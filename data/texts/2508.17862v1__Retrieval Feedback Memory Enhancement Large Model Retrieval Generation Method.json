{"metadata": {"pdf_filename": "2508.17862v1__Retrieval Feedback Memory Enhancement Large Model Retrieval Generation Method.pdf", "source": "arXiv"}, "text": "Retrieval Feedback Memory Enhancement Large Model\nRetrieval Generation Method\nLeqian Li1, Dianxi Shi2*, Jialu Zhou1, Xinyu Wei1, Mingyue Yang1, Songchang Jin3, Shaowu Yang1\n1College of Computer Science and Technology, National University of Defense Technology, china\n2Advanced Institute of Big Data, Beijing, China\n3Intelligent Game and Decision Lab, Beijing 100091, China\n{llq,dxshi,zhoujialu23,xinyuwei,yangmingyue216,shaowu.yang}@nudt.edu.cn\nAbstract\nLarge Language Models (LLMs) have shown remarkable ca-\npabilities across diverse tasks, yet they face inherent limita-\ntions such as constrained parametric knowledge and high re-\ntraining costs. Retrieval-Augmented Generation (RAG) aug-\nments the generation process by retrieving externally stored\nknowledge absent from the model\u2019s internal parameters.\nHowever, RAG methods face challenges such as informa-\ntion loss and redundant retrievals during multi-round queries,\naccompanying the difficulties in precisely characterizing\nknowledge gaps for complex tasks. To address these prob-\nlems, we propose Retrieval Feedback and Memory Retrieval\nAugmented Generation(RFM-RAG), which transforms the\nstateless retrieval of previous methods into stateful contin-\nuous knowledge management by constructing a dynamic\nevidence pool. Specifically, our method generates refined\nqueries describing the model\u2019s knowledge gaps using rela-\ntional triples from questions and evidence from the dynamic\nevidence pool; Retrieves critical external knowledge to it-\neratively update this evidence pool; Employs a R-Feedback\nModel to evaluate evidence completeness until convergence.\nCompared to traditional RAG methods, our approach enables\npersistent storage of retrieved passages and effectively dis-\ntills key information from passages to construct clearly new\nqueries. Experiments on three public QA benchmarks demon-\nstrate that RFM-RAG outperforms previous methods and im-\nproves overall system accuracy.\nIntroduction\nIn recent years, large language models (LLMs) have been\nwidely applied in various natural language processing (NLP)\ntasks owing to their advanced comprehension and gener-\nation capabilities (Radford et al. 2018; Chowdhery et al.\n2023; Touvron et al. 2023). However, the parameter knowl-\nedge of the model remains static after pre-training. There-\nfore, when answering questions beyond their pretraining\nscope or requiring up-to-date domain knowledge, they\nmay generate text that is syntactically fluent but factu-\nally ungrounded. This phenomenon is called hallucination\n(Maynez et al. 2020; Zhou et al. 2020).\nTo mitigate hallucination issues, Retrieval-Augmented\nGeneration (RAG)(Lewis et al. 2020) retrieves relevant\n*Corresponding author.\nCopyright \u00a9 2026, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: RFM-RAG employs an LLM to distill knowl-\nedge from retrieved results, dynamically updating an evi-\ndence pool. A R-Feedback Model then assesses the pool\u2019s\ncompleteness. If sufficient, evidence is passed to the genera-\ntion model for final response. Otherwise, we formulates new\nqueries combining evidence pool\u2019s content with the ques-\ntion for iterative retrieval. Compared to previous methods,\nRFM-RAG enables persistent knowledge retention and ex-\ntracts critical information to retrieve.\nknowledge from external sources in a single pass based on\nuser input and integrating the information into LLM prompts\nto enhance factual accuracy in responses. While effective for\nsimple knowledge-intensive tasks (Ram et al. 2023), this ap-\nproach performs poorly in complex scenarios requiring such\nas multi-step reasoning (Paranjape et al. 2023), fact verifica-\ntion (Thorne et al. 2018), and Long-form generation (Fabbri\net al. 2021). Compared to simple tasks, these tasks require\nhigher standards for the knowledge to be acquired. For ex-\nample, Long-form generation necessitates iterative knowl-\nedge gathering throughout the generation process. Multi-hop\nQA requires step dependent queries where each retrieval re-\narXiv:2508.17862v1  [cs.IR]  25 Aug 2025\n\nFigure 2: RFM-RAG dynamically constructs an evidence pool by processing retrieved results and formulates targeted queries\nuntil termination. The workflow begins with the original question as the initial query. An LLM then curates retrieved passages,\nfiltering noise while integrating relevant knowledge into the evidence pool. Then, the R-Feedback Model evaluates knowledge\nsufficiency. If deficient, new queries are created from core question entities and evidence-pool information, iteratively enriching\nthe evidence pool through retrieval. Upon achieving comprehensive evidence coverage, the LLM generates the final response.\nlies on prior outputs. In contrast, iterative retrieval methods\ngenerate multiple retrieval queries and modify the retrieval\nqueries based on feedback information through multi-round\nof retrieval refinement to obtain the final results(Asai et al.\n2024). Dynamic retrieval RAG performs multiple retrievals\nduring the LLM generation process. FLARE (Jiang et al.\n2023) uses the part of the last generated sentence to per-\nform retrieval when the LLM\u2019s confidence (i.e., the gener-\nation probability) on the next token is lower than certain\nthresholds. Some methods decompose the original question\ninto multiple sub-questions when dealing with multi-step\nQA problems, retrieve external information separately and\nintegrate multiple pieces of information as the answer.\nHowever, iterative RAG approaches suffer from two crit-\nical limitations. Generated outputs depends on retrieved\ndocuments. low-quality retrievals introduce noise that re-\nduce the accuracy of response. Repeated calls of the full\nretrieval-generation pipeline results in unnecessary resource\noverhead. We believe that when provided with sufficient\nknowledge, LLMs can generate accurate answers in a single\npass. Redundant generation steps in conventional iterative\nRAG may have hallucinations. Thus, comprehensive knowl-\nedge completeness assessment before LLMs input is essen-\ntial. Furthermore, since knowledge gaps change dynamically\nduring iteration, each retrieval requires precise queries tar-\ngeting the model\u2019s current state.\nTo address these limitations, we propose Retrieval-\nFeedback Augmented Memory Enhanced Large Model Re-\ntrieval and Generation(RFM-RAG). As shown in Fig.1, our\nmethod constructs a dynamic evidence pool where LLMs\nCOT prompting (Wei et al. 2022) organize and deduplicate\nretrieved contexts to eliminate low-quality results. By for-\nmulating new retrieval queries through combined integra-\ntion of the evidence pool and original question, we pre-\ncisely target knowledge gaps beyond existing evidence. This\nevidence pool undergoes iterative refinement through suc-\ncessive queries until the Retrieval Feedback Model (R-\nFeedback Model) considers the evidence collection final. In\nsummary, our main contributions are as follows:\n\u2022 We propose an iterative retrieval-based dynamic evi-\ndence pool construction method, leveraging chain-of-\nthought prompts to guide LLMs in relevance filtering,\nstructural organization, and deduplication of retrieved\ncontext. The refined information serves as validated ev-\nidence, progressively building a high-quality knowledge\nreservoir for final generation.\n\u2022 We design a targeted query generation mechanism that\npinpoints the knowledge gaps of LLMs, enabling pre-\ncise localization of missing information beyond the ev-\nidence pool. Additionally, we innovatively introduce a\ndedicated R-Feedback Model to evaluate the sufficiency\nof evidence, and release a specialized dataset for training.\n\u2022 We conduct comprehensive evaluations of previous RAG\nmethods and RFM-RAG across three benchmark datasets\nusing two distinct LLMs. The experimental results\ndemonstrate improvements achieved by RFM-RAG, con-\nfirming the efficacy of our approach.\n\nRelated Work\nRetrieval-Augmented Generation\nRAG effectively mitigates hallucination with single-round\nretrieval enhancement being the most straightforward ap-\nproach, retrieving knowledge using the original query, in-\ntegrating relevant passages and prompting the LLM with\naugmented input.Foundational studies have extensively ex-\nplored this paradigm (Khandelwal et al. 2019; Borgeaud\net al. 2022; Izacard and Grave 2020; Guu et al. 2020). How-\never, these methods are only suitable for simple tasks or un-\nambiguous queries.\nIn complex scenarios requiring multi-hop reasoning or\ninference, single-round retrieval often fails to capture the\nknowledge necessary for accurate generation precisely. As a\nresult, recent research has focused on advanced RAG strate-\ngies. IRCot(Trivedi et al. 2022) employs chain-of-thought\nreasoning to iteratively generate retrieval queries. Adaptive-\nRAG(Jiang et al. 2023) categorizes questions into three\nmodes based on complexity and dynamically adjusts re-\ntrieval rounds. Self-RAG(Asai et al. 2024) produces reflec-\ntive tokens to guide retrieval-generation interplay. DRA-\nGIN(Su et al. 2024) performs real-time retrieval activated\nby LLM uncertainty signals during generation.\nRetrieval Quality Assessment Metrics\nEvaluating the generated outputs of large language models\n(LLMs) is a critical step in assessing RAG effectiveness.\nThis process quantifies generation quality using multidimen-\nsional metrics including (factual accuracy, answer relevance,\nand text diversity), which collectively reflect RAG\u2019s com-\nprehensive performance (Es et al. 2024). For the core re-\ntrieval component of RAG, accurate evaluation can effec-\ntively avoid unnecessary retrieval steps. Current mainstream\napproaches rely on quantitative metrics, which compute sta-\ntistical similarity between retrieved passages and queries.\nMethods such as BLEU(Papineni et al. 2002), ROUGE(Lin\n2004), and METEOR(Banerjee and Lavie 2005) evaluate\nrelevance through surface term matching (e.g., n-gram over-\nlap) but fundamentally ignore semantic depth. While provid-\ning measurable evaluation standards, these approaches face\nsignificant limitations in real-world applications due to in-\nsufficient understanding of deep semantics.\nMethodology\nPrevious RAG methods suffer from over-reliance on single-\nround retrieval results and an inherent inability to accu-\nrately identify model knowledge gaps, frequently leading\nto outputs that are factually incorrect. To address these\nlimitations, we introduce the Retrieval-Feedback Memory-\nenhanced RAG (RFM-RAG) framework, detailed in this\nsection with architectural overview in Fig.2. Our method-\nology is based on three core principles: Dynamically con-\nstructing an evidence pool by aggregating and organizing\nretrieved passages for each retrieval. Using a retrieval feed-\nback model to terminate retrieval loops upon verifying evi-\ndence pool sufficiency. Generating iterative queries through\nrelational chain-based knowledge gap detection to address\nmissing information.\nDynamic Evidence Pool Construction\nWe define the vanilla LLM generation process as Ans =\nLLM(q), where the LLM directly generates answers from\nqueries. Traditional RAG methods follow Ans = LLM(q, e)\nwith e = R(C | q), where R denotes the retriever, e rep-\nresents relevant knowledge retrieved from corpus C given\nq, and both q and e are input to the LLM. This paradigm\nsuffers from incomplete retrieval and inaccurate knowledge\ngap identification due to the limitations of single-round\nretrieval. To overcome this, RFM-RAG constructs a dy-\nnamic evidence pool through iterative retrieval, leveraging\nR-Feedback Model(As details in the next section) to score\nevidence completeness and determine termination. The pro-\ncess initializes with the original question q0. Subsequent\nretrievals use generated queries qi, each of which obtains\nretrieved passages Ki = {k1, k2, . . . }. Using chain-of-\nthought prompting (Wei et al. 2022), we instruct the LLM\n(GPT-3.5-turbo)(Brown et al. 2020) to curate retrieved pas-\nsages (As shown in Fig.3). This curation process involves\nfiltering redundancies while extracting question-relevant ev-\nidence and incrementally augment the evidence pool. This\nevidence accumulation process is formally defined as:\nKi = R(C | qi)\nEi = LLMprompt1(qi, Ki)\n(1)\nE = {E0, E1, ..., Ei}\n(2)\nFigure 3: Prompt1 for Organizing Passages Using LLMs.\nBefore the R-Feedback Model decides to terminate the\nevidence pool construction, each retrieval iteration requires\nformulating a new query for extracting necessary informa-\ntion from external databases. Most RAG methods lever-\nage query expansion or rewriting techniques. These meth-\nods parse semantic features and metadata within queries.\nHowever, they fail to capture critical information from re-\ntrieved passages. Consequently, we propose a query genera-\ntion strategy based on knowledge gap detection, designed to\nmore identify missing knowledge in LLM responses by de-\ntecting entity deficiencies in the query and newly emerged\nrelevant entities in the evidence pool.\n\nWe quantify entity gaps with entity coverage metrics.\nSpecifically, chain-of-thought prompting instructs the LLM\nto extract key entities k and relational triples rk from the\nquestion, replacing unknown information with placehold-\ners <X>. For the question Is the director of Move (1970\nFilm) and the director of M\u00b4editerran\u00b4ee (1963 Film) the same\ncountry?, this extracts entities Move, M\u00b4editerran\u00b4ee and\ntriples (Move,director,<X>),(M\u00b4editerran\u00b4ee, director,<X>),\n(<X>,country,end). The entity coverage feature Sfk is com-\nputed as the proportion of knowledge about entity k present\nin the current evidence pool E. When Sfk falls below preset\nthreshold \u03b8, it indicates insufficient entity information in E,\nprompting addition to the gap list G\u2032 to represent unretrieved\nentity knowledge.\n(k, rk) = LLMprompt2(q0)\n(3)\nSfk = min(CkE\nLE\n, 1.0)\n(4)\nG\u2032 =\n\u001aAdd(G, k)\nif\nSfk < \u03b8\nG\notherwise\n(5)\nwhere CkE is the number of occurrences of entity k in the\ncurrent evidence pool E, LE is the length of the evidence\npool information. Appendix C introduces the prompt tem-\nplates used to extract entities and Entity-Relationship groups\nfrom the question.\nSubsequently, we extract new question-related entities\nfrom the evidence pool. We input the extracted relational\ntriples and evidence pool into the large model (As shown\nin Fig.4), enabling the model to retrieve missing informa-\ntion zk represented by placeholders in the triples from the\nevidence pool and add to the knowledge gap list G =\n{g1, g2, ..., gn, z1, z2, ..., zk}. This augmentation captures\nentities indirectly related to the question that cannot be ob-\ntained from the initial question. These entities form the core\nfor subsequent retrievals. Finally, a new query qi is con-\nstructed using lexical items in G, designed to cover the\nknowledge gaps requiring external knowledge base retrieval\nfor accurate question answering by the LLM.\nzk = LLMprompt3(rk, E)\n(6)\nR-Feedback Model\nWe design the R-Feedback Model as a feed-forward net-\nwork with double hidden layers and activation functions. We\ncompute the syntactic entity coverage feature Sf and seman-\ntic relevance feature Gf from the evidence pool E and the\ninitial question q0. These features serve as input to the R-\nFeedback Model, which decides when to terminate evidence\npool updates.\nTherefore, we utilize the entity coverage calculated in\nEquation 4 for each key entity k. We take the average of\nthe coverage features of all entities to obtain the overall syn-\ntactic coverage of the entire question in the evidence pool,\nwhich is used to describe the syntactic relevance between the\nquestion and the evidence pool. Where |E| is the number of\nentities extracted from q0:\nSf = sum(Sfk)\n|E|\n(7)\nFigure 4: Prompt3 for Extracting Placeholder-Represented\nEntities from Evidence Pool.\nCross-encoders process queries and paragraphs concur-\nrently through deep attention mechanisms, capturing com-\nplex semantic relationships with high accuracy. We derive\nsemantic relevance features Gf by processing the evidence\npool E and initial query q0 using cross-encoder, and then\nfuse the two features as input to the R-Feedback Model\nRFmodel:\nGf = Encodercross(q0, E)\n(8)\nLogitSG = RFmodel(Sf, Gf)\n(9)\nUsing the value of LogitSG, R-Feedback Model decides\nif the condition for updating the evidence pool has been met.\nQuestion: Which film\nwas released more\nrecently, Die sch\u00a8one\nLurette or Sabhash?\nContext: Hobby won the\nAward for Best First Time\nDirector. Karl Geary...\nLabel:0\nQuestion: Were Dan\nO\u2019Connor and Hale\nBaugh from the same\ncountry?\nContext: Daniel O\u2019Connor\nwas a Canadian politician,\nbusinessman... Hale Baugh\nwas an American modern\npentathlete. He competed...\nLabel: 1\nQuestion: Which film\nhas the director who was\nborn later, Il Diavolo In\nConvento or The\nEnchanting Enemy?\nContext: Il diavolo in corpo\nis an... The Enchanting\nEnemy is an Italian comedy\nfilm directed by Claudio\nGora and starring...\nLabel: 0\nTable 1: Examples of datasets for R-Feedback Model. In-\nformation relevant to the question and context is marked in\nbold, and entities with missing information in the context are\nmarked in italics.\n\n2WikiMultihopQA\nNaturalQA\nStrategyQA\nAverage\nLLM\nRAG Method\nEM\nACC\nEM\nACC\nACC\nEM\nACC\nGemma-2b\nNo Retrieval\n22.6\n43.0\n15.0\n24.6\n56.0\n18.8\n41.2\nVanilla RAG\n22.8\n38.4\n11.4\n26.0\n56.3\n17.1\n40.2\nProbing-RAG\n24.2\n43.6\n21.6\n35.0\n61.8\n22.9\n46.8\nAdaptive RAG\n21.6\n40.6\n11.4\n26.2\n54.7\n16.5\n40.5\nDRAGIN\n26.4\n28.8\n18.8\n22.2\n62.4\n22.6\n37.8\nRFM-RAG(Ours)\n29.2\n37.6\n30.6\n33.2\n63.2\n29.9\n44.7\nMistral-7b\nNo Retrieval\n16.4\n30.0\n13.2\n19.8\n62.4\n14.8\n37.4\nVanilla RAG\n21.6\n32.6\n16.8\n35.0\n60.7\n19.2\n42.7\nProbing-RAG\n23.0\n33.4\n20.8\n39.4\n61.5\n21.9\n44.7\nAdaptive RAG\n22.6\n31.6\n17.2\n37.4\n65.4\n19.9\n44.8\nDRAGIN\n23.2\n25.8\n16.8\n37.2\n70.3\n20.0\n44.4\nRFM-RAG(Ours)\n32.1\n36.7\n33.4\n42.8\n72.6\n32.7\n50.7\nTable 2: Experimental results on three different QA datasets. We indicate the highest performance in bold and underline the\nsecond highest.\nTraining R-Feedback Model\nTraining the retrieval feedback model requires dataset pairs\n((q, E), y)N\n1 , where q denotes the question, E represents a\nknowledge segment, and y \u22080, 1 indicates sufficiency of\nE to answer q. To generate these pairs, we use the eviden-\ntial chain corresponding to the answer to question q in the\ndataset to divide the context into supporting evidence and ir-\nrelevant information. Sufficient samples (y = 1) use gold\nsupporting evidence from the dataset as E, indicating E\nfully answers q without further retrieval. Insufficient sam-\nples (y = 0) assign irrelevant information to E, denoting E\ncannot answer q. Partially sufficient samples (y = 0) com-\nbine subsets of supporting with irrelevant information as E,\nsimulating scenarios where E contains relevant but incom-\nplete knowledge requiring additional retrieval.\nAs detailed in Table 1, our training dataset comprises\nthree data categories derived from the public 2WikiMulti-\nhopQA (Ho et al. 2020) dataset. To ensure a balanced dis-\ntribution of positive and negative samples, we randomly se-\nlected questions and generated paired samples for each cat-\negory: sufficient evidence (y = 1) and insufficient evidence\n(y = 0). The final dataset contains 10,000 training and 800\nvalidation samples. We trained the R-Feedback Model using\nthis dataset, with cross-entropy loss defined as follows:\nL = \u22121\nN\nN\nX\ni=1\n[yi log(pi) + (1 \u2212yi) log(1 \u2212pi)]\nWe provide details on the hyperparameters for training the\nR-Feedback Model in Appendix A.\nExperimental Setups\nDatasets\nFor performance assessment, we evaluate methods using\nthree open-domain QA datasets, randomly sampling 500\ntest examples per dataset. Comprehensive dataset and cor-\npus specifications are provided in Appendix B.\n2WikimultihopQA (Ho et al. 2020). It contains multi-\nhop questions that span more than two Wikipedia pages,\neach provided with 10 paragraphs. The dataset features fine-\ngrained paragraph annotations and a high proportion of dis-\ntractors, which enables rigorous testing of models\u2019 multi-\nhop reasoning in noisy environments.\nNaturalQA (Kwiatkowski et al. 2019). Answers must be\nfound in long documents to locate the exact fragments. Due\nto the real distribution of user questions and the challenge of\nlocating answers, this task effectively tests the model\u2019s abil-\nity to extract accurate information from long, open-domain\ntexts.\nStrategyQA (Geva et al. 2021). It contains binary ques-\ntions requiring implicit reasoning strategies without provid-\ning explicit evidence paragraphs. Characterized by strategic\nreasoning requirements, it assesses models\u2019 ability to con-\nstruct evidence chains and perform complex inference.\nBaselines\nWe choose the following Text Generation baselines for com-\nparison. No Retrieval. Directly generates answers from the\noriginal question without retrieval.Vanilla RAG(Lewis et al.\n2020). Relevant passages are retrieved from an external cor-\npus based on the initial question. The retrieved passages are\nthen added into the LLM\u2019s input. DRAGIN(Su et al. 2024).\nRetrieves when token-level confidence drops, using atten-\ntion weights to construct queries from contextually salient\nwords. Adaptive-RAG(Jeong et al. 2024). Classifies ques-\ntion complexity via fine-tuned classifier to dynamically ad-\njust retrieval steps. Probing-RAG(Baek et al. 2024). Lever-\nages intermediate-layer hidden states to determine need for\nadditional retrieval.\nAll methods were evaluated under few-shot settings: 4-\nshot on 2WikiMultihopQA and NaturalQA, 6-shot on Strat-\negyQA. Answer extraction used regular expression pattern\nmatching to structure free-form LLM outputs into precise\nfinal answers. For evaluation, we used answer-level exact\nmatch(EM) and accuracy(ACC) scores to compare extracted\nanswers against reference labels. Given diminishing accu-\nracy gains and significant latency increases beyond three re-\ntrieval rounds, we capped maximum number of retrievals at\nthree.\n\nFigure 5: EM and ACC scores for QA without retrieval and RFM-RAG based on Gemma-2b, Mistral-7b, and Gemma3-4b\nmodels. RFM-RAG outperforms the generation models themselves on all three datasets and all models.\nNaturalQA\nStrategyQA\nMethods\nR-Step\n\u2206\nEM\nR-Step\n\u2206\nACC\nGemma-2b\nRFM\n1.93\n30.6\n2.31\n63.2\nwo-RFM\n3\n1.07\u2193\n29.5\n3\n0.69\u2193\n62.8\nMistral-7b\nRFM\n2.34\n33.4\n2.62\n72.6\nwo-RFM\n3\n0.66\u2193\n35.8\n3\n0.38\u2193\n70.2\nTable 3: Comparison of averaged retrieval steps and EM, ACC (%) between RFM-RAG and the ablated wo-RFM approach\n(Evidence pool construction termination fixed at maximum retrieval count 3) using Gemma-2b and Mistral-7b models.\nImplementation Details\nWe employ BM25(Robertson and Jones 1976), a probabilis-\ntic sparse retrieval model based on (Robertson, Zaragoza\net al. 2009), which demonstrates superior performance in\nRAG, even surpassing certain dense retrievers. This is im-\nplemented via ElasticSearch for all methods to ensure fair-\nness. For 2WikiMultihopQA, we adopt IRCoT\u2019s(Trivedi\net al. 2022) document corpus. StrategyQA averages 2.7\nevidence documents per question and has no official cor-\npus, while NaturalQA provides only answer-containing doc-\numents. Consequently, we constructed dedicated corpora\nusing dataset contexts (details in Appendix B). All RAG\nmethods utilize Gemma-2b(Team et al. 2024) and Mistral-\n7b(Albert Q. Jiang et al. 2023) as QA models. For comput-\ning resources, we utilize A100 GPUs with 40GB memory. In\naddition, due to the significant costs associated with evalu-\nating retrieval-augmented generation models, we conducted\nexperiments with a single run.\nExperimental Results\nMain Results\nOur experiments comprehensively evaluated the perfor-\nmance of RFM-RAG on three datasets against various base-\nlines, with results shown in Table 2. Our observations in-\ndicate that in most cases, single-round retrieval RAG con-\nsistently outperformed direct LLMs generation in question\nanswering and confirming the efficacy of retrieval augmen-\ntation for knowledge-intensive QA tasks. The RFM-RAG\nmethod showed excellent performance on the majority of\nLLMs and datasets. Compared to no retrieval and single-\nround retrieval methods, on the Gemma-2b model, EM im-\nproved by approximately 11.1 and 12.8 percentage points,\nACC improved by 3.5 and 4.5 percentage points. On the\nMistral-7b model, EM improved by 17.9 and 13.5 percent-\nage points, ACC improved by approximately 13.3 and 8 per-\ncentage points. This demonstrates the robustness and effec-\ntiveness of RFM-RAG in terms of knowledge collection and\norganization, as well as its ability to detect knowledge gaps\nin the model.\nNotably, RFM-RAG demonstrates consistent perfor-\nmance gains on Gemma-2b, proving that models with\nfewer parameters can achieve competitive QA perfor-\nmance when provided with sufficient relevant information.\nAdaptive-RAG underperforms significantly across datasets.\nWhile it adjusts retrieval based on question complexity, the\nmethod lacks iterative enhancement targeting model\u2019s spe-\ncific knowledge gaps. The RFM-RAG we propose outper-\nforms all previous adaptive retrieval methods by avoiding\nredundant generation cycles. By constructing a dynamic ev-\nidence pool through detecting model knowledge gaps, our\nmethod achieves significant performance improvements.\nAnalysis\nRFM-RAG performance is unaffected by the generation\nmodel. To investigate the impact of the generation model\u2019s\ninherent capabilities on the retrieval augmentation meth-\nods, we conducted supplementary experiments on the latest\nmodel, Gemma3-4b (Team et al. 2025). The experimental\nsetup is identical to that of the main experiment: the retriever\nuses BM25 (implemented in ElasticSearch), the corpus is\nthe same as in Appendix B. The prompt engineering uses\nthe same mind chain template. Fig.5 compares the ability\nof the three generative models based solely on parameter-\n\nQuestion\nDRAGIN\nRFM-RAG(Ours)\nWho is the mother of\nthe director of film\nPolish-Russian War?\n(2WikiMultihopQA)\nQuery 1: mother director film Polish-Russian\nWar? director film Polish-Russian War\nKnowledge for LLM: bombs would be like\nthe early...The event attracted an audience...\nuniversity leaders to combat a wide array.. .\nQuery 2: mother director film Polish-Russian\nWar... mother Andrzej Wajda Zofia Wajda.\nKnowledge for LLM: bombs would be like the\nearly...The event attracted. .. mentioned a few\ntimes in the Torah and references.. .\nAnswer: Zofia Wajda\nEM: 0\nQuery 1: Who is the mother of the director\nof film Polish-Russian War?\nKnowledge for LLM: The director of the film\n\u201dPolish-Russian War\u201d is Xawery \u02d9Zu\u0142awski.\nQuery 2: Xawery \u02d9Zu\u0142awski.\nKnowledge for LLM: The director of the film\n\u201dPolish-Russian War\u201d is Xawery \u02d9Zu\u0142awski.\nMa\u0142gorzata Braunek is the mother of\nXawery \u02d9Zu\u0142awski, the Polish film director.\nAnswer: Ma\u0142gorzata Braunek\nEM: 1\nwhat is the name of\nthe rca victor dog?\n(NaturalQA)\nQuery 1: name rca victor dog Bristol, served\nmodel painting Francis Barraud titled Master\u2019s.\nKnowledge for LLM: were the first in provincial\n...Ginsberg, In this mode perfection is basic,...\nNipper(1884\u20131895)was a dog from Bristol, who\nserved as the model for a painting.\nQuery 2: image basis dog-and-gramophone\ntrademark, Berliner\u2019s successor Co. Victor Records)\nKnowledge for LLM: were the first in provincial...\nThis image was the basis for the dog-and... Berliner\u2019\nAmerican successor the Victor Talking Machine\nCo. (later known as RCA Victor).\nAnswer: Berliner\nEM: 0\nQuery 1: what is the name of the rca victor dog?\nKnowledge for LLM: Berliner\u2019s successor\nthe Victor Talking Machine Co. (later known as\nRCA Victor)\nQuery 2: Berliner\nKnowledge for LLM: Nipper(1884\u20131895)was a dog,\nwho served as the model for a painting.This image\nwas the basis for the dog-and-gramophone\ntrademark that was used by Berliner\u2019s successor\nthe Victor Talking Machine Co.(later known as\nRCA Victor).\nAnswer: Nipper\nEM: 1\nTable 4: Case study with the RFM-RAG and DRAGIN.\nized knowledge with the ability of our RFM-RAG to answer\nquestions on three datasets. For all three models, RFM-RAG\noutperforms the others across all datasets. Especially for the\nlatest generative model(Gemma3-4b), RFM-RAG improves\nthe EM or ACC score by 8.8 points on 2WikiMultihopQA,\n18.4 points on NaturalQA, and 11 points on StrategyQA,\nrelative to the model\u2019s inherent generative capability.\nEvaluating Retrieval Feedback Model\u2019s Iteration Ter-\nmination Efficacy. Compared to fixed-iteration baselines\nthat terminate without considering knowledge sufficiency,\nour method employs early termination when sufficient ev-\nidence is acquired. This strategy significantly reduces la-\ntency and mitigates noise from redundant retrievals. We em-\npirically compared the step counts and Exact Match (EM)\nscores between the Fixed-iteration baseline (wo-RFM) and\nRFM-RAG\u2019s adaptive termination on NaturalQA and Strate-\ngyQA datasets. Table 3 shows that unnecessary retrieval be-\nyond knowledge saturation leads to a reduction in accuracy\nby 0.4 to 2.4 percentage points on average, while RFM-RAG\nachieves latency reductions of 12-35% and maintains com-\nparable or superior accuracy, validating the efficacy of our\nretrieval feedback mechanism.\nCase Study. We conducted a case study comparing RFM-\nRAG and DRAGIN qualitatively on 2WikiMultihopQA\nand NaturalQA question pairs(Table 4), analyzing retrieval\nqueries, knowledge provisioning, and final answers. In Case\n1 (complex multi-hop QA), RFM-RAG extracts key entities\nfrom retrieval results as subsequent queries. The second re-\ntrieval provides targeted knowledge for accurate answer gen-\neration. Conversely, DRAGIN relies on generation-based\nknowledge inference after first retrieval, introducing uncer-\ntainty. DRAGIN extracts missing knowledge from model-\ngenerated information after the initial retrieval. However,\ndue to the uncertainty of model generation, its accuracy is\nweaker than the knowledge extracted from authentic evi-\ndence pool related to the question.\nIn Case 2, which requires information integration from\nmultiple knowledge sources, RFM-RAG processes and re-\ntains all retrieved evidence throughout iterations. During fi-\nnal generation, the LLM filters relevant information from\nthe complete evidence pool to formulate answers. DRAGIN\nfails to retain previously retrieved passages in subsequent\nretrievals. As a result, even when partial answers are gen-\nerated from prior knowledge, the lack of critical evidence\nundermines the integrity of the final conclusion.\nConclusion\nIn this work, we introduce RFM-RAG, a novel retrieval\npipeline that employs a relationship chain-based query gen-\neration pattern that enables precise multi-round of retrieval.\nDuring this process, the LLM organizes and deduplicates\nthe retrieved results to construct a comprehensive evidence\npool. To optimize the retrieval process, RFM-RAG incor-\nporates an R-Feedback Model, which is responsible for de-\ntermining when to stop updating the evidence pool during\nthe retrieval rounds. This model ensures that retrievals con-\ntinue only as long as necessary to gather relevant evidence.\nWe introduce both the training dataset and method for the\nR-Feedback Model and show that RFM-RAG outperforms\nprevious methods for various QA datasets.\n\nReferences\nAlbert Q. Jiang, A. M., Alexandre Sablayrolles; et al. 2023.\nMistral 7B. arXiv preprint arXiv:2310.06825.\nAsai, A.; Wu, Z.; Wang, Y.; Sil, A.; and Hajishirzi, H. 2024.\nSelf-rag: Learning to retrieve, generate, and critique through\nself-reflection.\nBaek, I.; Chang, H.; Kim, B.; Lee, J.; and Lee, H.\n2024.\nProbing-rag: Self-probing to guide language mod-\nels in selective document retrieval.\narXiv preprint\narXiv:2410.13339.\nBanerjee, S.; and Lavie, A. 2005. METEOR: An automatic\nmetric for MT evaluation with improved correlation with hu-\nman judgments. In Proceedings of the acl workshop on in-\ntrinsic and extrinsic evaluation measures for machine trans-\nlation and/or summarization, 65\u201372.\nBorgeaud, S.; Mensch, A.; Hoffmann, J.; Cai, T.; Ruther-\nford, E.; Millican, K.; Van Den Driessche, G. B.; Lespiau,\nJ.-B.; Damoc, B.; Clark, A.; et al. 2022. Improving language\nmodels by retrieving from trillions of tokens. In Interna-\ntional conference on machine learning, 2206\u20132240. PMLR.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners. Ad-\nvances in neural information processing systems, 33: 1877\u2013\n1901.\nChowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra,\nG.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.;\nGehrmann, S.; et al. 2023. Palm: Scaling language model-\ning with pathways. Journal of Machine Learning Research,\n24(240): 1\u2013113.\nEs, S.; James, J.; Anke, L. E.; and Schockaert, S. 2024. Ra-\ngas: Automated evaluation of retrieval augmented genera-\ntion. In Proceedings of the 18th Conference of the European\nChapter of the Association for Computational Linguistics:\nSystem Demonstrations, 150\u2013158.\nFabbri, A. R.; Kry\u00b4sci\u00b4nski, W.; McCann, B.; Xiong, C.;\nSocher, R.; and Radev, D. 2021. Summeval: Re-evaluating\nsummarization evaluation. Transactions of the Association\nfor Computational Linguistics, 9: 391\u2013409.\nGeva, M.; Khashabi, D.; Segal, E.; Khot, T.; Roth, D.;\nand Berant, J. 2021.\nDid aristotle use a laptop? a ques-\ntion answering benchmark with implicit reasoning strate-\ngies. Transactions of the Association for Computational Lin-\nguistics, 9: 346\u2013361.\nGuu, K.; Lee, K.; Tung, Z.; Pasupat, P.; and Chang, M.\n2020. Retrieval augmented language model pre-training. In\nInternational conference on machine learning, 3929\u20133938.\nPMLR.\nHo, X.; Nguyen, A.-K. D.; Sugawara, S.; and Aizawa,\nA. 2020.\nConstructing a multi-hop qa dataset for com-\nprehensive evaluation of reasoning steps.\narXiv preprint\narXiv:2011.01060.\nIzacard, G.; and Grave, E. 2020.\nLeveraging passage re-\ntrieval with generative models for open domain question an-\nswering. arXiv preprint arXiv:2007.01282.\nJeong, S.; Baek, J.; Cho, S.; Hwang, S. J.; and Park, J. C.\n2024. Adaptive-rag: Learning to adapt retrieval-augmented\nlarge language models through question complexity. arXiv\npreprint arXiv:2403.14403.\nJiang, Z.; Xu, F. F.; Gao, L.; Sun, Z.; Liu, Q.; Dwivedi-Yu,\nJ.; Yang, Y.; Callan, J.; and Neubig, G. 2023. Active retrieval\naugmented generation. In Proceedings of the 2023 Confer-\nence on Empirical Methods in Natural Language Process-\ning, 7969\u20137992.\nKhandelwal, U.; Levy, O.; Jurafsky, D.; Zettlemoyer, L.;\nand Lewis, M. 2019.\nGeneralization through memoriza-\ntion: Nearest neighbor language models.\narXiv preprint\narXiv:1911.00172.\nKwiatkowski, T.; Palomaki, J.; Redfield, O.; Collins, M.;\nParikh, A.; Alberti, C.; Epstein, D.; Polosukhin, I.; Devlin,\nJ.; Lee, K.; et al. 2019. Natural questions: a benchmark for\nquestion answering research. Transactions of the Associa-\ntion for Computational Linguistics, 7: 453\u2013466.\nLewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.;\nGoyal, N.; K\u00a8uttler, H.; Lewis, M.; Yih, W.-t.; Rockt\u00a8aschel,\nT.; et al. 2020.\nRetrieval-augmented generation for\nknowledge-intensive nlp tasks. Advances in neural infor-\nmation processing systems, 33: 9459\u20139474.\nLin, C.-Y. 2004. Rouge: A package for automatic evaluation\nof summaries. In Text summarization branches out, 74\u201381.\nMaynez, J.; Narayan, S.; Bohnet, B.; and McDonald, R.\n2020. On faithfulness and factuality in abstractive summa-\nrization. arXiv preprint arXiv:2005.00661.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.\nBleu: a method for automatic evaluation of machine trans-\nlation.\nIn Proceedings of the 40th annual meeting of the\nAssociation for Computational Linguistics, 311\u2013318.\nParanjape, B.; Lundberg, S.; Singh, S.; Hajishirzi, H.; Zettle-\nmoyer, L.; and Ribeiro, M. T. 2023. Art: Automatic multi-\nstep reasoning and tool-use for large language models. arXiv\npreprint arXiv:2303.09014.\nRadford, A.; Narasimhan, K.; Salimans, T.; Sutskever, I.;\net al. 2018. Improving language understanding by gener-\native pre-training.\nRam, O.; Levine, Y.; Dalmedigos, I.; Muhlgay, D.; Shashua,\nA.; Leyton-Brown, K.; and Shoham, Y. 2023.\nIn-context\nretrieval-augmented language models. Transactions of the\nAssociation for Computational Linguistics, 11: 1316\u20131331.\nRobertson, S.; Zaragoza, H.; et al. 2009. The probabilistic\nrelevance framework: BM25 and beyond. Foundations and\nTrends\u00ae in Information Retrieval, 3(4): 333\u2013389.\nRobertson, S. E.; and Jones, K. S. 1976. Relevance weight-\ning of search terms. Journal of the American Society for\nInformation science, 27(3): 129\u2013146.\nSu, W.; Tang, Y.; Ai, Q.; Wu, Z.; and Liu, Y. 2024. DRA-\nGIN: dynamic retrieval augmented generation based on the\ninformation needs of large language models. arXiv preprint\narXiv:2403.10081.\nTeam,\nG.;\nKamath,\nA.;\nFerret,\nJ.;\nPathak;\net\nal.\n2025.\nGemma 3 technical report.\narXiv preprint\narXiv:2503.19786.\n\nTeam, G.; Mesnard, T.; Hardin, C.; Dadashi, R.; Bhu-\npatiraju, S.; Pathak; et al. 2024.\nGemma: Open models\nbased on gemini research and technology. arXiv preprint\narXiv:2403.08295.\nThorne, J.; Vlachos, A.; Christodoulopoulos, C.; and Mittal,\nA. 2018. FEVER: a large-scale dataset for fact extraction\nand VERification. arXiv preprint arXiv:1803.05355.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi`ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; et al. 2023. Llama: Open and efficient founda-\ntion language models. arXiv preprint arXiv:2302.13971.\nTrivedi, H.; Balasubramanian, N.; Khot, T.; and Sabharwal,\nA. 2022. Interleaving retrieval with chain-of-thought rea-\nsoning for knowledge-intensive multi-step questions. arXiv\npreprint arXiv:2212.10509.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;\nChi, E.; Le, Q. V.; Zhou, D.; et al. 2022.\nChain-of-\nthought prompting elicits reasoning in large language mod-\nels. Advances in neural information processing systems, 35:\n24824\u201324837.\nZhou, C.; Neubig, G.; Gu, J.; Diab, M.; Guzman, P.; Zettle-\nmoyer, L.; and Ghazvininejad, M. 2020. Detecting hallu-\ncinated content in conditional neural sequence generation.\narXiv preprint arXiv:2011.02593.\n"}