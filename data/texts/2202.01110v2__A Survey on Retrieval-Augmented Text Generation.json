{"metadata": {"pdf_filename": "2202.01110v2__A Survey on Retrieval-Augmented Text Generation.pdf", "source": "arXiv"}, "text": "A Survey on Retrieval-Augmented Text Generation\nHuayang Li\u2665,\u2217\nYixuan Su\u2660,\u2217\nDeng Cai\u2666,\u2217\nYan Wang\u2663,\u2217\nLemao Liu\u2663,\u2217\n\u2665Nara Institute of Science and Technology\n\u2660University of Cambridge\n\u2666The Chinese University of Hong Kong\n\u2663Tencent AI Lab\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\nthisisjcykcd@gmail.com, brandenwang@tencent.com\nlemaoliu@gmail.com\nAbstract\nRecently, retrieval-augmented text generation\nattracted increasing attention of the compu-\ntational linguistics community.\nCompared\nwith conventional generation models, retrieval-\naugmented text generation has remarkable ad-\nvantages and particularly has achieved state-of-\nthe-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrieval-\naugmented text generation. It \ufb01rstly highlights\nthe generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable ap-\nproaches according to different tasks including\ndialogue response generation, machine trans-\nlation, and other generation tasks. Finally, it\npoints out some promising directions on top of\nrecent methods to facilitate future research.\n1\nIntroduction\nRetrieval-augmented text generation, as a new\ntext generation paradigm that fuses emerging deep\nlearning technology and traditional retrieval tech-\nnology, has achieved state-of-the-art (SOTA) per-\nformance in many NLP tasks and attracted the at-\ntention of the computational linguistics community\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,\n2021). Compared with generation-based counter-\npart, this new paradigm has some remarkable ad-\nvantages: 1) The knowledge is not necessary to be\nimplicitly stored in model parameters, but is explic-\nitly acquired in a plug-and-play manner, leading\nto great scalibility; 2) Instead of generating from\nscratch, the paradigm generating text from some re-\ntrieved human-written reference, which potentially\nalleviates the dif\ufb01culty of text generation.\nThis paper aims to review many representative\napproaches for retrieval-augmented text generation\ntasks including dialogue response generation (We-\nston et al., 2018), machine translation (Gu et al.,\n2018) and others (Hashimoto et al., 2018). We\n\u2217All authors contributed equally.\n\ufb01rstly present the generic paradigm of retrieval-\naugmented generation as well as three key com-\nponents under this paradigm, which are retrieval\nsources, retrieval metrics and generation models.\nThen, we introduce notable methods about\nretrieval-augmented generation, which are orga-\nnized with respect to different tasks. Speci\ufb01cally,\non the dialogue response generation task, exem-\nplar/template retrieval as an intermediate step has\nbeen shown bene\ufb01cial to informative response gen-\neration (Weston et al., 2018; Wu et al., 2019; Cai\net al., 2019a,b). In addition, there has been growing\ninterest in knowledge-grounded generation explor-\ning different forms of knowledge such as knowl-\nedge bases and external documents (Dinan et al.,\n2018; Zhou et al., 2018; Lian et al., 2019; Li et al.,\n2019; Qin et al., 2019; Wu et al., 2021; Zhang et al.,\n2021). On the machine translation task, we summa-\nrize the early work on how the retrieved sentences\n(called translation memory) are used to improve\nstatistical machine translation (SMT) (Koehn et al.,\n2003) models (Simard and Isabelle, 2009; Koehn\nand Senellart, 2010) and in particular, we inten-\nsively highlight several popular methods to inte-\ngrating translation memory to NMT models (Gu\net al., 2018; Zhang et al., 2018; Xu et al., 2020;\nHe et al., 2021). We also review the applications\nof retrieval-augmented generation in other genera-\ntion tasks such as abstractive summarization (Peng\net al., 2019), code generation (Hashimoto et al.,\n2018), paraphrase (Kazemnejad et al., 2020; Su\net al., 2021b), and knowledge-intensive generation\n(Lewis et al., 2020b). Finally, we also point out\nsome promising directions on retrieval-augmented\ngeneration to push forward the future research.\n2\nRetrieval-Augmented Paradigm\nIn this section, we \ufb01rst give a general formulation\nof retrieval-augmented text generation. Then, we\ndiscuss three major components of the retrieval-\naugmented generation paradigm, including the re-\narXiv:2202.01110v2  [cs.CL]  13 Feb 2022\n\nInput\nSources \n(Sec. 2.2):\nTraining \nCorpus\nExternal Data\nUnsupervised \nData\nMetrics\n(Sec. 2.3):\nSparse-vector \nRetrieval\nDense-vector \nRetrieval\nTask-specific \nRetrieval\nRetrieval Memory\nGeneration Model\nSec. 4: Machine \nTranslation\nSec. 5: Other \nTasks\nData \nAugmentation\nAttention \nMechanism\nSkeleton & \nTemplates\nInformation Retrieval\nTasks:\nSec. 3: Dialogue \nGeneration\nModels \n(Sec 2.4):\nOutput\nFigure 1: The overview of this survey.\ntrieval source, retrieval metric and integration meth-\nods.\n2.1\nFormulation\nMost text generation tasks can be formulated as a\nmapping from input sequence x to output sequence\ny : y = f(x). For instance, x and y could be the\ndialogue history and the corresponding response\nfor dialogue response generation, the text in the\nsource language and the translation in the target\nlanguage for machine translation, and so on.\nRecently, some researchers suggest to endow\nmodels the capability to access external memory\nvia some information retrieval techniques, so that\nthey can acquire more information in the generation\nprocess (Gu et al., 2018; Weston et al., 2018; Cai\net al., 2019b). The retrieval-augmented generation\ncan be further formulated as:\ny = f(x, z)\n(1)\nwhere z = {\u27e8xr, yr\u27e9} is a set of relevant instances\nretrieved from the original training set or external\ndatasets. The main idea of this paradigm is that yr\nmay bene\ufb01t the response generation, if xr (or yr)\nis similar (or relevant) to the input x. It is worth\nnoting that xr = \u2205when unsupervised retrieval\nsources are used. In general, the retrieval mem-\nory can be retrieved from three kinds of sources:\nthe training corpus, external datasets in the same\nformat with the training corpus, and large-scale\nunsupervised corpus (\u00a72.2). Metrics that evaluate\nthe relevance between text are varied as well, in\n\u00a72.3 we divided them into three categories: sparse-\nvector retrieval, dense-vector retrieval, and training-\nbased retrieval. Finally, how to integrate the re-\ntrieval memory to the generation model is also sig-\nni\ufb01cant, we also introduce some popular integra-\ntion approaches in \u00a72.4.\n2.2\nRetrieval Sources\nTraining Corpus\nMost previous studies search\nthe external memory from its training corpus (Song\net al., 2016; Gu et al., 2018; Weston et al., 2018).\nIn the inference time, retrieved examples with high\nrelevant scores could be regarded as extra refer-\nences and reduce model\u2019s uncertainty in generation.\nThe main motivation of those works is to to store\nknowledge not only in the model parameters but\nalso in an explicit and accessible form, making the\nmodel be able to re-access it during inference.\nExternal Data\nSome researchers also propose to\nretrieval relevant samples from external datasets\n(Su et al., 2021c; Xiao et al., 2021). In these stud-\nies, the retrieval pool is different with the training\ncorpus, which can further provide additional infor-\nmation that are not contained in the training corpus.\nThis is especially bene\ufb01cial for applications such\nas domain adaptation and knowledge update. For\nexample, Khandelwal et al. (2020a); Zheng et al.\n(2021a) employ the in-domain dataset as the exter-\nnal memory to achieve fast domain adaptation for\nmachine translation.\nUnsupervised Data\nOne limitation for previous\ntwo sources is that the datasets have to be super-\nvised datasets consisting of aligned input-output\npairs. For machine translation, Cai et al. (2021) pro-\npose a cross-lingual retriever to directly retrieve tar-\nget sentence from unsupervised corpus (i.e., mono-\nlingual corpus in the target language). The main\nidea is aligning source-side sentences and the corre-\nsponding target-side translations in a dense vector\nspace, i.e., aligning x and yr when xr is absent.\nAs a result, the retriever directly connects the dots\nbetween the source-side input and target-side trans-\nlations, enabling monolingual data in the target\n\nlanguage to be used alone as memories.\n2.3\nRetrieval Metrics\nSparse-vector Retrieval\nGiven an input se-\nquence x and a retrieval corpus, retrieval model\naims to retrieve a set of relevant examples z =\n{\u27e8xr, yr\u27e9} from the corpus. When a supervised\ncorpus is used, {\u27e8xr, yr\u27e9} is retrieved by measur-\ning the similarity between x and xr. For simi-\nlarity measurement, sparse-vector retrieval meth-\nods such as TF-IDF and BM25 (Robertson and\nZaragoza, 2009) are widely used. They match key-\nwords ef\ufb01ciently with an inverted index.\nDense-vector Retrieval\nHowever, these meth-\nods prefer examples with similar surfaces, and may\nfail to retrieve examples that are only semantically\nrelevant. To alleviate above problem, some stud-\nies (Cao and Xiong, 2018) attempt to retrieve in\ndense-vector space instead of the lexical overlap.\nRecent work (Lee et al., 2019) makes use of pre-\ntrained language models, which encodes the text to\nlow-dimensional dense vectors via BERT-based en-\ncoders. The retrieval score are computed via inner\nproducts between vectors.\nTask-speci\ufb01c\nRetrieval\nSimilarity-based\nre-\ntrieval is based on a simple heuristic. That is, the\nmore xr resembles with x, the more likely xr\nand yr will help the generation. However, the\nmost similar one by universal textual similarity\ndoes not necessarily serve the best for downstream\nmodels.\nIdeally, the retrieval metric would be\nlearned from the data in a task-dependent way: we\nwish to consider a memory only if it can indeed\nboost the quality of \ufb01nal generation. To this end,\nCai et al. (2021) propose to unify the memory\nretriever and its downstream generation model\ninto a learnable whole. Such memory retrieval is\nend-to-end optimized for task-speci\ufb01c objectives.\n2.4\nIntegration\nData Augmentation\nThere are several ways to\nintegrate the retrieved external memory in gener-\nation. One straightforward way is data augmen-\ntation, which constructs some augmented inputs\nby concatenating spans from {\u27e8xr, yr\u27e9} with the\noriginal input x. By training on the augmented\ninputs, a generation model implicitly leans how\nto integrate the retrieved information. Despite the\nsimplicity, this kind of methods works ef\ufb01ciently\nin lots of tasks (Song et al., 2016; Weston et al.,\n2018; Bulte and Tezcan, 2019).\nAttention\nMechanisms\nAnother\nintegration\nmethod\nis\nbased\non\nattention\nmechanisms\n(Bahdanau et al., 2014). The main idea of this\nfashion is adopting additional encoders (in various\narchitectures) to encode retrieved target sentences,\nand integrate them through attention (Cao and\nXiong, 2018; Gu et al., 2018; Bapna and Firat,\n2019). Since the attention mechanism is becoming\n(Bahdanau et al., 2014; Vaswani et al., 2017) a\nkey module in lots of NLP models, integrating\nretrieved memory through attention becomes a\nvery nature and ef\ufb01cient way.\nSkeleton Extraction\nIn the previous two meth-\nods, the downstream generation model learns how\nto \ufb01lter out irrelevant or even harmful informa-\ntion from the retrieved examples implicitly. There\nalso exist some works that try to explicitly extract\nuseful information, i.e., skeleton extraction, from\nthe retrieved memory (Cai et al., 2019a; Wu et al.,\n2019; Cai et al., 2019b). For example, one skeleton\nshould be a part of a whole utterance with irrelevant\ncontent masked, and the generation model only in-\ntegrate this skeleton in the generation process.\n3\nDialogue Response Generation\nBackground\nDialogue systems can be grouped\ninto two categories: chit-chat systems and task-\noriented systems. While task-oriented dialogue\nsystems are designed to accomplish speci\ufb01c user\ntasks such as air tickets booking, chit-chat dialogue\nsystems aim at giving a meaningful and \ufb02uent re-\nsponse for any dialogue history in the open domain.\nDialogue response generation in chit-chat dialogue\nsystem is challenging partly due to the diversity\nof possible responses to a single dialogue history\n(i.e., the one-to-many problem). The dialogue his-\ntory alone cannot decide a meaningful and speci\ufb01c\nresponse. Also, external knowledge that is not\npresent in the dialogue history are often necessary\nfor avoiding safe but boring responses. We focus\non recent efforts tackling the challenges to develop\nchit-chat dialogue systems.\nMost modern chit-chat dialogue systems can\nbe categorized into two classes, namely, retrieval-\nbased models and generation-based models. The\nretrieval-based models (Ji et al., 2014; Hu et al.,\n2014) directly copy an existing response from cu-\nrated dialogue corpora (i.e., the retrieval pool)\nwhen receiving a response request. The retrieved\nresponses are often informative and grammatical\nas they are collected from real-world conversa-\n\ntions and possibly post-edited by a human. How-\never, such systems perform poorly when a given\ndialogue history is substantially different from\nthose in the retrieval pool. On the other hand,\nthe generation-based models (Shang et al., 2015;\nVinyals and Le, 2015; Li et al., 2016a) generate\na new utterance from scratch. Those generation-\nbased models have better generalization capacity\nwhen handling unseen dialogue contexts. Never-\ntheless, the generated utterances are inclined to be\ndull and non-informative (e.g., \u201cI don\u2019t know\u201d, \u201cI\nthink so\u201d, \u201cMe too\u201d etc.) (Li et al., 2016a).\nShallow Integration\nAs discussed, retrieval-\nbased models may give informative but inappro-\npriate responses while generation-based models\noften do the opposite. It is desirable to combine the\nbest of both worlds. Early work (Qiu et al., 2017)\nattempts to re-rank the output from both models.\nFor a deep integration, Song et al. (2016) and Yang\net al. (2019) extend the standard SEQ2SEQ encoder-\ndecoder model (Bahdanau et al., 2014) with an ex-\ntra encoder for encoding the retrieval result. The\noutput of the extra encoder, along with the output\nfrom the original encoder for dialogue history, is\nused to feed the decoder. Weston et al. (2018) use\na single encoder that takes the concatenation of\nthe original dialogue history and the retrieved as\ninput. Wu et al. (2019) note that the retrieved infor-\nmation should be used in awareness of the context\ndifference, and further proposed to construct an\nedit vector by explicitly encoding the lexical differ-\nences between the input dialogue history and the\nretrieved dialogue history. Pandey et al. (2018) fur-\nther propose to weight different training instances\nby context similarity.\nDeep Integration\nTo prevent the in\ufb02ow of er-\nroneous information, Cai et al. (2019a) propose\na general framework that \ufb01rst extracts a skeleton\nfrom the retrieved response and then generates the\nresponse based on the extracted skeleton. This\nframework is also adopted for stylistic response\ngeneration (Su et al., 2021c). Gupta et al. (2021)\nsuggest to use the semantic structure of an exem-\nplar response, instead of the tokens of the exem-\nplar response, to guide generation. Despite their\ndifferences, a common issue is that the genera-\ntion model easily learns to ignore the retrieved re-\nsponse entirely and collapses to a vanilla seq2seq\nmodel. This happens with improper training in-\nstances. Due to the one-to-many nature, it hap-\npens frequently that a retrieved response (extracted\nskeleton) is suitable for responding to the query,\nbut inconsistent with the current target response.\nEarlier studies (Weston et al., 2018; Wu et al.,\n2019; Cai et al., 2019a) alleviate the above prob-\nlems by putting hard constraints on the data (e.g.,\ndiscarding data with low similarity of the retrieved\nresponse and the target response), which, however,\ngreatly reduces the amount of usable data. Cai\net al. (2019b) employ a random mechanism for\ngenerating the skeletons used for training, which\nextract skeletons from the corresponding responses\nwith some deliberate disturbance. Paranjape et al.\n(2021) propose to model the retriever after the pos-\nterior distribution of retrieval given the input and\nthe target output and train it jointly with the stan-\ndard retriever and the generator by maximizing the\nevidence lower bound (ELBo) in expectation over\nretrieval.\nKnowledge-Enhanced Generation\nThe afore-\nmentioned work demonstrates that retrieval-based\ndialogue systems can be used for building bet-\nter generation-based models. In general, this is\ndone by conditioning the generation on some re-\ntrieved responses. More traditionally, to infuse\nthe response with external knowledge, the retrieval\npool is not necessarily a dialogue corpus. In fact,\nknowledge-grounded dialogue response generation\nexploring different forms of knowledge such as\nknowledge bases and external documents (Dinan\net al., 2018; Zhou et al., 2018; Lian et al., 2019;\nLi et al., 2019; Qin et al., 2019; Wu et al., 2021;\nZhang et al., 2021; Komeili et al., 2021) has been\nactively explored.\nLimitations\nWe note that there are three major\nlimitations in existing work for dialogue response\ngeneration. First, current methods only use one\nretrieved response for generation. It can be more\nbene\ufb01cial to combine multiple retrieval responses.\nHowever, this can be dif\ufb01cult due to the one-to-\nmany nature of dialogue response generation. Sec-\nond, current methods use universal relevance score\nfor retrieval. It can be more effective if we can\nuse more customized retrieval metric especially\nfor controlled dialogue response generation (e.g.,\npersona, emotion, etc). Third, the retrieval pool\nof existing methods is limited to dialogue corpora\n(context-response pairs) or documents. It might\nbe useful to enlarge the retrieval pool by including\nmore corpora in other domains or in other modali-\n\nties. As discussed, there leaves plenty of possible\ndirections to explore in the future.\n4\nMachine Translation\nRetrieval augmented translation originates from hu-\nman translation scenarios (Somers, 2003). When\ntranslating \u02c6y from an input source sentence x, a hu-\nman translator typically involves a search engine to\nretrieve similar sentences {\u27e8xr, yr\u27e9} from a bilin-\ngual database. Such a technique called translation\nmemory is helpful to improve the translation qual-\nity and ef\ufb01ciency for human translators (Dillon\nand Fraser, 2006). As the development of ma-\nchine translation techniques, there is a surge of\ninterests in improving machine translation models\nwith translation memory. In the rest of this section,\nwe will review translation memory for both statisti-\ncal machine translation (SMT) and neural machine\ntranslation (NMT).\n4.1\nTranslation Memory in SMT\nGenerally, SMT includes three key components in\na pipeline manner such as phrase table extraction,\nparameter tuning and decoding (Koehn et al., 2003;\nChiang, 2007). As a result, many efforts have been\nmade to make use of translation memory (TM) on\ntop of each component.\nConstrained Decoding with TM\nConstrained\ndecoding is the most straightforward way to in-\ntegrating TM into SMT (Smith and Clark, 2009;\nKoehn and Senellart, 2010; Zhechev and Van Gen-\nabith, 2010; Ma et al., 2011). Its basic idea is\nto reuse the useful segments in yr while trans-\nlate other segments by SMT. Speci\ufb01cally, the ap-\nproach consists of three steps: 1) identify the un-\nmatched segments in both xr and x through the\nedit-distance algorithm; 2) identify the unmatched\nsegments in yr, each of which is aligned to one\nunmatched segment in xr by a word alignment\nalgorithm; 3) decode each unmatched segment in\nx by SMT and then use the result to replace its\ncorresponding unmatched segment in yr. Li et al.\n(2016b) further extend this approach from sentence\nlevel to phrase level. The advantage in constrained\ndecoding is that it does not require to change the\ntranslation model (including phrase table and pa-\nrameters) and can be applied in a plug-and-play\nway. This approach is successful when x is highly\nsimilar to xr; otherwise its performance is de-\ngraded largely, because it explicitly isolates TM\nmatching and SMT decoding and reuses the results\nin xr or not in a deterministic way.\nPhrase Table Aggregation with TM\nThere are\nalso notable efforts to augment the phrase table\nfor SMT by extracting translation rules from the\nretrieved bilingual sentences {\u27e8xr, yr\u27e9}.\nThen\nthey re-tune the parameters for the SMT model\nwhich makes use of translation knowledge from\n{\u27e8xr, yr\u27e9} in a implicit way when translating x.\nFor example, Bi\u00e7ici and Dymetman (2008); Simard\nand Isabelle (2009) directly combine the extracted\ntranslation rules into the phrase table in a shallow\ncombination way. They introduce an additional fea-\nture to indicate that whether translation rule is from\n{\u27e8xr, yr\u27e9} or not and then train all feature weights\nwith MERT (Och, 2003). One characteristic of\nthese work is that a translation rule extracted from\n{\u27e8xr, yr\u27e9} which can not exactly match any seg-\nments in x is useless even if it may contain some\nuseful words in its target side. To remedy this ob-\nservation, Wang et al. (2013, 2014) resort to a deep\ncombination way to using the extracted translation\nrules. For each rule in the phrase table, it designs\na generative model to reward the rules which are\nsimilar to those extracted from {\u27e8xr, yr\u27e9}. Then\nthis generative model is used as a feature in the log-\nlinear based SMT model whose weight is tuned\ntogether with other features by MERT. In addition,\nLi et al. (2014) employ a similar way to reward\nthe rules but it relies on a discriminative model\nwhich is easy to integrate potential features from\n{\u27e8xr, yr\u27e9}.\nParameter Tuning with TM\nUnlike the above\ntwo research lines, Liu et al. (2012, 2014) make use\nof translation memory only in tuning parameters.\nTo be speci\ufb01c, when translating an input sentence\nx, they \ufb01rstly retrieve many similar bilingual sen-\ntences {\u27e8xr, yr\u27e9}, and then tune the parameters on\ntop of the retrieved sentences as well as a given de-\nvelopment dataset in a sentence-wise manner, i.e.,\nit performs an independent tuning for each input\nsentence. To improve the ef\ufb01ciency of each tuning\nstep, it propose a local update on top of {\u27e8xr, yr\u27e9}\nfrom a baseline model.\nDespite the successes of translation memory in\nSMT, there are still some limitations for the above\nthree kinds of methods. Firstly, all these methods\nemploy fuzzy score for retrieval which is highly de-\npendent on word matching and thus can not recall\nsuch examples which are similar in word seman-\n\ntics but different in surface form. Secondly, these\nmethods integrate the retrieved examples into a\nmodule of SMT in the ways which can not make\nfull use of the knowledge in retrieved examples.\nFor example, the integration ways in the \ufb01rst two\nkinds (constrained decoding and phrase table ag-\ngregation) are heuristic and not optimized towards\ntranslation quality; the parameter tuning method\n\ufb01ne-tunes few parameters for log-linear based SMT\nwhich are not enough to preserve suf\ufb01cient knowl-\nedge from retrieved examples. Thirdly, since SMT\nperforms in a pipeline manner, it is intractable to\njointly optimize retrieval metrics as well as SMT\nmodels. Consequently, all these methods adopt an\noff-the-shelf metric for retrieval, leading to sub-\noptimal performance.\n4.2\nTranslation Memory in NMT\nTranslation memory has been widely explored in\nNeural Machine Translation (NMT). Depending\non when retrieval is involved, we can categorize\nprevious works into two classes: 1) an NMT model\nleans how to cooperate with the retrieval model in\nthe training phase; 2) an NMT model is only aware\nof the retrieved data in the inference phase.\nInference Phase\nThe key point of literature in\nthis line is to reward some target words based on\nwords in yr in the inference process. Thus, a de-\ncision can be made based on both the distribution\nof generation model and the additional reward of\nretrieval model. Some previous works propose to\nreward target words based on the sentence-level\nsimilarity between x and xr, and the word align-\nment between xr and yr. Given the input sentence\nx, Zhang et al. (2018) try to assign target words\nin \u02c6y with higher rewards, when they appear in yr\nand the aligned source words are in both xr and\nx. He et al. (2019) follow a similar framework\nand consider the position information of those tar-\nget words when rewarding. Those works reward\nthe target words in an explicit way, however, the\none-sentence-one-model approach (Li et al., 2016c;\nTurchi et al., 2017) propose to reward target word\nimplicitly. For each testing input x, their approach\nwill \ufb01rst \ufb01netune the translation model on retrieved\nmemory {\u27e8xr, yr\u27e9} and then translate x.\nOthers try to reward target words based on token-\nlevel similarity score. Most works in this line are\nbased on the dense retriever (Khandelwal et al.,\n2020a), e.g., faiss. Khandelwal et al. (2020a) build\na key-value datastore, where key h(xr, yr\n<t) is the\nhidden state at each time step when translating yr\nfrom xr, and value is its golden-truth target word\nyr\nt. Therefore, in the inference time, they can use\nthe h(x, \u02c6y<t) as query and reward target words\nwith similar hidden representations in the datas-\ntore. Although this method achieves signi\ufb01cant\nperformance gain, one drawback of it is the high la-\ntency. To address this issue, Meng et al. (2021) use\nsome heuristics, e.g., pre-\ufb01ltering, to avoid search-\ning on the entire datastore. The reward score of\nprevious works is got from some non-parametric\napproaches, however, Zheng et al. (2021a) propose\na light-weight network to learn the reward score.\nSince dense retrieval has the potential of cross-\nlingual retrieval, Zheng et al. (2021b) use a similar\napproach to achieve unsupervised domain adapta-\ntion, where a main change is to create the datastore\nbased on synthetic sources sentence and the real\ntarget sentences.\nTraining Phase\nDifferent from those model-\nagnostic approaches, previous works in this line\naim to train the generation model to learn how\nto cooperate with the retrieval model. It is also\nworth noting that most works in this line adopt\nthe sentence-level retrieval, when integrating the\nretrieval information in the training process. To\nachieve its goal, Bulte and Tezcan (2019) and\nHossain et al. (2020) propose a data augmenta-\ntion method to integrate the retrieved information,\nwhere x is concatenated with yr before feeding\ninto the model . Following the data augmentation\napproach, Xu et al. (2020) propose more matching\nmethods to determine including which retrieved\nexample in the source is better.\nThere also exist some works that propose new\narchitectures to integrate the retrieval information.\nUnder the RNN-based framework, Cao and Xiong\n(2018) and Gu et al. (2018) use the gating and at-\ntention mechanism to incorporate the retrieved tar-\nget sentences. When Transformer (Vaswani et al.,\n2017) becomes the backbone of NMT, some works\nalso use additional transformer encoders to en-\ncode retrieved target sentences, and integrate them\nthrough attention mechanism (Bapna and Firat,\n2019; Cao et al., 2019). Xia et al. (2019) repre-\nsent the retrieved target sentences in a different\ndata structure, i.e., a graph structure, and integrate\nit through attention mechanism. He et al. (2021)\npropose a light-weight method to encode the re-\ntrieved target sentences and leverage the alignment\ninformation to \ufb01lter out irrelevant information. Dif-\n\nferent from previous works that rely on bilingual\nmemories, Cai et al. (2021) propose a framework\nthat can retrieve the most similar target sentence in\na monolingual dataset, using a source sentence as\nquery.\nLimitations\nIn the section of SMT, we have\nshowed some limitations of the retrieval augmented\napproaches. There also exist some limitations in\nthe line of NMT. First, the information used for\nderiving reward scores is limited. The similarity\nbetween an input and retrieved examples is the\nprimary feature to derive reward scores.\nHow-\never, some information, e.g., frequencies of words\nand context, may also be bene\ufb01cial for integrating\nthe translation memory. Second, it remains to be\nan open question that when should we use the re-\ntrieved information and when not. In the inference\nphase, approaches tend to integrate the translation\nmemory excessively, e.g., at each time step, which\nnot only reduces the translation ef\ufb01ciency but may\nalso dampen the \ufb02uency of generated results.\n5\nOther Tasks\nIn addition to dialogue system and machine trans-\nlation, retrieval-augmented generation techniques\nhave shown to be bene\ufb01cial in many other tasks. In\nthe following, we highlight several key tasks that\napply retrieval-augmented generation approaches.1\nLanguage Modelling\nIt has been shown that\nproperly leveraging information from retrieval\nmemory could improve the performance of large\npre-trained language model. To build a more accu-\nrate language model, Khandelwal et al. (2020b) pro-\npose to incorporate a soft memory module into the\nsystem. Speci\ufb01cally, an index is built by caching\nthe hidden states of the training corpus. Then, the\nlanguage model accesses the index via k-NN search\nand displays a greatly improved performance. As\nanother example, Guu et al. (2020) propose a new\nparadigm that applies retrieval-augmented tech-\nnique into the pre-training of generative language\nmodel. During learning, they train a neural se-\nlector that dynamically samples a relevant text to\nguide the reconstruction of a corrupted input se-\nquence. In this way, the pre-trained model deliv-\ners better results by explicitly grounding on the\nretrieval memory. Lewis et al. (2020a) combine\nlanguage model pre-training with a paraphrasing\n1Here, we focus on tasks other than question answering.\nWe refer readers interested in QA to Chen and Yih (2020).\napproach. During learning, an input sequence to\nthe model is \ufb01rst corrupted. In the meantime, a set\nof multi-lingual texts are retrieved based on which\nthe model learns to reconstruct the original input\nsequence. Recently, Borgeaud et al. (2021) pro-\npose RETRO, a large pre-trained language model\nenhanced with retrieved documents, and obtained\ncomparable performances with GPT-3 using 25\u00d7\nfewer parameters.\nSummarization\nText summarization is another\nresearch\narea\nthat\nbene\ufb01ts\nfrom\nretrieval-\naugmented text generation.\nPeng et al. (2019)\npropose an adaptive decoding framework which\n\ufb01rst retrieves an exemplar document given the\nsource document. Then, the summarization of the\nsource document is derived through an adaptive\ngeneration process based on the retrieved template.\nDifferent from Peng et al. (2019), Cao et al.\n(2018) and Hossain et al. (2020) introduce an\nintermediate re-ranking stage into the generation\npipeline.\nSpeci\ufb01cally, before generating the\ndocument summary, the retrieval documents are\n\ufb01rst re-ranked based on their similarity scores\nwith respect to the source document. Then, the\ndocument summarization is produced by re-writing\nthe selected templates.\nParaphrase Generation\nTo address the lack of\nquality as well as diversity in the generation of para-\nphrases, Kazemnejad et al. (2020) propose a gen-\neration framework which \ufb01rst retrieves a sentence\nthat is similar to input sentence. Then, based on\nthe retrieved sentence, a neural editor produces the\nresulting paraphrased sentence. Chen et al. (2019)\ninvestigate a different aspect of paraphrasing, i.e.\nhow to control the linguistic syntax displayed in\nthe generated text. To achieve this goal, Chen et al.\n(2019) propose to \ufb01rst extract a sentential exem-\nplar that serves as the syntax template. A neural\nmodel then generates the paraphrase with desired\nlinguistic syntax following the retrieved exemplar.\nText Style Transfer\nTo improve the quality of\ngenerated text, Li et al. (2018) propose a retrieval-\naugmented framework which \ufb01rst retrieves texts\nthat are similar to the input based on lexical-level\nsimilarity. Then, the retrieved tokens that are irrel-\nevant to the source are deleted, and the output is\nderived from the edited template. Xiao et al. (2021)\nalso adopte this framework by incorporating re-\ntrieval information from two sources (i.e. sparse\nand dense memories) and obtained an improved\n\nmodel performance.\nData-to-Text Generation\nRecently, retrieval-\naugmented generation has been adapted to the task\nof data-to-text generation. To bridge the gap be-\ntween the structured data and natural language\ntext, Su et al. (2021a) propose a novel retrieval-\naugmented framework.\nSpeci\ufb01cally, given the\nsource data, a set of candidate texts are \ufb01rst re-\ntrieved from a large unlabelled corpus. Then, a\nneural selector is applied to measure the similari-\nties between the source data and candidate texts,\nand extract a set of more \ufb01ne-grained prototypes\nfrom the candidates. Lastly, a generation model\ntakes the prototypes as input to produce the text\nthat describes the given structured data.\nWhile retrieval-augmented generation has been\nwidely explored in the NLP community, we sug-\ngest that future research could extend this approach\nto tasks that involve data from multiple modali-\nties. For instance, with recent advancements in\nimage-text retrieval (Jia et al., 2021; Radford et al.,\n2021), the structural gap between images and texts\nis largely bridged. Some early studies (Zhang et al.,\n2020) have shown that information retrieved from\nimages could improve the performance of neural\nmachine translation model. Naturally, such meth-\nods could be extended to other multi-modal tasks,\nsuch as image captioning (Karpathy and Li, 2015).\nA similar idea could also be applied to tasks be-\nyond images, such as speech-to-text transcription\n(Gales and Young, 2007).\n6\nFuture Directions\nDespite the current success of retrieval augmented\ntext generation, there is still a long way to go as\ndiscussed in previous sections. We highlight some\ndirections to facilitate the future research as fol-\nlows:\nRetrieval Sensitivity\nThe performance of re-\ntrieval augmented text generation is very sensitive\nto the retrieval quality, i.e., the similarity between\nthe query and the retrieved examples. Currently, re-\ntrieval augmented text generation models perform\nwell when the retrieved examples are very simi-\nlar to the query. However, they are even worse\nthan the generation models without retrieval when\nthe retrieval examples are less similar. Therefore,\nit would be important to exploit new methods to\naddress such an issue on similarity.\nRetrieval Ef\ufb01ciency\nGenerally, if one enlarges\nthe retrieval memory to some extent, it would be\npossible to retrieve an example which is very simi-\nlar to the query.Unfortunately, the downside is that\nthe overall inference for the retrieval augmented\ngeneration models is less ef\ufb01cient due the consid-\nerable retrieval overhead. In this sense, it is urgent\nto consider some methods to trade off the retrieval\nmemory size and retrieval ef\ufb01ciency, for example,\ndata compression for the retrieval memory.\nLocal vs. Global Optimization\nTheoretically, it\nseems promising to jointly learn retrieval metrics\nand generation models. However, in practice, there\nis an essential gap about the retrieval metric be-\ntween the training and inference phrases. In the\ntraining phase, the loss is locally back-propagated\nto only a few retrieved examples while in the infer-\nence phase the metric is globally conducted among\nall examples in the memory. It would be interesting\nto narrow such a gap when learning a better metric\nfor generation tasks.\nMulti-Modalities\nWith recent advancement in\nimage-text retrieval, directly associating images\nwith relevant text becomes possible. This urges\nresearchers to investigate the possibility of retrieval-\nbased text generation in tasks that involve data from\ndifferent modalities. One typical task is image\ncaptioning. Beyond images, other tasks like speech-\nto-text transcription could potentially bene\ufb01t from\nretrieval-based generation methods as well.\nDiverse & Controllable Retrieval\nMost of the\nexisting approaches adopt a universal metric for\nretrieval, such as lexical similarities of sentences.\nFuture work should explore how to use customized\nmetrics for retrieval. This can be bene\ufb01cial for\nmore controlled text generation. For example, in-\nstances with emotions and styles may be more de-\nsirable in the personalized dialogue generation, par-\nallel data that contains speci\ufb01c terminologies is\nmore helpful in machine translation, and so on. On\nthe other hand, using a universal metric for retrieval\nmay lead to the lack of diversity of the retrieval re-\nsults. Collecting a diverse set of retrieval results\ncan improve the coverage of useful information.\nThus, considering multiple different metrics for re-\ntrieval may lead to generation with higher quality\nin the future.\n\n7\nConclusion\nIn this paper, we surveyed recent approaches for\nretrieval-augmented text generation. We reviewed\nand summarized the development of different com-\nponents of retrieval-augmented text generation in-\ncluding retrieval metrics, retrieval sources, and in-\ntegration paradigms. We gave in-depth discussions\nwhen retrieval-augmented text generation comes to\ndifferent applications including dialogue response\ngeneration, machine translation, and other genera-\ntion tasks. We also pointed out some future direc-\ntions for retrieval-augmented text generation.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014.\nNeural machine translation by jointly\nlearning to align and translate.\narXiv preprint\narXiv:1409.0473.\nAnkur Bapna and Orhan Firat. 2019. Non-parametric\nadaptation for neural machine translation. In Pro-\nceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 1 (Long and Short Papers), pages 1921\u20131931.\nErgun Bi\u00e7ici and Marc Dymetman. 2008.\nDynamic\ntranslation memory: Using statistical machine trans-\nlation to improve translation memory fuzzy matches.\nIn International Conference on Intelligent Text Pro-\ncessing and Computational Linguistics, pages 454\u2013\n465. Springer.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge van den Driessche, Jean-Baptiste Lespiau,\nBogdan Damoc, Aidan Clark, Diego de Las Casas,\nAurelia Guy, Jacob Menick, Roman Ring, Tom Hen-\nnigan, Saffron Huang, Loren Maggiore, Chris Jones,\nAlbin Cassirer, Andy Brock, Michela Paganini, Ge-\noffrey Irving, Oriol Vinyals, Simon Osindero, Karen\nSimonyan, Jack W. Rae, Erich Elsen, and Laurent\nSifre. 2021. Improving language models by retriev-\ning from trillions of tokens. CoRR, abs/2112.04426.\nBram Bulte and Arda Tezcan. 2019. Neural fuzzy re-\npair: Integrating fuzzy matches into neural machine\ntranslation. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 1800\u20131809.\nDeng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xi-\naojiang Liu, Wai Lam, and Shuming Shi. 2019a.\nSkeleton-to-response: Dialogue generation guided\nby retrieval memory.\nIn Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 1219\u20131228.\nDeng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiao-\njiang Liu, and Shuming Shi. 2019b.\nRetrieval-\nguided dialogue response generation via a matching-\nto-generation framework.\nIn Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1866\u20131875.\nDeng Cai, Yan Wang, Huayang Li, Wai Lam, and\nLemao Liu. 2021. Neural machine translation with\nmonolingual translation memory. In Proceedings of\nthe 59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 7307\u20137318, Online.\nAssociation for Computational Linguistics.\nQian Cao, Shaohui Kuang, and Deyi Xiong. 2019.\nLearning to reuse translations: Guiding neural ma-\nchine translation with examples.\narXiv preprint\narXiv:1911.10732.\nQian Cao and Deyi Xiong. 2018.\nEncoding gated\ntranslation memory into neural machine translation.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n3042\u20133047.\nZiqiang Cao, Wenjie Li, Sujian Li, and Furu Wei.\n2018. Retrieve, rerank and rewrite: Soft template\nbased neural summarization. In Proceedings of the\n56th Annual Meeting of the Association for Com-\nputational Linguistics, ACL 2018, Melbourne, Aus-\ntralia, July 15-20, 2018, Volume 1: Long Papers,\npages 152\u2013161. Association for Computational Lin-\nguistics.\nDanqi Chen and Wen-tau Yih. 2020.\nOpen-domain\nquestion answering. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics: Tutorial Abstracts, pages 34\u201337, On-\nline. Association for Computational Linguistics.\nMingda Chen, Qingming Tang, Sam Wiseman, and\nKevin Gimpel. 2019. Controllable paraphrase gen-\neration with a syntactic exemplar. In Proceedings of\nthe 57th Conference of the Association for Compu-\ntational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages\n5972\u20135984. Association for Computational Linguis-\ntics.\nDavid Chiang. 2007. Hierarchical phrase-based trans-\nlation. computational linguistics, 33(2):201\u2013228.\nSarah Dillon and Janet Fraser. 2006. Translators and\ntm: An investigation of translators\u2019 perceptions of\ntranslation memory adoption. Machine Translation,\n20(2):67\u201379.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2018. Wizard\nof wikipedia: Knowledge-powered conversational\nagents. arXiv preprint arXiv:1811.01241.\n\nMark J. F. Gales and Steve J. Young. 2007. The applica-\ntion of hidden markov models in speech recognition.\nFound. Trends Signal Process., 1(3):195\u2013304.\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Vic-\ntor OK Li. 2018. Search engine guided neural ma-\nchine translation. In Proceedings of the AAAI Con-\nference on Arti\ufb01cial Intelligence, volume 32.\nPrakhar Gupta, Jeffrey Bigham, Yulia Tsvetkov, and\nAmy Pavel. 2021. Controlling dialogue generation\nwith semantic exemplars.\nIn Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 3018\u20133029, On-\nline. Association for Computational Linguistics.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. REALM: retrieval-\naugmented language model pre-training.\nCoRR,\nabs/2002.08909.\nTatsunori B Hashimoto, Kelvin Guu, Yonatan Oren,\nand Percy S Liang. 2018. A retrieve-and-edit frame-\nwork for predicting structured outputs. In Advances\nin Neural Information Processing Systems, pages\n10052\u201310062.\nQiuxiang He, Guoping Huang, Qu Cui, Li Li, and\nLemao Liu. 2021. Fast and accurate neural machine\ntranslation with translation memory.\nIn Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 3170\u20133180.\nQiuxiang He, Guoping Huang, Lemao Liu, and Li Li.\n2019. Word position aware translation memory for\nneural machine translation.\nIn CCF International\nConference on Natural Language Processing and\nChinese Computing, pages 367\u2013379. Springer.\nNabil Hossain, Marjan Ghazvininejad, and Luke Zettle-\nmoyer. 2020.\nSimple and effective retrieve-edit-\nrerank text generation. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2532\u20132538.\nBaotian Hu, Zhengdong Lu, Hang Li, and Qingcai\nChen. 2014. Convolutional neural network architec-\ntures for matching natural language sentences. In\nNIPS, pages 2042\u20132050.\nZongcheng Ji, Zhengdong Lu, and Hang Li. 2014. An\ninformation retrieval approach to short text conver-\nsation. arXiv preprint arXiv:1408.6988.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung,\nZhen Li, and Tom Duerig. 2021. Scaling up visual\nand vision-language representation learning with\nnoisy text supervision. In Proceedings of the 38th In-\nternational Conference on Machine Learning, ICML\n2021, 18-24 July 2021, Virtual Event, volume 139 of\nProceedings of Machine Learning Research, pages\n4904\u20134916. PMLR.\nAndrej Karpathy and Fei-Fei Li. 2015. Deep visual-\nsemantic alignments for generating image descrip-\ntions. In IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2015, Boston, MA, USA,\nJune 7-12, 2015, pages 3128\u20133137. IEEE Computer\nSociety.\nAmirhossein Kazemnejad, Mohammadreza Salehi, and\nMahdieh Soleymani Baghshah. 2020.\nParaphrase\ngeneration by learning how to edit from samples. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 6010\u2013\n6021, Online. Association for Computational Lin-\nguistics.\nUrvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke\nZettlemoyer,\nand Mike Lewis. 2020a.\nNear-\nest neighbor machine translation.\narXiv preprint\narXiv:2010.00710.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020b. Generaliza-\ntion through memorization: Nearest neighbor lan-\nguage models. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nPhilipp Koehn, Franz J. Och, and Daniel Marcu. 2003.\nStatistical phrase-based translation. In Proceedings\nof the 2003 Human Language Technology Confer-\nence of the North American Chapter of the Associa-\ntion for Computational Linguistics, pages 127\u2013133.\nPhilipp Koehn and Jean Senellart. 2010. Convergence\nof translation memory and statistical machine trans-\nlation. In Proceedings of AMTA Workshop on MT\nResearch and the Translation Industry, pages 21\u201331.\nMojtaba Komeili, Kurt Shuster, and Jason Weston.\n2021.\nInternet-augmented dialogue generation.\narXiv preprint arXiv:2107.07566.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019.\nLatent retrieval for weakly supervised\nopen domain question answering.\narXiv preprint\narXiv:1906.00300.\nMike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Ar-\nmen Aghajanyan, Sida Wang, and Luke Zettlemoyer.\n2020a. Pre-training via paraphrasing. In Advances\nin Neural Information Processing Systems 33: An-\nnual Conference on Neural Information Processing\nSystems 2020, NeurIPS 2020, December 6-12, 2020,\nvirtual.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rock-\nt\u00e4schel, et al. 2020b.\nRetrieval-augmented gen-\neration for knowledge-intensive nlp tasks.\narXiv\npreprint arXiv:2005.11401.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016a. A diversity-promoting ob-\njective function for neural conversation models. In\nNAACL, pages 110\u2013119.\n\nJuncen Li, Robin Jia, He He, and Percy Liang. 2018.\nDelete, retrieve, generate: a simple approach to sen-\ntiment and style transfer. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, NAACL-HLT 2018, New\nOrleans, Louisiana, USA, June 1-6, 2018, Volume\n1 (Long Papers), pages 1865\u20131874. Association for\nComputational Linguistics.\nLiangyou Li, Andy Way, and Qun Liu. 2014.\nA\ndiscriminative framework of integrating translation\nmemory features into smt.\nIn Proceedings of the\n11th Conference of the Association for Machine\nTranslation in the Americas, volume 1, pages 249\u2013\n260.\nLiangyou Li, Andy Way, and Qun Liu. 2016b. Phrase-\nlevel combination of smt and tm using constrained\nword lattice.\nAssociation for Computational Lin-\nguistics (ACL).\nXiaoqing Li, Jiajun Zhang, and Chengqing Zong.\n2016c. One sentence one model for neural machine\ntranslation. arXiv preprint arXiv:1609.06490.\nZekang Li, Cheng Niu, Fandong Meng, Yang Feng,\nQian Li, and Jie Zhou. 2019.\nIncremental trans-\nformer with deliberation decoder for document\ngrounded conversations. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 12\u201321.\nRongzhong Lian, Min Xie, Fan Wang, Jinhua Peng,\nand Hua Wu. 2019. Learning to select knowledge\nfor response generation in dialog systems.\narXiv\npreprint arXiv:1902.04911.\nLemao Liu, Hailong Cao, Taro Watanabe, Tiejun Zhao,\nMo Yu, and Conghui Zhu. 2012. Locally training\nthe log-linear model for smt. In Proceedings of the\n2012 Joint Conference on Empirical Methods in Nat-\nural Language Processing and Computational Natu-\nral Language Learning, pages 402\u2013411.\nLemao Liu, Tiejun Zhao, Taro Watanabe, Hailong Cao,\nand Conghui Zhu. 2014. Discriminative training for\nlog-linear based smt: Global or local methods. ACM\nTransactions on Asian Language Information Pro-\ncessing (TALIP), 13(4):1\u201325.\nYanjun Ma, Yifan He, Andy Way, and Josef van Gen-\nabith. 2011.\nConsistent translation using discrim-\ninative learning-a translation memory-inspired ap-\nproach.\nIn Proceedings of the 49th Annual Meet-\ning of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1239\u20131248.\nYuxian Meng, Xiaoya Li, Xiayu Zheng, Fei Wu, Xi-\naofei Sun, Tianwei Zhang, and Jiwei Li. 2021.\nFast nearest neighbor machine translation.\narXiv\npreprint arXiv:2105.14528.\nFranz Josef Och. 2003. Minimum error rate training in\nstatistical machine translation. In Proceedings of the\n41st Annual Meeting of the Association for Compu-\ntational Linguistics, pages 160\u2013167, Sapporo, Japan.\nAssociation for Computational Linguistics.\nGaurav Pandey, Danish Contractor, Vineet Kumar, and\nSachindra Joshi. 2018. Exemplar encoder-decoder\nfor neural conversation generation. In ACL, pages\n1329\u20131338.\nAshwin Paranjape, Omar Khattab, Christopher Potts,\nMatei Zaharia, and Christopher D Manning. 2021.\nHindsight: Posterior-guided training of retrievers for\nimproved open-ended generation.\narXiv preprint\narXiv:2110.07752.\nHao Peng, Ankur P. Parikh, Manaal Faruqui, Bhuwan\nDhingra, and Das Dipanjan. 2019. Text generation\nwith exemplar-based adaptive decoding. In Proceed-\nings of the Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies.\nLianhui Qin, Michel Galley, Chris Brockett, Xiaodong\nLiu, Xiang Gao, William B Dolan, Yejin Choi, and\nJianfeng Gao. 2019. Conversing by reading: Con-\ntentful neural conversation with on-demand machine\nreading. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5427\u20135436.\nMinghui Qiu, Feng-Lin Li, Siyu Wang, Xing Gao, Yan\nChen, Weipeng Zhao, Haiqing Chen, Jun Huang,\nand Wei Chu. 2017. Alime chat: A sequence to se-\nquence and rerank based chatbot engine. In ACL,\npages 498\u2013503.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural lan-\nguage supervision. In Proceedings of the 38th In-\nternational Conference on Machine Learning, ICML\n2021, 18-24 July 2021, Virtual Event, volume 139 of\nProceedings of Machine Learning Research, pages\n8748\u20138763. PMLR.\nStephen Robertson and Hugo Zaragoza. 2009.\nThe\nprobabilistic relevance framework: BM25 and be-\nyond. Now Publishers Inc.\nLifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neu-\nral responding machine for short-text conversation.\nIn ACL, pages 1577\u20131586.\nMichel Simard and Pierre Isabelle. 2009. Phrase-based\nmachine translation in a computer-assisted transla-\ntion environment. Proceedings of the Twelfth Ma-\nchine Translation Summit (MT Summit XII), pages\n120\u2013127.\nJames Smith and Stephen Clark. 2009. Ebmt for smt:\na new ebmt-smt hybrid. In Proceedings of the 3rd\nInternational Workshop on Example-Based Machine\nTranslation, pages 3\u201310. Citeseer.\n\nHarold Somers. 2003.\nTranslation memory systems.\nBenjamins Translation Library, 35:31\u201348.\nYiping Song, Rui Yan, Xiang Li, Dongyan Zhao, and\nMing Zhang. 2016. Two are better than one: An en-\nsemble of retrieval-and generation-based dialog sys-\ntems. arXiv preprint arXiv:1610.07149.\nYixuan Su, Zaiqiao Meng, Simon Baker, and Nigel Col-\nlier. 2021a. Few-shot table-to-text generation with\nprototype memory. In Findings of the Association\nfor Computational Linguistics: EMNLP 2021, Vir-\ntual Event / Punta Cana, Dominican Republic, 16-\n20 November, 2021, pages 910\u2013917. Association for\nComputational Linguistics.\nYixuan Su, David Vandyke, Simon Baker, Yan Wang,\nand Nigel Collier. 2021b. Keep the primary, rewrite\nthe secondary: A two-stage approach for paraphrase\ngeneration. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021, pages\n560\u2013569, Online. Association for Computational\nLinguistics.\nYixuan Su, Yan Wang, Deng Cai, Simon Baker, Anna\nKorhonen, and Nigel Collier. 2021c. PROTOTYPE-\nTO-STYLE: dialogue generation with style-aware\nediting on retrieval memory. IEEE ACM Trans. Au-\ndio Speech Lang. Process., 29:2152\u20132161.\nMarco Turchi, Matteo Negri, M Farajian, and Marcello\nFederico. 2017. Continuous learning from human\npost-edits for neural machine translation.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998\u20136008.\nOriol Vinyals and Quoc Le. 2015. A neural conversa-\ntional model. In ICML (Deep Learning Workshop).\nKun Wang, Chengqing Zong, and Keh-Yih Su. 2013.\nIntegrating translation memory into phrase-based\nmachine translation during decoding.\nIn Proceed-\nings of the 51st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 11\u201321.\nKun Wang, Chengqing Zong, and Keh-Yih Su. 2014.\nDynamically integrating cross-domain translation\nmemory into phrase-based machine translation dur-\ning decoding.\nIn Proceedings of COLING 2014,\nthe 25th International Conference on Computational\nLinguistics: Technical Papers, pages 398\u2013408.\nJason Weston, Emily Dinan, and Alexander Miller.\n2018. Retrieve and re\ufb01ne: Improved sequence gen-\neration models for dialogue. In Proceedings of the\n2018 EMNLP Workshop SCAI: The 2nd Interna-\ntional Workshop on Search-Oriented Conversational\nAI, pages 87\u201392.\nYu Wu, Furu Wei, Shaohan Huang, Yunli Wang, Zhou-\njun Li, and Ming Zhou. 2019. Response generation\nby context-aware prototype editing. In Proceedings\nof the AAAI Conference on Arti\ufb01cial Intelligence,\nvolume 33, pages 7281\u20137288.\nZeqiu Wu, Michel Galley, Chris Brockett, Yizhe Zhang,\nXiang Gao, Chris Quirk, Rik Koncel-Kedziorski,\nJianfeng Gao, Hannaneh Hajishirzi, Mari Ostendorf,\net al. 2021. A controllable model of grounded re-\nsponse generation. In Proceedings of the AAAI Con-\nference on Arti\ufb01cial Intelligence, volume 35, pages\n14085\u201314093.\nMengzhou Xia, Guoping Huang, Lemao Liu, and\nShuming Shi. 2019. Graph based translation mem-\nory for neural machine translation. In Proceedings\nof the AAAI Conference on Arti\ufb01cial Intelligence,\nvolume 33, pages 7297\u20137304.\nFei Xiao, Liang Pang, Yanyan Lan, Yan Wang, Huawei\nShen, and Xueqi Cheng. 2021. Transductive learn-\ning for unsupervised text style transfer. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, EMNLP 2021, Vir-\ntual Event / Punta Cana, Dominican Republic, 7-11\nNovember, 2021, pages 2510\u20132521. Association for\nComputational Linguistics.\nJitao Xu, Josep M Crego, and Jean Senellart. 2020.\nBoosting neural machine translation with similar\ntranslations.\nIn Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1580\u20131590.\nLiu Yang, Junjie Hu, Minghui Qiu, Chen Qu, Jian-\nfeng Gao, W Bruce Croft, Xiaodong Liu, Yelong\nShen, and Jingjing Liu. 2019.\nA hybrid retrieval-\ngeneration neural conversation model. In Proceed-\nings of the 28th ACM international conference on in-\nformation and knowledge management, pages 1341\u2013\n1350.\nJingyi Zhang, Masao Utiyama, Eiichiro Sumita, Gra-\nham Neubig, and Satoshi Nakamura. 2018. Guiding\nneural machine translation with retrieved translation\npieces. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long Papers), pages 1325\u20131335.\nYizhe Zhang, Siqi Sun, Xiang Gao, Yuwei Fang, Chris\nBrockett, Michel Galley, Jianfeng Gao, and Bill\nDolan. 2021.\nJoint retrieval and generation train-\ning for grounded text generation.\narXiv preprint\narXiv:2105.06597.\nZhuosheng Zhang, Kehai Chen, Rui Wang, Masao\nUtiyama, Eiichiro Sumita, Zuchao Li, and Hai Zhao.\n2020.\nNeural machine translation with universal\nvisual representation. In 8th International Confer-\nence on Learning Representations, ICLR 2020, Ad-\ndis Ababa, Ethiopia, April 26-30, 2020. OpenRe-\nview.net.\n\nVentsislav Zhechev and Josef Van Genabith. 2010.\nSeeding statistical machine translation with trans-\nlation memory output through tree-based structural\nalignment.\nIn Proceedings of the 4th Workshop\non Syntax and Structure in Statistical Translation,\npages 43\u201351.\nXin Zheng, Zhirui Zhang, Junliang Guo, Shujian\nHuang, Boxing Chen, Weihua Luo, and Jiajun Chen.\n2021a. Adaptive nearest neighbor machine transla-\ntion. arXiv preprint arXiv:2105.13022.\nXin Zheng, Zhirui Zhang, Shujian Huang, Boxing\nChen, Jun Xie, Weihua Luo, and Jiajun Chen. 2021b.\nNon-parametric unsupervised domain adaptation for\nneural machine translation. In Findings of the As-\nsociation for Computational Linguistics: EMNLP\n2021, pages 4234\u20134241.\nKangyan Zhou, Shrimai Prabhumoye, and Alan W\nBlack. 2018. A dataset for document grounded con-\nversations. arXiv preprint arXiv:1809.07358.\n"}