{"metadata": {"pdf_filename": "2406.02266v1__Enhancing Retrieval-Augmented LMs with a Two-stage Consistency Learning Compress.pdf", "source": "arXiv"}, "text": "Enhancing RALMs with Consistency Learning Comp \n1 \n \nEnhancing Retrieval-Augmented LMs with a Two-stage \nConsistency Learning Compressor \nChuankai Xu1, Dongming Zhao2, Bo Wang1 and Hanwen Xing3, \n1College of  Intelligence and Computing, Tianjin University, Tianjin, China \n2Artificial Intelligence Laboratory, China Mobile Communication Group Tianjin Co., Ltd. \n 3University College London, London, WC1E 6BT, United Kingdom \nChuankai_Xu@tju.edu.cn waitman_840602@163.com  bo_wang@tju.edu.cn \n13902100085@qq.com \nAbstract. Despite the prevalence of retrieval-augmented language models (RALMs), the seamless \nintegration of these models with retrieval mechanisms to enhance performance in document-based \ntasks remains challenging. While some post-retrieval processing Retrieval-Augmented Generation \n(RAG) methods have achieved success, most still lack the ability to distinguish pertinent from \nextraneous information, leading to potential inconsistencies and reduced precision in the generated \noutput, which subsequently affects the truthfulness of the language model\u2019s responses. To address \nthese limitations, this work proposes a novel two-stage consistency learning approach for retrieved \ninformation compression in retrieval-augmented language models to enhance performance. By \nincorporating consistency learning, the aim is to generate summaries that maintain coherence and \nalignment with the intended semantic representations of a teacher model while improving faithfulness \nto the original retrieved documents. The proposed method is empirically validated across multiple \ndatasets, demonstrating notable enhancements in precision and efficiency for question-answering \ntasks. It outperforms existing baselines and showcases the synergistic effects of combining \ncontrastive and consistency learning paradigms within the retrieval-augmented generation \nframework. \nKeywords: Retrieved Augmented Language Model, Summary Generation, Contrastive Learning, \nConsistency Learning \n1 \nIntroduction \nRetrieval-augmented generation (RAG) has emerged as a promising paradigm for \nenhancing language model performance in document-based tasks. RAG comprises three \nstages: Retrieval, Augmentation, and Generation. In the Retrieval stage, the corpus is \ndivided into chunks, and vector indices are created using an encoder model. The \nAugmentation stage identifies and retrieves relevant chunks by comparing vector \nrepresentations of the query and the indexed chunks. Finally, in the Generation stage, the \nmodel generates a response conditioned on the information from the retrieved chunks. \nRecent advancements in RAG focus on refining each stage, including advanced \n\n2 \nC.Xu et al. \n \nencoding and indexing for improved corpus representation, sophisticated retrieval \nmechanisms to identify pertinent chunks, and novel generation strategies for accurate and \ncoherent responses. However, noise in retrieved documents challenges the seamless \nintegration of retrieval mechanisms with language models, particularly in controlling the \nrelevance and faithfulness of the generated outputs. \nTo address these limitations, we introduce the Contrastive and Consistency Learning \nPost-Retrieval Compressor (C2LPRCom) RAG framework. C2LPRCom aims to enhance \nrobustness against noise in retrieved documents while maintaining consistency with the \noriginal source. By incorporating contrastive and consistency learning, we enhance the \nmodel\u2019s ability to identify and utilize relevant information from retrieved documents, \nimproving the quality and faithfulness of the generated outputs. \n \nThis paper contributes to post-retrieval processing in RAG by: \n \na) \nProposing a fine-grained extractive compressor based on contrastive learning, \nintroducing a three-way data construction method to enhance localization accuracy \nduring extraction by leveraging granular positive, semi-positive, and negative \nsample pairs. \nb) \nIntroducing a two-stage consistency learning approach for constructing a lightweight \npost-retrieval information compressor in RAG frameworks. This method optimizes \ngenerated outputs to be relevant and faithful to retrieved information, employing a \nteacher-student paradigm to ensure high semantic similarity and robustness against \nnoise. \nc) \nConducting extensive experiments on three benchmark datasets, covering question-\nanswering and language modeling tasks. Results demonstrate significant \nimprovements in precision and inference efficiency, outperforming state-of- the-art \nbaselines and highlighting the potential of fine-grained contrastive and two-stage \nconsistency learning techniques in enhancing RAG performance. \n2 \nMethodology \n2.1 \nTask Definition \nSuppose the original input to the large model for a user is a sequence of N words: X= {x1, x2, \nx3, ..., xN }. K relevant documents D = {D1, ...Dk} (k=1,2,...K) have been retrieved from \nthe data pool based on the query. We aims to train such a model M\u03b8, which output a \nsummary sequence S of the shortest possible length L that contains all the relevant \ninformation from the K documents. The training data for M\u03b8 is created using the outputs \nof a contrastive selector and a distillation module. M\u03b8 is then fine-tuned on this crafted \ndataset using a two-stage consistency learning way to optimize its performance in \n\nEnhancing RALMs with Consistency Learning Comp \n3 \n \nproducing informative summaries. \n \n \nFig. 1: Overview of our proposed C2LPRCom architecture \nOur framework utilizes two complementary modules. The contrastive retriever enhanced \nselection model ranks and selects the optimal sentences Sj from the retrieved document D \nto form the sequence S. The distillation learning module takes the user\u2019s question and the \nretrieved relevant documents as input and outputs the best sequence S2 as the summary. \nThis summary is then incorporated with a prompt template to serve as the input for the \nsubsequent large language model, which generates the final response to the original query \nX. \n2.2 \nContrastive Retriever on Sentence-Level \nSome matured pre-trained dense retriever methods is prevailing and success- fully applied \nin some perplex language tasks, like Contriever[2], ColBert[3].But towards some specific \ntasks, they still need to be more fine-grained to escape introducing bias and noise, which \ncould improve the performance of downstream task. \nThe training process of the encoder is similar to training common question- answering \nmodels. The loss used in model training takes the form of contrastive learning, similar \nto[4]: \n\n4 \nC.Xu et al. \n \n( ,\n)\nexp(\n)\nlog\n( ,\n)\nexp(\n)\n1\nc\nsim x si\nL\nsim x s j\nn\nw\nw\nj\ni\ni\n\uf074\n\uf074\n= \u2212\uf0e5\n\uf0e5=\n                              (1) \nwhere sim(x, si) is the inner product between embeddings, used to measure the similarity \nbetween sentences. \u03c4 is the temperature parameter used to normalize the similarity. wi is \nthe weight assigned to each sample, with wi being 1 for positive and negative samples, \nand the value assigned in Algorithm 2 for semi-positive samples. The training objective of \nthe model aims to minimize Lc and maximize the distance between positive and negative \nsamples, while semi-positive samples lie in between based on their weights. We have \nattached pseudo code in 2 in Appendix. \n \n \nFig. 2: Overview of our refined contrastive retriever \n2.3 \nDistillation Module \nThe algorithm flow is shown in Algorithm 1, which illustrates the process of data \ndistillation and subsequent consistency learning based on the outputs of the teacher and \nstudent models. In the first stage, we do not perform perturbation or data augmentation on \nthe user input and retrieved results. For each user input xi and the retrieved documents \nDi, we utilize a fine-tuned selector in the upstream to choose the top-k candidate sentences \nCS(xi, Di). In the midstream distillation module, we obtain the corresponding output S\u2032 for \neach template in pi by combining xi and Di as input to the teacher model L\u2032. \nSimultaneously, we input whether to concatenate S\u2032 with xi and Di to the student large \nlanguage model L. If concatenating the teacher model output S\u2032 helps the student model \ngenerate a reply with a higher score, we further utilize the teacher model output to construct \nthe distilled training set; otherwise, we discard the teacher model output, indicating that \nthe teacher large language model cannot perform better than the student model for that \n\nEnhancing RALMs with Consistency Learning Comp \n5 \n \ndata point. We then calculate the similarity between the encoded CS(xi, Di) and the \nteacher model output S\u2032, and select the top-5 from the selector output as the result se, \nwhich is concatenated with the distiller output to form the constructed first-stage fine-\ntuning dataset T1. \nSimilarly, in the second stage of fine-tuning data construction, we add different data \naugmentations to the inputs of the upstream selector, midstream distillation model, and \ndownstream fine-tuning model. The remaining process is the same as in the first stage, \nexcept that we additionally require the fine-tuning model output under perturbation to \nmaintain semantic consistency with the upstream and midstream constructed fine-tuning \ndata T2. During training, in the first M rounds, fully supervised fine-tuning is performed \nbased on the first-stage constructed dataset. In subsequent rounds, in addition to the first-\nstage training loss, the consistency learning training loss is also added. \n \nAlgorithm 1 Generate the Distilled Training Data \nInput: lightweight model M\u03b8, Student LM L, Teacher LM L\u2032, Fine-tuned Selector \nCS, Input Training Data {Xi, Di, yi}T, Xi for User Input , Retrieved Relevant \nDocument Di, Ground Truth yi, Data Augmentation Set \u03a0, Prompt Template Set \n{pi}n. \nOutput: Distillation Training Dataset {T1, T2}, T1 for Stage 1, T2 for Stage 2   \nT1, T2 \u2192 \u2205, \u03c0, \u03c0\u2032 \u2208 \u03a0 \nfor i = 1 to T do \nsc, sc\u2032 \u2192 \u2212\u221e; st , s\u2032t, \ud835\udc60\ud835\udc61\n\ud835\udc52, \ud835\udc60\ud835\udc61\n\ud835\udc52\u2032 \nfor j = 1 to n do \notj, ot\u2032 \u2192 L\u2032(pj ; xi; Di), L\u2032(pj ; \u03c0(xi); \u03c0(Di)) \nif sc < SCORE(L, yi, [otj ; xi]) then \nst \u2192 otj ; sc = SCORE(L, yi, [otj ; xi]) \nend if \nif sc\u2032 < SCORE(L, yi, [ot\u2032 ; \u03c0(xi)]) then \ns\u2032t\u2192 ot\u2032j ; sc\u2032 = SCORE(L, yi, [ot\u2032 ; \u03c0(xi)]) \nend if  \nend for \n\ud835\udc60\ud835\udc61\ud835\udc52\u2192 argTop5\ud835\udc60\ud835\udc57\u2208\ud835\udc60\ud835\udc61\ud835\udc52<Enc(CS(xi, Di)), Enc(L\u2032(pj; xi; Di)) > \n\ud835\udc60\ud835\udc61\ud835\udc52\u2032 \u2192 argTop5\ud835\udc60\ud835\udc57\u2208\ud835\udc60\ud835\udc61\ud835\udc52 <Enc(CS(\u03c0(xi), \u03c0(Di))), Enc(L\u2032(pj; \u03c0(xi); \u03c0(Di)))> \nif sc < SCORE(L, yi, [xi]) then \nT1 \u222a {(xi, Di, \ud835\udc60\ud835\udc61\ud835\udc52)}, T2 \u222a {(\u03c0(xi), \u03c0(Di), \ud835\udc60\ud835\udc61\ud835\udc52\u2032)} \ncontinue \nend if \nT1 \u222a {(xi, Di, [\ud835\udc60\ud835\udc61\ud835\udc52; st])}, T2 \u222a {(\u03c0\u2032(xi), \u03c0\u2032(Di), [\ud835\udc60\ud835\udc61\ud835\udc52\u2032; s\u2032t ])} \nend for \nreturn {T1, T2} \n \nFor the language modeling task, SCORE(L, yi, [sj; xi]) = logpL(yi|sj; xi),which represents \nthe log-likelihood score of the output of the LLM L with respect to the reference answer, \ngiven the concatenation of the selected best candidate sentence and the user input. In the \n\n6 \nC.Xu et al. \n \ncase of question answering (QA) tasks, SCORE employs the Exact Match between strings \nas the evaluation metric. \n2.4 \nTwo-stage Consistency Learning \nAfter obtaining the distilled training datasets T1, T2, a two-stage consistency learning \napproach is employed for model training. During the initial stage, the model undergoes \nfine-tuning solely guided by the supervision loss. Subsequently, in the second stage, the \nmodel is trained under the joint supervision of both the supervised loss and the \nconsistency loss, enabling the model to learn from both labeled data and the consistency \nbetween the predictions of the model on different views of the input data. \nThe fully supervised loss Ls employs cross-entropy loss to measure the discrepancy \nbetween the model-generated sequence and the ground-truth answer. \nFor a single sample sequence, the cross-entropy loss is defined as follows on a per word \nbasis: \nn\n1\n2\n( (\n1)\n1\n( ,\n)\n(\n|\n,\n,\n,...,\n)\ns\ni\ni\nij\ni\ni\ni\ni j\nj\nL x y\nlogP y\nx y\ny\ny\n\u2212\n=\n= \u2212\uf0e5\n                    (2) \n1\n1/\n( ,\n)\ns\ni\ni\nj\nN\nL\nN\nL x y\n=\n=\n\uf0e5\n                                               (3) \nFor the consistency learning loss, it primarily consists of two components: By inputting the \nconstructed training set [se\u2032; s\u2032] under perturbation and the output M\u03b8(\u03c0\u2032(xi, Di)) of the \nlightweight model M\u03b8 into the BGE-en-large[8] encoder, we obtain their embedded \nrepresentations. We then separately calculate the L2 norm between se\u2032 and M\u03b8(\u03c0\u2032(xi, Di)), \nas well as between s\u2032 and M\u03b8(\u03c0\u2032(xi, Di)), as the final loss. The formulation is as follows: \n2\n2\n1\n1/\n||\n( ')\n(\n(\n(\n \n))) |\n \n'\n,\n|\nN\ni\nj\ncg\ni\nL\nN\nenc s\nenc M\ns\nD\n\uf071\uf070\n=\n=\n\u2212\n\uf0e5\n                (4) \n2\n1\n'\n2\n1/\n||\n(\n)\n(\n(\n(\n))) ||\n \n'\n \n,\nN\ni\nj\ne\nce\nt\ni\nL\nN\nenc s\nenc M\ns\nD\n\uf071\uf070\n=\n=\n\u2212\n\uf0e5\n               (5) \nThe total training loss is expressed as, where \u03bb1, \u03bb2 are ramp-up weighting factor, \ncontrolling the trade-off between supervised loss and consistency loss. On first stage, they \nare set to zero : \n1\n2\ns\ncg\nce\nL\nL\nL\nL\n\uf06c\n\uf06c\n=\n+\n+\n                                                   (6) \n3 \nExperimentation \n3.1 \nDatasets and Measure Metrics \nWe conduct our experiment on three selected datasets for two tasks\u2013Question Answering \n(Non-reasoning & Reasoning) and language modeling. \n\nEnhancing RALMs with Consistency Learning Comp \n7 \n \nWikiText-103[4]: is extracted from Wikipedia articles, is designed for language model \ntraining and evaluation, focusing on long-term dependencies. It contains over 100 million \ntokens, with 101,425,671 tokens in the training set, 213,886 tokens in the validation set, \nand 241,211 tokens in the test set. \nNatural Question[5]: a large-scale dataset for evaluating the natural language \ngeneration ability of models. It consists of a training set with 307,373 annotated question-\nanswer pairs and a validation set and test set, each containing 7,830 and 7,840 5-way \nannotated data pairs, respectively. \nHotpotQA[6]: aims to require models to perform question answering by integrating \nmulti-hop reasoning and information from multiple documents. It includes a large number \nof questions, relevant documents, and correct answers, with approximately 90,000 training \nexamples and 7,400 validation examples. \nFollowing the metrics in work[7], we choose model output\u2019s perplexity(PPL) to evaluate \nthe performance of model\u2019s language modeling ability, and Exact Match(EM) and \nToken-level F1 to assess model\u2019s performance on QA task. \n3.2 \nHyperparameters and Other Settings \nThis study utilizes the NLTK package for sentence segmentation and filters out data pairs \nlacking negative or semi-positive examples. Document retrieval is performed using \nBM25[13]. For the distillation module, we employ the GPT-3.5-turbo[9] API with a \ntemperature of 0.7 and Top p of 1. In the language modeling task, three prompts are used \n(as shown in the WikiText-103 data column), with the lowest perplexity result selected as \nthe target answer. Approximately 2% of the training data (36,000 examples) are randomly \nsampled to generate three summaries per example. \nThe contrastive selector initializes \ud835\udc52\ud835\udc5b\ud835\udf03 with Contriever[4], fine-tuned on the MS \nMarco[10] dataset. With 110M parameters, it is optimized using Adam with a learning \nrate of 1e-5, 2000 warmup steps, and trained over 5 epochs with a batch size of 64. \nEvaluation results are selected based on validation set performance. In the language \nmodeling task, GPT2 and GPT2-XL[9] are employed as the lightweight models, with \nGPT2-XL transferred directly from GPT2. For the QA task, LLaMA2 (13B)[11] serves as \nthe student model, while GPT-3.5-turbo is the teacher model. The lightweight model uses \nT5-large[12] with 770M parameters for fine-tuning, following a similar Adam \noptimization strategy over 6 epochs. The first three epochs are fully supervised, followed \nby three epochs incorporating consistency learning, with a batch size of 32. BGE-en-large \nis used as the sentence encoder Enc. \nFor data augmentation, two main strategies are adopted: token-level augmentation \n(synonym replacement and random token insertion/deletion) and sentence-level \nparaphrasing[14]. Due to computational constraints, 30% of sentences from retrieved \ndocuments are randomly sampled for augmentation. Different augmentation types are \nemployed for downstream fine-tuning and up- stream/midstream modules. \n\n8 \nC.Xu et al. \n \n3.3 \nBaselines \nFor the language modeling task, there are two heuristic baseline models at the token and \nphrase levels: \n\u2013 Bag-of-Words (BoW):Converting the retrieved documents into an ordered list of words and \nconcatenates them; \n\u2013 Named Entity (NE):Extracting an ordered list of named entities from the retrieved \ndocuments and concatenates them. \n\u2013 BM25: We select it as one representation of classic retrieval algorithms. \n\u2013 Contriever: Our initialization dense retrieve model, fine-tuned on MS MARCO. \n\u2013 Random: We random select a sentence from retrieved documents. \n\u2013 RECOMP[7](extractive):The \nSOTA \npost-retrieve \nRAG \nmodel \nby \nextractively \nsummarizing the retrieved documents. \n\u2013 Upperbound: Upperbound performance is obtained by traversing all the sentences in the \nretrieved document w.r.t input query X to achieve the best evaluation score. \nFor QA task, we adopt the following baselines: \n\u2013 GPT-turbo-3.5:We directly get the response by inputting query X. \n\u2013 T5:We use the off-the-shelf T5-large version. \n\u2013 RECOMP(abstractive):The \nSOTA \npost-retrieve \nRAG \nmodel \nby \nabstractively \nsummarizing the retrieved documents. \nFor the dynamic sampling selector in the language modeling task, we use BM25 and \nContriever as baseline models, which rank sentences based on their similarity to the input \nx. For the question answering task, we report using BM25 and Contriever fine-tuned on \nthe MS MARCO dataset as comparative baseline models. \n3.4 \nResults and Analysis \nIn the language modeling task, Table 1 presents the performance of baseline models \ncompared to our model. First, GPT2 and GPT2-XL perform worse without retrieval than \nwith any retrieval-based input. Second, concatenation based on named entities (NE) \nperforms worse than the bag-of-words (BoW) model, and both are inferior to directly \ninputting retrieved documents. This may be because BoW and NE provide less effective \ninformation. \nNotably, when the selector module is active (using only the selector\u2019s retrieved results \nas a summary), there is a significant difference in average token length between the upper \nbound scheme and generated methods. Our method shows a smaller gap in average token \nlength between the two modules, indicating better compression of original retrieved \ndocuments and more refined output, improving training and inference efficiency. \nCompared to SOTA models like GPT-3.5 and RECOMP, our proposed modules show \nimproved output length and model perplexity. For instance, the consistency learning \nmodule outperforms using the entire document for retrieval-augmented generation, \n\nEnhancing RALMs with Consistency Learning Comp \n9 \n \nindicating protection from noise in retrieved documents. In the question-answering task, \nas shown in Table 2, all retrieval-augmented methods improve EM and F1 metrics \ncompared to non-retrieval methods, consistent with previous studies. Unlike the language \nmodeling task, appending five documents as retrieval enhancement yields more \nsignificant improvements than a single document, indicating that more effective context \nenhances model performance. Across all datasets, the upper bound performance of the \nselector model surpasses the generation model. The upper bound performance is \ncalculated by choosing the best from N candidate sentences for the selector model, while \nthe generation model selects the optimal result from the teacher and student large \nlanguage model outputs. \nTable 1: Performance of automatic sampling contrastive model \nGPT2(117M) \ntokens \nPPL \nGPT2-XL(1.5B) \ntokens \nPPL \nZero-shot \n0 \n37.84 \n0 \n19.89\nUpperbound \n32 \n30.61 \n32 \n16.69\nPrepend Document(TOP1) \n147 \n32.99 \n147 \n18.11\nPrepend Document(TOP5) \n521 \n35.89 \n521 \n19.65\nBoW \n67 \n36.27 \n66 \n18.99\nNE \n33 \n37.37 \n35 \n19.75\nExtractive models \nBM25 \n33 \n36.87 \n33 \n19.23\nContriever \n34 \n35.78 \n34 \n19.09\nRandom \n27 \n37.01 \n27 \n19.63\nRECOMP(extractive) \n31 \n33.67 \n31 \n18.19\nOurs(w/o distill module) \n30 \n33.12 \n30 \n18.01\nGenerated models \nGPT-3.5 \n35 \n34.98 \n35 \n18.82\nT5 \n16 \n37.92 \n16 \n19.98\nRECOMP(abstractive) \n16 \n33.68 \n16 \n18.19\nOurs(w/o selector) \n15 \n33.11 \n30 \n17.99\nOurs \n17 \n32.98 \n31 \n17.87\nThe trained summary retrieval-augmented generation modules, both the selector and \ndistillation modules, show performance improvements. On the NQ dataset, the selector \nmodule achieves a 5% compression rate while losing 2 EM points compared to appending \nthe complete document. On the HotpotQA dataset, which requires multi-step document \nunderstanding, the selector module achieves an 11% compression rate while losing 2.4 EM \npoints. Further research is needed for summarization in complex tasks like HotpotQA, as \nlarge language models, though competitive in single-document summarization, struggle \nwith synthesizing information from multiple documents. \nWe use the number of tokens in the generated summary and inference speed, measured by \nGPU time, as indicators of model efficiency. Table 3 reports the GPU time for LLaMA2 \n\n10 \nC.Xu et al. \n \n(13B) on the NQ dataset, run on six RTX 4090 GPUs. For generating retrieval-augmented \nsummaries, RECOMP and our method were run on two RTX 4090 GPUs, with the \nconsistency module using T5-large and the dynamic sampling module using Contriver. Our \nmethod significantly improves efficiency compared to appending the top-5 documents, \neven when accounting for summary generation time. The dynamic sampling module is \nparticularly efficient. Inference speed is influenced more by model parameter size and the \nbenchmark model than by token count. For example, the consistency learning module \n(770M parameters) incurs more latency than the dynamic sampling module (110M \nparameters). Both of our modules demonstrate significant improvements in inference \nefficiency compared to RECOMP, with a 9% to 16% improvement rate. \nTable 2: Performance of Seletor and Distillation Module on QA task. \nNQ \navg # token \nF1 \nEM \nHotpotQA \navg # token \nF1 \nEM \nZero-shot \n0 \n29.67 22.05 \n0 \n26.43 17.95 \nPrepend Document(TOP1) \nPrepend Document(TOP5) \n135 \n465 \n31.23 23.72 \n36.28 28.23 \n141 \n689 \n40.66 28.87 \n43.34 32.31 \nBoW \nNE \n459 \n344 \n36.12 27.89 \n30.88 23.13 \n264 \n165 \n35.76 25.21 \n31.54 22.12 \nExtractive models \nRandom \n33 \n30.78 23.10 \n62 \n29.52 20.87 \nBM25 \n37 \n33.43 25.45 \n75 \n37.82 26.72 \nContriever \n37 \n31.54 29.81 \n79 \n39.12 28.12 \nRECOMP(extractive) \n38 \n43.92 36.41 \n76 \n39.23 28.13 \nOurs(w/o distill module) \n36 \n44.74   37.12 \n72 \n39.91 28.83 \nGenerated models \nGPT-turbo-3.5 \n57 \n46.14 36.89 \n110 \n39.89 30.12 \nT5 \n11 \n34.42 25.71 \n8 \n33.01 23.01 \nRECOMP(abstractive) \n37 \n45.16 36.92 \n66 \n37.51 28.04 \nOurs(w/o selector) \n30 \n46.33    37.76 \n59 \n38.02    28.79 \nOurs \n31 \n46.53    37.93 \n62 \n38.39    28.95 \nTable 3: The inference efficiency under different circumstances \nModel \ntoken Generated Time Inference Time     Total Time \nNo context \nAppend TOP5 retrieved docs \n0 \n465 \n0 \n0 \n3988s \n9891s \n3988s \n9891s \nRECOMP \n37 \n898s \n4561s \n5459s \nOurs(w/o distill) \n30 \n783s \n4319s \n5002s \nOurs(w/o selector) \n30 \n101s \n4117s \n4218s \n4 \nConclusion \n\nEnhancing RALMs with Consistency Learning Comp \n11 \n \nIn conclusion, this paper proposes a novel two-stage consistency learning frame- work, \nC2LPRCom, to enhance the robustness of retrieval-augmented language models (RALMs) \nagainst noise in retrieved documents while maintaining consistency with the original \nsource information. The framework incorporates a fine-grained extractive compressor \nbased on contrastive learning and a two-stage consistency learning approach for constructing \na lightweight post-retrieval information compressor. Extensive experiments on question-\nanswering and language modeling tasks demonstrate that the proposed method \nsignificantly improves the precision and inference efficiency of RAG frameworks, \noutperforming state-of-the-art baselines. The synergistic combination of contrastive and \nconsistency learning paradigms within the retrieval-augmented generation framework \nshows great potential for enhancing the performance of RALMs. Future research could \nexplore further improvements to summary generation models for complex tasks and \ninvestigate the synthesis of information from multiple documents. \nAcknowledgements. This work was supported by the National Key R&D Program of \nChina(2022YFC3301900;2022YFC3301901), National Natural Science Foundation of \nChina(62376188,62272340,62276187,62376192). \nReferences \n1. Cai H, Chen H, Song Y, et al. Data Manipulation: Towards Effective In- stance Learning \nfor Neural Dialogue Generation via Learning to Augment and Reweight[C]//Proceedings of the \n58th Annual Meeting of the Association for Computational Linguistics. 2020: 6334-6343. \n2. Gautier I, Mathilde C, Lucas H, et al. Unsupervised dense information retrieval with contrastive \nlearning[J]. Transactions on Machine Learning Research, 2022. \n3. Khattab O, Zaharia M. Colbert: Efficient and effective passage search via con textualized late \ninteraction over bert [C]. In Proceedings of the 43rd International ACMSIGIR conference on \nresearch and development in Information Retrieval, 2020: 3948. \n4. Karpukhin V, Oguz B, Min S, et al. Dense Passage Retrieval for Open-Domain Question \nAnswering [C/OL] // Webber B, Cohn T, He Y, et al. In Proceedings of the 2020 Conference on \nEmpirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, \n2020, 2020: 67696781. \n5. Kwiatkowski T, Palomaki J, Redfield O, et al. Natural questions: a benchmark for question \nanswering research [J]. Transactions of the Association for Computational Linguistics, 2019, 7: \n453466. \n6. Yang Z, Qi P, Zhang S, et al. HotpotQA: A Dataset for Diverse, Explainable Multi- hop Question \nAnswering [C/OL] // Riloff E, Chiang D, Hockenmaier J, et al. In Proceedings of the 2018 \nConference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October \n31- November 4, 2018, 2018: 23692380. \n7. XuF,ShiW,ChoiE. RECOMP:Improving Retrieval-Augmented LMs with Context Compression \nand Selective Augmentation [C/OL]. In The Twelfth International Conference on Learning \nRepresentations, 2024. \n8. BAAI. Flagembedding. https://github.com/FlagOpen/FlagEmbedding, 2023. \n9. Tom B. Brown and Benjamin Mann et al. Language models are few-shot learners, 2020. \n10. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and \nLi Deng. MS MARCO: A human generated machine reading com- prehension dataset. CoRR, \nabs/1611.09268, 2016. \n\n12 \nC.Xu et al. \n \n11. Raffel C, Shazeer N, Roberts A, et al. Exploring the limits of transfer learning with a unified text-to-\ntext transformer [J]. Journal of machine learning research, 2020, 21 (140): 167 \n12. Touvron H, Martin L, Stone K, et al. Llama 2: Open foundation and fine-tuned chat models [J]. \narXiv preprint arXiv:2307.09288, 2023. \n13. Robertson S, Zaragoza H, et al. The probabilistic relevance framework: BM25 and beyond [J]. \nFoundations and Trends\u0151 in Information Retrieval, 2009, 3 (4):333389. \nCai H, Chen H, Song Y, et al. Data Manipulation: Towards Effective Instance Learning for \nNeural Dialogue Generation via Learning to Augment and Reweight [C]. In Proceedings of the \n58th Annual Meeting of the Association for Computational Linguistics, 2020: 63346343. \n1.A  Appendix \n \nAlgorithm 2 Contrastive Retriever Training Process \nInput: encoder en\u03b8, LM L, Input Data{xi, Si, yi}T , User Input Xi,Si = {sj}n Ground \nTruth yi, Threshold K, \u03b4. \nOutput: contrastive fine-tuned retriever en\u03b8 \nNe, T \u2192 \u2205 \nfor i = 1 to T do \nsc \u2192 \u2205 \ntopi \u2192 argmaxsj\u2208{Si}SCORE(L, yi, [sj; xi]) \nfor j = 1 to n do \npi \u2192< en\u03b8(xi), en\u03b8(sj) >; sc \u2192 sc\u222a {pi, sj} \nend for \nsc \u2192 DESCENDINGSort(sc)[: K]according topi \nfor m = 1 to K do \n\u2206pi = pm \u2212 p1 \nif \u2206pi < \u03b4 then \nwm \u2192 1/\u2206pi \nNe \u2192 Ne \u222a {xi, topi, smwm} \nelse \nT \u2192 T \u222a {xi, topi, sm} \nend if end for \nend for \nen\u03b8 = Finetune(T, en\u03b8) \nreturn en\u03b8 \n"}