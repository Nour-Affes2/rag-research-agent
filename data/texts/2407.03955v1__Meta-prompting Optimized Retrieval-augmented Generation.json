{"metadata": {"pdf_filename": "2407.03955v1__Meta-prompting Optimized Retrieval-augmented Generation.pdf", "source": "arXiv"}, "text": "arXiv:2407.03955v1  [cs.CL]  4 Jul 2024\nMeta-prompting Optimized Retrieval-augmented\nGeneration\nJo\u00e3o Rodrigues and Ant\u00f3nio Branco\nUniversity of Lisbon\nNLX\u2014Natural Language and Speech Group, Dept of Informatics\nFaculdade de Ci\u00eancias (FCUL), Campo Grande, 1749-016 Lisboa, Portugal\n{jarodrigues,antonio.branco}@fc.ul.pt\nAbstract. Retrieval-augmented generation resorts to content retrieved\nfrom external sources in order to leverage the performance of large lan-\nguage models in downstream tasks. The excessive volume of retrieved\ncontent, the possible dispersion of its parts, or their out of focus range\nmay happen nevertheless to eventually have a detrimental rather than an\nincremental e\ufb00ect. To mitigate this issue and improve retrieval-augmented\ngeneration, we propose a method to re\ufb01ne the retrieved content before it\nis included in the prompt by resorting to meta-prompting optimization.\nPut to empirical test with the demanding multi-hop question answer-\ning task from the StrategyQA dataset, the evaluation results indicate\nthat this method outperforms a similar retrieval-augmented system but\nwithout this method by over 30 %.\nKeywords: RAG \u00b7 Retrieval-Augmented Generation \u00b7 Prompt Opti-\nmization \u00b7 Large Language Models \u00b7 Meta-prompting \u00b7 Multi-hop QA\n1\nIntroduction\nPre-trained Large Language Models (LLMs) [32,22] are known for their hallu-\ncinations [12] and for their further limitations regarding truthfulness [17]. To\ntackle these issues, remediation techniques have been explored such as, for in-\nstance, \ufb01ne-tuning [10], prompt-engineering [18] or Retrieval-Augmented Gen-\neration [15], initiated by Houlsby et al. [9] among several others.\n1.1\nRetrieval-augmented generation\nFocusing on Retrieval-Augmented Generation (RAG), this approach seeks to\nenhance truthfulness and curb hallucinations by expanding the initial prompt,\nwhich contains the initial query, with additional content retrieved from sources\nthat are external to the LLM. Such additional content is obtained with the\nhelp of an auxiliary Retrieval Model where the retrieval model may be a simple\nJacquard model or a vector database that extracts relevant content from external\nsources and pass it on to a Large Language Model that generates an appropriate\nresponse given the original query and the extracted content. If this external\n\n2\nJo\u00e3o Rodrigues and Ant\u00f3nio Branco\ncontent is unstructured text, it may be of di\ufb00erent lengths, such as sentences,\nparagraphs or full documents, among others.\nBy feeding LLM\u2019s knowledge, and curbing its whim, with further knowledge\nfrom external sources, more accurate answers are likely to be provided.\nCompared with other techniques, such as \ufb01ne-tuning or prompt-engineering,\nRAG key advantage is the ease with which newer, up-to-date content is taken\nadvantage of, as this does not require the costly compute of re-training neural\nnetworks (as in \ufb01ne-tuning) or the costly human labour for the creation of further\nmanually designed prompts (as in prompt-engineering). To be sure, all these\ntechniques can nevertheless be mixed and function together.\n1.2\nPrompt optimization\nUsually, the pieces of content retrieved may be from heterogeneous sources and\nthey tend to lack a connecting thread. They may also be redundant or may be of\nvery high volume. These, among other aspects, may end up having a detrimental\ne\ufb00ect and eventually jeopardizing the generation task, rather than enhancing it.\nTo mitigate this problem, we present a method that consists of adding an\nintermediate step between the retrieval of the external content and the entering\nof the expanded prompt into the LLM to \ufb01nally obtain the response to the\ninitial query. Aiming at improving the performance of this generation-LLM, this\nintermediate step seeks to obtain a re\ufb01ned version of the external knowledge.\nThis re\ufb01nement is accomplished by means of an auxiliary transformation-\nLLM that is entered with a prompt containing the pieces of retrieved contents,\npreceded by an instruction with the request for the sought re\ufb01nement.\nFor example, if several pages of Wikipedia are retrieved as possible relevant\ncontent, the transformation-LLM processes this content and may generate a\nsummary or remove unnecessary information from that original content.\nTurning to this re\ufb01nement instruction, this is obtained by an automatic pro-\ncedure that is preliminary to running the RAG system made of transformation-\nand generation-LLMs, and it is undertaken by yet a third LLM.\nInspired in Yang et al. [35], in this procedure a meta-prompt is used as\ninput to this third, optimizer-LLM for this to iteratively generate new tentative\ninstructions, score them, and retain, in the meta-prompt itself, a list with the\ntop k ones that induce better performance for the RAG system. By the end\nof this optimization process, the best scoring instruction in this list is the one\nretained to be used in the re\ufb01nement step of the retrieved contents with the\ntransformation-LLM.\nThis meta-prompt contains a meta-instruction and a list of tentative instruc-\ntions that is aimed at being updated during this process with new instructions\nthat induce better RAG performance. After a new tentative instruction is gen-\nerated, its contribution to approximate the gold output to the initial query is\nscored, and the list of tentative instructions in the meta-prompt is possibly up-\ndated so that it retains the top-performing ones so far. This is iterated, and an\noptimization trajectory is hence accomplished to eventually \ufb01nd the new re\ufb01ne-\nment instruction that maximizes the success of the RAG system.\n\nMeta-prompting Optimized Retrieval-augmented Generation\n3\n***\nIn this paper, we propose a method for RAG to be enhanced with the re-\n\ufb01nement of the retrieved content, a re\ufb01nement that is optimized by resorting to\niterative meta-prompting. This is a novel method that can be combined with\nprevious approaches aimed at enhancing RAG.\nWe report on the experiments performed to put this method to the test. This\napproach is extrinsically evaluated by being embedded in a demanding question-\nanswering downstream task. Its performance demonstrates that it is an e\ufb00ective\nmethod to enhance RAG by improving by 30% the performance of a baseline\nRAG without this method, and that it can be combined with other previous\nstate of the art methods for RAG enhancement proposed in the literature.\nThe remainder of this paper is structured as follows: Section 2 discusses\nrelated work; Section 3 describes the method proposed in this study; Section 4\nreports on the models and dataset resorted to; Section 5 presents the experiments\nundertaken and their evaluation, and discusses the results obtained; Section 6\naddresses future research paths; and \ufb01nally, Section 7 closes this paper with\nconcluding remarks.\n2\nRelated work\nPrompt optimization has gained traction as an e\ufb00ective mechanism for enhancing\nLLMs in several downstream tasks [1][14].\nThe earliest approaches in prompt optimization sought to directly optimize\nthe prompt embedding space, such as pre\ufb01x-tuning [16] or OptiPrompt [37].\nThese aimed at optimizing a sequence of continuous task-speci\ufb01c vectors applied\nto the prompt to leverage downstream tasks.\nMore recent studies have introduced further techniques to enhance prompts,\nsuch as chain-of-thought [34] and tree-of-thoughts [36]. The former involves ex-\ntending prompts with a few manually written chain of thought demonstrations\nas examples, which results in improved performance across various tasks, includ-\ning arithmetic, commonsense and symbolic reasoning. The latter builds upon the\nchain-of-thought by considering multiple reasoning paths, self-evaluating choices,\nand by making global decisions by looking ahead or backtracking when necessary.\nOther methods for optimizing prompts include searching through a pool of\nprompt candidates generated by an LLM, employing principled planning algo-\nrithms based on Monte Carlo tree search [33], or applying iterative local edit\noperations at a syntactic phrase-level split within the prompts [21].\nFurther proposals encompass EvoPrompt [7], which uses evolutionary opera-\ntors over a prompt population for optimization, while Sabbatella et al. employs\nBayesian Optimization within a prompt search space [26], reinforcement learning\nto rewrite prompts [13] or a prompt optimization that integrates human-design\nfeedback rules to suggest improvements automatically [5].\nRecently, Yang et al. [35] introduced OPRO, leveraging LLMs as optimiz-\ners through meta-prompts, which are natural language descriptions that guide\n\n4\nJo\u00e3o Rodrigues and Ant\u00f3nio Branco\nprompt optimization. It was applied to optimize prompts by retrieving and re-\nranking top-K relevant instructions with respect to an initial instruction, and by\nappending them to the global task description.\nIn contrast, to enhance RAG, we propose a method to optimize the prompt\nthat di\ufb00ers from the previous proposals in the literature.\nA prompt for RAG includes a query and the content retrieved from external\nsources on the basis of that query. It may contain also an instruction about\nhow to handle the query or how the retrieved content should be used by the\ngeneration-LLM to answer it. Related work for RAG enhanced with prompt\noptimization has concentrated on optimizing the instruction and/or the query.\nDi\ufb00erently from previous approaches, our method focuses instead on optimizing\nthe version of the retrieved content that is included in the prompt entered into the\ngeneration-LLM. Hence, rather than being an approach alternative to previous\nones, it is a new one that is complementary to them and may be combined.\n3\nMethod\nThe objective of our method is to enhance the RAG performance of a generation-\nLLM by means of the improvement of its input prompt, which is made of a query\nintroduced by the user and of pieces of content retrieved from external sources on\nthe basis of that query. Before it is entered into the generation-LLM, this prompt\nis improved by means of a re\ufb01nement of the retrieved content, performed by a\ntransformation-LLM.\nI have some prompts along with their corresponding scores. The prompts are arranged\nin ascending order based on their scores, where higher scores indicate better quality.\nTogether with relevant information extracted from a database, these prompts are given as\ninput to a large language model in order to optimize the provided relevant information.\nSeveral techniques may help the optimization, such as re-ranking paragraphs, cleaning,\n\ufb01ltering and summarization. Write your new prompt taking into account the previous\nones and aiming to achieve a higher score.\nprompt:\nSummarize the main idea of the previous text.\nscore:\n3.0\nprompt:\nSummarize the main points in 30 words or less.\nscore:\n3.0\nTable 1. Meta-prompt - An example of a meta-prompt: in black, the top paragraph\nwith the meta-instruction actually used in the experiments; below, in green, the list of\ntop performing instructions so far, and the respective scores.\nAnd before a \ufb01rst query is accepted to put the RAG system to use, the prompt\nto be used with the transformation-LLM for re\ufb01nement purposes is optimized.\n\nMeta-prompting Optimized Retrieval-augmented Generation\n5\nThis prompt includes a re\ufb01nement instruction and the pieces of retrieved content\nto be re\ufb01ned. It is optimized by means of the optimization of this instruction\nthrough iterative meta-prompting.\nThis meta-prompting optimization is undertaken by an optimizer-LLM that\nis entered with a (meta-)prompt that includes a (meta-)instruction and a list\nof tentative re\ufb01nement instructions and respective performance scores. These\nscores are obtained by running the RAG with the tentative re\ufb01nement instruction\nthrough a sample of training examples and evaluating the output against the\nrespective gold responses.\nFocusing on the optimization phase, a meta-prompt is used that contains\nboth the description of the optimization problem and the history with previous\nbest solutions for the instruction. Such meta-prompt is iteratively entered into\nthe optimizer-LLM, and at each iteration that history is possibly updated with\ngenerated instructions if these support better performance for the task at stake\nin the generation phase. The instruction selected out of this optimization process\nis the best scoring one in the history obtained as this iteration is over.\nAn example of a meta-prompt is in Table 1, and a detailed description of\nthis optimization via iterative meta-prompting is presented in Algorithm 1.\nAlgorithm 1 Optimization with meta-prompting\n1: Input: Dataset D with n examples, each containing a query q, retrieved contents\nc and the answer a; meta-prompt metaP with the description of the optimization\ntask and with a list of instructions and respective scores\n2: Output: List of scored instructions and the best scoring instruction\n3: while optimizing prompt do\n4:\nEnter meta-prompt to optimizer-LLM\n5:\nGenerate new instructions I\n6:\nSelect a random subset E of examples e from D\n7:\nfor each instruction Ij do\n8:\nfor each example ek in E do\n9:\nAssemble prompt T ransP from Ij and contents ck\n10:\nEnter T ransP to transformation-LLM\n11:\nGenerate transformed contents tc\n12:\nAssemble prompt T askP from query qk and tc\n13:\nInput T askP to generation-LLM\n14:\nGenerate answer and evaluate it against gold ak\n15:\nend for\n16:\nCompute Ijscore\n17:\nend for\n18:\nUpdate metaP by replacing its worst scoring instruction by Ij and Ijscore if\nthis is better scored\n19: end while\n\n6\nJo\u00e3o Rodrigues and Ant\u00f3nio Branco\n4\nDataset and models\nTo empirically assess the performance gains of the proposed method, it was\nintegrated into an RAG for question-answering whose performance provides for\nits extrinsic evaluation.\n4.1\nTask and dataset\nMulti-hop question answering requires taking into account disparate pieces of\ncontent to get at the answer for a query, which constitutes a most demanding\nscenario for the task of question answering.\nWe resorted to a most complex benchmark for multi-hop question-answering\navailable in the literature, the StrategyQA dataset [6,19,11,8], which contains\n2,780 queries, each associated with related content made of paragraphs and the\nrespective yes or no answer. Based on Wikipedia content, this dataset covers\na range of diverse topics and the task consists in, given a query, to provide an\naccurate answer to it together with the passages retrieved from Wikipedia with\nthe most relevant content to get at that answer \u2014 Table 2 displays an example.\nQuery\nCould $1 for each 2009 eclipse buy a copy of TIME magazine in 2020?\nContent #1\nIt set out to tell the news through people, and for many decades through\nthe late 1960s, the magazine\u2019s cover depicted a single person. [...] Raymond\nFielding also noted that Larsen was \"originally circulation manager and\nthen general manager of Time, later publisher of Life, for many years\npresident of Time Inc., and in the long history of the corporation the\nmost in\ufb02uential and important \ufb01gure after Luce\"\nContent #2\nTotal eclipses are rare because the timing of the new moon within the\neclipse season needs to be more exact for an alignment between the ob-\nserver (on Earth) and the centers of the Sun and Moon. [...] because to-\ntality exists only along a narrow path on the Earth\u2019s surface traced by the\nMoon\u2019s full shadow or umbra.\nContent #3\nAt least two lunar eclipses and as many as \ufb01ve occur every year, although\ntotal lunar eclipses are signi\ufb01cantly less common. If the date and time of\nan eclipse is known, the occurrences of upcoming eclipses are predictable\nusing an eclipse cycle, like the saros.\nAnswer\nYes\nTable 2. StrategyQA - An example from the StrategyQA, with a query, three of the\nmost relevant pieces of content, and the respective answer.\nTo provide for the evaluation of the proposed method, and isolate the ac-\ncrued performance induced by it, thus disregarding possible \ufb02uctuation or loss\nof performance due to the retrieval process, only the gold pieces of content from\na test set should be taken into account. Since the answers are not provided in the\noriginal test set of StrategyQA, a new test set for the present evaluation exercise\nhad to be built. Accordingly, we divided the original training set into two parts:\na new test set with 490 of the original training examples, which matches the size\nof the original test set, and a new training set containing a subset with 1800 such\nexamples. The resulting train and test sets have an average query length of 9.6\nwords, and 2.33 contents (paragraphs) per query and are almost balanced. The\ntraining set contains 834 yes answers and 966 no answers (46.32 % / 53.68 %).\nThe test set contains 237 yes answers and 253 no answers (48.40 % / 51.60 %).\n\nMeta-prompting Optimized Retrieval-augmented Generation\n7\n4.2\nModels\nTwo Transformer-based language models with 70 Billion parameters were used,\na pre-trained Llama-2-70b and an instruct model Llama-2-70b-chat \ufb01ne-tuned\nfor dialogue use [31]. These models were trained and \ufb01ne-tuned with a context\nlength of 4k tokens over 2 Trillion tokens on a mix of publicly available data.\nIn general, the default Llama2 model hyper-parameters were applied and\nno hyper-parameters search bound was performed. All language models use a\ntemperature value of 1.0, a maximum of 64 generation tokens for the new in-\nstructions, a maximum of 128 generation tokens for the re\ufb01ned content, and a\nmaximum of 64 generation tokens for the response to the task. The optimization\nrun was performed for two days on two NVIDIA A100 40GB GPUs.\nAll software and versioning along with hyper-parameters are fully described\nin the source code of these experiments.1\n4.3\nEvaluation procedure and metrics\nBased on empirical experimentation, we arrived at a meta-prompt, presented\nin Table 1, that indicates the aim of the optimization problem and includes a\nstarting example instruction.2\nThe instruction optimization was iterated over 100 steps. At each step, 3\ninstructions were generated, each such instruction was evaluated on a random\nsample of 6 training examples, and the meta-prompt was eventually updated to\ncontaining the 8 top scoring queries so far. When this iteration was concluded,\nthe best scoring instruction was retained as the optimized instruction.\nWe compare against the same generation-LLM using test queries and associ-\nated pieces of content, that is without the later being re\ufb01ned by the transformation-\nLLM under the instruction that was optimized by the optimizer-LLM.\nFor the StrategyQA task, a Boolean answer is expected. Accuracy is thus the\nmetric used for evaluating the match between the answer output by the system\nand the gold answer in the data set. Accuracy score is given by the proportion\nof correct answers, and a generated response was counted as correct if the gold\nanswer was found in the exact beginning of it. The response underwent minimal\nnormalization, with just lowercasing.\nAs for the instrumental process of instruction optimization, it is worth recall-\ning that the evaluation is performed over sample examples from the training set.\nFor a tentatively generated instruction, a correct answer to it counted 1 point; an\nincorrect answer, in turn, counted 0.5 points if it was nevertheless in a Boolean\nformat, or counted 0 points otherwise. The maximum possible score was thus 6\npoints, given each tentative instruction was evaluated against 6 sampled queries\nas indicated above.\n1 For\nthe\nsake\nof\nreproducibility,\ndata\nand\ncode\nare\navailable\nat\nhttps://github.com/nlx-group/rag-meta-prompt\n2 The starting instruction is \"Clean and organize the previous text.\"\n\n8\nJo\u00e3o Rodrigues and Ant\u00f3nio Branco\n5\nResults and discussion\nIn this section, we report on the evaluation exercise undertaken to assess the\nproposed method and discuss its results, summarized in Table 3\n5.1\nExperiments\nAll in all, six experiments were undertaken, two resorting to the model Llama-2-\n70b, developed with a pre-training regime only, and four resorting to the model\nLlama-2-70b-chat, which resulted from further \ufb01ne-tuning it with dialogue data.\nModel\nMethod\nAccuracy\nLlama-2-70b\nquery\n17 (3.46 %)\nLlama-2-70b\nquery+contents\n33 (6.73 %)\nLlama-2-70b-chat\nquery\n81 (16.53 %)\nLlama-2-70b-chat\nquery+contents (plain RAG)\n128 (26.12 %)\nLlama-2-70b-chat\nre\ufb01ned query+contents (ours)\n170 (34.69 %)\nLlama-2-70b-chat\nref. query+contents no iteration\n127 (25.92 %)\nTable 3. Evaluation - From the total 490 test set examples, the number of correct\nanswers is presented and the respective accuracy.\nBoth these models were used in two evaluation scenarios. In one of these\nscenarios\u2014noted as query in Table 3\u2014, the response to the query entered was\nprovided by the LLM alone, with no further content from external sources being\nentered. In the other scenario, in turn,\u2014noted as query+contents\u2014, further\ncontent from external sources was included in the prompt as well. The perfor-\nmance scores for these two scenarios with the two models are displayed in the\ntop four rows 3.\nExternal, non-parametric content improves generation As expected,\nand in line with results in the literature, the retrieval-augmented generation\n(26.12%) outperforms the plain generation based solely on the query (16.53%).\nFine-tuning improves generation Also as expected, and by a very large\nmargin, better performance scores were obtained with Llama-2-70b-chat, which\nhad been \ufb01ne-tuned on dialogue tasks, namely 16.53% against 3.46%, with the\nquery only, and 26.12% against 6.73%, with the query and external content.\nRetrieved content re\ufb01nement via meta-prompting optimization im-\nproves RAG \u2014 the proposed method is e\ufb00ective The model Llama-2-70b-\nchat was thus retained and two further evaluation scenarios were considered.\nA scenario with the application of the proposed method\u2014noted as re\ufb01ned\nquery+contents\u2014, where the external content was re\ufb01ned with the help of an\ninstruction optimized with meta-prompting.\nThe performance scores indicate that, with 34.69% accuracy, our proposed\nmethod of enhanced RAG outperforms plain RAG, with 26.12%, thus contribut-\ning for a large improvement of over 8.5 percentage points, that represents here\nan improvement rate of almost 33%.\n\nMeta-prompting Optimized Retrieval-augmented Generation\n9\nRetrieved content re\ufb01nement via \"brute force\" optimization does\nnot improve RAG A sixth scenario\u2014noted as re\ufb01ned query+contents no\niter\u2014was also considered. Here the external content was re\ufb01ned as in the pro-\nposed method, but the instruction was re\ufb01ned under an alternative way that\ndispensed with iterative meta-prompting.\nAll in all, 300 tentative instructions are generated during all optimization\nsteps \u2014recall that we had 100 iteration steps with 3 tentative instructions gen-\nerated per step with meta-prompting optimization. To dispense with this iter-\native meta-prompting, the same number of 300 new tentative instructions were\ngenerated at once, in a \"brute force\" fashion. By the end of this process, all ten-\ntative instructions were scored with the same scoring function as in the proposed\nmethod, and the top instruction was evaluated on the test set.\nThis \"brute force\" optimization approach, scoring 25.92%, is outperformed\nnot only by the proposed method of meta-prompting optimization, with 34.69%,\nbut even also by the baseline, plain RAG, with 26.12%.\nStatistical signi\ufb01cance To assess the statistical signi\ufb01cance of the improve-\nments by our method, we employed the unpaired t-test.3 We evaluated the base-\nline system with three seeds and did the same for the meta-prompting optimized\nsystem. Both samples are independent and one may assume the samples are nor-\nmally distributed. Applying the unpaired t-test, a two-tailed P value equal to\n0.0004 was obtained, which is considered statistically signi\ufb01cant.\n5.2\nExamining the tentative instructions\nTable 4 presents the top generated prompts. The best scoring prompt (last row),\nwith 5.5 (out of a maximum of 6), was obtained at iteration step 46 (out of 300\nsteps in total), and a good prompt (\ufb01rst row) can be obtained with only 28 steps.\nWhen reading the best prompt (last row), one realizes that it aims to improve\nthe task through the summarization of the retrieved contents, considering their\nbroader context, and identifying the main theme or message. It appears thus\nlike a reasonable prompt a human might have thoughtfully arrived at if aiming\nat improving the performance of the task.\nIt is reasonable to assume that the meta-prompt iteration in subsequent steps\nused this query and its score in its search for further tentative instructions, with\nthe generated instructions in three subsequent steps (58, 65 and 72) being some\nderivation of it (second, fourth and \ufb01fth rows).\nWhen taking a look at the entire set of generated queries, a high \ufb02uctuation\nof the evaluation scores can be observed along the iteration steps. This is likely\ndue to some interim, generated instructions happening to perform poorly.\n5.3\nExamining the responses\nTo gain insight about where our method outperformed the plain RAG baseline,\nwe examined the \ufb01rst 10 instances where our method correctly provided the an-\n3 The unpaired t-test evaluates if there exists a statistically signi\ufb01cant distinction\nbetween the means of two independent samples by comparing them.\n\n10\nJo\u00e3o Rodrigues and Ant\u00f3nio Branco\nGenerated instruction\nScore\nIter. step\nSummarize the previous text in 2-3 sentences, while also considering\nthe broader context, the author\u2019s intent, the potential implications of\nthe information, and also identify the main theme or message and its\nsigni\ufb01cance, and also analyze the impact of the information on the\nreader.\n5\n65\nSummarize the previous text in 1 sentence, while also considering the\nbroader context, the author\u2019s intent, the potential implications of the\ninformation, and also identify the main theme or message and its sig-\nni\ufb01cance, and also analyze the impact of the information on the reader,\nand also provide recommendations for further\n5\n72\nSummarize the previous text in 2-3 sentences, while also considering\nthe broader context, the author\u2019s intent and the potential implications\nof the information, and also identify the main theme or message.\n5.5\n46\nTable 4. Top meta-prompting optimized instructions scoring 5 or higher, with\nrespective scores and iteration steps at which they were obtained, ordered top to bot-\ntom, with the top-scoring, retained instruction in the last row.\nswer while the baseline failed. Among these, in six cases, the baseline provided\na verbose response and might have failed the exact-match evaluation criterion\nused.4 In the remaining four cases, the baseline either answered incorrectly, re-\nsponded with a query, or failed to provide an answer.\nConversely, we reviewed the \ufb01rst 10 instances where our method failed to\nprovide the correct answer while the baseline succeeded. We observed that our\nmethod exhibited a verbose response behavior in \ufb01ve cases that eventually ar-\nrived at the correct answer but failed the exact-match evaluation criterion. In\ntwo other cases, our method gave a verbose response without providing an an-\nswer, while in two remainder cases, it provided an incorrect response. Finally, in\none instance, our method did not provide any response.\nBoth methods seem thus to be similarly penalized by the evaluation criterion\nfor not providing straight answers when the correct answer may happen to be\nincluded in the verbose response.\n6\nFuture work\nWhile providing a method that e\ufb00ectively enhances RAG, our proposal paves\nthe way for future research, such as the exploration of optimal hyper-parameters,\nre\ufb01ning content retrieved without gold content, scaling up with larger models,\nexploring further evaluation functions, or tackling other downstream tasks. A\nsigni\ufb01cant number of hyper-parameters remain unexplored, which is an oppor-\ntunity to further enhance the e\ufb03cacy of this method.\nIt is worth noting that the evaluation with exact matching is a binary task,\nand achieving an exact match with a task demanding a more complex string\nmatch still needs to be studied, questioning the need for additional training, a\ndi\ufb00erent meta-prompt, or a di\ufb00erent approach.\n4 An example of a verbose response: [query] Was Superhero \ufb01ction invented in the digi-\ntal format? [response] The answer is no; superhero \ufb01ction did not originate in digital\nformat. Superheroes have their roots in pulp magazines, comic strips, and comic\nbooks, which were all print media formats before the advent of digital technology.\n\nMeta-prompting Optimized Retrieval-augmented Generation\n11\nIt will be interesting also to study the interaction of our proposed method\nwith the Portuguese language [20,2] with the existing family of LLMs [24,30,29]\nand multi-modal LLMs [27] as also with other tasks such as argument mining\n[25], exploring data spuriousness and others [4,28,3,23].\n7\nConclusion\nThis paper introduces a novel method that enhances RAG and that can be\ncombined with previous approaches for RAG enhancement. It consists in re\ufb01ning\nthe retrieved content included in the prompt entered into the generation model\nwith the help of a re\ufb01nement instruction that was obtained by means of meta-\nprompting optimization.\nIt reports also on the empirical assessment of this proposal by means of it\nbeing embedded in a most demanding multi-hop question answering task. The\nevaluation results indicate that it is highly e\ufb00ective in as much as it outperforms\nRAG without this method by over 30%.\nAcknowledgments. This research was partially supported by: PORTULAN CLARIN\n\u2014 Research Infrastructure for the Science and Technology of Language, funded by Lis-\nboa 2020, Alentejo 2020 and FCT (PINFRA/22117/2016); ACCELERAT.AI - Multi-\nlingual Intelligent Contact Centers, funded by IAPMEI (C625734525-00462629);\nReferences\n1. Aarohi Srivastava, A.R., et al.: Quantifying and extrapolating the capabilities of\nlanguage models. Transactions on Machine Learning Research (2023)\n2. Branco, A., Mendes, A., Quaresma, P., Gomes, L., Silva, J., Teixeira, A.: Infras-\ntructure for the science and technology of language PORTULAN CLARIN. In:\nRehm, G., Bontcheva, K., Choukri, K., Haji\u010d, J., Piperidis, S., Vasil,jevs, A. (eds.)\nProceedings of the 1st International Workshop on Language Technology Platforms.\npp. 1\u20137. European Language Resources Association, Marseille, France (May 2020),\nhttps://aclanthology.org/2020.iwltp-1.1\n3. Branco, A., Rodrigues, J., Salawa, M., Branco, R., Saedi, C.: Comparative probing\nof lexical semantics theories for cognitive plausibility and technological usefulness.\nIn: Proceedings of the 28th International Conference on Computational Linguistics.\npp. 4004\u20134019 (2020)\n4. Branco, R., Branco, A., Ant\u00f3nio Rodrigues, J., Silva, J.R.: Shortcutted common-\nsense: Data spuriousness in deep learning of commonsense reasoning. In: Moens,\nM.F., Huang, X., Specia, L., Yih, S.W.t. (eds.) Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language Processing. pp. 1504\u20131521.\nAssociation for Computational Linguistics, Online and Punta Cana, Domini-\ncan Republic (Nov 2021). https://doi.org/10.18653/v1/2021.emnlp-main.113,\nhttps://aclanthology.org/2021.emnlp-main.113\n5. Chen, Y., Arkin, J., Hao, Y., Zhang, Y., Roy, N., Fan, C.: Prompt optimization in\nmulti-step tasks (promst): Integrating human feedback and preference alignment.\narXiv preprint arXiv:2402.08702 (2024)\n\n12\nJo\u00e3o Rodrigues and Ant\u00f3nio Branco\n6. Geva, M., Khashabi, D., Segal, E., Khot, T., Roth, D., Berant, J.: Did aristotle\nuse a laptop? a question answering benchmark with implicit reasoning strategies.\nTransactions of the Association for Computational Linguistics 9, 346\u2013361 (2021)\n7. Guo, Q., Wang, R., Guo, J., Li, B., Song, K., Tan, X., Liu, G., Bian, J., Yang, Y.:\nConnecting large language models with evolutionary algorithms yields powerful\nprompt optimizers. arXiv preprint arXiv:2309.08532 (2023)\n8. Ho, X., Duong Nguyen, A.K., Sugawara, S., Aizawa, A.: Constructing a multi-hop\nQA dataset for comprehensive evaluation of reasoning steps. In: Proceedings of\nthe 28th International Conference on Computational Linguistics. pp. 6609\u20136625.\nInternational Committee on Computational Linguistics (Dec 2020)\n9. Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Ges-\nmundo, A., Attariyan, M., Gelly, S.: Parameter-e\ufb03cient transfer learning for nlp.\nIn: International conference on machine learning. pp. 2790\u20132799. PMLR (2019)\n10. Howard, J., Ruder, S.: Universal language model \ufb01ne-tuning for text classi\ufb01cation.\nIn: Proceedings of the 56th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers). pp. 328\u2013339 (2018)\n11. Inoue, N., Stenetorp, P., Inui, K.: R4C: A benchmark for evaluating RC systems\nto get the right answer for the right reason. In: Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics. pp. 6740\u20136750 (Jul 2020)\n12. Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y.J., Madotto, A.,\nFung, P.: Survey of hallucination in natural language generation. ACM Computing\nSurveys 55(12), 1\u201338 (2023)\n13. Kong, W., Amba Hombaiah, S., Zhang, M., Mei, Q., Bendersky, M.: Prewrite:\nPrompt rewriting with reinforcement learning. arXiv e-prints pp. arXiv\u20132401\n(2024)\n14. Lester, B., Al-Rfou, R., Constant, N.: The power of scale for parameter-e\ufb03cient\nprompt tuning. In: Proceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing. pp. 3045\u20133059 (2021)\n15. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., K\u00fcttler, H.,\nLewis, M., Yih, W.t., Rockt\u00e4schel, T., et al.: Retrieval-augmented generation for\nknowledge-intensive nlp tasks. Advances in Neural Information Processing Systems\n33, 9459\u20139474 (2020)\n16. Li, X.L., Liang, P.: Pre\ufb01x-tuning: Optimizing continuous prompts for generation.\nIn: Proceedings of the 59th Annual Meeting of the Association for Computational\nLinguistics. pp. 4582\u20134597 (2021)\n17. Lin, S., Hilton, J., Evans, O.: Truthfulqa: Measuring how models mimic human\nfalsehoods. In: Proceedings of the 60th Annual Meeting of the Association for\nComputational Linguistics. pp. 3214\u20133252 (2022)\n18. Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., Neubig, G.: Pre-train, prompt, and\npredict: A systematic survey of prompting methods in natural language processing.\nACM Computing Surveys 55(9), 1\u201335 (2023)\n19. Min, S., Wallace, E., Singh, S., Gardner, M., Hajishirzi, H., Zettlemoyer, L.: Com-\npositional questions do not necessitate multi-hop reasoning. In: Proceedings of the\n57th Annual Meeting of the Association for Computational Linguistics (Jul 2019)\n20. Os\u00f3rio, T.F., Leite, B., Lopes Cardoso, H., Gomes, L., Rodrigues, J., Santos, R.,\nBranco, A.: PORTULAN ExtraGLUE datasets and models: Kick-starting a bench-\nmark for the neural processing of Portuguese. In: Zweigenbaum, P., Rapp, R.,\nSharo\ufb00, S. (eds.) Proceedings of the 17th Workshop on Building and Using Com-\nparable Corpora (BUCC) @ LREC-COLING 2024. pp. 24\u201334. ELRA and ICCL,\nTorino, Italia (May 2024), https://aclanthology.org/2024.bucc-1.3\n\nMeta-prompting Optimized Retrieval-augmented Generation\n13\n21. Prasad, A., Hase, P., Zhou, X., Bansal, M.: Grips: Gradient-free, edit-based in-\nstruction search for prompting large language models. In: Proceedings of the 17th\nConference of the EACL. pp. 3845\u20133864 (2023)\n22. Ra\ufb00el, C., Shazeer: Exploring the limits of transfer learning with a uni\ufb01ed text-to-\ntext transformer. Machine Learning Research 21, 5485\u20135551 (2020)\n23. Rodrigues, J., Branco, R., Silva, J., Saedi, C., Branco, A.: Predicting brain activa-\ntion with wordnet embeddings. In: Proceedings of the Eight Workshop on Cognitive\nAspects of Computational Language Learning and Processing. pp. 1\u20135 (2018)\n24. Rodrigues, J., Gomes, L., Silva, J., Branco, A., Santos, R., Cardoso, H.L., Os\u00f3rio,\nT.: Advancing neural encoding of portuguese with transformer albertina pt. In:\nEPIA Conference on Arti\ufb01cial Intelligence. pp. 441\u2013453. Springer (2023)\n25. Rodrigues, J.A., Branco, A.: Transferring con\ufb02uent knowledge to argument min-\ning. In: Calzolari, N., Huang, C.R., Kim, H., Pustejovsky, J., Wanner, L., Choi,\nK.S., Ryu, P.M., Chen, H.H., Donatelli, L., Ji, H., Kurohashi, S., Paggio, P., Xue,\nN., Kim, S., Hahm, Y., He, Z., Lee, T.K., Santus, E., Bond, F., Na, S.H. (eds.)\nProceedings of the 29th International Conference on Computational Linguistics.\npp. 6859\u20136874. International Committee on Computational Linguistics, Gyeongju,\nRepublic of Korea (Oct 2022), https://aclanthology.org/2022.coling-1.597\n26. Sabbatella, A., Ponti, A., Giordani, I., Candelieri, A., Archetti, F.: Prompt opti-\nmization in large language models. Mathematics 12(6) (2024)\n27. Santos, R., Branco, A., Silva, J.R.: Cost-e\ufb00ective language driven image editing\nwith LX-DRIM. In: Proceedings of the First Workshop on Performance and In-\nterpretability Evaluations of Multimodal, Multipurpose, Massive-Scale Models. pp.\n31\u201343. International Conference on Computational Linguistics, Virtual (Oct 2022),\nhttps://aclanthology.org/2022.mmmpie-1.5\n28. Santos, R., Rodrigues, J., Branco, A., Vaz, R.: Neural text categorization with\ntransformers for learning portuguese as a second language. In: Progress in Arti\ufb01cial\nIntelligence: 20th EPIA Conference on Arti\ufb01cial Intelligence, EPIA 2021, Virtual\nEvent, September 7\u20139, 2021, Proceedings 20. pp. 715\u2013726. Springer (2021)\n29. Santos, R., Rodrigues, J., Gomes, L., Silva, J.R., Branco, A., Lopes Cardoso,\nH., Os\u00f3rio, T.F., Leite, B.: Fostering the ecosystem of open neural encoders for\nPortuguese with albertina PT* family. In: Melero, M., Sakti, S., Soria, C. (eds.)\nProceedings of the 3rd Annual Meeting of the Special Interest Group on Under-\nresourced Languages @ LREC-COLING 2024. pp. 105\u2013114. ELRA and ICCL,\nTorino, Italia (May 2024), https://aclanthology.org/2024.sigul-1.14\n30. Santos, R., Silva, J.R., Gomes, L., Rodrigues, J., Branco, A.: Advancing generative\nAI for Portuguese with open decoder gerv\u00e1sio PT*. In: Melero, M., Sakti, S., Soria,\nC. (eds.) Proceedings of the 3rd Annual Meeting of the Special Interest Group on\nUnder-resourced Languages @ LREC-COLING 2024. pp. 16\u201326. ELRA and ICCL,\nTorino, Italia (May 2024), https://aclanthology.org/2024.sigul-1.3\n31. Touvron, H., Martin, L., Stone, K., et al.: Llama 2: Open foundation and \ufb01ne-tuned\nchat models. arXiv preprint arXiv:2307.09288 (2023)\n32. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n\u0141., Polosukhin, I.: Attention is all you need. Advances in neural information pro-\ncessing systems 30 (2017)\n33. Wang, X., Li, C., Wang, Z., Bai, F., Luo, H., Zhang, J., Jojic, N., Xing, E.P., Hu,\nZ.: Promptagent: Strategic planning with language models enables expert-level\nprompt optimization. arXiv preprint arXiv:2310.16427 (2023)\n34. Wei, J., Wang, X., Schuurmans, D., et al.: Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in NIPS 35 (2022)\n\n14\nJo\u00e3o Rodrigues and Ant\u00f3nio Branco\n35. Yang, C., Wang, X., Lu, Y., Liu, H., Le, Q.V., Zhou, D., Chen, X.: Large language\nmodels as optimizers. arXiv preprint arXiv:2309.03409 (2023)\n36. Yao, S., Yu, D., Zhao, J., et al.: Tree of thoughts: Deliberate problem solving with\nlarge language models. Advances in NIPS 36 (2024)\n37. Zhong, Z., Friedman, D., Chen, D.: Factual probing is [mask]: Learning vs. learning\nto recall. In: Proceedings of the 2021 Conference of the NACL (2021)\n"}