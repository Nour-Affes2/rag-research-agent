{"metadata": {"pdf_filename": "2406.19150v1__RAVEN Multitask Retrieval Augmented Vision-Language Learning.pdf", "source": "arXiv"}, "text": "RAVEN: Multitask Retrieval Augmented Vision-Language Learning\nVarun Nagaraj Rao1*, Siddharth Choudhary2, Aditya Deshpande3\u2020, Ravi Kumar Satzoda2,\nSrikar Appalaraju2\u2021\n1Princeton University, 2AWS AI Labs, 3Apple\nAbstract\nThe scaling of large language models to encode\nall the world\u2019s knowledge in model parame-\nters is unsustainable and has exacerbated re-\nsource barriers. Retrieval-Augmented Genera-\ntion (RAG) presents a potential solution, yet its\napplication to vision-language models (VLMs)\nis underexplored. Existing methods focus on\nmodels designed for single tasks. Furthermore,\nthey\u2019re limited by the need for resource inten-\nsive pretraining, additional parameter require-\nments, unaddressed modality prioritization and\nlack of clear benefit over non-retrieval base-\nlines. This paper introduces RAVEN, a multi-\ntask retrieval augmented VLM framework that\nenhances base VLMs through efficient, task-\nspecific fine-tuning. By integrating retrieval\naugmented samples without the need for addi-\ntional retrieval-specific parameters, we show\nthat the model acquires retrieval properties that\nare effective across multiple tasks.\nOur re-\nsults and extensive ablations across retrieved\nmodalities for the image captioning and VQA\ntasks indicate significant performance improve-\nments compared to non retrieved baselines \u2013 +1\nCIDEr on MSCOCO, +4 CIDEr on NoCaps,\nand nearly a +3% accuracy on specific VQA\nquestion types. This underscores the efficacy of\napplying RAG approaches to VLMs, marking\na stride toward more efficient and accessible\nmultimodal learning.\n1\nIntroduction\nThe rapid growth in model sizes in NLP, as high-\nlighted by OpenAI\u2019s LLM progression from GPT-\n2\u2019s 1.5 billion parameters (Radford et al., 2019)\nto GPT-3\u2019s 175 billion (Brown et al., 2020), and\nfurther to over a trillion in GPT-4 (OpenAI, 2023),\nis a source of increasing concern. This trend re-\nquires more data and computational power, lead-\n*Work conducted during an internship at Amazon.\n\u2020Work done while at Amazon.\n\u2021Correspondence to: srikara@amazon.com\ning to higher carbon emissions and presenting sig-\nnificant obstacles for less-resourced researchers\n(Strubell et al., 2019). In response, the field is\npivoting to approaches like Retrieval-Augmented\nGeneration (RAG) (Lewis et al., 2020), which in-\ncorporates external non-parametric world knowl-\nedge into a pretrained language model, removing\nthe necessity of encoding all information directly\ninto the model\u2019s parameters. However, this strat-\negy is not yet widely applied in vision-language\nmodels (VLMs) (Li et al., 2022; Wang et al., 2021;\nAlayrac et al., 2022; Chen et al., 2022c; Radford\net al., 2021; Wang et al., 2022b), which process\nboth image and textual data, and are typically more\nresource-intensive. Moreover, VLMs often rely\non massive datasets like LAION-5B (Schuhmann\net al., 2022), presenting a significant opportunity\nfor performance gains through retrieval augmenta-\ntion.\nThe scant prior work exploring retrieval augmen-\ntation applied to VLMs, although promising, is\nbeset with several limitations. Most importantly,\nthey rely on pretraining with retrieval specific pa-\nrameters (Hu et al., 2023; Ramos et al., 2023b;\nYang et al., 2023); as a result the performance im-\nprovement over non-retrieval baselines cannot be\nestablished and the benefit due to retrieval augmen-\ntation cannot be independently discerned. Next,\nmodel architectures are suited to only a single task,\nand therefore, experimental evaluation is also only\npresented on a single task e.g. on image caption-\ning (Ramos et al., 2023b,a; Yasunaga et al., 2023);\nother image-to-text tasks like VQA are ignored.\nFurther, the decision on which modality to prior-\nitize during retrieval - textual, visual, or a combi-\nnation of both - is not established. Some works\n(Yasunaga et al., 2023; Chen et al., 2022a) retrieve\nand concatenate both image and text, while others\n(Ramos et al., 2023a,b; Yang et al., 2023) only re-\ntrieve text, even though they all evaluate on image-\nto-text tasks. Finally, we also observe that overlaps\narXiv:2406.19150v1  [cs.CV]  27 Jun 2024\n\nbetween the retrieval and pre-training/fine-tuning\ndatasets exist; for example, Ramos et al. (2023a,b)\npretrain and retrieve from MSCOCO. This can\nconfound the benefits attributed to the RAG ap-\nproach, underscoring the need for a larger and non-\noverlapping external memory.\nIn this paper, we present RAVEN (see Figure 1),\na multitask retrieval augmented framework adapt-\nable to any multitask base VLM. The framework\ndoes not rely on pretraining with retrieval specific\nparameters, and is suitable to a variety of tasks.\nImportantly, the design of RAVEN allows for a\ncomprehensive investigation of the performance\nbenefits over non-retrieval baselines, and implica-\ntions of retrieving and using different modalities.\nSpecifically, our key contributions are as follows:\n1. We are the first to design a multitask re-\ntrieval augmented VLM framework (RAVEN),\nwhich relies on only fine-tuning, no retrieval\nspecific trainable parameters and is adaptable\nto any multitask base VLM.\n2. Our method allows for comprehensive abla-\ntions which examine the trade-offs between\nretrieval modalities and their advantages rel-\native to non-retrieval baselines while using a\nnon-overlapping and larger external memory.\n3. We demonstrate the benefits and limitations\nof our approach on Image Captioning and\nVQA through quantitative and qualitative anal-\nysis.\nOur results achieve a new state-of-\nthe-art performance improvement compared\nto non retrieved baselines: +1 CIDEr on\nMSCOCO, +4 CIDEr on NoCaps (using mag-\nnitudes of fewer parameters than prior works),\nand nearly a +3% accuracy on specific VQA\nquestion types.\nBroadly, our work expands the empirical knowl-\nedge on RAG techniques and contributes to the\nrapidly growing body of work focusing on their\napplications to multitask VLMs. Ultimately, this\nwork establishes a clearer understanding of the role\nof retrieval augmentation in VLMs, paving the way\nfor more efficient and sustainable approaches in the\nfield.\n2\nRelated Work\n2.1\nVision Language Models\nVision language models are an emerging type of\nmulti-modal AI system that can process both vi-\nsual and textual data (Appalaraju et al., 2024,\n2021) They build upon recent advances in com-\nputer vision and natural language processing to\ngenerate textual descriptions of images, answer\nvisual questions, and perform other vision-and-\nlanguage tasks. Earlier works in this direction\nunified multiple tasks like image captioning, im-\nage classification etc. using a simple sequence-to-\nsequence framework. Some notable examples in-\nclude OFA (Wang et al., 2022b), GIT (Wang et al.,\n2022a), SimVLM (Wang et al., 2021). Recent\nvision-language models (Biten et al., 2022) aug-\nment pre-trained large language models with visual\nencoder. For example, Frozen (Tsimpoukelli et al.,\n2021), Flamingo (Alayrac et al., 2022), BLIP (Li\net al., 2022), InstructBLIP (Dai et al., 2023),\nLLaVA (Liu et al., 2023), MiniGPT-4 (Zhu et al.,\n2023), Kosmos-1 (Huang et al., 2023), Pali (Chen\net al., 2022c). In this work, we use OFA (Wang\net al., 2022b) as the baseline rather than using\nVLMs augmented with pretrained LLMs. This\nchoice allows us to remove the effects of in-context\nlearning abilities of the pretrained language mod-\nels from the resulting enhancement brought by\nretrieval-augmented vision-language modeling.\n2.2\nRetrieval Augmented Generation in NLP\nRetrieval augmentation has become an important\ntechnique for improving natural language process-\ning models. One of the first works in this area\nwas kNN-LM by Khandelwal et al. (Khandelwal\net al., 2020) who showed how interpolating over\nnearest neighbors from any text collection could\nimprove generalization.\nThis was followed by\nRETRO (Borgeaud et al., 2021), which scaled up\nthe retrieval corpus to trillions of tokens. Another\nline of work has focused on integrating Wikipedia\npassages directly into models like REALM (Guu\net al., 2020), RAG (Lewis et al., 2020), and\nFiD (Izacard and Grave, 2021). By retrieving and\nconditioning on relevant Wikipedia passages, these\nmodels can better perform knowledge-intensive\ndownstream tasks like question answering. Over-\nall, retrieval augmentation has proven to be a\nhighly effective way of injecting knowledge into\nlanguage models to improve their capabilities.\nThe techniques have progressed from simple cor-\npus retrieval to integrated and scalable architec-\ntures that retrieve from large knowledge bases like\nWikipedia.\n\nFigure 1: Illustration of our RAVEN framework. Given an input image, we retrieve image-text pairs from an external\nmemory. Subsequently, we use a multitask pretrained base vision-language model (VLM) to encode the retrieved\nsamples along with the query and decode to generate an output by attending over both the query and retrieved\nsamples.\n2.3\nRetrieval Augmented Generation in\nVLMs\nRecent years have seen significant progress in ex-\ntending retrieval-augmented generation to vision-\nlanguage models.\nOne of the earliest works\nis Multimodal Retrieval-Augmented Transformer\n(MuRAG) which utilizes non-parametric multi-\nmodal memory for language generation improve-\nment (Chen et al., 2022a). In image-to-text genera-\ntion, Smallcap (Ramos et al., 2023b), exhibits com-\npetitive performance on COCO and other domains\nthrough retrieval from target-domain data. Sarto et\nal.(Sarto et al., 2022) use kNN memory for image\ncaptioning, enhancing knowledge retrieval from\nexternal corpora. Re-ViLM (Yang et al., 2023),\nbuilt upon the Flamingo (Alayrac et al., 2022), and\nsupports retrieving the relevant knowledge from\nthe external database for zero and in-context few-\nshot image-to-text generations. Recently, Iscen\net al. (Iscen et al., 2023) proposed to equip con-\ntrastive vision-text models with the ability to refine\ntheir embedding with cross-modal retrieved infor-\nmation from a memory at inference time, which\ngreatly improved their zero-shot predictions. Hu\net al. (Hu et al., 2023) presented REVEAL that\nlearns to encode world knowledge into a large-\nscale memory, and to retrieve from it to answer\nknowledge-intensive queries, and achieves state-of-\nthe-art results on visual question answering and im-\nage captioning. In text-to-image generation, Chen\net al. (Chen et al., 2022b) presented Re-Imagen that\nuses retrieved information to produce high-fidelity\nand faithful images, even for rare or unseen entities.\nRA-CM3 is the first multimodal model that can\nretrieve and generate mixtures of text and images\nand exhibits novel capabilities such as knowledge-\nintensive image generation and multi-modal in-\ncontext learning (Yasunaga et al., 2023).\nOur multitask framework, RAVEN, extends be-\nyond RA-CM3 by supporting both captioning and\nVQA, and it diverges from REVEAL (Hu et al.,\n2023) by attaining retrieval capabilities solely\nthrough fine-tuning, eliminating the need for pre-\ntraining and additional retrieval-specific parame-\nters; and is adaptable to any base VLM.\n3\nProposed Approach\n3.1\nRAVEN Framework\nOur framework, RAVEN, is illustrated in Figure\n1. At a high level, given a multimodal input con-\nsisting of images and text, we use a retriever to\nretrieve relevant image-text pairs from a large ex-\nternal memory. Subsequently, we use a pretrained\nmultitask encoder-decoder VLM which refers to\nthe retrieved context in addition to the multimodal\nquery and generates a textual output. Importantly,\nwe demonstrate that through short, but efficient,\ntask specific fine-tuning of the base VLM, with\nconcatenated retrieval augmented samples and no\nadditional retrieval-specific trainable parameters,\nthe model acquires retrieval properties which gen-\neralizes to multiple tasks. We now describe both\nthese components in detail.\n\n3.2\nMultimodal Retriever\nOur semantic search based retrieval system, relies\non the Facebook AI Similarity Search (FAISS) li-\nbrary (Douze et al., 2017). FAISS enables high-\ndimensional vector indexing within an external\nmemory and facilitates efficient search through an\napproximate nearest neighbor approach based on a\nspecified similarity measure, such as dot-product\nsimilarity. We utilize the publicly available Laion-\n5B (Schuhmann et al., 2022) image-based index\nwhich consists of 5 billion images and correspond-\ning alt text.\nTo describe the retrieval steps in detail, we first\nencode the query image using a CLIP-based image\nencoder (Radford et al., 2021) into a dense vector.\nNext, we follow the Dense Retrieval method out-\nlined in Karpukhin et al. (2020) to retrieve the top\n\u2018k\u2019 (k can be specified by the user) image-text pairs\nby scoring the query (image) and memory data as\nfollows:\nscore(query, memory) = E(query)T E(memory)\n(1)\nwhere E is the CLIP-based image encoder. Fi-\nnally, we perform Maximum Inner Product Search\n(MIPS) over the memory to obtain the top \u2018k\u2019 candi-\ndate image-text pairs sorted according to the score.\nOur retrieval approach ensures that the retrieved\nsamples, which are provided as additional con-\ntext to the model, along with the query image,\nare relevant, diverse and in the style of our tar-\nget datasets. Relevance is easily ensured through\nsampling based on the top similarity score. How-\never, simply sampling based on relevance score\ncan result in exact or near duplicates resulting in\npoor performance. To avoid this redundancy and\nenhance diversity, we exclude near duplicate im-\nages. Finally, to use COCO-style captions rather\nthan the noisy image alt text in Laion-5B, we map\nthe retrieved samples from Laion-5B down to the\nLaion-COCO 600M 1 subset, whose captions are\nsynthetically generated using a BLIP model trained\non COCO-style captions. This can result in some\nmissing data due to lack of matches with LAION-\nCOCO 600M and also due to failure of LAION-\nCOCO 600M raw image downloads. Our approach\nis robust to these missing samples.\n3.3\nBase Vision-Language Model (VLM)\nRAVEN relies on a multitask, multimodal encoder-\ndecoder base VLM which can easily leverage addi-\n1https://laion.ai/blog/laion-coco/\ntional multimodal context from an external mem-\nory.\nArchitecture.\nFor image encoding, we use a\nResNet, and for text encoding we use a byte-pair\nencoding (BPE) to convert the text sequence into\na subword sequences, and then embed them into\nfeatures. We adopt a unified vocabulary encom-\npassing linguistic and visual tokens, incorporating\nsubwords, image codes, and location tokens. The\nbase architecture is the transformer; this serves as\nthe backbone for the encoder-decoder framework.\nTo enhance stability and hasten convergence, the\nmodel uses head scaling for self-attention, post-\nattention layer normalization (LN), and LN fol-\nlowing the first layer of FFN. For positional infor-\nmation, separate absolute position embeddings are\nused for text and images. Notably, we decouple\nposition correlation from token embeddings and\npatch embeddings, while employing 1D relative\nposition bias for text and 2D relative position bias\nfor images.\nVL Tasks.\nAll cross-modal tasks are cast\nas Seq2Seq generation. We focus on 2 popular\nimage-to-text tasks, image captioning and visual\nquestion answering (VQA). For image captioning,\nthe model adeptly adopts the Seq2Seq format, gen-\nerating captions based on both the provided image\nand the input textual prompt, \u201cWhat does the image\ndescribe?\u201d. For VQA, the model takes in the im-\nage and the question as inputs, learning to generate\naccurate responses.\nNeed for Retrieval in VL tasks.\nRetrieval\ncan benefit performance in VL tasks as contextual\ninformation can be crucial for guiding models to ac-\ncurate answers. Moreover, the retrieval mechanism\ncan mitigate bias by sourcing information from di-\nverse datasets, countering the influence of biased\ntraining data. Specifically, in VQA, image con-\ntent, such as object attributes, strongly correlates\nwith questions and answers, making captions valu-\nable auxiliary information while similar/retrieved\nimages are less informative (Gur et al., 2021). In\ncaptioning, additional textual context resembles\nfew-shot inference (Yasunaga et al., 2023).\nReasons for OFA(Wang et al., 2022b) as a VLM\nbackbone.\nWe list 4 reasons for choosing\nOFA rather than alternates like Beit-3 (Wang et al.,\n2023) and Open Flamingo (Awadalla et al., 2023):\nFirst, OFA is naturally suited to our approach as it\nunifies multiple modalities and tasks into a single\nSeq2Seq model; the multitask backbone is a delib-\nerate design choice that underscores the versatility\n\nof our approach and is a foundational element cru-\ncial to our model\u2019s architecture. Second, we can\neasily endow the model retrieval augmented capa-\nbilities through short, but efficient, task specific\nfine-tuning with no additional trainable parameters.\nMoreover, we intentionally avoided recent MLLM\nmodels like LLaVa or Flamingo which contain an\nLM to not add additional trainable parameters, re-\nmove their in-context learning ability and isolate re-\ntrieval capabilities within an encoder-decoder back-\nbone, a first in the field. Third, the codebase is open\nsource, modular and easy to extend. Finally, the\nbase OFA model is not very large (182M param-\neters) given our compute and finance limitations,\nbut sufficient to demonstrate the benefits of our\nframework.\n4\nExperiments\nIn this section, we evaluate the performance of our\napproach under the fine-tuning setting on various\nimage captioning and VQA benchmarks. We aim\nto demonstrate the benefits of retrieval augmenta-\ntion on the generated captions and answers through\nretrieving relevant knowledge from a large exter-\nnal non-overlapping database with the fine-tuning\ndatasets. Our experiments show clear benefits of\nour approach compared to non-retrieval baselines.\nFurthermore, the performance is competitive with\nsimilarly sized models, and even exceeds the per-\nformance of existing widely used captioning and\nVQA models several magnitudes larger.\n4.1\nTraining Setup\n4.1.1\nData\nWe make use of an external memory and task spe-\ncific fine-tuning datasets in our implementation.\nFor captioning, we use the MSCOCO 2014 Karpa-\nthy Splits for fine-tuning and NoCaps for a zero-\nshot evaluation. For VQA, we use the VQA v2\ndataset augmented with VG-QA questions during\nfine-tuning. We use Laion-5B index as our exter-\nnal memory and map down to Laion-COCO 600M\nsubset to retrieve image-caption pairs. The datasets\nare summarized in Table 1 and 2. Notably, unlike\nprior work, we ensure the fine-tuning datasets and\nexternal memory do not have any overlap, to real-\nize the true benefits of retrieval augmentation in\npractical settings.\nMissing Samples:\nRetrieved data can be missing\nfor 2 reasons: (1) lack of matches of the Laion-\n5B retrieved samples with the Laion-COCO 600M\nDataset\nSplit\n# of images\n(original)\n# of images\n(caption)\n# of images\n(caption + image)\nSize - w or w/o\nretrieval\nMSCOCO\nKarpathy\nSplit (2014)\ntrain\n113287\n108780\n107800\n37G / 64G\nval\n5000\n4776\n4725\n330M / 573M\ntest\n5000\n4817\n4778\n329M / 576M\nNoCaps\nval\n4500\n4275\n4239\n295M / 512M\nTable 1: Captioning dataset summary\nSplit\n# of samples\n# of images\n(original)\n# of images\n(caption)\n# of images\n(caption + image)\nSize - w or w/o\nretrieval\ntrain\n1,358,769\n121,277\n116,439\n115,387\n106G / 151G\nval\n10,402\n2,000\n1,924\n1,906\n653M / 1.2G\ntest-dev\n107,394\n36,807\n35,107\n34,760\n28G / 50G\ntest-std\n447,793\n81,434\n77,856\n77,098\n28G / 50G\nTable 2: VQA v2 dataset summary\nsubset, and (2) raw image download failure. For\ncaptioning, we only work on the subset of samples\nwhich have both retrieved captions and images. We\nvalidate that augmentation with images is not use-\nful, and subsequently decide to only use retrieved\ncaptions for augmentation. For VQA, we retain the\noriginal dataset, and missing captions are handled\nwith an empty string. This allows us to evaluate our\nresults on the VQA evaluation server. Importantly,\nthe model learns to be robust to samples which\nmay not have corresponding retrieved context at\ninference; a scenario common in practice.\n4.1.2\nImplementation\nOur retriever uses the off-the-shelf CLIP image en-\ncoder (Radford et al., 2021) for both the query and\nmemory encoders. We use FAISS (Douze et al.,\n2017) to index the external Laion-5B image-based\nmemory and perform MIPS-based top-50 retrieval.\nWe then map down to the Laion-COCO 600M sub-\nset ensuring to select, when it exists, the top-1 im-\nage (excluding exact or near duplicates), and all\nassociated metadata, including the top caption, all\ncaptions and alt text. The retrieved samples are con-\ncatenated with the original samples in the TSV file\nprovided as input during the fine-tuning process.\nWe ensure our fine-tuning process is able to op-\nerate in resource constrained settings. We use a\nlightweight OFA-base (Wang et al., 2022b) model\ncheckpoint of 182M parameters as our multitask\nVLM. The maximum sequence length is 1024. We\nfine-tune the model for 8-12 hours, upto 10 epochs,\non 4 V100 32GB GPU\u2019s. Our implementation is in\nPyTorch. We increase the max source length from\n80 upto 600 to account for the retrieved samples.\nOtherwise, we rely on the task-specific default hy-\nperparameters in the OFA-base run scripts.\nFollowing the OFA implementation, we optimize\n\nthe model with the standard cross-entropy loss.\nGiven an input image i, a prompt t, and an output y,\nwe minimize the loss L = \u2212P|y|\nj=i log P\u03b8(yj|y <\nj, i, t) where \u03b8 refers to the model parameters. For\ninference, we decode using beam search, to en-\nhance the quality of generation. For the VQA task,\nwe employ a trie-based search to only search over\na bounded set of vocabulary (top 3129 VQA v2\nanswers) to prevent labels out of the closed label\nset during inference.\n4.2\nEvaluation Setup\n4.2.1\nBaselines\nWe establish baselines to gauge the performance of\nRAVEN in comparison to various configurations:\nCaptioning.\n(1) Retrieval Only: This baseline\ninvolves using the top caption retrieved from the\nmemory as the generated output. It serves as a\nbenchmark to assess the additional benefits gained\nthrough fine-tuning the OFA-base model. (2) Zero\nShot In-Context Retrieval: During inference, this\nbaseline directly concatenates the retrieved top cap-\ntion and all captions with the prompt. The objec-\ntive is to evaluate the model\u2019s capacity to leverage\nretrieved context without any pretraining or fine-\ntuning. (3) No Retrieved Samples: In this scenario,\nthe model undergoes fine-tuning solely on the tar-\nget dataset without incorporating any retrieved con-\ntext. This baseline helps establish a performance\nreference point.\nVQA.\nNo Retrieved Samples: Similar to the\ncaptioning task, this baseline involves fine-tuning\nthe model exclusively on the target dataset without\nincorporating any retrieved context.\nIn all cases, we report performance gains rela-\ntive to the \u201cNo Retrieved Samples\u201d baselines to\nhighlight the efficacy of our proposed approach.\nNotably, most prior work fail to report this base-\nline making it challenging to assess the benefits of\nretrieval augmentation.\nAdditionally, we provide a comparative analysis\nby reporting recent baselines and the current State-\nof-the-Art (SOTA) for both captioning and VQA\ntasks. This comparative assessment considers per-\nformance metrics and the number of parameters,\noffering a comprehensive view of the landscape\nand positioning our model within the current state-\nof-the-art research.\n4.2.2\nMetrics\nIn evaluating the performance of RAVEN for cap-\ntioning, we employ two key metrics: BLEU@4\nand CIDEr. BLEU@4 measures the quality of gen-\nerated captions by assessing the overlap of n-grams\n(in this case, four-grams) between the generated\ncaption and reference captions. Meanwhile, the\nCIDEr metric gauges the diversity and distinctive-\nness of generated captions by considering consen-\nsus across multiple reference captions.\nFor the VQA task, we utilize accuracy as the\nevaluation metric. This measure is computed using\nthe Eval.ai server.\n4.2.3\nAblations\nWe explore three distinct sets of ablations for both\ncaptioning and VQA: text-only, image-only, and\ncombined image and text. To the best of our knowl-\nedge, we are the first to comprehensively discern\nthe impact of text and image modalities in retrieval\naugmented VLMs, providing valuable insights to\nmodel practitioners.\nCaptioning.\nFor the text-only ablation, we ex-\nperiment with various combinations, concatenating\none or more of the top caption, all captions, and\nimage alt text. This helps us discern the impact of\ntextual information in isolation. In the image-only\nablation, we alter the patch size, doubling it, and\nemploy a horizontal concatenation strategy. If a re-\ntrieved image is present, we concatenate it with the\nquery image. In cases where the retrieved image is\nabsent, we duplicate the query image. This analysis\nprovides valuable insights into the model\u2019s reliance\non visual information alone. For the combined im-\nage and text ablation, we adopt a similar approach\nto the image-only case for processing images. Si-\nmultaneously, we concatenate the top caption and\nall captions to the text prompt. This exploration\nallows us to understand the synergistic effects of\nboth modalities.\nVQA.\nBuilding on insights gained from the cap-\ntioning task, where naive image fusion through\nconcatenation proved less useful (see Table 3),\nwe hypothesize that captions serve as good aux-\niliary information in image-to-text tasks, while\nsimilar/retrieved images are less informative, since\nthe content of the image and the objects contained\nis often very correlated with the question and an-\nswer. Therefore, in the VQA ablations, we exclu-\nsively consider text concatenation scenarios. This\ninvolves combining one or more of the top cap-\ntion, all captions, and alt text when available. In\ninstances where the retrieved sample is missing, we\nconcatenate with an empty string.\n\nRetrieval\nModality\nMSCOCO\nNoCaps\nImage\nText\n# of\nParameters\nAblation Description\nBLEU@4\nCIDEr\nCIDEr\nOur Approach (Image, Text, Image+Text Retrieval)\n-\n-\n-\nretrieval only\n0.1905\n74.98\n71.68\n-\n-\n182M\nzero shot in-context retrieval with top caption + all captions\n0.3777\n128.91\n103.99\n-\n-\n182M\nno retrieved samples\n0.4102\n137.25\n106.69\n-\n\u2713\n182M\ntop caption\n0.4102\n138.23* (+0.98)\n109.76\n-\n\u2713\n182M\nalt text\n0.4125\n137.19\n106.81\n-\n\u2713\n182M\nall captions concatenated\n0.4057\n137.70\n109.72\n-\n\u2713\n182M\ntop caption + all captions\n0.4108\n138.17* (+0.92)\n111.00 (+ 4.31)\n-\n\u2713\n182M\ntop caption + all captions + alttext\n0.4104\n138.03\n109.88\n\u2713\n-\n182M\nimage\n0.4087\n136.95\n106.22\n\u2713\n\u2713\n182M\nimage + top caption + all captions\n0.4081\n136.85\n107.28\nImage Captioning Baselines (Fine-tuning)\n-\n-\n420M\nRe-ViLM (base, (Yang et al., 2023))\n0.378\n129.1\n105.2\n-\n-\n364M\nFlamingo (base, re-implementation from (Yang et al., 2023))\n0.370\n128.0\n102.8\n-\n-\n252M\nBLIPCapFilt-L (Li et al., 2022)\n0.404\n136.7\n113.2\n-\n-\n172M\nVL-T5 (Cho et al., 2021)\n0.346\n116.1\n4.4\n-\n-\n1.4B\nSimVLM (huge, (Wang et al., 2021))\n0.406\n143.3\n110.3\n-\n-\n5.1B\nGIT2 (current SOTA (Wang et al., 2022a))\n0.432\n146.4\n126.9\n*Gain with respect to the non retrieved baseline is comparable to the only prior work which reported it for the MSCOCO captioning task (Sarto et al., 2022)\nTable 3: Fine-tuning evaluation results using cross-entropy optimization on MSCOCO, and NoCaps benchmarks,\ncompared with different image captioning baselines. For NoCaps, we finetune on MSCOCO karpathy train following\nprior works (Li et al., 2022), and perform zero-shot evaluation. We use the Laion-5B image index mapped down\nto the Laion-COCO 600M subset as our external memory. We report BLEU@4 and CIDer scores for different\nmethods and show the gain in the best performing models compared to the non-retrieved baseline.\nTest-Dev Accuracy %\n# of\nParameters\nAblation Description\nnumber\nother\nyes/no\noverall\nOur Approach (Text Retrieval)\n182M\nno retrieved samples\n58.55\n67.47\n90.12\n75.89\n182M\nalttext\n61.10\n67.94\n90.10\n76.29\n182M\nalttext + all captions\n57.84\n67.92\n90.46\n76.06\n182M\ntop caption + all captions\n61.33* (+ 2.78%)\n68.27*(+ 0.80%)\n90.54* (+0.42%)\n76.75* (+0.86%)\nVQA Baselines (Fine-tuning)\n122M\nUnifiedVLP (Zhou et al., 2020)\n52.10\n60.30\n87.20\n70.50\n252M\nBLIPCapFilt-L (Li et al., 2022)\n-\n-\n-\n78.25\n1.4B\nSimVLM (huge, (Wang et al., 2021))\n-\n-\n-\n80.30\n80B\nFlamingo (Alayrac et al., 2022)\n-\n-\n-\n82.00\n55B\nPaLI-X (2023) - current SOTA (Chen et al., 2022c)\n-\n-\n-\n86.10\n*Gain with respect to the non retrieved baseline surpasses that of the only prior work which reported it for the VQA v2 task (Gur et al., 2021)\nTable 4: Finetuning evaluation results on VQA v2 benchmarks, compared with the non retrieval VQA baseline. We\nfinetune our method on VQA v2 train split using a subset of the OFA dataset. We report Test-Dev accuracy % from\nthe eval.ai server for different methods.\n5\nResults\n5.1\nQuantitative Analysis\nCaptioning.\nThe results for image captioning,\npresented in Table 3, reveal notable insights. Base-\nline comparisons indicate that both the retrieval-\nonly and zero-shot in-context retrieval fall short\nof the no-retrieved samples baseline, underscor-\ning the value of fine-tuning on the target dataset.\nThe absence of zero-shot in-context retrieval ca-\npabilities may be attributed to the absence of a\nlanguage model in the transformer-based encoder-\ndecoder VLM architecture. In the text-only abla-\ntion, concatenating with the top caption and/or all\ncaptions yields optimal performance, demonstrat-\ning a gain of nearly 1 CIDEr point on MSCOCO\nand up to 4 CIDEr points on zero-shot NoCaps.\nThe gain with respect to the non retrieved base-\nline is comparable to the only prior work which\nreported it (+1.2 CIDEr score) for the MSCOCO\n\nFigure 2: Examples of the retriever output given a query image.\ncaptioning task (Sarto et al., 2022). This empha-\nsizes the valuable contextual information provided\nby retrieved captions. However, concatenating with\nalt text proves less effective due to its inherent\nnoise. Both image-only and combined image and\ntext concatenation exhibit performance below the\nnon-retrieved baseline, suggesting that retrieved im-\nages and naive concatenation introduce noise rather\nthan relevant context. In fine-tuning settings, our\nmodel performs competitively with similar-sized\nmodels such as BLIP. Notably, in the zero-shot set-\nting on NoCaps, our model surpasses SimVLM\n(1.4B vs 182M parameters), achieving a CIDEr\nscore of 111.0 compared to 110.3.\nVQA.\nGiven the limited efficacy observed in\nthe use of retrieved image for captioning (see Ta-\nble 3), we exclusively explore text augmentation\nstrategies for VQA. The results, presented in Table\n4, align with the captioning outcomes, affirming\nthe efficacy of text-only augmentation. Notably,\nacross all question categories, text-only augmenta-\ntion yields improvements in accuracy ranging from\n0.42% to 2.78%. The gain with respect to the non\nretrieved baseline surpasses that of the only prior\nwork which reported it (+0.36% accuracy) for the\nVQA v2 task (Gur et al., 2021). The highest perfor-\nmance is achieved through concatenating the top\ncaption and all captions with the question, while\nthe addition of alt text introduces noise, resulting\nin lower performance. The overall performance of\nour model in VQA remains competitive and com-\nparable to similar-sized models, underscoring its\nrobustness in leveraging textual information for\naccurate question answering.\n5.2\nQualitative Analysis\nIn this section, we present qualitative examples\nthat elucidate the efficacy and limitations of our\napproach.\nFigure 3: Examples where RAVEN succeeds in generat-\ning the correct answer.\nRetriever Output.\nFigure 2 illustrates the out-\nput of the retriever for a given query image. The\nretrieved images align with the query image, em-\nphasizing relevance. However, Laion-5B\u2019s image\nalt text is observed to be noisy and differs from\nthe required COCO-style captions. Mapping down\nto synthetically generated BLIP captions from the\nLAION-COCO 600M subset, mitigates the style\nissue by mimicking the COCO caption style, and\noffers more valuable context to the model.\nIncorporating World Knowledge.\nFigure\n3 demonstrates VQA outputs leveraging world\nknowledge.\nThe model adeptly utilizes entity-\nrich captions from the retriever to disambiguate\nbetween entities, as seen in the bear image distin-\nguishing logs from rocks. Additionally, the model\naccurately identifies nuanced details, such as a boy\nsquatting while playing baseball, by leveraging\n\nrelevant context in the captions, such as the term\n\u201ccrouches.\"\nRetriever Failures.\nDespite successes, retrieved\ncontext may not consistently contribute to specific\nquestions, particularly when inquiries concern en-\ntities not prominently featured in the image. This\nissue is more pronounced in tasks such as VQA,\nrather than in captioning, where general knowledge\nabout the image is often sufficient to generate high\nquality and diverse captions. Illustrated in Figure\n4, failure cases for VQA depict relevant but insuf-\nficiently informative captions. For instance, cap-\ntions for an elephant image focus on the foreground\nelephant, neglecting details about the background\nmountains and forest. Similarly, captions for a cake\nimage lack information about the cake lifter in the\ncorner.\nFigure 4: Examples where RAVEN fails in generating\nthe correct answer.\nMultimodal Query Embedding.\nConsidering\nscenarios where retrieved context may lack speci-\nficity, we propose the joint use of image and text\nmodalities as input to the retriever, when available.\nFigure 5 demonstrates an example where creating\na multimodal query embedding by averaging im-\nage and text embeddings separately results in rel-\nevant captions addressing both the image and the\nquestion. Comprehensive exploration of scenarios\nwhere specific entity properties lack corresponding\ncaptions is deferred to future work.\n6\nConclusion and Future Work\nTo address escalating model size and computa-\ntional demands, we propose a retrieval augmen-\ntation framework, an alternative to storing ex-\ntensive world knowledge within model param-\neters.\nOur contributions introduce a multitask,\nmultimodal retrieval-augmented vision-language\nmodel, demonstrating adaptability across multi-\nFigure 5: An example depicting the benefits of using\na multimodal query embedding (average of image and\nquestion embedding). This results in the retrieval of\ncaptions relevant to both the image and question.\nple tasks through computationally efficient task-\nspecific fine-tuning. Utilizing concatenated multi-\nmodal retrieval-augmented samples from an exter-\nnal non-overlapping memory, without additional\ntrainable parameters, our single model acquires\nrobust retrieval properties. This showcases bene-\nfits in both captioning and VQA tasks using a uni-\nfied approach. Notably, extensive ablations across\ntext, image, and image-text modalities, systemat-\nically compared against non-retrieved baselines,\nprovide valuable insights. Our findings underscore\nthat retrieval augmentation, particularly with text\nin image-to-text tasks, optimally enhances perfor-\nmance, especially in the zero-shot setting.\nFuture directions involve refining sampling\nstrategies for enhanced diversity, exploring alterna-\ntive image fusion approaches, and investigating a\nmixture of experts to afford the model flexibility\nin leveraging retrieved context. Additionally, we\npropose extending retrieval over a composite index\n(image+text) to further optimize performance.\n7\nLimitations\nWe use a relatively small model to demonstrate\nperformance on 2 tasks. While we acknowledge\nthe demonstrating our approach on more tasks and\nlarger models would be beneficial, we defer this\nto future work due to compute and financial con-\nstraints. RAVEN\u2019s current capability to handle di-\nverse tasks like image captioning and VQA within a\nsingle model framework already stands as a signifi-\ncant advancement; and is sufficient to demonstrate\nthe benefit of our framework.\n\nReferences\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,\nAntoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm\nReynolds, et al. 2022. Flamingo: a visual language\nmodel for few-shot learning. Advances in Neural\nInformation Processing Systems, 35:23716\u201323736.\nSrikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota,\nYusheng Xie, and R. Manmatha. 2021. Docformer:\nEnd-to-end transformer for document understand-\ning. In Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), pages 993\u2013\n1003.\nSrikar Appalaraju, Peng Tang, Qi Dong, Nishant\nSankaran, Yichu Zhou, and R. Manmatha. 2024.\nDocformerv2: Local features for document under-\nstanding. AAAI, abs/2306.01733.\nAnas Awadalla, Irena Gao, Josh Gardner, Jack Hes-\nsel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe,\nYonatan Bitton, Samir Gadre, Shiori Sagawa, Je-\nnia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel\nIlharco, Mitchell Wortsman, and Ludwig Schmidt.\n2023. Openflamingo: An open-source framework for\ntraining large autoregressive vision-language models.\narXiv preprint arXiv:2308.01390.\nAli Furkan Biten, Ron Litman, Yusheng Xie, Srikar\nAppalaraju, and R Manmatha. 2022. Latr: Layout-\naware transformer for scene-text vqa. In Proceedings\nof the IEEE/CVF conference on computer vision and\npattern recognition, pages 16548\u201316558.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\nTrevor Cai, Eliza Rutherford, Katie Millican, George\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\nGuy, Jacob Menick, Roman Ring, T. W. Hennigan,\nSaffron Huang, Lorenzo Maggiore, Chris Jones, Al-\nbin Cassirer, Andy Brock, Michela Paganini, Geof-\nfrey Irving, Oriol Vinyals, Simon Osindero, Karen\nSimonyan, Jack W. Rae, Erich Elsen, and L. Sifre.\n2021. Improving language models by retrieving from\ntrillions of tokens. In International Conference on\nMachine Learning.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nWenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and\nWilliam Cohen. 2022a. Murag: Multimodal retrieval-\naugmented generator for open question answering\nover images and text. In Proceedings of the 2022\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 5558\u20135570.\nWenhu Chen, Hexiang Hu, Chitwan Saharia, and\nWilliam W. Cohen. 2022b. Re-imagen: Retrieval-\naugmented\ntext-to-image\ngenerator.\nArXiv,\nabs/2209.14491.\nXi Chen, Xiao Wang, Soravit Changpinyo, AJ Pier-\ngiovanni, Piotr Padlewski, Daniel Salz, Sebastian\nGoodman, Adam Grycner, Basil Mustafa, Lucas\nBeyer, et al. 2022c.\nPali: A jointly-scaled mul-\ntilingual language-image model.\narXiv preprint\narXiv:2209.06794.\nJaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. 2021.\nUnifying vision-and-language tasks via text genera-\ntion. In International Conference on Machine Learn-\ning, pages 1931\u20131942. PMLR.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony\nMeng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang\nLi,\nPascale\nFung,\nand\nSteven\nHoi.\n2023. Instructblip: Towards general-purpose vision-\nlanguage models with instruction tuning. Preprint,\narXiv:2305.06500.\nMatthijs Douze, Jeff Johnson, and Herv\u00e9 Jegou. 2017.\nFaiss: A library for efficient similarity search.\nShir Gur, Natalia Neverova, Chris Stauffer, Ser-Nam\nLim, Douwe Kiela, and Austin Reiter. 2021. Cross-\nmodal retrieval augmentation for multi-modal classi-\nfication. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2021, pages 111\u2013123.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Ming-Wei Chang. 2020. REALM: Retrieval-\naugmented language model pre-training.\narXiv\npreprint arXiv:2002.08909.\nZiniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-\nWei Chang, Yizhou Sun, Cordelia Schmid, David A\nRoss, and Alireza Fathi. 2023. Reveal: Retrieval-\naugmented visual-language pre-training with multi-\nsource multimodal knowledge memory. In Proceed-\nings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 23369\u201323379.\nShaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,\nSaksham Singhal, Shuming Ma, Tengchao Lv, Lei\nCui, Owais Khan Mohammed, Qiang Liu, Kriti Ag-\ngarwal, Zewen Chi, Johan Bjorck, Vishrav Chaud-\nhary, Subhojit Som, Xia Song, and Furu Wei. 2023.\nLanguage is not all you need: Aligning perception\nwith language models. ArXiv, abs/2302.14045.\nAhmet Iscen, Alireza Fathi, and Cordelia Schmid. 2023.\nImproving image recognition by retrieving from web-\nscale image-text data. 2023 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR),\npages 19295\u201319304.\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 874\u2013880, Online. Association for Computa-\ntional Linguistics.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\n\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP). Association for\nComputational Linguistics.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough Memorization: Nearest Neighbor Language\nModels. In International Conference on Learning\nRepresentations (ICLR).\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rock-\nt\u00e4schel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459\u20139474.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven\nHoi. 2022. Blip: Bootstrapping language-image pre-\ntraining for unified vision-language understanding\nand generation. In International Conference on Ma-\nchine Learning, pages 12888\u201312900. PMLR.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023. Visual instruction tuning.\nR OpenAI. 2023. Gpt-4 technical report. arXiv, pages\n2303\u201308774.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In International confer-\nence on machine learning, pages 8748\u20138763. PMLR.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nRita Ramos, Desmond Elliott, and Bruno Martins.\n2023a. Retrieval-augmented image captioning. In\nProceedings of the 17th Conference of the European\nChapter of the Association for Computational Lin-\nguistics, pages 3648\u20133663.\nRita Ramos, Bruno Martins, Desmond Elliott, and Yova\nKementchedjhieva. 2023b. Smallcap: lightweight\nimage captioning prompted with retrieval augmenta-\ntion. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages\n2840\u20132849.\nSara Sarto, Marcella Cornia, Lorenzo Baraldi, and Rita\nCucchiara. 2022. Retrieval-augmented transformer\nfor image captioning. In Proceedings of the 19th\nInternational Conference on Content-based Multime-\ndia Indexing, pages 1\u20137.\nChristoph Schuhmann, Romain Beaumont, Richard\nVencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\nTheo Coombes, Aarush Katta, Clayton Mullis,\nMitchell Wortsman, et al. 2022. Laion-5b: An open\nlarge-scale dataset for training next generation image-\ntext models. Advances in Neural Information Pro-\ncessing Systems, 35:25278\u201325294.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 3645\u20133650, Florence, Italy. Asso-\nciation for Computational Linguistics.\nMaria Tsimpoukelli, Jacob Menick, Serkan Cabi,\nSM Eslami, Oriol Vinyals, and Felix Hill. 2021. Mul-\ntimodal few-shot learning with frozen language mod-\nels. Proc. Neural Information Processing Systems.\nJianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie\nLi, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and\nLijuan Wang. 2022a. Git: A generative image-to-text\ntransformer for vision and language. arXiv preprint\narXiv:2205.14100.\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai\nBai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren\nZhou, and Hongxia Yang. 2022b. Ofa: Unifying ar-\nchitectures, tasks, and modalities through a simple\nsequence-to-sequence learning framework. In Inter-\nnational Conference on Machine Learning, pages\n23318\u201323340. PMLR.\nWenhui Wang,\nHangbo Bao,\nLi Dong,\nJohan\nBjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,\nOwais Khan Mohammed, Saksham Singhal, Subho-\njit Som, and Furu Wei. 2023. Image as a foreign\nlanguage: BEiT pretraining for vision and vision-\nlanguage tasks. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition.\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yu-\nlia Tsvetkov, and Yuan Cao. 2021. Simvlm: Simple\nvisual language model pretraining with weak super-\nvision. arXiv preprint arXiv:2108.10904.\nZhuolin Yang, Wei Ping, Zihan Liu, Vijay Kor-\nthikanti, Weili Nie, De-An Huang, Linxi Fan, Zhid-\ning Yu, Shiyi Lan, Bo Li, et al. 2023.\nRe-vilm:\nRetrieval-augmented visual language model for zero\nand few-shot image captioning.\narXiv preprint\narXiv:2302.04858.\nMichihiro Yasunaga, Armen Aghajanyan, Weijia Shi,\nRich James, Jure Leskovec, Percy Liang, Mike Lewis,\nLuke Zettlemoyer, and Wen-tau Yih. 2023. Retrieval-\naugmented multimodal language modeling. In Pro-\nceedings of the 40th International Conference on\nMachine Learning, ICML\u201923. JMLR.org.\nLuowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu,\nJason Corso, and Jianfeng Gao. 2020. Unified vision-\nlanguage pre-training for image captioning and vqa.\nIn Proceedings of the AAAI conference on artificial\nintelligence, pages 13041\u201313049.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny. 2023. Minigpt-4: Enhancing\n\nvision-language understanding with advanced large\nlanguage models. arXiv preprint arXiv:2304.10592.\n"}