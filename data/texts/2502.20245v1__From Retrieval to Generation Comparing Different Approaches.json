{"metadata": {"pdf_filename": "2502.20245v1__From Retrieval to Generation Comparing Different Approaches.pdf", "source": "arXiv"}, "text": "From Retrieval to Generation: Comparing Different Approaches\nAbdelrahman Abdallah, Jamshid Mozafari, Bhawna Piryani,\nMohammed Ali, Adam Jatowt\nUniversity of Innsbruck\n{abdelrahman.abdallah, jamshid.mozafari, bhawna.piryani,\nmohammed.ali, adam.jatowt}@uibk.ac.at\nAbstract\nKnowledge-intensive tasks, particularly open-\ndomain question answering (ODQA), docu-\nment reranking, and retrieval-augmented lan-\nguage modeling, require a balance between re-\ntrieval accuracy and generative flexibility. Tra-\nditional retrieval models such as BM25 and\nDense Passage Retrieval (DPR) efficiently re-\ntrieve from large corpora but often lack seman-\ntic depth. Generative models like GPT-4-o pro-\nvide richer contextual understanding but face\nchallenges in maintaining factual consistency.\nIn this work, we conduct a systematic evalua-\ntion of retrieval-based, generation-based, and\nhybrid models, with a primary focus on their\nperformance in ODQA and related retrieval-\naugmented tasks. Our results show that dense\nretrievers, particularly DPR, achieve strong per-\nformance in ODQA with a top-1 accuracy of\n50.17% on NQ, while hybrid models improve\nnDCG@10 scores on BEIR from 43.42 (BM25)\nto 52.59, demonstrating their strength in doc-\nument reranking.\nAdditionally, we analyze\nlanguage modeling tasks using WikiText-103,\nshowing that retrieval-based approaches like\nBM25 achieve lower perplexity compared to\ngenerative and hybrid methods, highlighting\ntheir utility in retrieval-augmented generation.\nBy providing detailed comparisons and prac-\ntical insights into the conditions where each\napproach excels, we aim to facilitate future op-\ntimizations in retrieval, reranking, and genera-\ntive models for ODQA and related knowledge-\nintensive applications1.\n1\nIntroduction\nThe increasing complexity of knowledge-intensive\ntasks, particularly open-domain question answer-\ning (ODQA) and retrieval-augmented applica-\ntions, necessitates advanced approaches to effi-\nciently retrieve and generate relevant informa-\ntion. Traditionally, retrieval-based methods have\n1The code and the dataset will be available after acceptance\nof the paper.\nplayed a central role in these tasks, with mod-\nels like BM25 (Robertson and Zaragoza, 2009)\nserving as foundational tools for extracting rel-\nevant documents.\nHowever, the limitations of\nkeyword-based retrieval prompted the development\nof dense retrieval models such as Dense Passage\nRetrieval (DPR) (Karpukhin et al., 2020a) and\nContriever (Izacard et al., 2021), which leverage\ntransformer-based architectures to encode queries\nand documents into dense representations. While\ndense retrieval models improve over sparse meth-\nods, they introduce new challenges.\nFirst, re-\ntrieval corpora are typically divided into fixed\nchunks (Karpukhin et al., 2020a), which can lead\nto retrieving irrelevant content.\nSecond, dual-\nencoder architectures encode queries and docu-\nments separately, limiting direct interaction be-\ntween them (Khattab et al., 2021). Finally, dense\nretrieval models require pre-encoding and storing\ndocument embeddings, which constrains scalability\nand hinders their ability to leverage large language\nmodels (LLMs) (Levine et al., 2022).\nTo address these limitations, generative models\nsuch as GPT-3.5 and InstructGPT (Brown et al.,\n2020; Ouyang et al., 2022) offer an alternative\nby directly generating contextualized responses in-\nstead of retrieving existing documents. Approaches\nlike GenRead (Yu et al., 2022) first generate rel-\nevant text and then use it for answer prediction.\nHowever, generative models often struggle with\nfactual consistency and may hallucinate informa-\ntion (Huang et al., 2023), making them less re-\nliable for knowledge-intensive tasks. Given the\ntrade-offs between retrieval and generation, hy-\nbrid models have emerged to integrate the strengths\nof both approaches. Merging Generator and Re-\ntriever (MGR) (Abdallah and Jatowt, 2023; Zhang\net al., 2023) combines generated and retrieved doc-\numents, allowing models to refine answers while\nmaintaining factual accuracy. However, hybrid\nmodels introduce challenges in document selec-\narXiv:2502.20245v1  [cs.CL]  27 Feb 2025\n\ntion, requiring effective reranking strategies to\nprioritize relevant information. Recent work in\nRetrieval-Augmented Generation (RAG) and In-\nContext Learning (ICL) (Lewis et al., 2020; Ram\net al., 2023) highlights the importance of reranking\ntechniques in improving answer quality.\nThis paper extends prior work on retrieval-\naugmented language models (Ram et al., 2023),\nby incorporating generated documents into the re-\ntrieval process. We evaluate retrieval, generation,\nand hybrid models with a primary focus on ODQA\nand retrieval-augmented tasks, examining their im-\npact on document selection, answer generation, and\nlanguage modeling. Figure 1 provides an overview\nof these tasks and the experimental setup, illustrat-\ning how different approaches are compared in terms\nof retrieval effectiveness, generative capability, and\nreranking strategies. First, we compare the accu-\nracy of retrieval models such as BM25 (Robertson\nand Zaragoza, 2009), MSS (Sachan et al., 2021),\nMSS-DPR (Sachan et al., 2021), Contriever (Izac-\nard et al., 2021), and DPR (Karpukhin et al., 2020a)\nwith generation-based models. Next, we examine\nhow combining retrieval and generation methods\naffects performance in hybrid models. We also in-\nvestigate strategies for reranking documents\u2014an\nessential step in hybrid models\u2014to determine how\nbest to select the most relevant content for down-\nstream tasks. Additionally, we investigate how\nhybrid models combining retrieval and genera-\ntion perform in ODQA and Information Retrieval\n(IR) tasks, evaluating datasets like BEIR (Thakur\net al., 2021) and TREC (Craswell et al., 2020).\nFinally, we evaluate the impact of these methods\non language modelling in reducing perplexity. In\ngeneral, we try to answer a question: Which ap-\nproach\u2014retriever, generator, or hybrid\u2014is best\nsuited for ODQA, language modeling and infor-\nmation retrieval tasks? Our contributions are as\nfollows:\n1. We provide a comparison of retrieval and\ngeneration-based models, focusing on their ef-\nfectiveness in ODQA and retrieval-augmented\napplications by incorporating generated docu-\nments into the retrieval process.\n2. We evaluate the impact of combining retrieval\nand generation methods in hybrid models, ex-\namining how these models perform across dif-\nferent tasks.\n3. We explore advanced document reranking\nLLaMA 3.1 70B\nGPT 3.5\nBM25\nMSS-DPR\nContriever\nUPR\nRankGPT\nDPR\nMSS\nBM25\nOpen-Domain QA\nInformation Retrieval\nLanguage Modelling\nRAG\nRetriever\nGenerator\nLLaMA 3.1 70B\nGPT 3.5\nGenerator\nRetriever\nBM25\nLLaMA 3.1 70B\nGPT 3.5\nGenerator\nRetriever\nReranker\nRAG\nEM\nF1\nRecall\nContains\nMetrics\nUPR\nRankGPT\nReranker\nnCDG@\nMetrics\nUPR\nRankGPT\nReranker\nPerplexity\nMetrics\nTriviaQA\nNaturalQA\nWebQ\nDataset\nBIER\nTREC\nDataset\nWikiText 103\nDataset\nFigure 1: Overview of experimental setup across the three\ntasks of open-domain question answering (QA), Informa-\ntion Retrieval, and Language Modeling.\nmethods, demonstrating how reranking en-\nhances Information Retrieval (IR) accuracy\nand improves the performance of hybrid mod-\nels in ODQA tasks.\n4. We\nprovide\npractical\ninsights\ninto\nthe\nstrengths and limitations of retrieval, gener-\nation, and hybrid approaches in ODQA and\nretrieval-augmented applications\n2\nRelated Work\nAdvancements in Open-Domain Question Answer-\ning (ODQA), Document Reranking, and Language\nModeling (LM) have led to three main approaches:\nretriever-based, generator-based, and hybrid mod-\nels, along with retrieval-augmented techniques for\nimproving factual consistency in text generation.\nRetriever-based methods focus on identifying\nrelevant documents before processing. Sparse mod-\nels like BM25 (Robertson and Zaragoza, 2009) rely\non lexical matching, while dense retrieval meth-\nods such as DPR (Karpukhin et al., 2020a) and\nContriever (Izacard et al., 2021) encode queries\nand documents into dense vectors for improved re-\ntrieval. Further refinements include ANCE (Xiong\net al., 2020), ColBERT (Khattab et al., 2021), and\nMSS-DPR (Sachan et al., 2021). Some models\nbypass full-document retrieval, using dense phrase\nretrieval (Lee et al., 2020; Seo et al., 2019) to ex-\ntract answer spans directly.\nGenerator-based models generate responses\nrather than retrieving documents. LLMs like GPT-\n3.5 and InstructGPT (Brown et al., 2020; Ouyang\net al., 2022) perform well in open-ended QA but\nstruggle with factual accuracy, often hallucinating\n\ninformation (Huang et al., 2023).\nModels like\nGenRead (Yu et al., 2022) and DocGen (Askari\net al., 2023) generate contextual documents be-\nfore extracting answers. Pretrained models like T5-\nRC (Raffel et al., 2020) and BART-based retriever-\nreader architectures (Lewis et al., 2020) attempt to\nimprove reliability, but consistency issues persist.\nHybrid approaches integrate retrieval and gen-\neration, balancing factual grounding with contex-\ntual flexibility. RAG (Lewis et al., 2020) condi-\ntions generation on retrieved documents, while\nFiD (Izacard and Grave, 2020) enhances multi-\ndocument conditioning. Merging Generator and\nRetriever (MGR) (Abdallah and Jatowt, 2023;\nZhang et al., 2023) dynamically selects between\nretrieved and generated content. Neural rerankers\nlike MonoBERT (Nogueira and Cho, 2019) and\nDuoBERT (Nogueira et al., 2020) refine retrieval\nresults. Hybrid models have also been effective\nin In-Context Learning (ICL) (Ram et al., 2023),\nallowing LLMs to retrieve external knowledge dy-\nnamically.\nRetrieval-augmented language models improve\nfactual consistency by conditioning text genera-\ntion on retrieved knowledge.\nkNN-LM (Khan-\ndelwal et al., 2020) enhances predictions using\nnearest-neighbor retrieval but faces scalability\nchallenges (He et al., 2021; Alon et al., 2022).\nRetrieve-and-Read models like REALM (Guu\net al., 2020a) and RETRO (Borgeaud et al., 2022)\nintegrate retrieval into masked modeling or cross-\nattention to improve factual reliability. These ap-\nproaches bridge the gap between traditional LMs\nand knowledge-grounded systems, enhancing gen-\nerated response accuracy.\n3\nApproaches\n3.1\nRetriever Models\nLet D = d1, d2, . . . , dM be a collection of evi-\ndence documents representing a retrieval corpus.\nGiven a query q, an Information Retrieval (IR)\nmodel selects a subset of relevant passages Z \u2282D,\none or more of which will ideally contain the\ncorrect answer to q.\nOur setup supports pas-\nsages obtained from any retriever, whether based\non sparse representations like BM25 (Robertson\nand Zaragoza, 2009) or dense representations such\nas DPR (Karpukhin et al., 2020a), MSS (Sachan\net al., 2021), MSS-DPR (Sachan et al., 2021),\nand Contriever (Izacard et al., 2021). BM25 is a\ntraditional sparse retriever that ranks documents\nbased on term frequency-inverse document fre-\nquency (TF-IDF). DPR encodes queries and doc-\numents into dense vector representations using\npre-trained transformers. The similarity between\na query q and a document d is calculated as\nthe dot product of their dense embeddings, i.e.,\nsim(q, d) = EQ(q)\u22a4EP (d), where EQ and EP\nare the encoders for the query and document, re-\nspectively. In addition to DPR, we test models such\nas MSS, which focuses on masked salient span\nprediction, and MSS-DPR, which extends DPR\nwith additional pre-training using MSS. Another\ndense retriever, Contriever, is trained in an unsu-\npervised manner using contrastive learning on text\nparagraphs. We assume that each retriever pro-\nvides the top-K most relevant passages, denoted as\nZ = {z1, z2, . . . , zK}.\n3.2\nGenerator Model\nThe generator approach is an alternative to the tra-\nditional retriever. Rather than relying on retriev-\ning documents from an external corpus, it prompts\nan LLM to generate contextual documents based\non a query.\nFormally, given a question q, the\nmodel generates a set of contextual documents\nG = {g1, g2, . . . , gn}, where each document gi\nis generated by a large language model, such as\nInstructGPT (Ouyang et al., 2022), conditioned on\nthe query q.\n3.3\nZero-Shot LLM Reranking\nZero-shot retrieval reranking (Abdallah et al.,\n2025c,a) aims to reorder retrieved or generated\ndocuments within an Information Retrieval (IR)\npipeline, without relying on training data specific\nto the task or dataset. We explore two methods\nfor reranking: using LLMs to generate a question\nfrom a document and a zero-shot ranking approach\nbased on semantic matching.\nIn the first approach, inspired by the Unsuper-\nvised Passage Re-ranking (UPR) (Sachan et al.,\n2022), a pre-trained LLM estimates the likelihood\nof generating a question given the passage. For-\nmally, for each passage zi from the set of top-K re-\ntrieved passages Z, we compute the relevance score\np(q | zi), where q is the input question. The score\nis estimated by calculating the likelihood of gener-\nating the question q given the passage zi. The LLM\ncomputes the average log-likelihood of question to-\nkens: log p(q | zi) =\n1\n|q|\nP\nt log p(qt | q<t, zi; \u0398)\nwhere \u0398 are the parameters of the LLM. The gen-\nerated question serves as a query, and we rank the\n\npassages based on how well the passage can pro-\nduce the question. The second approach, called\nRankGPT (Sun et al., 2023), utilizes a permutation-\nbased approach for re-ranking top-K retrieved doc-\numents using LLMs like GPT-3.5 and GPT-4. Un-\nlike traditional methods that evaluate documents\nindependently, RankGPT optimizes the order of\nall retrieved documents by considering their rele-\nvance to a query. RankGPT inputs a set of retrieved\ndocuments into an LLM, where each document is\ntagged with an identifier (e.g., [1], [2], etc.). The\nLLM generates a ranking by outputting a permuta-\ntion of these identifiers based on their relevance to\nthe input query. This process directly produces a\nranked list without relying on intermediate scoring,\nleveraging the LLM\u2019s understanding of instructions\nfor ranking.\n3.4\nRetrieval Augmented Generation\nLanguage model\nIn-context learning enables LLMs to utilize exter-\nnal knowledge without altering their internal param-\neters. This approach enriches the context by incor-\nporating both retrieved and generated documents\ndirectly into the model\u2019s input sequence. Given a\nsequence of tokens x1, . . . , xn, where x1, . . . , xi\u22121\nrepresents the tokens preceding the current token\nxi, the goal is to predict xi. The standard for-\nmulation is: p(x1, . . . , xn) = Qn\ni=1 p\u03b8(xi|x<i),\nwhere p\u03b8(xi|x<i) is the conditional probability\nof generating xi based on its prefix x<i, with \u03b8\nrepresenting the model\u2019s parameters. We extend\nthis by incorporating a set of retrieved documents\nR(x<i) and a set of generated documents G(q),\nwhere q is the query derived from the input prefix.\nThe generation probability is: p(x1, . . . , xn) =\nQn\ni=1 p\u03b8(xi|[x<i; R(x<i); G(q)]), where [a; b; c]\ndenotes the concatenation of sequences a, b, and c.\nThis setup allows the LLM to condition its output\non both retrieved knowledge and newly generated\ncontent, providing a richer context for generation.\nFor each query q derived from the prefix to-\nkens x<i, a set of top-k relevant documents\n{d1, d2, . . . , dk} is retrieved, forming R(q). Simul-\ntaneously, the model generates a set of contextual\ndocuments G(q) = {g1, g2, . . . , gn}, where each\ngi is conditioned on q. These documents serve as\nadditional context to inform the answer generation.\nThe combined influence of retrieved and generated\ndocuments allows the model to maximize the like-\nlihood of generating the next token xi: di\u2217, gj\u2217=\narg maxd\u2208R(q),g\u2208G(q) p\u03b8(xi|[x<i; d; g]), where di\u2217\nis the most relevant retrieved document and gj\u2217is\nthe most informative generated document for the\ngiven prefix and query. By balancing the factual\naccuracy of retrieved documents with the creative\nand contextual insights from generated documents,\nthis hybrid approach enhances the model\u2019s ability\nto address complex information needs. Building\nupon (Ram et al., 2023), which focuses on retrieval-\naugmented language models (RALMs), we extend\nthe framework by incorporating generated docu-\nments into the retrieval process alongside retrieved\npassages. Unlike prior work, which primarily re-\nlies on external retrieval, we explore the trade-offs\nbetween retrieved and generated context within\nhybrid models, analyzing their impact on factual\nconsistency and response diversity. Our analysis\nin Section 4 reveals that while this hybrid retrieval-\naugmented generation approach improves recall\nin QA tasks, it introduces challenges related to re-\ntrieval redundancy, factual consistency, and ranking\nbiases in hybrid models.\n4\nExperimental Setup\n4.1\nDatasets\nThe evaluation is conducted for ODQA on Nat-\nural Questions (NQ) (Kwiatkowski et al., 2019),\nTriviaQA (Joshi et al., 2017) and WebQuestions\n(WebQ) (Berant et al., 2013), following the same\nsetup as in (Yu et al., 2022; Izacard and Grave,\n2020; Lee et al., 2019), while for Information Re-\ntrieval we use TREC (Craswell et al., 2020) and\nBEIR (Thakur et al., 2021) and finally WikiText-\n103 (Merity et al., 2016) for Language modelling.\nODQA datasets: NQ is derived from real user\nqueries made through Google Search, where an-\nswers are extracted as spans from Wikipedia arti-\ncles. The dataset consists of 79,168 examples for\ntraining, 8,757 for development, and 3,610 for test-\ning. TriviaQA, a dataset constructed from trivia\nand quiz-league websites, contains open-domain\nquestions with well-defined answers. For ODQA,\nwe use its unfiltered version, which includes 78,785\ntraining examples, 8,837 development examples,\nand 11,313 test examples. WebQ consists of ques-\ntions sourced via the Google Suggest API, with\nanswers mapped to Freebase entities. It includes\n3,417 training, 361 development, and 2,032 test\nexamples.\nEvidence Passages:\nFor ODQA, we used\nthe preprocessed English Wikipedia dump from\nDecember 2018, as released by Karpukhin et\n\nRetriever\nNQ\nTQA\nWEBQ\nTop-1\nTop-5\nTop-10\nTop-20\nTop-50\nTop-100\nTop-1\nTop-5\nTop-10\nTop-20\nTop-50\nTop-100\nTop-1\nTop-5\nTop-10\nTop-20\nTop-50\nTop-100\nUnsupervised Retrievers\nMSS\n19.28\n41.25\n51.27\n59.97\n69.56\n75.57\n30.76\n52.65\n60.52\n67.18\n74.58\n79.11\n11.66\n29.04\n39.12\n49.21\n61.12\n68.36\nBM25\n22.11\n43.77\n54.46\n62.94\n72.74\n78.25\n46.30\n66.28\n71.74\n76.41\n80.56\n83.15\n18.90\n41.83\n52.17\n62.40\n71.70\n75.49\nContriever\n22.16\n47.29\n58.73\n67.87\n76.01\n80.55\n34.16\n59.49\n68.00\n73.91\n79.84\n82.94\n19.98\n43.45\n56.40\n65.70\n74.85\n80.12\nSupervised Retrievers\nDPR\n48.67\n68.78\n74.54\n79.20\n83.71\n85.71\n57.47\n72.40\n76.50\n79.77\n82.97\n85.10\n44.83\n65.01\n70.62\n74.61\n78.69\n81.64\nMSS-DPR\n50.17\n71.88\n77.48\n81.44\n85.98\n88.14\n61.64\n75.21\n79.15\n81.85\n84.92\n86.58\n44.24\n65.01\n71.65\n76.92\n81.84\n84.55\nGenerator\nGenRead\n45.76\n65.32\n71.22\n-\n-\n-\n69.41\n79.72\n82.85\n-\n-\n-\n51.03\n69.05\n73.23\n-\n-\n-\nTable 1: Performance of different retrieval and generative models on Natural Questions (NQ), TriviaQA (TQA), and\nWebQuestions (WEBQ).\nal. (Karpukhin et al., 2020a). Each Wikipedia ar-\nticle is split into non-overlapping passages of 100\nwords. This corpus contains over 21 million pas-\nsages and serves as the evidence set from which rel-\nevant documents are retrieved for answering ques-\ntions in QA tasks, we downloaded and used this\ncorpus from Rankify, as described in (Abdallah\net al., 2025b).\nInformation Retrieval (IR): TREC (Craswell\net al., 2020) is a well-established benchmark\ndataset used in IR. For our evaluation, we utilize\nthe test sets from the 2019 and 2020 TREC Deep\nLearning (DL) tracks: (i) TREC-DL19, which in-\ncludes 43 queries, and (ii) TREC-DL20, compris-\ning 54 queries. BEIR Benchmark (Thakur et al.,\n2021): is a heterogeneous benchmark covering 18\nretrieval tasks, including fact-checking, question\nanswering, and domain-specific retrieval (biomedi-\ncal, scientific, etc.).\nLanguage Modeling: WikiText-103 (Merity\net al., 2016): is a large-scale dataset of over 100\nmillion tokens sourced from long, context-rich\nWikipedia articles. This dataset is a standard for\nevaluating language modeling tasks.\nAll experiments and dataset processing were con-\nducted using the Rankify2 framework, which pro-\nvides a unified toolkit for retrieval, re-ranking, and\nretrieval-augmented generation (Abdallah et al.,\n2025b).\n4.2\nRetrieval and Generative Models\nRetrieval Models: We used five retrieval mod-\nels in our experiments: BM25, a sparse vector-\nbased method; DPR, a dense dual-encoder model\nthat maximizes similarity between questions and\nrelevant passages; MSS, a dense retriever pre-\ntrained on predicting masked spans like named\nentities; MSS-DPR, combining MSS pre-training\nwith DPR\u2019s fine-tuning for improved performance;\nand Contriever, an unsupervised dense retriever\n2https://github.com/DataScienceUIBK/Rankify\nRetriever\nNQ\nWEBQ\nTop-1\nTop-5\nTop-10\nTop-20\nTop-50\nTop-100\nTop-1\nTop-5\nTop-10\nTop-20\nTop-50\nTop-100\nGenerator+UPR\nGenRead\n44.76\n64.18\n71.22\n-\n-\n-\n51.72\n67.13\n73.23\n-\n-\n-\nRetriever+UPR\nMSS\n35.90\n60.91\n66.70\n71.44\n74.10\n75.57\n29.97\n51.03\n58.46\n63.19\n66.93\n68.36\nBM25\n36.84\n61.72\n68.45\n72.63\n76.68\n78.25\n33.12\n56.45\n63.73\n69.14\n73.92\n75.49\nContriever\n36.73\n63.49\n71.69\n76.32\n79.50\n80.55\n33.96\n59.94\n67.18\n73.08\n78.20\n80.12\nDPR\n44.21\n71.86\n78.92\n82.16\n84.90\n85.71\n41.44\n67.18\n72.93\n77.12\n80.41\n81.64\nMSS-DPR\n43.74\n72.88\n80.44\n84.71\n87.26\n88.14\n39.96\n65.40\n73.08\n78.30\n82.63\n84.55\nCombined+UPR\nMSS\n43.16\n64.68\n75.32\n82.58\n85.37\n86.29\n48.87\n66.49\n72.83\n78.89\n81.20\n82.33\nBM25\n43.46\n64.52\n75.73\n82.96\n86.09\n87.09\n48.28\n66.78\n73.33\n78.94\n82.38\n83.76\nContriever\n43.27\n64.99\n76.34\n83.93\n87.15\n87.78\n47.93\n67.67\n73.97\n79.48\n83.56\n84.99\nDPR\n44.07\n65.90\n78.17\n86.68\n89.39\n90.11\n49.46\n68.06\n74.90\n81.89\n85.19\n86.27\nMSS-DPR\n43.96\n65.60\n77.87\n87.51\n90.42\n91.27\n48.13\n68.11\n74.46\n81.64\n85.48\n87.40\nGenerator+RankGPT\nGenRead\n50.97\n64.74\n71.22\n-\n-\n-\n55.71\n67.77\n73.23\n-\n-\n-\nRetriever+RankGPT\nMSS\n43.52\n63.19\n68.34\n70.28\n73.85\n75.57\n35.58\n53.30\n58.76\n61.91\n65.8\n68.36\nBM25\n48.98\n66.76\n70.86\n73.71\n76.40\n78.25\n42.27\n60.19\n65.75\n69.39\n73.43\n75.49\nContriever\n46.87\n67.09\n71.58\n75.29\n78.98\n80.55\n41.93\n63.24\n68.8\n73.23\n77.12\n80.12\nDPR\n50.47\n75.24\n80.00\n82.71\n84.88\n85.71\n48.28\n68.85\n74.26\n77.31\n79.82\n81.64\nMSS-DPR\n54.88\n75.35\n81.47\n84.88\n87.20\n88.14\n49.56\n69.83\n75.15\n79.28\n82.43\n84.55\nCombined+RankGPT\nMSS\n53.60\n68.37\n76.95\n81.88\n83.96\n86.12\n54.77\n68.6\n73.72\n76.43\n79.53\n82.04\nBM25\n53.46\n68.48\n76.98\n82.22\n84.65\n86.87\n55.07\n69.29\n75.34\n78.35\n81.5\n83.86\nContriever\n53.05\n68.25\n77.26\n83.05\n85.84\n87.67\n55.86\n68.90\n74.51\n78.74\n82.38\n84.84\nDPR\n56.15\n69.70\n80.08\n86.93\n88.92\n90.06\n56.10\n70.52\n78.30\n82.33\n84.25\n85.93\nMSS-DPR\n56.79\n70.55\n80.58\n87.73\n89.81\n91.16\n56.50\n69.59\n77.90\n82.53\n85.63\n87.30\nTable 2: Performance comparison of various retrieval\nand generation methods combined with UPR and\nRankGPT for reranking on NQ and WebQ datasets.\noptimized for zero-shot performance through con-\ntrastive learning.\nGenerative Models: For the generation-based\nretrieval, we employ GenRead (Yu et al., 2022) a\ngenerative model designed for open-domain QA\ntasks, which first generates contextual documents\nbased on the query and then predicts the final an-\nswer using those generated documents.\n4.3\nLanguage Models\nWe tested a range of LLMs in our experiments,\nfocusing on both generation and reranking tasks:\nGPT-2 (Radford et al., 2019): A transformer-based\nautoregressive language model trained on WebText.\nWe experimented with the small (110M), medium\n(345M), large (774M), and extra-large (1.5B) ver-\nsions of GPT-2 to observe how model size impacts\nperformance. OPT (Zhang et al., 2022): We ex-\nperimented with various OPT models ranging from\n125M to 13B parameters to analyze their perfor-\nmance across retrieval and generation tasks. GPT-\nNeo (Black et al., 2021): An autoregressive lan-\nguage model trained on the Pile dataset (Gao et al.,\n2021). We evaluated its performance on WikiText-\n103 using both retrieval and generation configura-\n\ntions.\n4.4\nReranking Methods\nWe explored two reranking techniques to optimize\nthe combination of retrieved and generated doc-\numents: UPR: Based on the T5 series (Raffel\net al., 2020), which consists of encoder-decoder\ntransformers pre-trained on text denoising tasks.\nWe used the T5-lm-adapt (Lester et al., 2021)\nand T0 (Sanh et al., 2022) models for reranking.\nRankGPT: A reranking approach using GPT-3.5\nand LLama 3 (70B) to evaluate the relevance of\nboth retrieved and generated documents. These\nmodels dynamically rank documents to ensure the\nmost relevant results are presented in QA tasks.\n5\nOpen-Domain QA Results\n5.1\nRetrieval and Generation Results:\nIn this section, we evaluate the performance of vari-\nous retrieval-based models and a generative model,\nGenRead, on three open-domain QA datasets: NQ,\nTriviaQA, and WebQ. We compare unsupervised re-\ntrievers (BM25, MSS, and Contriever), supervised\nretrievers (DPR and MSS-DPR), and the GenRead\ngenerative model to understand their effectiveness\nin different retrieval settings. Table 1 presents the\nresults for Top-1, Top-5, Top-10, Top-20, Top-50,\nand Top-100 retrieval accuracies across the three\ndatasets. For the unsupervised retrieval models,\nContriever achieves the highest Top-1 accuracy\non NQ, outperforming BM25 and MSS by captur-\ning deeper semantic relationships between queries\nand passages, consistent with findings by (Izacard\net al., 2021). However, the supervised retrievers\nshow a clear advantage, with MSS-DPR achiev-\ning the highest Top-1 accuracy of 50.17% on NQ,\ndemonstrating the impact of training on specific\nQA datasets like NQ (Sachan et al., 2021).\nThe generative model, GenRead, achieves com-\npetitive performance, particularly in TriviaQA,\nwhere it outperforms all retrievers with a Top-1\naccuracy of 69.41%. This indicates the model\u2019s\nability to generate contextually relevant passages\neven when traditional retrieval may fall short. How-\never, generating more passages like retrieving doc-\numents (such as Top-50 or Top-100) increases the\ncomputational costs associated with generating doc-\numents and the risk of repetitive content (Ganguli\net al., 2022). Generative models require signifi-\ncant resources to create contextually relevant docu-\nments, which becomes increasingly demanding as\nthe number of generated documents increases. Ad-\nditionally, there is a tendency for generative mod-\nels to produce similar or redundant outputs when\ntasked with generating numerous responses for a\nsingle query (Ganguli et al., 2022).\n5.2\nRe-ranking Results\nIn this section, we evaluate the impact of com-\nbining retrieval and generation through reranking\nmethods, specifically using UPR and RankGPT, to\nrefine document selection for open-domain tasks.\nTable 2 illustrates the performance of different\nretrieval models paired with UPR and RankGPT\nacross NQ and WebQ datasets. UPR enhances\nthe precision of document ranking, as evidenced\nby DPR\u2019s improvement in Top-10 accuracy from\n74.54% to 80.44% on the NQ dataset when com-\nbined with UPR. RankGPT, which leverages the\nsemantic capabilities of large language models for\nreranking, achieves further gains, particularly for\nhybrid methods. For example, MSS-DPR with\nRankGPT achieves a Top-10 accuracy of 81.47%\non NQ, compared to 77.87% without reranking.\nCombining retrieval and generative outputs, es-\npecially with methods like Combined+RankGPT,\nprovides a balance between broad retrieval cov-\nerage and specific contextual information from\nLLMs. This is demonstrated by the MSS-DPR\nmethod achieving a Top-100 accuracy of 91.16%\non NQ when paired with RankGPT. Addition-\nally, on the TriviaQA dataset as shown in Fig-\nure 3 (see Appendix A.1), the UPR method\u2019s\nperformance comparison reveals that the Gen-\nRead+UPR retriever reaches the highest Top-1\naccuracy at 69.74%, while DPR+GenRead+UPR\nand MSS-DPR+Gen+UPR follow closely with\nTop-1 accuracies of 67.50% and 67.30%, respec-\ntively. Hybrid methods like BM25+Gen+UPR and\nDPR+Gen+UPR excel in Top-10 accuracy, achiev-\ning 84.41% and 85.03%, respectively, showing the\nbenefit of combining generative context with re-\ntrieval outputs. However, we note that RankGPT\u2019s\nreranking process is computationally expensive,\ncosting over 1, 500 for evaluations across NQ and\nWebQA, which limited its application to only UPR\nfor the TriviaQA dataset in our experiments.\n5.3\nIn-Context for Open-Domain QA\nIn this section ,we evaluate a Retrieval-Augmented\nGeneration (RAG) setting, where retrieval-based\nmodels (BM25, MSS-DPR) are combined with gen-\nerative models (e.g., InstructGPT, GenRead). The\n\nRetriever\n# Mode\nLLama-3.3 8B\nLLama-3.1 8B\nGemma-2-2b\nGemma-2-9b\nLlama-2-13b-hf\nMistral-7B-v0.1\nNQ\nTriviaQA\nWebQA\nNQ\nTriviaQA\nWebQA\nNQ\nTriviaQA\nWebQA\nNQ\nTriviaQA\nWebQA\nNQ\nTriviaQA\nWebQA\nNQ\nTriviaQA\nWebQA\nG\n24.68\n52.23\n15.45\n21.49\n48.74\n14.51\n27.01\n59.91\n19.34\n28.28\n63.02\n18.65\n28.06\n62.64\n20.32\n27.01\n62.64\n16.09\nBM25\nR\n14.90\n42.10\n10.23\n12.82\n40.13\n9.25\n14.02\n43.28\n14.71\n19.81\n57.55\n14.96\n21.14\n57.90\n19.54\n11.19\n52.85\n6.40\nR+G\n25.51\n53.29\n15.05\n22.29\n49.69\n13.97\n28.39\n59.89\n19.29\n28.45\n63.50\n19.05\n26.62\n61.35\n19.00\n25.68\n60.45\n15.65\nG+R\n25.67\n53.24\n15.00\n23.29\n50.29\n13.43\n28.50\n59.87\n19.73\n28.42\n62.94\n19.34\n26.79\n62.01\n19.24\n23.71\n58.56\n13.44\nUPR\n23.49\n55.85\n17.27\n24.24\n55.22\n17.67\n26.23\n58.71\n19.78\n23.41\n58.74\n15.94\n27.59\n61.60\n20.37\n25.18\n59.64\n17.18\nRankGPT\n28.45\n-\n19.73\n26.65\n-\n18.55\n30.36\n-\n21.11\n30.75\n-\n21.06\n29.22\n-\n21.99\n25.35\n-\n17.18\nMSS\nR\n12.82\n31.90\n7.38\n11.19\n30.74\n6.88\n13.96\n33.05\n14.71\n19.78\n50.93\n14.96\n21.52\n51.75\n20.62\n11.08\n42.69\n6.40\nR+G\n25.54\n51.39\n14.96\n21.57\n47.75\n14.46\n28.48\n58.63\n19.29\n28.56\n62.53\n18.80\n26.81\n58.64\n19.05\n25.90\n56.28\n14.57\nG+R\n25.31\n51.81\n15.20\n22.68\n48.75\n13.43\n28.42\n58.47\n19.78\n28.31\n61.78\n18.55\n27.48\n59.56\n18.75\n23.10\n52.15\n13.09\nUPR\n23.35\n53.78\n17.08\n24.43\n54.97\n16.70\n26.20\n58.15\n19.73\n23.10\n57.50\n15.85\n27.29\n59.23\n20.72\n23.77\n57.24\n17.42\nRankGPT\n28.17\n-\n19.05\n25.84\n-\n17.37\n29.17\n-\n19.93\n29.97\n-\n19.64\n27.56\n-\n20.77\n23.77\n-\n16.98\nContriever\nR\n15.29\n36.25\n10.67\n13.24\n35.29\n9.35\n13.96\n33.05\n14.71\n19.78\n50.93\n14.96\n20.47\n42.69\n19.98\n11.08\n42.69\n6.40\nR+G\n25.59\n51.78\n15.10\n22.18\n48.38\n13.87\n28.78\n58.86\n20.28\n28.84\n62.85\n19.59\n27.45\n56.43\n19.59\n25.35\n56.43\n15.31\nG+R\n25.70\n52.07\n15.50\n23.13\n48.91\n13.87\n28.75\n58.64\n20.13\n28.37\n61.97\n19.09\n27.17\n52.66\n19.88\n23.02\n54.39\n13.44\nUPR\n23.24\n53.48\n17.32\n24.21\n55.71\n17.62\n26.26\n58.37\n19.83\n23.21\n57.64\n16.14\n26.57\n59.60\n20.72\n25.10\n57.26\n17.27\nRankGPT\n30.55\n-\n19.78\n28.86\n-\n17.62\n32.11\n-\n20.67\n32.44\n-\n19.88\n30.39\n-\n20.72\n25.10\n-\n17.27\nDPR\nR\n28.08\n45.88\n19.83\n23.21\n43.62\n14.32\n13.99\n33.05\n14.71\n19.78\n50.93\n14.96\n21.94\n51.07\n19.83\n11.11\n42.69\n6.40\nR+G\n28.94\n54.41\n24.62\n24.62\n50.61\n14.32\n30.25\n60.16\n20.18\n30.83\n63.72\n19.93\n28.81\n58.00\n20.32\n27.70\n58.00\n16.44\nG+R\n28.14\n54.50\n25.51\n25.01\n51.65\n14.81\n27.92\n60.17\n20.72\n29.92\n63.18\n19.39\n27.92\n60.01\n20.72\n25.01\n54.64\n14.71\nUPR\n23.60\n53.41\n18.06\n24.74\n56.07\n19.64\n26.51\n58.71\n19.78\n23.38\n57.85\n16.04\n27.45\n60.05\n20.62\n25.25\n57.53\n17.17\nRankGPT\n31.74\n-\n20.42\n29.58\n-\n19.64\n34.16\n-\n22.15\n34.04\n-\n21.01\n32.77\n-\n22.15\n25.26\n-\n17.18\nMSS+DPR\nR\n28.17\n47.69\n23.68\n23.68\n45.72\n13.92\n13.96\n33.05\n14.71\n19.78\n50.93\n14.96\n21.47\n51.35\n19.83\n11.08\n42.69\n6.40\nR+G\n29.41\n54.53\n24.09\n24.09\n50.72\n14.96\n28.48\n60.56\n19.29\n28.45\n62.53\n18.96\n28.45\n59.50\n19.78\n27.40\n58.03\n16.44\nG+R\n28.61\n54.48\n25.54\n25.07\n52.20\n14.76\n28.61\n60.25\n19.78\n28.31\n61.78\n18.55\n28.73\n59.96\n20.77\n25.07\n54.39\n14.96\nUPR\n23.10\n53.65\n16.68\n25.24\n55.52\n18.98\n23.10\n58.64\n16.68\n25.24\n57.47\n16.09\n23.10\n60.00\n19.98\n25.23\n57.47\n16.08\nRankGPT\n31.94\n-\n21.41\n29.95\n-\n18.99\n32.51\n-\n21.41\n32.13\n-\n18.99\n32.61\n-\n21.95\n25.24\n-\n16.09\nTable 3: Zero-shot results of in-context learning on the test set of NQ, TriviaQA, and WebQ measured by exact\nmatch. Bold values indicate the best performance within each retriever, while underlined values represent the\nbest overall performance. Please refer to the Appendix for additional evaluation metrics (e.g., Recall and F1-score)\npresented in Tables 7, 8, and 9.\nGPT-2 S\nGPT-2 M\nGPT-2 L\nGPT-2 XL\nopt-125m\nopt-350m\nopt-1.3b\nopt-2.7b\nopt-6.7b\nopt-13b\ngpt-neo-1.3B\ngpt-neo-2.7B\ngpt-neo-6B\nLanguage Model\n10\n15\n20\n25\n30\n35\n40\n45\n50\nPerplexity\n42.20\n33.23\n27.61\n24.91\n48.12\n37.22\n23.74\n20.94\n17.52\n16.39\n20.45\n18.35\n13.73\n29.56\n21.45\n18.08\n16.56\n32.03\n24.41\n15.91\n13.80\n11.72\n10.83\n14.62\n12.85\n10.05\n32.14\n24.46\n20.89\n19.22\n36.56\n28.20\n18.60\n16.87\n14.13\n13.35\n16.53\n14.97\n11.58\n34.77\n27.12\n23.09\n20.94\n39.39\n30.57\n20.25\n17.84\n15.07\n14.23\n17.38\n15.76\n12.09\nLlama-3.3 70B Generator Document\nContext\nNo Context\nR\nG\nR+G\nG+R\nGPT-2 S\nGPT-2 M\nGPT-2 L\nGPT-2 XL\nopt-125m\nopt-350m\nopt-1.3b\nopt-2.7b\nopt-6.7b\nopt-13b\ngpt-neo-1.3B\ngpt-neo-2.7B\ngpt-neo-6B\nLanguage Model\n10\n15\n20\n25\n30\n35\n40\n45\nPerplexity\n42.20\n33.23\n27.61\n24.91\n48.12\n37.22\n23.74\n20.94\n17.52\n16.39\n20.45\n18.35\n13.73\n39.27\n28.68\n23.83\n21.50\n43.58\n32.32\n20.34\n17.54\n14.63\n13.46\n18.83\n16.35\n12.32\n30.92\n22.78\n19.17\n17.51\n34.23\n25.71\n16.68\n14.67\n12.31\n11.41\n15.53\n13.70\n10.58\n31.56\n23.48\n19.69\n17.91\n34.88\n26.27\n17.01\n14.80\n12.47\n11.52\n15.70\n13.86\n10.73\nGPT3.5 Generator Document\nContext\nNo Context\nR\nG\nR+G\nG+R\nPerplexity results on the WikiText-103 test set for various models\nFigure 2: Perplexity Comparison for Language Modeling with Retrieval, Generation, and Hybrid Context Strategies\nusing Different Generator Documents. The figure is divided into two subplots, each representing a different\ndocument generator used for providing context: (a) Llama-3 70B Generator Document and (b) GPT3.5 Generator\nDocument. In both subplots, language model perplexity is evaluated under several context strategies: \u2019No Context\u2019\n(baseline), \u2019R\u2019 (Retrieval-only using BM25 from Wikipedia), \u2019G\u2019 (Generation-only, context from a generated\ndocument), \u2019R+G\u2019 (Retrieval followed by Generation), and \u2019G+R\u2019 (Generation followed by Retrieval).\nretrieved documents serve as input to a large lan-\nguage model, which generates the final response.\nSimilar to prior works (Karpukhin et al., 2020b;\nIzacard and Grave, 2020), our setup involves re-\ntrieval (R), generation (G), and hybrid models\n(R+G, G+R). Furthermore, we implement rerank-\ning techniques such as UPR and RankGPT to re-\nfine the document selection process. Our reader\nmodel (LLama-3.3 8B) gets the question along\nwith its corresponding retrieved documents and\nreturns the answer.\nReader models are simply\na frozen large LM (not pre-trained, fine-tuned).\nTable 3, shows that hybrid models (R+G, G+R)\nwhen combined with reranking approaches such as\nRankGPT offer a more balanced approach across\nvarious datasets. For example, BM25 R+G achieves\na score of 25.51 on NQ, in comparison with both\nretrieval-only (14.90) and generation-only (24.68)\nmodels in the LLama V3.3 8B setup. In particu-\nlar, RankGPT consistently enhances performance,\nwith BM25+RankGPT achieving 28.45 on NQ and\n19.73 on WebQA, which highlights the effective-\nness of reranking in refining document selection\nfor question answering.\n5.4\nSupervised open-domain QA\nIn this section, we evaluate the Fusion-in-Decoder\n(FiD) model (Izacard and Grave, 2020) using\nLLama-2 7B. We integrate retrieved documents\nfrom DPR with generated content, leveraging both\nretrieval and generative for enhanced question-\nanswering EM. We compare Finetuned LLama-2\n7B with other Retrieve-Read models, including\nDPR (Karpukhin et al., 2020a), REALM (Guu\net al., 2020b), RAG (Lewis et al., 2020), and GEN-\nREAD (Yu et al., 2022). Table 4 presents the EM\n\nModels\nNQ\nTriviaQA\nWebQ\nRetriever Only\nREALM (Guu et al., 2020b)\n40.4\n-\n40.7\nDPR (Karpukhin et al., 2020a)\n41.5\n56.8\n41.1\nRAG (Lewis et al., 2020)\n44.5\n56.1\n45.2\nFiD-l (Izacard and Grave, 2020)\n46.7\n61.9\n48.1\nFiD-xl (Izacard and Grave, 2020)\n50.1\n66.3\n50.8\nFiD-xl (Izacard and Grave, 2020)\n45.0\n70.1\n53.6\nFiD (Izacard and Grave, 2020)\n51.4\n67.6\n50.5\nEMDR (Singh et al., 2021)\n52.5\n71.4\n48.7\nRFiD-large (Wang et al., 2023)\n54.3\n72.6\n-\nDensePhrases (Lee et al., 2020)\n14.5\n34.4\n17.3\nDensePhrases (Lee et al., 2021)\n41.3\n53.5\n41.5\nGenerator\nGenRead (FiD-l) (Yu et al., 2022)\n40.3\n67.8\n51.5\nGenRead (FiD-l) (Yu et al., 2022)\n43.5\n70.2\n53.3\nGenRead (FiD-xl) (Yu et al., 2022)\n42.6\n69.6\n52.6\nGenRead (FiD-xl) (Yu et al., 2022)\n45.6\n71.6\n54.4\nGenerator and Retriever\nCombine Document\n57.4\n75.7\n53.6\nTable 4: Exact match (EM) performance of Llama 2-7B\ntrained on retrieved and generated documents compared\nto baseline models across NQ, TriviaQA, and WebQ\ndatasets using DPR and Generated Documents.\nMethod\nDL19 DL20 Covid NFCorpus Touche DBPedia SciFact Signal News Robust04 BEIR (Avg)\nBM25\n50.58 47.96 59.47\n30.75\n44.22\n31.80\n67.89\n33.05 39.52\n40.70\n43.42\nSupervised\nmonoBERT (340M)\n70.50 67.28 70.01\n36.88\n31.75\n41.87\n71.36\n31.44 44.62\n49.35\n47.16\nmonoT5 (220M)\n71.48 66.99 78.34\n37.38\n30.82\n42.42\n73.40\n31.67 46.83\n51.72\n49.07\nmonoT5 (3B)\n71.83 68.89 80.71\n38.97\n32.41\n44.45\n76.57\n32.55 48.49\n56.71\n51.36\nTART-Rerank (FLAN-T5-XL)\n-\n-\n75.10\n36.03\n27.46\n42.53\n74.84\n25.84 40.01\n50.75\n-\nCohere Rerank-v2\n73.22 67.08 81.81\n36.36\n32.51\n42.51\n74.44\n29.60 47.59\n50.78\n49.45\nCohere Embedding-large\n-\n-\n80.10\n34.70\n27.60\n37.20\n72.10\n30.60 46.10\n48.90\n47.16\nOpenAI Embedding-ada\n-\n-\n81.30\n35.80\n28.00\n40.20\n73.60\n32.90 49.50\n50.90\n49.02\nUnsupervised\nUPR (FLAN-T5-XL)\n53.85 56.02 68.11\n35.04\n19.69\n30.91\n72.69\n31.91 43.11\n42.43\n42.99\nPromptagator++ (zero-shot)\u2020\n-\n-\n76.0\n36.0\n27.8\n41.3\n73.6\n-\n-\n-\n-\nPromptagator++ (few-shot)\n-\n-\n76.2\n37.0\n38.1\n43.4\n73.1\n-\n-\n-\n-\nBM25+RankGPT\n65.80 62.91 76.67\n35.62\n36.10\n44.47\n70.43\n32.12 48.85\n50.62\n49.37\nBM25+GenRead+RankGPT\n72.21 68.93 82.81\n38.59\n35.82\n45.64\n75.46\n33.64 50.35\n58.48\n52.59\nTable 5: Results (nDCG@10) on TREC and BEIR.\nBest performing unsupervised and overall system(s)\nare marked bold. For generating documents we used\nLLama-3.1 70B.\nfor Llama 2-7B trained on retrieved and generated\ndocuments across the benchmarks. As seen, DPR\ncombined with the generative model achieves com-\npetitive results, with an EM score of 57.4 on the\nNQ test set, 75.7 on TriviaQA, and 53.6 on WebQ.\nThis performance is compared with baseline mod-\nels such as REALM, and the FiD variants, showing\nimprovements in most cases. For instance, the\nTrained LLama Model on generated and retrieved\noutperforms FiD-xl (50.1 EM on NQ) with a 7.3%\nincrease when using DPR.\n6\nLanguage Modeling Performance\nIn this section, we explore the performance of\nretrieval (R), generation (G), and hybrid mod-\nels (R+G and G+R) for language modeling on\ntheir impact on perplexity, a widely used evalu-\nation metric for language models. The models\nare evaluated on the WikiText-103 test set us-\ning GPT2, OPT and GPT-Neo, as presented in Fig-\nure 2 (see Table 6 in the Appendix). We aim to\nanalyse how retrieving documents from Wikipedia\naffects perplexity and can generate documents\nthat can help also like in the ODQA task. Table\n6 shows that using GPT-2 with BM25 retrieval\nachieves a perplexity of 29.56, outperforming the\ngeneration-based (Llama-3.3 70, GPT3.5) model,\nwhich yields a perplexity of 42.20 and 39.27. The\nperplexity of retrieval models is Perplexity =\nexp\n\u0010\n\u22121\nN\nPN\ni=1 log p\u03b8(wi|context)\n\u0011\n,\nwhere N\nrepresents\nthe\ntotal\nnumber\nof\nwords\nand\np\u03b8(wi|context), where wi is the i-th word in the\nsequence, and p\u03b8(wi|context) represents the proba-\nbility assigned by the model \u03b8 given the retrieved\ncontext. Hybrid models that combine retrieval and\ngeneration in two configurations do not outperform\nretrieval-only models as seen in Table 6. For in-\nstance, in LLama-3.3 70B, the R+G setup yields a\nperplexity of 32.14, compared to the retrieval-only\nmodel\u2019s 24.91.\n7\nInformation Retrieval Performance\nFinally,\nwe\npresent\nan\nevaluation\nof\nthe\nBM25+GEN+RankGPT method against state-of-the-\nart supervised and unsupervised models for in-\nformation retrieval on the TREC and BEIR\nbenchmarks.\nWe focus on nDCG@10 scores\nacross BEIR datasets.\nThe supervised base-\nlines include monoBERT (Nogueira and Cho,\n2019), monoT5 (Nogueira et al., 2020), and Co-\nhere Rerank.\nThe unsupervised baselines in-\nclude UPR (Sachan et al., 2022), and Promp-\ntagator++ (Dai et al., 2022).\nTable 5 presents\nthat the BM25+GEN+RankGPT consistently outper-\nforms BM25 across all benchmarks, achieving the\nhighest nDCG@10 scores on BEIR and TREC\ndatasets.\nFor example, on Robust04 dataset,\nBM25+GEN+RankGPT achieves an nDCG@10 score\nof 58.48, compared to 43.42 for BM25. Similarly,\non SciFact, the hybrid model reaches 45.64, out-\nperforming both supervised and unsupervised base-\nlines like monoT5 (44.45) and UPR (30.91).\n8\nConclusion\nThis study compares retrieval-based, generation-\nbased, and hybrid models across QA, reranking,\ninformation retrieval, and language modeling. Re-\ntrieval models like BM25 and DPR excel in factual\naccuracy, while generative models provide contex-\ntual richness but struggle with consistency. Hybrid\nmodels effectively balance retrieval and genera-\n\ntion, enhancing QA and IR performance. However,\nin language modeling, hybrid and generative ap-\nproaches do not consistently outperform retrieval-\nbased methods, underscoring the importance of\nretrieval for factual accuracy.\nLimitations\nWhile our study demonstrates promising results in\nopen-domain question answering (ODQA), docu-\nment reranking, and retrieval-augmented language\nmodeling, several limitations warrant further atten-\ntion:\n1. The computational complexity of hybrid mod-\nels, which combine retrieval and generation,\nincreases with both the size of the corpus and\nthe length of documents. This can lead to\nslower processing times, especially for large-\nscale datasets.\n2. The effectiveness of dense retrievers like DPR\nis highly dependent on the quality and diver-\nsity of the corpus used for training. Poorly\nrepresentative datasets may lead to reduced\nperformance in real-world applications.\n3. While hybrid models show significant im-\nprovements in document reranking, they are\nsensitive to the interplay between the retrieval\nand generation components.\nInconsistent\nalignment between these components could\nlead to suboptimal performance in certain sce-\nnarios.\n4. Our evaluation is primarily limited to standard\nbenchmarks, such as NQ and BEIR, which\nmay not fully capture the diverse nature of\nreal-world knowledge-intensive tasks.\nBe-\nsides other types of questions and retrieval\ntasks, the analysis should be extended to\ndomain-specific scenarios, especially ones\nwith low tolerance for errors and hallucina-\ntions like Medical (Kim et al., 2024) or Legal\nQA (Abdallah et al., 2023).\nEthical Considerations and Licensing\nOur research utilizes the GPT models, which is\navailable under the OpenAI License and Apache-\n2.0 license, and the Llama model, distributed un-\nder the Llama 3 Community License Agreement\nprovided by Meta. We ensure all use cases are\ncompliant with these licenses. Additionally, the\ndatasets employed are sourced from repositories\npermitting academic use. We are releasing the ar-\ntifacts developed during our study under the MIT\nlicense to facilitate ease of use and adaptations by\nthe research community. We have ensured that all\ndata handling, model training, and dissemination\nof results are conducted in accordance with ethical\nguidelines and legal stipulations associated with\neach used artifact.\nReferences\nAbdelrahman Abdallah and Adam Jatowt. 2023.\nGenerator-retriever-generator: A novel approach to\nopen-domain question answering. arXiv preprint\narXiv:2307.11278.\nAbdelrahman Abdallah, Jamshid Mozafari, Bhawna\nPiryani, Mohammed M. Abdelgwad, and Adam Ja-\ntowt. 2025a. DynRank: Improve passage retrieval\nwith dynamic zero-shot prompting based on ques-\ntion classification. In Proceedings of the 31st Inter-\nnational Conference on Computational Linguistics,\npages 4768\u20134778, Abu Dhabi, UAE. Association for\nComputational Linguistics.\nAbdelrahman Abdallah, Jamshid Mozafari, Bhawna\nPiryani, Mohammed Ali, and Adam Jatowt. 2025b.\nRankify: A comprehensive python toolkit for re-\ntrieval, re-ranking, and retrieval-augmented gener-\nation. arXiv preprint arXiv:2502.02464.\nAbdelrahman Abdallah, Jamshid Mozafari, Bhawna\nPiryani, and Adam Jatowt. 2025c. Asrank: Zero-shot\nre-ranking with answer scent for document retrieval.\narXiv preprint arXiv:2501.15245.\nAbdelrahman Abdallah, Bhawna Piryani, and Adam\nJatowt. 2023. Exploring the state of the art in legal\nqa systems. Journal of Big Data, 10(1):127.\nUri Alon, Frank Xu, Junxian He, Sudipta Sengupta, Dan\nRoth, and Graham Neubig. 2022. Neuro-symbolic\nlanguage modeling with automaton-augmented re-\ntrieval. In ICML.\nArian Askari, Mohammad Aliannejadi, Chuan Meng,\nEvangelos Kanoulas, and Suzan Verberne. 2023. Ex-\npand, highlight, generate: RL-driven document gen-\neration for passage reranking. In Proceedings of the\n2023 Conference on Empirical Methods in Natural\nLanguage Processing, pages 10087\u201310099, Singa-\npore. Association for Computational Linguistics.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on freebase from\nquestion-answer pairs. In Proceedings of the 2013\nconference on empirical methods in natural language\nprocessing, pages 1533\u20131544.\nSid Black, Leo Gao, Phil Wang, Connor Leahy,\nand Stella Biderman. 2021.\nGPT-Neo:\nLarge\nScale Autoregressive Language Modeling with Mesh-\nTensorflow. If you use this software, please cite it\nusing these metadata.\n\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, Diego\nDe Las Casas, Aurelia Guy, Jacob Menick, Roman\nRing, Tom Hennigan, Saffron Huang, Loren Mag-\ngiore, Chris Jones, Albin Cassirer, Andy Brock,\nMichela Paganini, Geoffrey Irving, Oriol Vinyals,\nSimon Osindero, Karen Simonyan, Jack Rae, Erich\nElsen, and Laurent Sifre. 2022. Improving language\nmodels by retrieving from trillions of tokens. In\nICML.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In Advances in Neural Information Process-\ning Systems, pages 1877\u20131901.\nNick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel\nCampos, and Ellen M Voorhees. 2020. Overview\nof the trec 2019 deep learning track. arXiv preprint\narXiv:2003.07820.\nZhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo\nNi, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B\nHall, and Ming-Wei Chang. 2022. Promptagator:\nFew-shot dense retrieval from 8 examples. arXiv\npreprint arXiv:2209.11755.\nDeep Ganguli,\nDanny Hernandez,\nLiane Lovitt,\nAmanda Askell, Yuntao Bai, Anna Chen, Tom Con-\nerly, Nova Dassarma, Dawn Drain, Nelson Elhage,\net al. 2022. Predictability and surprise in large gen-\nerative models. In Proceedings of the 2022 ACM\nConference on Fairness, Accountability, and Trans-\nparency, pages 1747\u20131764.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2021.\nThe pile: An\n800gb dataset of diverse text for language modeling.\nCoRR, abs/2101.00027.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Ming-Wei Chang. 2020a. REALM: Retrieval-\naugmented language model pre-training. In ICML.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Mingwei Chang. 2020b. Retrieval augmented\nlanguage model pre-training. In International confer-\nence on machine learning, pages 3929\u20133938. PMLR.\nJunxian He,\nGraham Neubig,\nand Taylor Berg-\nKirkpatrick. 2021. Efficient nearest neighbor lan-\nguage models. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 5703\u20135714, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,\nZhangyin Feng, Haotian Wang, Qianglong Chen,\nWeihua Peng, Xiaocheng Feng, Bing Qin, et al. 2023.\nA survey on hallucination in large language models:\nPrinciples, taxonomy, challenges, and open questions.\narXiv preprint arXiv:2311.05232.\nGautier Izacard and Edouard Grave. 2020. Leverag-\ning passage retrieval with generative models for\nopen domain question answering. arXiv preprint\narXiv:2007.01282.\nGautier Izacard, Patrick Lewis, Xin Du, Kurt Shuster,\nSebastian Riedel, and Jason Weston. 2021. Unsu-\npervised dense information retrieval with contrastive\nlearning. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 1077\u20131089.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. arXiv preprint arXiv:1705.03551.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020a. Dense passage retrieval for\nopen-domain question answering. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 6769\u2013\n6781.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020b. Dense passage retrieval for\nopen-domain question answering. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 6769\u2013\n6781, Online. Association for Computational Lin-\nguistics.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In International Conference on Learning\nRepresentations.\nOmar Khattab, Christopher Potts, and Matei Zaharia.\n2021. Relevance-guided supervision for openqa with\ncolbert. Transactions of the association for computa-\ntional linguistics, 9:929\u2013944.\nYunsoo Kim, Jinge Wu, Yusuf Abdulle, and Honghan\nWu. 2024. MedExQA: Medical question answer-\ning benchmark with multiple explanations. In Pro-\nceedings of the 23rd Workshop on Biomedical Natu-\nral Language Processing, pages 167\u2013181, Bangkok,\nThailand. Association for Computational Linguistics.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research. Transactions of the\nAssociation for Computational Linguistics, 7:453\u2013\n466.\n\nJinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi\nChen. 2020.\nLearning dense representations of\nphrases at scale. arXiv preprint arXiv:2012.12624.\nJinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi\nChen. 2021.\nLearning dense representations of\nphrases at scale. Preprint, arXiv:2012.12624.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019.\nLatent retrieval for weakly supervised\nopen domain question answering. arXiv preprint\narXiv:1906.00300.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. arXiv preprint arXiv:2104.08691.\nYoav Levine, Itay Dalmedigos, Ori Ram, Yoel Zeldes,\nDaniel Jannai, Dor Muhlgay, Yoni Osin, Opher\nLieber, Barak Lenz, Shai Shalev-Shwartz, et al. 2022.\nStanding on the shoulders of giant frozen language\nmodels. arXiv preprint arXiv:2204.10019.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rock-\nt\u00e4schel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459\u20139474.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels. arXiv preprint arXiv:1609.07843.\nRodrigo Nogueira and Kyunghyun Cho. 2019. Pas-\nsage\nre-ranking\nwith\nbert.\narXiv\npreprint\narXiv:1901.04085.\nRodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2020.\nDocument ranking with a pretrained sequence-to-\nsequence model. arXiv preprint arXiv:2003.06713.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in neural in-\nformation processing systems, 35:27730\u201327744.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a unified text-to-text\ntransformer. Journal of machine learning research,\n21(140):1\u201367.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. 2023. In-context retrieval-augmented lan-\nguage models. Transactions of the Association for\nComputational Linguistics, 11:1316\u20131331.\nStephen Robertson and Hugo Zaragoza. 2009.\nThe\nprobabilistic relevance framework: Bm25 and be-\nyond. Found. Trends Inf. Retr., 3(4):333\u2013389.\nDevendra Singh Sachan, Mike Lewis, Mandar Joshi,\nArmen Aghajanyan, Wen-tau Yih, Joelle Pineau, and\nLuke Zettlemoyer. 2022. Improving passage retrieval\nwith zero-shot question generation. arXiv preprint\narXiv:2204.07496.\nDevendra Singh Sachan, Mostofa Patwary, Mohammad\nShoeybi, Neel Kant, Wei Ping, William L Hamilton,\nand Bryan Catanzaro. 2021. End-to-end training of\nneural retrievers for open-domain question answering.\narXiv preprint arXiv:2101.00408.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\net al. 2022. Multitask prompted training enables\nzero-shot task generalization. In International Con-\nference on Learning Representations.\nMinjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur P\nParikh, Ali Farhadi, and Hannaneh Hajishirzi.\n2019.\nReal-time open-domain question answer-\ning with dense-sparse phrase index. arXiv preprint\narXiv:1906.05807.\nDevendra Singh, Siva Reddy, Will Hamilton, Chris\nDyer, and Dani Yogatama. 2021. End-to-end train-\ning of multi-document reader and retriever for open-\ndomain question answering. Advances in Neural\nInformation Processing Systems, 34:25968\u201325981.\nWeiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang\nWang, Pengjie Ren, Zhumin Chen, Dawei Yin, and\nZhaochun Ren. 2023. Is ChatGPT good at search?\ninvestigating large language models as re-ranking\nagents. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 14918\u201314937, Singapore. Association for\nComputational Linguistics.\nNandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Ab-\nhishek Srivastava, and Iryna Gurevych. 2021. Beir:\nA heterogenous benchmark for zero-shot evalua-\ntion of information retrieval models. arXiv preprint\narXiv:2104.08663.\nCunxiang Wang, Haofei Yu, and Yue Zhang. 2023.\nRfid: Towards rational fusion-in-decoder for open-\ndomain question answering.\narXiv preprint\narXiv:2305.17041.\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul Bennett, Junaid Ahmed, and Arnold\nOverwijk. 2020. Approximate nearest neighbor neg-\native contrastive learning for dense text retrieval.\narXiv preprint arXiv:2007.00808.\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong\nXu, Mingxuan Ju, Soumya Sanyal, Chenguang\nZhu, Michael Zeng, and Meng Jiang. 2022. Gen-\nerate rather than retrieve:\nLarge language mod-\nels are strong context generators.\narXiv preprint\narXiv:2209.10063.\n\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nYunxiang Zhang, Muhammad Khalifa, Lajanugen Lo-\ngeswaran, Moontae Lee, Honglak Lee, and Lu Wang.\n2023. Merging generated and retrieved knowledge\nfor open-domain QA. In Proceedings of the 2023\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 4710\u20134728, Singapore. As-\nsociation for Computational Linguistics.\nA\nAppendix\nThis appendix provides detailed insights into the\nretrieval and reranking results discussed in the\nmain paper. We present performance evaluations\nof different retrieval methods, including BM25,\nDPR, MSS, and hybrid approaches (R+G, G+R)\nacross multiple models. The results demonstrate\nthe impact of reranking and retrieval-augmented\ngeneration (RAG) techniques on various question-\nanswering benchmarks.\nA.1\nRetrieval Performance on TriviaQA\nRetrieval plays a fundamental role in the TriviaQA\ndataset, where models must extract relevant infor-\nmation from large document collections to answer\ntrivia-based, open-domain questions. This section\nprovides a detailed comparison of various retrieval\nmethods, including sparse retrievers like BM25,\ndense retrievers such as DPR, and generator mod-\nels. The retrieval effectiveness of these models is\nmeasured using Top-1, Top-5, and Top-10 accu-\nracy, which represent the percentage of cases in\nwhich a correct document appears within the top-k\nretrieved results.\nThe results show that DPR achieves the highest\nTop-1 accuracy at 75.4%, significantly outperform-\ning BM25, which achieves only 54.0%. This indi-\ncates that sparse retrieval methods struggle with the\ncomplexity of trivia-style questions, while dense\nretrieval models that leverage learned representa-\ntions of queries and documents exhibit superior\nretrieval effectiveness. MSS-DPR follows closely,\nwith a Top-1 accuracy of 73.5%, suggesting that ad-\nditional pretraining techniques further enhance re-\ntrieval performance. Generative augmentation also\nproves valuable, as GenRead achieves a Top-1 accu-\nracy of 69.7%, surpassing BM25 and approaching\nthe effectiveness of dense retrievers.\nThe advantages of generator approaches become\nmore evident in the Top-5 and Top-10 accuracy\nmetrics.\nMSS-DPR+Gen leads with an 85.0%\nTop-5 accuracy, followed closely by DPR+Gen\nat 84.4%, indicating that the combination of re-\ntrieval and generation improves ranking effective-\nness. BM25+Gen also sees significant improve-\nments, achieving 84.4% in Top-5 accuracy, com-\npared to BM25 alone at 73.6%. In the Top-10\nretrieval setting, hybrid models consistently outper-\nform retrieval-only methods, with DPR+Gen reach-\ning 85.2% and MSS-DPR+Gen achieving 85.0%.\nThese findings confirm that hybrid approaches,\nwhich integrate retrieval with generative document\nexpansion, provide more robust and reliable re-\ntrieval for complex QA tasks.\nA.2\nLanguage Model Perplexity Evaluation\nThis section evaluates the impact of retrieval on\nlanguage modeling performance, measured using\nperplexity (PPL). Lower perplexity values indicate\na model\u2019s ability to generate more fluent and con-\ntextually appropriate text, making it a key metric\nfor evaluating generative language models. We\ncompare various configurations, including retrieval-\nbased (R), generative (G), and hybrid retrieval-\ngeneration approaches (R+G, G+R), across differ-\nent model sizes, including GPT-2, OPT, and GPT-\nNeo, on the WikiText-103 benchmark.\nThe results indicate that retrieval (R) consistently\nimproves language model performance, leading to\nsubstantial reductions in perplexity. For instance,\nGPT-2 Small achieves a perplexity of 37.5 with-\nout any augmentation, which drops to 29.56 with\nBM25-based retrieval, representing a 21.2% im-\nprovement. Similarly, GPT-2 XL sees its perplexity\nreduced from 20.0 to 16.56 when retrieval is ap-\nplied, highlighting the benefits of factual grounding\nin reducing language modeling uncertainty. Larger\nmodels such as OPT-13B achieve the lowest per-\nplexity, with a reduction from 12.7 to 10.83 when\nretrieval is used.\nGenerative (G), in contrast, does not always lead\nto improvements in perplexity. In several cases,\ngenerating context without retrieval increases per-\nplexity, as seen with GPT-2 XL, where perplexity\nrises from 20.0 to 24.91 when using generation\nalone. This suggests that generative models may\nintroduce hallucinations, making their predictions\nless certain and increasing the likelihood of gener-\nating inaccurate text. The same trend is observed in\nsmaller models, such as OPT-125M, where perplex-\nity worsens from 40.1 to 48.12 with generation.\nHybrid approaches that combine retrieval and\n\nGenRead\nMSS\nBM25\nContriever\nDPR\nMSS-DPR\nMSS+Gen\nBM25+Gen\nContriever+Gen\nDPR+Gen\nMSS-DPR+Gen\n0\n20\n40\n60\n80\n100\nTop Accuracy (%)\n69.7\n54.0\n73.6\n57.9\n75.4\n73.5\n65.6\n66.0\n65.5\n67.5\n67.3\n79.8\n70.5\n82.8\n78.5\n80.8\n78.0\n78.8\n78.7\n79.2\n80.0\n79.8\n82.8\n73.6\n85.0\n83.8\n81.9\n78.9\n83.8\n84.4\n84.4\n85.2\n85.0\nTriviaQA: Top k Accuracy for Different Retrievers and Generator with UPR Reranker\nTop-1\nTop-5\nTop-10\nFigure 3: Comparison of retrieval methods using the UPR approach on the TriviaQA dataset,\nhighlighting Top-1, Top-5, and Top-10 accuracy (Gen refers to GenRead method). The\nresults showcase the impact of different retrieval and reranking strategies.\nModel\nNo Context\nLLama-3.3 70B\nGPT3.5\nR\nG\nR+G\nG+R\nG\nR+G\nG+R\nGPT-2 S\n37.5\n29.56\n42.20\n32.14\n34.77\n39.27\n30.92\n31.56\nGPT-2 M\n26.3\n21.45\n33.23\n24.46\n27.12\n28.68\n22.78\n23.48\nGPT-2 L\n22.0\n18.08\n27.61\n20.89\n23.09\n23.83\n19.17\n19.69\nGPT-2 XL\n20.0\n16.56\n24.91\n19.22\n20.94\n21.50\n17.51\n17.91\nopt-125m\n40.1\n32.03\n48.12\n36.56\n39.39\n43.58\n34.23\n34.88\nopt-350m\n30.4\n24.41\n37.22\n28.20\n30.57\n32.32\n25.71\n26.27\nopt-1.3b\n19.2\n15.91\n23.74\n18.60\n20.25\n20.34\n16.68\n17.01\nopt-2.7b\n16.4\n13.80\n20.94\n16.87\n17.84\n17.54\n14.67\n14.80\nopt-6.7b\n13.8\n11.72\n17.52\n14.13\n15.07\n14.63\n12.31\n12.47\nopt-13b\n12.7\n10.83\n16.39\n13.35\n14.23\n13.46\n11.41\n11.52\ngpt-neo-1.3B\n17.5\n14.62\n20.45\n16.53\n17.38\n18.83\n15.53\n15.70\ngpt-neo-2.7B\n15.1\n12.85\n18.35\n14.97\n15.76\n16.35\n13.70\n13.86\ngpt-neo-6B\n11.6\n10.05\n13.73\n11.58\n12.09\n12.32\n10.58\n10.73\nTable 6: Perplexity results on the WikiText-103 test\nset for various models (GPT-2, OPT, GPT-Neo) using\nretrieval (R), generation (G), and hybrid approaches\n(R+G and G+R). Llama-3 70B and GPT3.5 used for\ngenerated Documents.\ngeneration yield mixed results.\nThe retrieval-\nfirst strategy (R+G) consistently outperforms the\ngeneration-first approach (G+R), as seen in GPT-\n2 XL, where R+G achieves a perplexity of 19.22\ncompared to 20.94 for G+R. The results confirm\nthat retrieval should precede generation for maxi-\nmum benefit, ensuring that the generative model\nconditions its output on factually accurate retrieved\ninformation.\nA.3\nQA performance comparison on multiple\nbenchmarks\nThis section presents a detailed zero-shot evalu-\nation of various retrieval and reranking methods\nwithin a retrieval-augmented generation (RAG)\nframework. The performance of these methods\nis assessed across three widely used question-\nanswering benchmarks: Natural Questions (NQ),\nTriviaQA, and WebQuestions (WebQ). The study\ncompares different retriever and reranker models,\nincluding BM25, Multi-Stage Search (MSS), Con-\ntriever, Dense Passage Retrieval (DPR), and hybrid\napproaches such as retrieval + generation (R+G)\nand generation + retrieval (G+R). The results are\nreported for multiple state-of-the-art large lan-\nguage models, including LLaMA-3 8B, LLaMA-\n3.1 8B, Gemma-2 (2B and 9B), LLaMA-2-13B,\nand Mistral-7B-v0.1.\nA.3.1\nRetrieval and RAG Performance on\nLLaMA-3 8B and LLaMA-3.1 8B\nThe performance results for LLaMA-3 8B and\nLLaMA-3.1 8B across different retrieval ap-\nproaches are presented in Table 7. The models\nare evaluated using three primary metrics: Exact\nMatch (EM), Recall, and Consistency (Con). The\nEM score measures the percentage of responses\nthat exactly match the ground truth, Recall rep-\nresents the proportion of relevant documents re-\ntrieved, and Consistency assesses the stability of\nthe model\u2019s generated answers across multiple re-\ntrieval settings.\nFor LLaMA-3 8B, the DPR+G+R hybrid method\nachieves the highest EM score on TriviaQA\n(54.50%) and NQ (28.14%), while for WebQ, the\nBM25+G and DPR+G methods perform compa-\nrably with 15.45% and 15.50% EM, respectively.\nIn contrast, traditional BM25 retrieval alone ex-\nhibits significantly lower performance, achieving\nonly 14.90% EM on NQ, 42.10% on TriviaQA,\nand 10.23% on WebQ. The R+G (Retriever first,\nthen Generator) approach consistently outperforms\n\nG+R, with DPR+R+G reaching 28.94% EM on\nNQ and 37.31% on TriviaQA. This demonstrates\nthat conditioning retrieval before generation is a\nsuperior strategy for answer synthesis.\nFor LLaMA-3.1 8B, a general performance im-\nprovement is observed over LLaMA-3 8B. The\nDPR+R+G method improves EM on NQ to 30.83%\nand TriviaQA to 37.89%, indicating that a refined\nretriever-generator interaction further enhances\nretrieval-augmented generation. The MSS+DPR\napproach achieves a strong balance between re-\ntrieval recall and consistency, with Recall reaching\n49.23% on NQ and 73.67% on TriviaQA, while\nmaintaining high consistency.\nA.3.2\nPerformance of RAG on Gemma-2 (2B\nand 9B) Models\nThe retrieval and reranking evaluation on Gemma-\n2-2B and Gemma-2-9B is presented in Table 8.\nThe larger Gemma-2-9B model significantly out-\nperforms its 2B counterpart across all benchmarks,\ndemonstrating the benefits of model scaling in\nretrieval-augmented question answering.\nOn Gemma-2-2B, DPR+R+G achieves 30.25%\nEM on NQ and 37.73% on TriviaQA, while the\ntraditional BM25 retriever falls behind, achieving\n14.02% EM on NQ and 43.28% on TriviaQA. The\ngenerative augmentation (G) alone leads to better\nrecall but performs worse in terms of EM than\nretrieval-augmented methods, with 46.85% Recall\non NQ and 72.99% on TriviaQA.\nThe Gemma-2-9B model exhibits substantial im-\nprovements, with DPR+R+G achieving 30.83%\nEM on NQ and 37.89% on TriviaQA, mirroring\nthe performance trends observed with LLaMA-3.1\n8B. Interestingly, BM25+G surpasses the retrieval-\nonly (BM25 R) approach by achieving 63.02%\nRecall on TriviaQA, reinforcing that generative\naugmentation benefits sparse retrieval methods.\nHowever, the best-performing approach remains\nMSS+DPR+R+G, which achieves a Reciprocal\nRank of 49.23% on NQ and 73.67% on TriviaQA,\nemphasizing the importance of hybrid search.\nA.3.3\nRetrieval-Augmented QA with\nLLaMA-2-13B and Mistral-7B-v0.1\nTable 9 presents the results for LLaMA-2-13B\nand Mistral-7B-v0.1, two widely used open-source\nmodels. LLaMA-2-13B achieves better retrieval\nperformance than Mistral-7B, especially when\ncombining dense retrieval with reranking.\nFor LLaMA-2-13B, DPR+R+G consistently\nachieves the highest scores, with 30.91% EM\non NQ and 38.12% on TriviaQA, surpassing the\nBM25+R baseline (21.14% EM on NQ and 57.90%\non TriviaQA). However, BM25+G shows better\nrecall (73.63% on TriviaQA) compared to dense\nretrieval methods, supporting the argument that\ngenerative augmentation enhances sparse retrieval.\nThe hybrid approach (DPR+G+R) further improves\nretrieval, attaining EM scores of 30.66% on NQ\nand 37.98% on TriviaQA, slightly trailing behind\nDPR+R+G but still outperforming retrieval-only\nbaselines.\nFor Mistral-7B-v0.1, the results indicate that\nBM25 alone is highly ineffective, achieving only\n11.19% EM on NQ and 52.85% on TriviaQA. The\nDPR+G+R hybrid model achieves 25.07% EM\non NQ and 28.92% on TriviaQA, demonstrating\nthat retrieval-first approaches remain more effec-\ntive than generation-first pipelines. Interestingly,\nUPR (Unsupervised Passage Reranking) achieves\n25.24% EM on NQ and 31.25% on TriviaQA, prov-\ning to be a strong alternative to traditional DPR\nreranking.\n\nLLama V3.3 8B\nLLama V3.1 8B\nRetriever\n# Doc\nNQ\nTriviaQA\nWebQ\nNQ\nTriviaQA\nWebQ\nEM\nRecall\nCon\nEM\nRecall\nCon\nEM\nRecall\nCon\nEM\nRecall\nCon\nEM\nRecall\nCon\nEM\nRecall\nCon\nBM25\nR\n14.90 26.68 19.91 42.10 56.90 50.11 10.23 24.95 16.92 12.82 27.14 20.27 40.13 58.40 51.40\n9.25\n24.92 17.22\nG\n24.68 45.43 33.85 52.23 66.86 59.61 15.45 45.34 35.28 21.49 44.92 33.65 48.74 66.27 58.68 14.51 45.73 35.03\nR+G\n25.51 45.22 34.29 53.29 70.04 62.75 15.05 42.75 32.77 22.29 45.46 34.37 49.69 70.45 62.98 13.97 43.91 33.56\nG+R\n25.67 45.93 34.70 53.24 69.54 62.34 15.00 44.09 33.56 23.29 46.10 34.93 50.29 70.50 63.00 13.43 43.53 32.72\nUPR\n23.49 41.02 31.55 55.85 69.59 62.65 17.27 44.15 32.68 24.24 40.68 31.47 55.22 68.14 61.78 17.67 41.68 30.71\nRankGPT 28.45 42.73 34.02\n-\n-\n-\n19.73 40.57 29.82 26.65 40.89 32.44\n-\n-\n-\n18.55 37.92 28.10\nMSS\nR\n12.82 22.96 17.36 31.90 44.27 37.75\n7.38\n18.9\n11.81 11.19 23.37 17.81 30.74 45.82 38.91\n6.88\n19.30 12.40\nG\n24.95 45.82 34.15 51.69 66.58 59.22 15.84 46.06 35.77 21.46 45.29 33.90 47.98 65.57 57.84 14.12 45.25 45.25\nR+G\n25.54 44.79 33.96 51.39 67.52 59.99 14.96 43.14 32.48 21.57 44.88 33.93 47.75 68.22 60.41 14.46 44.46 33.31\nG+R\n25.31 45.34 34.43 51.81 67.72 60.12 15.20 43.75 33.12 22.68 45.55 34.68 48.75 68.02 60.27 13.43 44.14 33.07\nUPR\n23.35 40.52 31.55 53.78 68.67 61.63 17.08 43.97 32.14 24.43 41.08 31.77 54.97 68.46 61.73 16.70 42.20 31.30\nRankGPT 28.17 41.67 33.21\n-\n-\n-\n19.05 39.33 29.58 25.84 39.07 31.41\n-\n-\n-\n17.37 36.38 27.21\nContriever\nR\n15.29 27.36 20.99 36.25 49.52 49.52 10.67 28.28 20.22 13.24 27.68 21.88 35.29 50.96 44.19\n9.35\n28.71 20.32\nG\n24.70 46.02 34.48 51.48 66.44 59.03 15.89 45.59 35.23 21.32 45.53 34.04 48.39 65.82 58.03 14.61 45.67 34.84\nR+G\n25.59 45.28 34.32 51.78 67.90 60.45 15.10 43.49 32.87 22.18 45.26 34.45 48.38 68.84 61.47 13.87 45.07 33.80\nG+R\n25.70 46.07 35.04 52.07 68.21 60.98 15.50 44.08 33.21 23.13 46.18 35.12 48.91 68.32 60.82 13.87 44.87 34.10\nUPR\n23.24 41.01 31.61 53.48 68.70 61.82 17.32 44.11 32.97 24.21 40.99 31.63 55.71 68.74 61.98 17.62 37.73 28.44\nRankGPT 30.55 44.25 35.40\n-\n-\n-\n19.78 41.18 31.10 28.86 42.42 33.68\n-\n-\n-\n17.62 37.73 28.44\nDPR\nR\n28.08 45.40 36.37 45.88 61.24 54.58 19.83 40.27 30.98 23.21 44.99 36.03 43.62 62.61 55.97 14.32 38.27 28.98\nG\n25.06 45.81 34.34 51.66 66.51 59.08 15.45 46.32 36.59 21.41 45.37 34.15 48.21 65.76 57.96 14.76 45.32 34.59\nR+G\n28.94 48.82 37.31 54.41 70.83 63.68 24.50 47.45 35.94 24.62 48.50 37.28 50.61 71.30 63.99 14.32 46.55 34.94\nG+R\n28.14 48.87 37.03 54.50 70.53 63.42 25.51 48.60 37.50 25.51 50.28 38.53 51.65 71.66 64.30 14.81 46.59 35.23\nUPR\n23.60 41.18 31.77 53.41 68.60 61.60 18.06 44.01 32.48 24.74 41.32 31.75 56.07 69.14 62.41 19.64 39.77 29.87\nRankGPT 31.74 46.41 36.76\n-\n-\n-\n20.42 40.84 31.00 29.58 43.92 34.65\n-\n-\n-\n19.64 39.77 29.87\nMSS+DPR\nR\n28.17 46.72 37.00 47.69 63.66 57.08 13.92 40.87 30.57 23.68 46.70 37.53 45.72 65.24 58.53 13.92 38.97 29.67\nG\n24.73 45.46 33.85 51.64 66.90 59.40 14.51 48.74 37.73 21.80 45.28 33.90 47.98 65.51 57.83 14.27 46.14 35.03\nR+G\n29.41 49.73 38.25 54.53 70.86 63.82 14.96 49.48 38.84 24.09 48.80 37.34 50.72 71.67 64.35 14.96 46.40 34.94\nG+R\n28.61 49.27 37.36 54.48 71.16 63.91 14.76 48.80 37.77 25.54 50.30 38.61 52.20 72.19 64.86 14.76 46.42 35.33\nUPR\n23.10 40.81 31.25 53.65 68.61 61.69 16.68 44.07 32.48 25.24 41.17 31.83 55.52 68.76 61.90 18.99 40.76 30.22\nRankGPT 31.94 45.95 36.59\n-\n-\n-\n21.41 42.54 32.14 29.95 45.00 36.49\n-\n-\n-\n18.99 40.76 30.22\nTable 7: Zero-shot results of in-context learning on The test set of NQ, TriviaQA, and WebQ uses the LLama 3/3.1\n8B Model as RAG\n\nGemma-2-2b\nGemma-2-9b\nRetriever\n# Doc\nNQ\nTriviaQA\nWebQ\nNQ\nTriviaQA\nWebQ\nEM\nRecall\nCon\nEM\nRecall\nCon\nEM\nRecall\nCon\nEM\nRecall\nCon\nEM\nRecall\nCon\nEM\nRecall\nCon\nBM25\nR\n14.02 25.55 18.53 43.28 52.78 47.01 14.71 37.74 27.21 19.81 26.95 22.05 57.55 65.53 60.29 14.96 24.87 20.13\nG\n27.01 46.85 35.35 59.91 72.99 66.36 19.34 50.19 38.93 28.28 46.46 36.04 63.02 75.33 68.83 18.65 50.57 39.37\nR+G\n28.39 46.41 35.62 59.89 73.39 66.53 19.29 47.44 35.93 28.45 45.09 35.54 63.50 75.23 68.50 19.05 45.38 34.94\nG+R\n28.50 46.50 35.68 59.87 72.99 66.21 19.73 48.22 36.47 28.42 45.67 35.96 62.94 75.44 68.64 19.34 45.83 35.53\nUPR\n26.23 44.37 33.99 58.71 71.59 64.79 19.78 48.16 36.47 23.41 41.52 32.24 58.74 71.92 65.20 15.94 46.16 34.15\nRankGPT 30.36 45.96 36.09\n-\n-\n-\n21.11 45.13 34.35 30.75 43.48 35.46\n-\n-\n-\n21.06 39.32 31.15\nMSS\nR\n13.96 25.41 18.50 33.05 42.24 36.23 14.71 37.74 27.21 19.78 26.86 22.08 50.93 58.57 53.28 14.96 24.92 20.18\nG\n27.06 46.87 35.29 59.27 72.32 65.49 19.34 50.19 38.93 27.95 46.77 36.01 62.67 74.93 68.41 18.65 50.57 39.37\nR+G\n28.48 46.36 35.48 58.63 71.48 64.65 19.29 47.05 35.78 28.56 45.27 35.65 62.53 73.97 67.27 18.80 44.44 34.01\nG+R\n28.42 46.18 35.57 58.47 71.27 64.46 19.78 47.79 36.07 28.31 45.53 35.37 61.78 73.76 67.07 18.55 45.11 35.09\nUPR\n26.20 44.14 33.77 58.15 71.02 64.10 19.73 48.92 36.96 23.10 41.49 32.05 57.50 71.15 64.33 15.85 47.20 34.65\nRankGPT 29.17 45.00 35.32\n-\n-\n-\n19.93 44.61 34.10 29.97 42.59 34.85\n-\n-\n-\n19.64 36.91 29.72\nContriever\nR\n13.96 25.41 18.50 33.05 42.24 36.23 14.71 37.74 27.21 19.78 26.86 22.08 50.93 58.57 53.28 14.96 24.92 20.18\nG\n27.06 46.87 35.29 59.27 72.32 65.49 19.34 50.19 38.93 27.95 46.77 36.01 62.67 74.93 68.41 18.65 50.57 39.37\nR+G\n28.78 46.98 36.12 58.86 71.68 64.81 20.28 48.42 37.30 28.84 45.33 35.90 62.85 73.89 67.21 19.64 46.01 35.43\nG+R\n28.75 46.70 35.98 58.64 71.33 64.57 20.13 48.24 36.86 28.37 45.46 36.07 61.97 73.87 67.07 19.09 45.95 36.17\nUPR\n26.26 44.26 33.82 58.37 71.17 64.23 19.83 48.32 36.61 23.21 41.46 32.08 57.64 71.20 64.36 16.14 46.12 34.01\nRankGPT 32.11 47.86 38.31\n-\n-\n-\n20.67 45.77 34.89 32.44 44.83 36.79\n-\n-\n-\n19.88 38.57 30.02\nDPR\nR\n13.99 25.44 18.53 33.05 42.24 36.23 14.71 37.74 27.21 19.78 26.86 22.08 50.93 58.57 53.28 14.96 24.92 20.18\nG\n27.06 46.87 35.29 59.27 72.32 65.49 19.34 50.19 38.93 27.92 46.75 35.98 62.67 74.93 68.41 18.65 50.57 39.37\nR+G\n30.25 48.89 37.73 60.16 73.19 66.28 20.18 48.36 36.66 30.83 47.93 37.89 63.72 74.99 68.27 19.93 46.16 35.88\nG+R\n30.72 48.78 37.45 60.17 72.98 66.22 20.52 48.93 36.86 29.92 47.48 37.12 63.18 75.00 68.29 19.39 46.59 36.32\nUPR\n26.51 44.62 34.16 58.71 71.72 64.78 19.78 48.62 36.86 23.38 41.80 32.38 57.85 71.54 64.75 16.04 46.56 34.50\nRankGPT 34.16 50.26 39.42\n-\n-\n-\n21.21 46.41 35.48 34.04 47.14 38.64\n-\n-\n-\n21.01 40.41 31.69\nMSS+DPR\nR\n13.96 25.41 18.50 33.05 42.24 36.23 14.71 37.74 27.21 19.78 26.86 22.08 50.93 58.57 53.28 14.96 24.92 20.18\nG\n27.06 46.87 35.29 59.27 72.32 65.49 19.34 50.19 38.93 28.25 46.47 36.01 62.67 74.93 68.41 18.60 46.85 36.32\nR+G\n30.91 49.23 38.12 60.56 73.67 66.82 20.72 48.67 36.91 30.58 47.85 37.73 63.76 75.20 68.48 19.93 46.62 36.32\nG+R\n30.66 49.15 37.98 60.25 73.53 66.68 21.26 49.67 37.70 29.94 47.61 37.40 63.17 75.19 68.37 20.13 46.90 37.01\nUPR\n26.54 44.67 34.21 58.64 71.50 64.55 19.49 48.03 36.02 23.46 41.75 32.38 57.75 71.34 64.48 15.94 45.96 33.56\nRankGPT 32.51 49.77 39.53\n-\n-\n-\n21.41 47.58 36.52 32.13 46.55 38.29\n-\n-\n-\n21.06 40.72 32.04\nTable 8: Zero-shot results of in-context learning on The test set of NQ, TriviaQA, and WebQ uses the Gemma\nModel as RAG.\n\nLlama-2-13b-hf\nMistral-7B-v0.1\nRetriever\n# Mode\nNQ\nTriviaQA\nWebQ\nNQ\nTriviaQA\nWebQ\nEM\nRecall\nCon\nEM\nRecall\nCon\nEM\nRecall\nCon\nEM\nRecall\nCon\nEM\nRecall\nCon\nEM\nRecall\nCon\nBM25\nR\n21.14 30.82 24.46 57.90 65.27 59.57 19.54 37.38 27.51 11.19 13.45 11.80 52.85 58.11 53.82\n6.40\n8.46\n6.84\nG\n28.06 44.60 34.21 62.64 73.63 67.13 20.32 45.54 34.40 27.01 41.30 32.19 62.64 72.63 66.19 16.09 33.01 24.70\nR+G\n26.62 41.96 32.47 61.35 73.01 66.34 19.00 43.61 32.53 25.68 37.68 30.11 60.45 69.76 63.49 15.65 29.42 22.54\nG+R\n26.79 43.16 33.35 62.01 73.14 66.67 19.24 43.70 32.68 23.71 34.64 27.87 58.56 67.61 61.63 13.44 26.74 20.08\nUPR\n27.59 42.65 32.99 61.60 71.62 65.00 20.37 44.09 33.76 25.18 40.47 31.11 59.64 69.91 63.20 17.18 40.89 30.46\nRankGPT 29.22 43.00 34.21\n-\n-\n-\n21.99 41.25 31.15 25.35 40.47 31.36\n-\n-\n-\n17.18 40.90 30.46\nMSS\nR\n21.52 30.92 24.18 51.75 58.58 53.35 20.62 39.45 29.68 11.08 13.33 11.66 42.69 47.12 43.40\n6.40\n8.46\n6.84\nG\n28.01 43.22 33.74 60.66 72.01 65.43 19.44 44.80 34.20 27.15 41.40 32.19 61.28 71.06 64.71 16.09 33.01 24.70\nR+G\n26.81 42.48 33.02 58.64 70.39 63.87 19.05 42.74 32.14 25.90 38.34 30.58 56.28 64.82 59.03 14.57 27.91 20.96\nG+R\n27.48 43.97 33.88 59.56 71.14 64.63 18.75 42.48 31.64 23.10 33.75 27.26 52.15 59.83 54.59 13.09 24.77 18.31\nUPR\n27.29 42.28 32.63 59.23 69.93 63.56 20.72 44.50 33.96 23.77 40.41 30.97 57.24 67.67 61.04 17.42 42.17 31.20\nRankGPT 27.56 41.77 33.05\n-\n-\n-\n20.77 44.50 33.96 23.77 39.08 29.28\n-\n-\n-\n16.98 39.87 29.43\nContriever\nR\n20.47 30.13 23.85 42.69 47.12 43.40 19.98 37.74 27.61 11.08 13.33 11.66 42.69 47.12 43.40\n6.40\n8.46\n6.84\nG\n26.79 43.16 33.35 61.28 71.06 64.71 19.69 44.88 33.76 27.15 41.40 32.19 61.28 71.06 64.71 16.09 33.01 24.70\nR+G\n27.45 42.89 33.27 56.43 65.00 59.20 19.59 43.25 32.19 25.35 37.57 30.08 56.43 65.00 59.20 15.31 27.88 20.82\nG+R\n27.17 43.64 33.74 52.66 60.52 55.33 19.88 44.21 33.61 23.02 33.53 27.29 54.39 62.52 57.18 13.44 25.79 19.24\nUPR\n26.57 41.99 32.44 59.60 70.02 63.58 20.72 44.65 33.56 25.10 40.43 31.02 57.26 67.63 61.11 17.27 40.48 29.97\nRankGPT 30.39 44.60 36.09\n-\n-\n-\n20.72 44.65 33.56 25.10 40.48 31.02\n-\n-\n-\n17.27 40.48 29.97\nDPR\nR\n21.94 31.36 24.88 51.07 57.97 52.71 19.83 37.35 28.05 11.11 13.36 11.69 42.69 47.12 43.40\n6.40\n8.46\n6.84\nG\n28.12 44.10 34.32 60.85 71.87 65.44 20.47 45.05 34.15 27.15 41.40 32.19 61.28 71.06 64.71 16.09 33.01 24.70\nR+G\n28.81 44.73 34.82 58.00 67.03 61.14 20.32 45.14 34.25 27.70 40.40 32.47 58.00 67.03 61.14 16.44 32.14 24.31\nG+R\n27.92 44.15 34.27 60.01 71.44 64.93 20.72 45.02 34.15 25.01 35.90 29.06 54.64 62.57 57.27 14.71 28.39 21.75\nUPR\n27.45 42.93 33.02 60.05 70.49 63.96 20.62 44.81 33.91 25.26 40.71 31.36 57.53 67.80 61.31 17.18 41.20 30.66\nRankGPT 32.77 47.56 38.06\n-\n-\n-\n22.15 44.37 33.81 25.26 40.71 31.36\n-\n-\n-\n17.18 41.20 30.66\nMSS+DPR\nR\n21.47 31.26 24.60 51.35 58.26 53.01 19.83 37.37 27.61 11.08 13.33 11.66 42.69 47.12 43.40\n6.40\n8.46\n6.84\nG\n28.20 43.75 34.07 60.44 71.60 65.09 20.13 45.50 34.30 27.15 41.40 32.19 61.28 71.06 64.71 16.09 33.01 24.70\nR+G\n28.45 45.21 35.21 59.50 71.17 64.79 19.78 44.57 33.46 27.40 39.95 31.94 58.03 67.43 61.49 16.44 31.62 23.97\nG+R\n28.73 44.67 34.79 59.96 71.90 65.57 20.77 45.09 34.10 25.07 35.95 28.92 54.39 62.52 57.18 14.96 29.13 22.00\nUPR\n27.45 42.91 33.10 60.00 71.52 64.07 19.98 44.81 33.32 25.24 40.59 31.25 57.47 67.80 61.12 16.09 44.33 33.32\nRankGPT 32.61 48.90 40.19\n-\n-\n-\n21.95 43.73 33.32 25.24 40.59 31.25\n-\n-\n-\n16.09 44.33 33.32\nTable 9: Zero-shot results of in-context learning on the test set of NQ, TriviaQA, and WebQ using Llama-2-13b-hf\nand Mistral-7B-v0.1 as RAG\n"}