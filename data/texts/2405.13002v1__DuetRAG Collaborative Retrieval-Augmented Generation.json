{"metadata": {"pdf_filename": "2405.13002v1__DuetRAG Collaborative Retrieval-Augmented Generation.pdf", "source": "arXiv"}, "text": "DUETRAG: COLLABORATIVE RETRIEVAL-AUGMENTED\nGENERATION\nA PREPRINT\nDian Jiao\nZhejiang University\njd_dcd@zju.edu.cn\nLi Cai\nZhejiang University\n22321284@zju.edu.cn\nJingsheng Huang\nZhejiang University\njingsheng@zju.edu.cn\nWenqiao Zhang\nZhejiang University\nwenqiaozhang@zju.edu.cn\nSiliang Tang\nZhejiang University\nsiliang@zju.edu.cn\nYueting Zhuang\nZhejiang University\nyzhuang@zju.edu.cn\nMay 24, 2024\n1\nAbstract\nRetrieval-Augmented Generation (RAG) methods augment the input of Large Language Models (LLMs) with relevant\nretrieved passages, reducing factual errors in knowledge-intensive tasks. However, contemporary RAG approaches\nsuffer from irrelevant knowledge retrieval issues in complex domain questions (e.g., HotPot QA) due to the lack of\ncorresponding domain knowledge, leading to low-quality generations. To address this issue, we propose a novel\nCollaborative Retrieval-Augmented Generation framework, DuetRAG. Our bootstrapping philosophy is to simultane-\nously integrate the domain fintuning and RAG models to improve the knowledge retrieval quality, thereby enhancing\ngeneration quality. Finally, we demonstrate DuetRAG\u2019s matches with expert human researchers on HotPot QA.\n2\nIntroduction\nLarge language models (LLMs) have demonstrated significantly enhanced performance compared to preceding efforts\nwithin the realm of question answering (QA) (Achiam et al., 2023; Wei et al., 2022). Nonetheless, in the domain\nof domain-specific knowledge QA, LLMs frequently exhibit subpar performance attributable to an insufficiency of\npertinent knowledge. LLMs, when fine-tuned on domain-specific knowledge, acquire specialized domain knowledge to\naddress queries via fine-tuning on an extensive corpus of domain-specific information. However, this methodology\ntypically necessitates copious amounts of data and entails substantial training overhead, while concurrently exposing\nthe model to the risk of catastrophic forgetting (Kirkpatrick et al., 2017), thereby impeding its ability to achieve desired\noutcomes in QA across alternate domains. The paradigm of question answering grounded in retrieval-augmented\ngeneration (RAG) facilitates the integration of external knowledge into LLMs (Lewis et al., 2020), thereby enabling\nthem to extract contextually pertinent information from external repositories in instances where the model lacks internal\nknowledge, thus improving the performance of large language models in domain-specific knowledge question answering.\nHowever, traditional RAG suffers from the illusion problem (Zhang et al., 2023), leading to incorrect answers, and\nwhen faced with more complex questions, limited by the performance of the retriever, RAG may encounter difficulties\nin answering based on poorly correlated or even erroneous external documents.\nIn this paper, we propose a novel Collaborative Retrieval-Augmented Generation framework, called DuetRAG, based\non RAG and domain knowledge fine-tuning. This framework leverages both fine-tuned models and RAG to generate\nanswers to questions, and a referee model is employed to determine the selection of the final answer. The aim of\nDuetRAG is not only to equip models with domain-specific knowledge but also to enable them to utilize external\ndocuments to obtain answers when internal knowledge is uncertain. By complementing internal and external knowledge,\nDuetRAG aims to enhance the robustness of the model. We provide a detailed description of the DuetRAG methodology\narXiv:2405.13002v1  [cs.CL]  12 May 2024\n\nDuetRAG: Collaborative Retrieval-Augmented Generation\nA PREPRINT\nand its performance on various datasets in Section 3. DuetRAG outperforms existing RAG and fine-tuned models on\nthe HotPot QA dataset (Yang et al., 2018), presenting a new pipeline for QA.\n3\nRelated Work\nRetrieval-Augmented Language Models Augmenting language models with relevant information obtained from\nvarious external knowledge bases has been shown to significantly improve the performance of various NLP tasks,\nincluding language modeling (Guu et al., 2020; Borgeaud et al., 2022; Shi et al., 2023; Lin et al., 2023) and open domain\nquestion answering (Izacard et al., 2022; Zhang et al., 2024). RAG mainly adopts the \"retrieve then read\" paradigm.\nSpecifically, the input question is first used as the query, then the retrieval module retrieves relevant documents from\nthe external knowledge base, and finally the retrieved documents and questions are merged into a complete input\nto generate final output. For example, RETRO (Borgeaud et al., 2022) modifies the autoregressive LM to focus on\nrelevant documents through chunked cross-attention, thereby introducing new parameters to the model. REPLUG (Shi\net al., 2023) assumes black-box access to LM and optimizes it by fine-tuning the retriever. RAFT (Zhang et al., 2024)\nproposes a fine-tuned data that additionally contains relevant documents and answers with reasoning chains to train\nlanguage models for domain-specific open-book settings.\nFinetuning for RAG Recently, related work has studied how to improve the overall performance by fine-tuning the\nLLM or retriever in the RAG framework. For example, RADIT (Lin et al., 2023) proposes a dual-instruction fine-tuning\nframework to fine-tune both the LLM and the retriever simultaneously. InstructRetro (Wang et al., 2023) pre-trains a\nlarger autoregressive large-scale language model with retrieval function and performs instruction fine-tuning based on\nit. ChatQA (Liu et al., 2024) additionally proposes a context-enhanced instruction fine-tuning stage, specifically to\nenhance the model\u2019s ability to perform context awareness in conversational QA. RAFT (Zhang et al., 2024) proposes a\nkind of fine-tuning data that additionally contains related documents and answers with reasoning chains to fine-tune\nLLM and improve LLM\u2019s ability to understand the retrieved documents under the RAG framework.\nMulti-Model collaboration Benefiting from the extensive knowledge exhibited by large language models, the use of\nmulti-model collaboration to solve or improve tasks is gradually being studied. CAMEL (Li et al., 2024a) proposes a\ncooperative agent framework called role-playing that enables agents to cooperate on complex tasks. Du et al. (2023)\nand Liang et al. (2023) explore enhancing the factuality of specific tasks, e.g., translation and arithmetic reasoning, by\nfacilitating \u201cdebates\u201d among multiple models. Corex (Sun et al., 2023) has explored three collaboration paradigms in\nreasoning \u2013 debate, code review and retrieve. In addition, model collaboration can also be used between models of\ndifferent sizes. Li et al. (2024b) generates prompts through a small model to guide the generation of LLM.\n4\nDuetRAG: Collaborative Retrieval-Augmented Generation\nIn this section, we introduce DuetRAG, a novel model-collaborative approach for retrieval-based argument generation,\nas shown in Figure 1. The DuetRAG framework relies on the synergy of three models: the Reciter, which utilizes\nfine-tuning to internalize a knowledge base; the Discoverer, which employs retrieval and refinement to obtain external\nknowledge; and the Arbiter, which evaluates and summarizes the answers from the first two models. We will first\npresent the Problem Formalization, followed by a detailed introduction to our proposed DuetRAG.\n4.1\nProblem Formalization\nFormally, for a specific domain task T, we have access to a training set Train{Qtrain, Dtrain, Atrain}, Here, Qtrain\nrepresents the questions in the training set, Dtrain is a collection of specific documents that contain the answers\nto the questions in the training set, and Atrain is the correct answers to the questions Qtrain. During the testing\nphase, we have access to a test set Test{Qtest, D}, where Qtest is the set of questions from the test task, and D is the\nexternal knowledge collection. For the training set Train{Qtrain, Dtrain, Atrain}, Dtrain can be further classified into\ndocuments d\u2217\n1, d\u2217\n2, ..., d\u2217\ni that contain direct and relevant information to the answers, and noise documents d1, d2, ..., dj\nthat are irrelevant to the answers.\n4.2\nOverview of DuetRAG\nThe core of our work, DuetRAG, involves the collaboration of three models: the question-answering model Mi, which\nutilizes internal knowledge through fine-tuning based on domain expertise to answer questions; the question-answering\nmodel Me, which utilizes external knowledge by fine-tuning based on domain knowledge and question-answering\nscenarios to answer questions; and the referee model Mj, which evaluates the answers from the aforementioned\nquestion-answering models and selects the answer with the highest confidence.\n2\n\nDuetRAG: Collaborative Retrieval-Augmented Generation\nA PREPRINT\nDomain Based Fine-tuning\nModel-Collaborative Inference\nQuestion\nInternal Knowledge \nBased Model\nExternal Knowledge \nBased Model\nAnswer A\nAnswer B\nRetrieved External \nKnowledge\n?\n?\n?\nReferee Model\nFinal Answer\nSemantic Alignment\nQuestion\nAnswer\nClassification Based \nReferee Model\nQ1\nQ2\nQ3\n...\nQn\nQ1\nQ2\nQ3\n...\nQn\nA1\nA2\nA3\n...\nAn\nQ1\nQ2\nQ3\n...\nQn\nA1\nA2\nA3\n...\nAn\nQuestion\nKnowledge\nAnswer\n: Answer A\n: Answer A\n: Answer B\n: Answer B\n: Answer A\n: Answer B\nSummarization Based \nReferee Model\nSemantic Alignment\nQuestion\nAnswer\nClassification Based \nReferee Model\nQ1\nQ2\nQ3\n...\nQn\nA1\nA2\nA3\n...\nAn\nQuestion\nKnowledge\nAnswer\n: Answer A\n: Answer B\nSummarization Based \nReferee Model\nWere Scott Derrick-son and Ed Wood of the same nationality?\nQuestion:\nExternal Knowledge \nBased Model\nExternal Knowledge \nBased Model\nDomain Specific \nKnowledge\n\u221a\n\u221a\nX\nDomain Specific \nKnowledge\n\u221a\n\u221a\nX\nInternal Knowledge \nBased Model\nInternal Knowledge \nBased Model\nExternal Knowledge \nBased Model\nWere Scott Derrick-son and Ed Wood of the same nationality?\nQuestion:\nExternal Knowledge \nBased Model\nDomain Specific \nKnowledge\n\u221a\n\u221a\nX\nInternal Knowledge \nBased Model\nExternal Knowledge \nBased Model\nFigure 1: Overview of our DUET method.\nDuring the Fine-tuning Stage, Mi learns domain-specific knowledge and recalls relevant knowledge within the model,\nwhile Me learns how to retrieve and summarize answers from external documents. Mj is tasked with assessing the\ncredibility of answers and generating the highest-confidence result based on existing answers.\n4.3\nFine-tuning with Domain Knowledge\nOur work, DuetRAG, employs a domain knowledge fine-tuning approach based on LoRA to optimize the Internal\nKnowledge Based Model Mi and the External Knowledge Based Model Me. For a specific domain T and training\nset Ttrain, we combine domain questions Qtrain, domain knowledge Dtrain, and question answers Atrain into a\nprompt q, d, a used for LoRA fine-tuning. The weights of the large model contain a vast amount of knowledge learned\nduring the training phase, and through domain knowledge fine-tuning, the model\u2019s knowledge relevant to the target\ndomain will be reactivated. At the same time, the large language model possesses reasoning and summarization\ncapabilities. Through domain knowledge fine-tuning, the model can learn to extract and answer questions related to\nexternal knowledge.\nOur work on the referee model Mj, responsible for assessing the credibility of answers and generating the final result,\ninvolves the design and implementation of two methods: classification and summarization. The classification method is\ninspired by the training approach of CLIP. It constructs data q, a based on the training set Ttrain to train Mj, achieving\nsemantic alignment between questions and answers. The summarization method utilizes the inherent reasoning and\nsummarization capabilities of the large language model itself. By constructing prompts and fine-tuning domain\nknowledge, Mj learns to independently weigh the credibility of Mi and Me\u2019s answers and respond accordingly.\n4.4\nModel-Collaborative Inference\nDuring the Inference stage, DuetRAG employs a multi-model collaboration approach to obtain answers. For a given\nquestion q \u2208Qtest, the Internal Knowledge Based Model Mi attempts to provide a direct answer ai. Simultaneously,\nDuetRAG utilizes a retriever to obtain relevant external knowledge documents Dq = {d1, ..., dn}, and the External\nKnowledge Based Model Me uses this external knowledge to answer, resulting in answer ae. If the referee model Mj\nis implemented using the classification method, Mj will score the confidence levels of ai and ae separately based on\n3\n\nDuetRAG: Collaborative Retrieval-Augmented Generation\nA PREPRINT\nTable 1: Overall Performance of DuetRAG\nAccuracy(GPT-4 Eval)\nLLaMA2-7B\n12.7\nLLaMA2-7B + RAG\n9.2\nDSF(LLaMA2-7B)\n20.8\nDSF+RAG(LLaMA2-7B)\n25.4\nInternal Model Mi\n23.1\nExternal Model Me\n32.7\nDuetRAG\n36.3\nquestion q, selecting the answer aj with the highest score. We note that the External Knowledge Based Model has\naccess to more knowledge, so when there is only a small difference in confidence scores between the two answers, Mj\nis designed to lean towards selecting ae. If Mj is implemented using the summarization method, a prompt constructed\nbased on q, ai, and ae will be provided to Mj, and the answer aj will be obtained. This prompt contains information\nsuch as task instructions and answer preferences.\n5\nEvaluation\nWe conducted a series of experiments to validate the performance of DuetRAG. Our experiments were based on the\nLLAMA-7B model (Touvron et al., 2023) and the HotpotQA dataset (Yang et al., 2018). For the Ablation Study, we\ntested the performance of replacing modules in DuetRAG with GPT-3.5 and other methods. In future versions, we\nplan to evaluate DuetRAG\u2019s performance on a wider range of task sets and assess its transferability based on different\nmodels.\n5.1\nDatasets\nOur experiments were conducted based on the HotpotQA dataset and a collection of external knowledge from Wikipedia.\nIn subsequent versions, we aim to include validation work on additional datasets.\nHotpotQA (Yang et al., 2018) is an open-domain question-answering dataset based on Wikipedia, primarily focused on\ncommon knowledge such as movies, sports, history facts, etc. Most of the questions in HotpotQA are in the form of\nmulti-hop questions, requiring multiple steps of reasoning and summarization to arrive at an answer. In our experiments,\nwe used the first 5,000 data samples from the official training set containing 70,000 samples as training data, and\nthe first 1,000 data samples from the official validation set containing 7,000 samples as validation data. The external\nknowledge documents were sourced from Wikipedia and the fullwiki version provided by the HotpotQA dataset.\n5.2\nBaselines\nWe consider the following baselines for our experiments. All baselines were based on the LLAMA-7B model. In future\nversions, we will test more base models to validate the transferability of DuetRAG.\n\u2022 LLAMA 2-7B model with 0-shot prompting: this is the commonly used model for QA tasks. We provide\nclearly written instructions without any reference documents.\n\u2022 LLAMA 2-7B model with RAG(Llama2+RAG): much similar to the previous setting, expect we include\nreference documents. This is a popular technique when dealing with domain specific QA tasks.\n\u2022 Domain specific fine-tuning with 0-shot prompting(DSF) : Performing standard supervised fine-tuning\nwithout documents in context. Much similar to internal knowledge based model in DuetRAG except DSF is\nfine-tuned with full trainning data.\n\u2022 Domain specific fine-tuning with RAG(DSF+RAG): Performing standard supervised fine-tuning without\ndocuments in context. Much similar to external knowledge based model in DuetRAG.\n5.3\nResults\nIn Table 1, we validate the performance of DuetRAG based on the aforementioned datasets and baselines. Compared\nto LLAMA2-7B models based on the 0-shot prompt and RAG, DuetRAG shows nearly a two-fold improvement,\n4\n\nDuetRAG: Collaborative Retrieval-Augmented Generation\nA PREPRINT\nTable 2: Performance of Mj via Multiple Approach\nClassification\nSummarization\nAlignment\nText Feature\nLLaMA2-7B\nChatGPT-3.5\nAcc. of Mj\n36.3\n34.7\n33.3\n39.3\nAcc. Compared with Me\n+3.6\n+2.0\n+0.6\n+6.6\ndemonstrating strong adaptability to domain-specific questions. Additionally, compared to Domain Specific Fine-tuning\nand RAG Based Domain Specific Fine-tuning, DuetRAG also exhibits significant improvements. Since HotpotQA is a\nmulti-hop question-answering dataset, where retriever results may not always cover all documents containing relevant\nanswer information, the performance of RAG-based models does not significantly surpass 0-shot prompt-based models.\nFurthermore, compared to Domain Specific Fine-tuned models trained on the entire training set, the performance of our\nMi and Me exceeds the former two, indicating potential overfitting of the models on excessive training data. Finally,\nthe performance of DuetRAG surpasses that of Mi and Me, demonstrating the effectiveness of the strategy designed\nfor Mj.\n5.4\nAblation Study\nFor the Ablation Study, we tested the effectiveness of employing different methods as the referee model Mj, including\nclassification methods based on semantic alignment, text feature-based classification methods, an ensemble method\nbased on 0-shot LLaMA-7B, and an ensemble method utilizing ChatGPT-3.5 (in Table 2). We can see that the\nclassification method based on semantic alignment improved accuracy by 3.6 percent, surpassing the direct text\nfeature-based classification method. Although the Summarization Method using ChatGPT-3.5 demonstrated significant\nperformance enhancements, these improvements are primarily attributed to the powerful summarization capabilities of\nChatGPT due to its large-scale training. However, similar summarization methods applied to the 0-shot LLaMA2-7B\nmodel did not yield particularly favorable results, likely due to the limited summarization capabilities of LLaMA2-7B\nitself.\n6\nConclusion\nDUET is a novel multi-model collaboration framework designed to enhance the performance of question-answering\nmodels in specific domains with few training samples and complex environments. We have identified several key design\ndecisions, such as simultaneously using an Internal Knowledge-Based Model and an External Knowledge-Based Model\nto generate answers, and using a third model to generate the answer with the highest confidence level. Our evaluation\non HotpotQA highlights the significant potential of DUET. Looking ahead, we anticipate that Retrieval-Augmented\nGeneration (RAG) within the domain will continue to attract attention in both industry and academia. Unlike general\nRAG, our work addresses the practical scenario where LLMs are tasked with answering questions using domain-specific\nknowledge. Meanwhile, we will continue to update this work and validate the reliability and transferability of DUET\non more application scenarios and base models.\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774,\n2023.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-\nof-thought prompting elicits reasoning in large language models. Advances in neural information processing systems,\n35:24824\u201324837, 2022.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran\nMilan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in\nneural networks. Proceedings of the national academy of sciences, 114(13):3521\u20133526, 2017.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler,\nMike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks.\nAdvances in Neural Information Processing Systems, 33:9459\u20139474, 2020.\n5\n\nDuetRAG: Collaborative Retrieval-Augmented Generation\nA PREPRINT\nYue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong\nChen, et al. Siren\u2019s song in the ai ocean: a survey on hallucination in large language models. arXiv preprint\narXiv:2309.01219, 2023.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christo-\npher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint\narXiv:1809.09600, 2018.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model\npre-training. In International conference on machine learning, pages 3929\u20133938. PMLR, 2020.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van\nDen Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving\nfrom trillions of tokens. In International conference on machine learning, pages 2206\u20132240. PMLR, 2022.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau\nYih. Replug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652, 2023.\nXi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn,\nGergely Szilvasy, Mike Lewis, et al.\nRa-dit: Retrieval-augmented dual instruction tuning.\narXiv preprint\narXiv:2310.01352, 2023.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand\nJoulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models.\narXiv preprint arXiv:2208.03299, 2022.\nTianjun Zhang, Shishir G Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and Joseph E Gonzalez. Raft:\nAdapting language model to domain specific rag. arXiv preprint arXiv:2403.10131, 2024.\nBoxin Wang, Wei Ping, Lawrence McAfee, Peng Xu, Bo Li, Mohammad Shoeybi, and Bryan Catanzaro. Instructretro:\nInstruction tuning post retrieval-augmented pretraining. arXiv preprint arXiv:2310.07713, 2023.\nZihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Mohammad Shoeybi, and Bryan Catanzaro. Chatqa: Building gpt-4 level\nconversational qa models. arXiv preprint arXiv:2401.10225, 2024.\nGuohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for\"\nmind\" exploration of large language model society. Advances in Neural Information Processing Systems, 36, 2024a.\nYilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning\nin language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023.\nTian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shum-\ning Shi. Encouraging divergent thinking in large language models through multi-agent debate. arXiv preprint\narXiv:2305.19118, 2023.\nQiushi Sun, Zhangyue Yin, Xiang Li, Zhiyong Wu, Xipeng Qiu, and Lingpeng Kong. Corex: Pushing the boundaries of\ncomplex reasoning through multi-model collaboration. arXiv preprint arXiv:2310.00280, 2023.\nZekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, and Xifeng Yan. Guiding large language models\nvia directional stimulus prompting. Advances in Neural Information Processing Systems, 36, 2024b.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288, 2023.\n6\n"}