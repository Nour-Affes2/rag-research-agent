{"metadata": {"pdf_filename": "2408.00555v1__Alleviating Hallucination in Large Vision-Language Models with Active Retrieval .pdf", "source": "arXiv"}, "text": "arXiv:2408.00555v1  [cs.CV]  1 Aug 2024\nAlleviating Hallucination in Large Vision-Language Models\nwith Active Retrieval Augmentation\nXIAOYE QU\u2020, Huazhong University of Science and Technology, China\nQIYUAN CHEN\u2020, Zhejiang University, China\nWEI WEI\u2217, Huazhong University of Science and Technology, China\nJIASHUO SUN, Xiamen University, China\nJIANFENG DONG, Zhejiang Gongshang University, China\nDespite the remarkable ability of large vision-language models (LVLMs) in image comprehension, these\nmodels frequently generate plausible yet factually incorrect responses, a phenomenon known as hallucina-\ntion. Recently, in large language models (LLMs), augmenting LLMs by retrieving information from external\nknowledge resources has been proven as a promising solution to mitigate hallucinations. However, the re-\ntrieval augmentation in LVLM signi\ufb01cantly lags behind the widespread applications of LVLM. Moreover,\nwhen transferred to augmenting LVLMs, sometimes the hallucination degree of the model is even exacer-\nbated. Motivated by the research gap and counter-intuitive phenomenon, we introduce a novel framework,\nthe Active Retrieval-Augmented large vision-language model (ARA), speci\ufb01cally designed to address halluci-\nnations by incorporating three critical dimensions: (i) dissecting the retrieval targets based on the inherent\nhierarchical structures of images. (ii) pinpointing the most e\ufb00ective retrieval methods and \ufb01ltering out the\nreliable retrieval results. (iii) timing the retrieval process to coincide with episodes of low certainty, while cir-\ncumventing unnecessary retrieval during periods of high certainty. To assess the capability of our proposed\nARA model in reducing hallucination, we employ three widely used LVLM models (LLaVA-1.5, Qwen-VL, and\nmPLUG-Owl2) across four benchmarks. Our empirical observations suggest that by utilizing \ufb01tting retrieval\nmechanisms and timing the retrieval judiciously, we can e\ufb00ectively mitigate the hallucination problem. We\nhope that this study can provide deeper insights into how to adapt the retrieval augmentation to LVLMs for\nreducing hallucinations with more e\ufb00ective retrieval and minimal retrieval occurrences.\nACM Reference Format:\nXiaoye Qu\u2020, Qiyuan Chen\u2020, Wei Wei\u2217, Jiashuo Sun, and Jianfeng Dong. 2018. Alleviating Hallucination in\nLarge Vision-Language Models with Active Retrieval Augmentation. J. ACM 37, 4, Article 111 (August 2018),\n20 pages. https://doi.org/XXXXXXX.XXXXXXX\n\u2020The \ufb01rst two authors contribute equally.\n\u2217Corresponding author.\nAuthors\u2019 Contact Information: Xiaoye Qu\u2020, xiaoye@hust.edu.cn, Huazhong University of Science and Technology, China;\nQiyuan Chen\u2020, chenqiyuan1012@gmail.com, Zhejiang University, China; Wei Wei\u2217, weiw@hust.edu.cn, Huazhong Uni-\nversity of Science and Technology, China; Jiashuo Sun, gasolsun36@gmail.com, Xiamen University, China; Jianfeng Dong,\ndongjf24@gmail.com, Zhejiang Gongshang University, China.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for pro\ufb01t or commercial advantage and that copies bear this notice and\nthe full citation on the \ufb01rst page. Copyrights for components of this work owned by others than the author(s) must be\nhonored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists,\nrequires prior speci\ufb01c permission and/or a fee. Request permissions from permissions@acm.org.\n\u00a9 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM 1557-735X/2018/8-ART111\nhttps://doi.org/XXXXXXX.XXXXXXX\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n111:2\nQ et al.\nOurs: Yes, there is a clock in the image.\nVanilla: There is no clock in the image.\nIs there a clock\nin the image?\nRetrieval \nLVLM\nFig. 1. Given an input image and corresponding query, the vanilla LVLM fails to identify the clock within\nthe image and outputs a hallucinated answer. However, by employing the retrieval augmentation strategy\nproposed in this paper, the LVLM model can accurately answer the query.\n1\nINTRODUCTION\nRecently, Large Vision-Language Models (LVLMs) [2, 4, 8, 22, 26, 47, 56] have gained widespread\napplication across various scenarios due to their signi\ufb01cant capacity to generate contextually ap-\npropriate text for visual inputs. Bene\ufb01ting from the advancements in model architecture, training\nmethods, and data diversity, LVLMs exhibit superior performance for various tasks such as visual\nquestion answering [11, 32] and image captioning [36, 51]. Despite these advancements, LVLMs\nstill su\ufb00er from a signi\ufb01cant challenge termed \u201challucination\u201d, whereby the models produce se-\nmantically plausible but factually inaccurate text, misaligned with the ground-truth content of the\nassociated image [7, 12, 18, 49, 55, 57]. This problem damages the practical employment of LVLMs,\nparticularly in \ufb01elds that require accurate content generation, such as medical [19, 45] and ro-\nbotics [27, 39], potentially resulting in severe consequences. Hence, addressing the hallucination\nproblem is paramount for bolstering the credibility of LVLMs across real-world scenarios.\nTo mitigate hallucinations in LVLMs, a series of attempts have been recently proposed and\ncan be broadly classi\ufb01ed into two categories. The \ufb01rst class of approach retrains the LVLMs with\nconstructed hallucination-related datasets by supervised \ufb01ne-tuning (SFT) [10, 24, 44], or Rein-\nforcement Learning from Human Feedback (RLHF) [42, 50]. Although e\ufb00ective, these methods in-\ntroduce signi\ufb01cantly additional training costs. The other solutions focus on designing more robust\ndecoding strategies [7, 12, 18, 49]. For example, VCD [18] introduces visual contrastive decoding\nto contrast output distributions derived from original and distorted visual inputs. In this way, the\nLVLM reduces the over-reliance on statistical bias and unimodal prior. Although these methods are\ntraining-free, they still su\ufb00er from the limitations of LVLMs\u2019 static parametric capacity. Recently,\nin the realm of large language models (LMs), augmenting LLMs [14, 16, 34] by retrieving informa-\ntion from external knowledge resources has shown promise in reducing language hallucinations.\nFurthermore, this retrieval augmentation serves as a \ufb02exible way to extend beyond the model\u2019s\ninherent knowledge without the burden of extensive training costs. In this paper, we aim to pro-\npose a novel framework to augment LVLMs with external knowledge by introducing an Active\nRetrieval-Augmented large vision-language model (ARA) for mitigating hallucinations. As shown\nin Figure 1, given an input image and query, the LVLM tends to produce hallucinated answers.\nInstead, our model can accurately identify the most relevant pairs and output the correct answer.\nConcretely, our proposed ARA is grounded in three critical dimensions for mitigating halluci-\nnations in LVLMs. Firstly, given the inherently hierarchical nature of images, simple full-image\nretrieval may result in noise and irrelevant outcomes, thus it is imperative to decompose the tar-\nget object causing hallucination from the input image for more accurate retrieval. Secondly, in\ncontrast to augmenting LLMs, LVLMs o\ufb00er diverse retrieval modalities due to their multimodal\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\nAlleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation\n111:3\ninputs. In this way, discerning the most e\ufb00ective retrieval technique and securing trustworthy re-\nsults is crucial for ensuring retrieving performance. Lastly, excessive retrieval activations may lead\nto undue time expenditure and unnecessary retrieval. By initiating retrieval only during periods\nof low LVLM certainty and knowledge de\ufb01ciency, one can circumvent unnecessary or improper\ninformation retrieval.\nBased on the above analysis, given the input image and query, our proposed ARA \ufb01rst devises\na coarse-to-\ufb01ne retrieving paradigm. The target object is \ufb01rst extracted from the input query and\nsubsequently positioned within a particular region of the input image. To build a robust retriev-\ning system, we simultaneously engage in coarse-grained and \ufb01ne-grained retrieval process which\nretrieves the full image as well as the speci\ufb01c regions pertaining to the target object. Recognizing\nthe plurality of retrieval strategies due to the multimodal input, an extensive analysis of di\ufb00erent\nmethods is conducted to determine the most optimal retrieving approach. Subsequent to acquir-\ning pertinent text and image pairs from external databases, a reranking strategy is employed to\neliminate unreliable outcomes, thereby augmenting LVLMs with the appropriate external informa-\ntion. Particularly, retrieval operations are circumvented when the LVLM exhibits high certainty,\nto prevent the inclusion of super\ufb02uous retrieval that may introduce redundant information. In\nthis context, retrieval is activated using a di\ufb03culty metric that relies on the mutual information\nbetween multimodal inputs.\nTo assess the e\ufb03cacy of ARA, empirical evaluations are performed using three prevalent LVLMs\nacross four benchmarks related to hallucination challenges. The promising results demonstrate\nthat our framework can e\ufb00ectively mitigate hallucinations. To sum up, our contributions are sum-\nmarized as follows:\n\u2022 Our research thoroughly investigates the three critical dimensions of augmenting LVLMs\nwith external knowledge, including accurately dissecting the retrieval targets, pinpointing\nthe most e\ufb00ective retrieval methods, and the sensible triggering of the retrieval process.\n\u2022 We introduce an Active Retrieval-Augmented large vision-language model (ARA) for mitigat-\ning hallucination. Our empirical evidence indicates that, by employing appropriate retrieval\nmechanisms and carefully timing the retrievals, we can signi\ufb01cantly address the issue of\nhallucination while maintaining a moderate frequency of retrieval.\n\u2022 Through comprehensive experiments, we demonstrate the superior performance of our pro-\nposed ARA in alleviating hallucinations using three well-known LVLMs across four hallucination-\nrelated benchmarks. The results in our paper also suggest that with optimal retrieval settings,\nthe potential of augmenting LVLMs can be more e\ufb00ectively harnessed.\n2\nRELATED WORK\n2.1\nLarge Visual-Language Models\nRecently, large-scale models have attracted signi\ufb01cant attention and gained widespread applica-\ntions [30, 40, 41, 46, 58]. LVLMs signi\ufb01cantly enhance the interaction between humans and AI\nin a more natural and intuitive ways and demonstrate remarkable capabilities in understand-\ning and generating multi-modal content. With the aid of advanced Large Language Models like\nLLaMA [43] and Qwen [1], a batch of LVLMs such as LLaVA-1.5 [26], Qwen-VL [2], and mPLUG-\nOwl2 [47] have emerged, which can comprehend and generate a wide array of content by utilizing\ninformation from distinct modalities like texts and images. The training of LVLMs is divided into\ntwo critical phases: feature alignment and instruction \ufb01ne-tuning. The former aims to model visual\nand textual inputs coherently, while the latter strives to enhance the ability of LVLMs to compre-\nhend user queries. Despite the advancements, these LVLMs continue to face signi\ufb01cant challenges\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n111:4\nQ et al.\nwith hallucination issues. Thus, in this paper, we focus on solving hallucination problems to pro-\nmote the use of LVLMs in practical scenarios.\n2.2\nHallucination in LVLMs\nIn LVLMs, hallucination refers to models that generate seemingly plausible outputs inclusive of\nobjects, attributes, or relations that do not correspond with the images [20, 48, 52]. Approaches to\naddress hallucinations in LVLMs have largely fallen into two camps. The \ufb01rst emphasizes model\nre\ufb01nement through supervised \ufb01ne-tuning or application of reinforcement learning techniques.\nFor instance, LRV [25] seeks to diminish hallucinatory outputs by utilizing broad and varied su-\npervised \ufb01ne-tuning datasets. For methods based on reinforcement learning, LLaVA-RLHF [42] is\nthe pioneer in applying Reinforcement Learning with Human Feedback (RLHF) to mitigate hallu-\ncination in LVLMs. This approach is further re\ufb01ned by RLHF-V [50], which incorporates detailed\ncorrective human feedback. Considering the instability and training di\ufb03culty of RLHF, Zhao et\nal. [53] employ Direct Preference Optimization (DPO) and build a hallucination-aware dataset for\nalleviating hallucination. Although While these methods have yielded notable advancements, they\nalso introduce signi\ufb01cant training overheads and are prone to over\ufb01tting the training data. The\nsecond category consists of training-free strategies designed to circumvent hallucination without\nincurring additional training expenses. VCD [18] endeavors to rectify the model\u2019s over-reliance on\nlinguistic priors and statistical leanings by comparing the output distributions from both unaltered\nand visually perturbed inputs. OPERA [12] mitigates overcon\ufb01dence in model predictions by inte-\ngrating a penalty term into the model logits during beam search decoding, further enhanced with\na strategic rollback feature. Although these methods are e\ufb00ective, they still su\ufb00er from the limita-\ntions of LVLMs\u2019 static parametric capacity. In this paper, di\ufb00erent from the above two paradigms,\nwe focus on augmenting LVLMs with external knowledge to mitigate hallucination in LVLMs.\n2.3\nRetrieval-Augmented Generation\nIn the realm of large language models (LLMs), Retrieval-Augmented Generation (RAG) has been\nwidely used and shown promising results in mitigating hallucinations. RAG in LLMs retrieves rel-\nevant information from an external knowledge base before LLMs respond to a query [14, 16, 34],\nthereby enabling them to collaboratively generate responses by leveraging the retrieved external\nnon-parameterized knowledge alongside their internal parameterized knowledge. However, its ap-\nplication in LVLMs has not been thoroughly explored. Since LVLMs present a multimodal nature,\n\ufb01ndings pertinent to LLMs cannot be indiscriminately extrapolated to LVLMs. Predominantly, ex-\nisting research on retrieval augmentation in multimodal tasks has been con\ufb01ned to image caption-\ning [3, 35\u201337] or image generation [6], with a narrow focus that overlooks the broader implications\nof RAG in LVLMs, especially concerning its capacity for hallucination reduction. In this paper, our\nresearch conducts an extensive evaluation of RAG\u2019s applicability in LVLMs to mitigate hallucina-\ntion by tailoring retrieval strategies and optimizing active retrieval processes. It is worth noting\nthat the active retrieving strategy has also been discussed in [15] in LLM. However, they propose\na token-level con\ufb01dence for triggering the retrieval, which is not applicable in LVLMs and we will\ninvestigate it in the ablation study.\n3\nMETHOD\nIn this section, we \ufb01rst describe the generation procedure of the LVLMs, laying the groundwork for\ncomprehending our approach. Subsequently, we will introduce our Active Retrieval-Augmentation\nlarge vision-language model (ARA) in detail. As shown in Figure 2, given the input image and\nquery, the LVLM \ufb01rst analyzes them based on mutual information and determines whether to trig-\nger the retrieval process. If the retrieval is necessary, our ARA will follow a coarse-to-\ufb01ne paradigm\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\nAlleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation\n111:5\nWhat color is the clothing \nthat this person is wearing?\nInput Image\nCoarse-grained Retrieval\nFine-grained Retrieval\nTop1:\nThe snowboard \nis airborne.\nTop2:\nA man wearing \na coat.\nTop3:\nSkateboarder \nwearing red shirt.\nKnowledge\nBase\nTop1:\nA skier is performing \non a snowy slope.\nTop2:\nA person is sking \non the snow slope.\nTop3:\nA man on snowboard \nthat is in the snow.\nK\nl d\nKnowledge\nBase\nLVLM\nRetrieve\nOr Not\u6f26\nCoarse / Fine \nRetrieval\nKnowledge\nBase\nVisual \nFeatures\nVisual Encoder\nInput Query\nIndex\nInput Query\nExtract \nobjects\nGrounding\nVisual \nFeatures\nIndex\nVisual Encoder\nImage \nCaption\nRerank\nPairs\nOutput answer:\nRed\nTop1\nTop2\nTopK\nInput Image\nInput Image\nJoint Decoding\nOriginal answer:\nWhite\nFig. 2. The overall pipeline of our Active Retrieval-Augmented large vision-language model (ARA). Given\nthe input image and query, our model first actively determines whether the input pair requires retrieval. If\nretrieval is not necessary, the LVLM will directly answer the input query based on the image. Otherwise, our\nmodel will perform a coarse-to-fine retrieval based on the input image. Subsequently, the retrieved image\nand text pairs are further reranked according to the caption of the input image. Finally, the retrieved coarse-\ngrained and fine-grained pairs are jointly decoded to produce the final answer.\nto dissect the retrieval targets and conduct jointly coarse-grained and \ufb01ne-grained retrieval. Fol-\nlowing this, a reranking of the retrieval outcomes is performed for additional re\ufb01nement. Finally,\nthe LVLM harnesses this externally sourced knowledge to generate the \ufb01nal response.\n3.1\nLVLM Input and Decoding\nThe input of LVLMs contains both image and text. Typically, the LVLMs use a vision encoder (e.g.\nCLIP) to extract visual tokens from the input images, and then map them into the input space\nof LLMs with a linear projection module. Subsequently, the mapped visual tokens are used as\npart of the LLM input, along with the text input. In this paper, we denote the visual tokens as\nx\ud463= {\ud4650, \ud4651, . . . ,\ud465\ud441\u22121}. Here \ud441is the length of the visual tokens and it is a \ufb01xed number in most\ncases. Correspondingly, the input text is tokenized and we denote it as x\ud45d= {\ud4650,\ud4651, . . . ,\ud465\ud440\u22121}. The\nimage and text tokens are concatenated as the \ufb01nal input sequence and we denote it as {\ud465\ud456}\ud447\u22121\n\ud461=0\nthat \ud447= \ud441+ \ud440.\nDuring decoding, LVLM gets the probabilities for the next token prediction, formally:\n\ud45d(\ud465\ud461|\ud465<\ud461) = SoftMax[\ud43f\ud449\ud43f\ud440(\u210e\ud461)]\ud465\ud461,\n\ud465\ud461\u2208V,\n(1)\nwhere \u210e\ud461denotes the output hidden states at time step t for the input image and query, \ud465<\ud461to\nsimplify the sequence {\ud465\ud456}\ud461\u22121\n\ud456=0 and V means the whole vocabulary set. For greedy decoding, the\ntoken with the highest probability will be chosen.\n3.2\nActive Triggering Retrieval\nAs shown in Figure 2, given the input image and query, our ARA model \ufb01rst determines whether\nthe input pair requires retrieval. In this way, we can circumvent unnecessary retrieval during\nperiods of high certainty. In this section, we propose three active retrieval methods to explore the\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n111:6\nQ et al.\ndi\ufb03culty metrics that in\ufb02uence when to retrieve, including Con\ufb01dence-aware Active Retrieval,\nQuestion-aware Active Retrieval, and Image-aware Active Retrieval. The \ufb01rst method relies on\nthe con\ufb01dence of the model output, and the latter two matrices depend on the mutual information\nbetween the inputs. An ideal metric should e\ufb00ectively reduce the frequency of retrievals and can\neasily apply to di\ufb00erent LVLMs.\nCon\ufb01dence-aware Active Retrieval. We feed the image and query into LVLMs to generate the\nanswer. If the LVLM is con\ufb01dent about the answer, we accept it without retrieving additional\ninformation. Speci\ufb01cally, we actively trigger retrieval if any token of the answer has a con\ufb01dence\nlower than the threshold \ud703. In this method, \ud703= 0 indicates retrieval will never be used and \ud703= 1\nmeans that all inputs will use the retrieval. This strategy is inspired by [15] in LLM.\nQuery-aware Active Retrieval. To better evaluate the di\ufb03culty of queries from a view of lan-\nguage bias, we evaluate the degree of LVLMs answering relying on the input image. To this end,\nwe de\ufb01ne a di\ufb03culty metric using mutual information based on the di\ufb00erence below:\n\ud440\ud456\ud457= log \ud443(\ud44e\ud456\ud457|\ud449\ud456,\ud444\ud456)\n\ud443(\ud44e\ud456\ud457)\n\u2212log \ud443(\ud44e\ud456\ud457|\ud444\ud456)\n\ud443(\ud44e\ud456\ud457)\n= log \ud443(\ud44e\ud456\ud457|\ud449\ud456,\ud444\ud456)\n\ud443(\ud44e\ud456\ud457|\ud444\ud456) .\n(2)\nwhere \ud443(\ud44e\ud456\ud457|\ud444\ud456) indicates the output probability with only query input to the LVLMs and \ud443(\ud44e\ud456\ud457|\ud449\ud456,\ud444\ud456)\nmeans the output probability with both query and image as model input. The higher \ud440\ud456\ud457indicates\nmore visual reasoning is needed to answer. \ud440\ud456\ud457< 0 illustrates that the answer excessively depends\non the input query. Thus, we de\ufb01ne a threshold \ud703and trigger retrieval if \ud440\ud456\ud457< \ud703, where insu\ufb03-\ncient visual information is provided for answering the query. Incorporating retrieved information\nwill enhance the con\ufb01dence of LVLMs to answer.\nImage-aware Active Retrieval. Similar to the above query-aware active retrieval, we use the\nprobability di\ufb00erence between the original and noisy image to evaluate the image information to\nanswer the query. During the implementation, we use the method in [18] to add noise.\n\ud437\ud456\ud457= log \ud443(\ud44e\ud456\ud457|\ud449\ud456,\ud444\ud456)\n\ud443(\ud44e\ud456\ud457)\n\u2212log \ud443(\ud44e\ud456\ud457|\ud449\u2032\n\ud456,\ud444\ud456)\n\ud443(\ud44e\ud456\ud457)\n= log \ud443(\ud44e\ud456\ud457|\ud449\ud456,\ud444\ud456)\n\ud443(\ud44e\ud456\ud457|\ud449\u2032\n\ud456,\ud444\ud456) .\n(3)\nwhere \ud443(\ud44e\ud456\ud457|\ud449\ud456,\ud444\ud456) indicates the output probability with query and input image, \ud443(\ud44e\ud456\ud457|\ud449\u2032\n\ud456,\ud444\ud456)\nmeans the output probability with query and noisy image. Analogously, we de\ufb01ne a boundary \ud703\nand trigger retrieval if \ud437\ud456\ud457< \ud703.\n3.3\nCoarse-to-Fine Hierarchical Retrieval\nGiven the inherently hierarchical nature of images, simple full-image retrieval may result in noise\nand irrelevant outcomes. As shown in Figure 2, using the full image for retrieval can not obtain the\nmost relevant information for reasoning, namely \u201cred shirt\". Thus it is imperative to decompose\nthe target object causing hallucination from the input image for more accurate retrieval. In the\nfollowing, we will \ufb01rst describe the coarse-grained retrieval and then introduce the \ufb01ne-grained\nretrieve.\nCoarse-grained Retrieve. Initially, based on the input image, we use the CLIP to extract the\nvisual embedding. Similarly, we build the embeddings for the images and corresponding texts in\nthe knowledge base. Then, we can retrieve the relevant images from knowledge bases with the\nsimilarity between images. Finally, the top-K image and text pairs will be fetched for the following\nreasoning. Bene\ufb01ting from the multimodal encoding ability of CLIP, we can also retrieve the top-K\npairs by matching the input image with texts in the database. We will compare di\ufb00erent retrieving\nmethods in the ablation study.\nFine-grained Retrieve. While full-image retrieval can incorporate valuable information, it may\noverlook \ufb01ne-grained details, such as diminutive objects and intricate attribute data. Therefore, it\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\nAlleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation\n111:7\nis crucial to propose a \ufb01ne-grained retrieval mechanism that concentrates on the distinct objects\nwithin an image. To conduct \ufb01ne-grained retrieval, we initially extract key entities from the input\nquery utilizing the large language model LLaMA2-7B [43]. This extraction is facilitated by in-\ncontext learning [9], where demonstrations are presented to the LLM, which subsequently outputs\nthe speci\ufb01c entities from the input query. Following this, Grounding Dino [28] is deployed to\nidentify the objects in the image that correspond to the extracted entities. As illustrated in Figure 2,\nwe crop the targeted object and then perform retrieval in a manner akin to the previously described\ncoarse-grained retrieval.\n3.4\nReranking Retrieving Results\nThroughout the above retrieving process, the similarity is computed by the CLIP embeddings be-\ntween images. In this way, the visually similar but semantically di\ufb00erent pairs may be recalled. To\navoid noisy retrieving results which bring di\ufb00erent semantic, after obtaining the retrieved pairs\nfor both coarse-grained and \ufb01ne-grained retrieval, we subsequently propose a reranking strategy\ndesigned to recalibrate the retrieved outcomes. Speci\ufb01cally, we adopt the LVLM to describe the\ninput image and generate a detailed caption. For coarse-grained retrieval, the input full image is\ncharacterized, while describing the cropped image for the \ufb01ne-grained retrieval. In this way, we\nare able to derive the semantics of the input image. Then, we extract the textual embeddings for\nboth image captions and captions of retrieved images with the CLIP. Finally, we can rerank the\nretrieved pairs according to the semantic similarity between these captions. In this stage, we also\nexplore other reranking methods, such as k-reciprocal encoding [54] or using an external visual\nentailment tool [23]. We will compare them in the ablation study.\n3.5\nJoint Coarse-grained and Fine-grained Decoding\nAfter retrieving and ranking the pairs, we can conduct a joint coarse-grained and \ufb01ne-grained de-\ncoding to yield the \ufb01nal responses. According to the di\ufb00erent ways of using the retrieved results,\nwe have two schemes including probability-level fusion and instance-level fusion. The former in-\ntegrates the results of coarse-grained and \ufb01ne-grained retrieval into a single prompt for inference,\nwhile the latter conducts separate inferences on the coarse-grained and \ufb01ne-grained retrieval re-\nsults, followed by a probabilistic-level weighting.\nProbability-level Fusion. In this paradigm, we use the prompt for both coarse-grained retrieval\nand \ufb01ne-grained retrieval: \u201cHere are the image-caption pairs similar to the test image: \u27e8Retrieval\nPairs\u27e9. Based on these pairs and this image: \u27e8Input Image\u27e9. Answer this question: \u27e8Input query\u27e9\".\nHere we denote the retrieval-augmented LVLMs as below:\n\ud45d\ud450\ud45c\ud44e\ud45f\ud460\ud452(\ud465\ud461|\ud465<\ud461) = SoftMax[\ud43f\ud449\ud43f\ud440([\ud445\ud43c: \u210e\ud461])]\ud465\ud461,\n\ud465\ud461\u2208V,\n(4)\nwhere \ud445\ud43cindicates the hidden states of retrieved image-level pairs. For simplicity, we directly\nuse [:] here. We use \ud45d\ud450\ud45c\ud44e\ud45f\ud460\ud452to represent decoding with coarse-grained retrieval. Similarly,\n\ud45d\ud453\ud456\ud45b\ud452(\ud465\ud461|\ud465<\ud461) = SoftMax[\ud43f\ud449\ud43f\ud440([\ud445\ud442: \u210e\ud461])]\ud465\ud461,\n\ud465\ud461\u2208V,\n(5)\nwhere \ud445\ud442indicates the retrieved \ufb01ne-grained object-level pairs. We represent the decoding with\nobject-level \ufb01ne retrieval by \ud45d\ud453\ud456\ud45b\ud452(\ud465\ud461|\ud465<\ud461). For the probability-level fusion, we can combine them\nwith a hyperparameter \ud6fcin [0,1] to control the importance of two kinds of retrieval.\n\ud45d\ud450\ud45c\ud44e\ud45f\ud460\ud452&\ud453\ud456\ud45b\ud452(\ud465\ud461|\ud465<\ud461) = \ud6fc\ud45d\ud450\ud45c\ud44e\ud45f\ud460\ud452(\ud465\ud461|\ud465<\ud461) + (1 \u2212\ud6fc)\ud45d\ud453\ud456\ud45b\ud452(\ud465\ud461|\ud465<\ud461),\n(6)\nInstance-level Fusion. For this kind of fusion, we can integrate both coarse-grained retrieval and\n\ufb01ne-grained retrieval with prompts: \u201cHere are the image-caption pairs similar to the test image:\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n111:8\nQ et al.\n\u27e8Coarse-grained Retrieval Pairs\u27e9. Here are the image-caption pairs: \u27e8Fine-grained Retrieval Pairs\u27e9\nsimilar to the \u27e8entity\u27e9in the input image. Based on these pairs and this input image: \u27e8Input Image\u27e9.\nAnswer this question: \u27e8Input query\u27e9.\" Here we extract the entity from the input query and \ufb01ll it in\nthe prompt.\n\ud45d\ud450\ud45c\ud44e\ud45f\ud460\ud452+\ud453\ud456\ud45b\ud452(\ud465\ud461|\ud465<\ud461) = SoftMax[\ud43f\ud449\ud43f\ud440([\ud445\ud43c: \ud445\ud442: \u210e\ud461])]\ud465\ud461,\n\ud465\ud461\u2208V,\n(7)\n4\nEXPERIMENT\nThis section will elaborate on the evaluation of our proposed Active Retrieval-Augmented large\nvision-language model (ARA) across di\ufb00erent LVLMs.\n4.1\nEvaluation Metrics\nPOPE [20], the Polling-based Object Probing Evaluation proposed a simpli\ufb01ed method for evalu-\nating object hallucinations. In this evaluation framework, the LVLM is tasked with determining\nthe presence of speci\ufb01c objects in a given image, with a balanced distribution of queries for ob-\nject presence and absence. The evaluation protocol encompasses three distinct sampling settings:\nrandom, popular, and adversarial, each employing unique strategies for constructing negative sam-\nples. In the random setting, objects not present in the image are chosen randomly. The popular\nsetting selects missing objects from a high-frequency pool, whereas the adversarial setting pri-\noritizes coexisting objects that could be easily mistaken for existing objects in the image. The\nPOPE benchmark aggregates data from three diverse sources: MSCOCO [21], A-OKVQA [38], and\nGQA [13]. Within each dataset, every sampling setting comprises 500 images, with 6 questions\nposed for each image, resulting in a total of 27,000 question-answer pairs derived from the devel-\nopment sets of these datasets. The evaluation predominantly assesses metrics such as accuracy,\nprecision, recall, and F1 score.\nMME [48] is a comprehensive benchmark designed speci\ufb01cally for evaluating LVLMs on multiple\ndimensions. It includes ten sub-tasks related to perception and four sub-tasks emphasizing cog-\nnition. According to previous studies [7, 18, 49], we utilize the existence and count subsets for\nobject-level hallucination evaluation, and the position and color subsets for attribute-level halluci-\nnation evaluation. Performance is quanti\ufb01ed by accuracy and accuracy+, which is also the o\ufb03cial\nevaluation method.\nMMStar [5], foundational to vision-reliant multi-modal evaluation, consists of 1,500 challenge\nsamples meticulously chosen by human experts. It is tailored to systematically benchmark six key\ncapabilities alongside eighteen granular facets, aiming to rigorously assess the multi-modal pro\ufb01-\nciency of LVLMs using a selection of samples that have been carefully re\ufb01ned and equilibrated.\nMMbench [29] is collected from multiple sources, including public datasets and the Internet, and\ncurrently, contains 2974 multiple-choice questions, covering 20 ability dimensions. Similar to MME,\nwe focus on hallucination-related ability. Thus, we select 4 ability subsets including object local-\nization, attribute recognition, spatial relationship, and action recognition as our evaluation bench-\nmarks. These subsets form a comprehensive assessment of hallucinations, addressing issues at the\nobject-level, attribute-level, and relation-level comprehensively.\n4.2\nImplementation Details\nIn the case of coarse-grained retrieval, we use the COCO dataset [21] as the retrieval database. For\n\ufb01ne-grained retrieval, we use the VisualGenome [17] to build the \ufb01ne-grained retrieval database\nas it provides a detailed caption for each grounding box in the image. CLIP-Large is used to extract\nimage embeddings for retrieval. To decompose target objects from the input query, we extract key\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\nAlleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation\n111:9\nTable 1. Results on POPE. Regular decoding denotes greedy decoding, whereas RAR refers to our method\nwith augmentation.\nDataset\nSetting\nModel\nDecoding\nAccuracy\u2191\nPrecision\nRecall\nF1 Score\u2191\nMSCOCO\nRandom\nLLaVA1.5\nRegular\n86.50\n97.40\n75.00\n84.75\nRAR\n89.43\n95.96\n82.33\n88.63\nQwen-VL\nRegular\n82.13\n98.88\n65.00\n78.44\nRAR\n84.27\n98.22\n69.80\n81.61\nmPLUG-Owl2\nRegular\n85.97\n95.15\n75.80\n84.38\nRAR\n89.60\n94.33\n84.27\n89.01\nPopular\nLLaVA1.5\nRegular\n85.53\n95.02\n75.00\n83.83\nRAR\n87.47\n92.07\n82.00\n86.74\nQwen-VL\nRegular\n81.93\n98.29\n65.00\n78.25\nRAR\n84.03\n97.58\n69.80\n81.38\nmPLUG-Owl2\nRegular\n84.63\n92.06\n75.80\n83.14\nRAR\n86.10\n87.68\n84.00\n85.80\nAdversarial\nLLaVA1.5\nRegular\n83.60\n90.71\n74.87\n82.03\nRAR\n84.53\n87.37\n80.73\n83.92\nQwen-VL\nRegular\n80.97\n95.41\n65.07\n77.37\nRAR\n82.90\n94.58\n69.80\n80.32\nmPLUG-Owl2\nRegular\n82.37\n87.32\n75.73\n81.11\nRAR\n82.77\n82.40\n83.33\n82.86\nA-OKVQA\nRandom\nLLaVA1.5\nRegular\n88.63\n93.54\n83.00\n87.95\nRAR\n90.27\n90.81\n89.60\n90.20\nQwen-VL\nRegular\n83.60\n95.90\n70.20\n81.06\nRAR\n86.53\n95.74\n76.47\n85.03\nmPLUG-Owl2\nRegular\n86.47\n91.07\n80.87\n85.66\nRAR\n88.60\n89.33\n87.67\n88.49\nPopular\nLLaVA1.5\nRegular\n85.53\n87.43\n83.00\n85.16\nRAR\n85.63\n85.85\n85.33\n85.59\nQwen-VL\nRegular\n83.47\n95.55\n70.20\n80.94\nRAR\n85.97\n94.40\n76.47\n84.49\nmPLUG-Owl2\nRegular\n82.43\n83.48\n80.87\n82.15\nRAR\n82.77\n80.17\n87.07\n83.48\nAdversarial\nLLaVA1.5\nRegular\n79.13\n77.04\n83.00\n79.91\nRAR\n79.33\n76.96\n83.73\n80.20\nQwen-VL\nRegular\n78.97\n85.13\n70.20\n76.95\nRAR\n80.50\n83.18\n76.47\n79.68\nmPLUG-Owl2\nRegular\n74.70\n73.38\n80.87\n76.94\nRAR\n75.63\n71.54\n85.13\n77.75\nGQA\nRandom\nLLaVA1.5\nRegular\n88.87\n93.64\n83.40\n88.22\nRAR\n90.10\n91.57\n88.33\n89.92\nQwen-VL\nRegular\n82.63\n95.45\n68.53\n79.78\nRAR\n85.60\n96.27\n74.07\n83.72\nmPLUG-Owl2\nRegular\n85.17\n91.05\n78.00\n84.02\nRAR\n86.90\n89.51\n83.60\n86.45\nPopular\nLLaVA1.5\nRegular\n84.57\n85.39\n83.40\n84.38\nRAR\n84.70\n84.27\n85.33\n84.80\nQwen-VL\nRegular\n80.40\n89.86\n68.53\n77.76\nRAR\n83.43\n91.41\n73.80\n81.67\nmPLUG-Owl2\nRegular\n78.67\n79.05\n78.0\n78.52\nRAR\n79.70\n78.07\n82.60\n80.27\nAdversarial\nLLaVA1.5\nRegular\n81.63\n80.55\n83.40\n81.95\nRAR\n81.83\n80.43\n84.13\n82.24\nQwen-VL\nRegular\n78.40\n85.38\n68.53\n76.04\nRAR\n80.93\n85.86\n74.07\n79.53\nmPLUG-Owl2\nRegular\n76.40\n75.58\n78.00\n76.77\nRAR\n76.53\n75.38\n78.80\n77.05\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n111:10\nQ et al.\nTable 2. Results on the hallucination subset of MME.\nModel\nDecoding\nObject-level\nAttribute-level\nTotal Scores\u2191\nExistence\u2191\nCount\u2191\nPosition\u2191\nColor\u2191\nLLaVA-1.5\nRegular\n195.00\n126.67\n106.67\n140.00\n605.00\nRAR\n195.00\n145.00\n133.33\n175.00\n648.33\nQwen-VL\nRegular\n165.00\n145.00\n103.33\n185.00\n598.33\nRAR\n170.00\n140.00\n118.33\n195.00\n623.33\nmPLUG-Owl2\nRegular\n170.00\n145.00\n76.67\n163.33\n555.00\nRAR\n175.00\n148.33\n78.33\n163.33\n565.00\nentities from the input query utilizing the large language model LLaMA2-7B [43]. Grounding Dino\n[28] is deployed to identify the objects in the image that correspond to the extracted entities. If\nthe objects can not be located in the image, we only use coarse-grained retrieval for the following\nreasoning process. In this paper, we implement our ARA model on three LVLMs, including LLaVA\n1.5, Qwen-VL, and mPLUG-Owl2 to evaluate the e\ufb00ectiveness of our method. For LLaVA 1.5 and\nmPLUG-Owl2, we use the retrieved texts to augment, while we apply the retrieved image and text\npairs to boost Qwen-VL. For these three LVLMs, we implement augmentation using 3, 4, and 5\nretrieval components respectively. For active retrieval, we use the query-aware active retrieval.\nFor the hyperparameter \ud6fcpresented in Equation 6, we apply a value of 0.8 for the POPE, MMstar,\nand the object-level subsets within the MME benchmark, as well as for OL, SR, and ACR in the\nMMbench. A value of 0.4 is utilized for the remaining evaluation subsets. For active triggering\nretrieval, we use query-aware active retrieval for its simplicity and e\ufb00ectiveness. The threshold of\nactive triggering is determined by grid search.\nIn our experiments, to have a strict comparison of our RAR and vanilla LVLMs, we consistently\nutilize greedy decoding instead of other probability sampling methods. With this strategy, the\ntoken with the highest probability from the post-softmax distribution is directly selected as the\nnext token.\n4.3\nExperiment Results\nResults on POPE. Experimental results on POPE under the random, popular, and adversarial set-\ntings are summarized in Table 1. This benchmark mainly focuses on the object-level hallucination.\nSpeci\ufb01cally, the performances of our ARA consistently surpass the baseline results on all of the\nLVLMs. This suggests its pivotal role in augmenting LVLMs with retrieval, thereby reducing in-\nstances of object-level hallucination. In a more detailed model-speci\ufb01c analysis, RAR demonstrates\nvaried e\ufb00ects across di\ufb00erent LVLMs. For all three LVLMs, the F1 score elevation is predominantly\ndriven by a recall boost (e.g., up to 10 points for mPLUG-Owl2), showcasing retrieving to external\nknowledge base can e\ufb00ectively help detect object presences.\nResults on MME Hallucination Subset. The MME subset evaluations extend beyond POPE\u2019s\nscope, encompassing both object-level and attribute-level hallucination. Results in Table 2 show\nthat our ARA leads to a uniform enhancement in addressing object-level hallucination for all\nmodels, except the count score of Qwen-VL. Meanwhile, our ARA demonstrates signi\ufb01cant im-\nprovements in attribute-level Color scores, contributing to substantial overall performance gains.\nThese improvements are attributable to our designed coarse-to-\ufb01ne retrieval paradigms, which\ne\ufb00ectively focus on the target object.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\nAlleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation\n111:11\nTable 3. Results on the MMStar benchmark. CP (coarse perception), FP (fine-grained perception), IR (in-\nstance reasoning), LR (logical reasoning), MA (mathematics), ST (science & technology).\nModel\nDecoding\nCP\nFGP\nIR\nLR\nMA\nST\nAve.\nLLaVA-1.5\nRegular\n0.556\n0.264\n0.388\n0.276\n0.236\n0.208\n0.321\nRAR\n0.660\n0.416\n0.464\n0.340\n0.320\n0.252\n0.409\nQwen-VL\nRegular\n0.540\n0.348\n0.460\n0.288\n0.292\n0.204\n0.355\nRAR\n0.584\n0.428\n0.524\n0.324\n0.260\n0.280\n0.400\nmPLUG-Owl2\nRegular\n0.544\n0.268\n0.420\n0.328\n0.236\n0.172\n0.328\nRAR\n0.636\n0.396\n0.480\n0.392\n0.312\n0.252\n0.411\nTable 4. Results on the hallucination subsets of MMbench benchmark. OL (object localization), ATR (at-\ntribute recognition), SR (spatial relationship), and ACR (action recognition).\nModel\nDecoding\nOL\nATR\nSR\nACR\nAve.\nLLaVA1.5\nRegular\n0.6032\n0.8750\n0.3785\n0.9581\n0.7147\nRAR\n0.6540\n0.8902\n0.4350\n0.9628\n0.7293\nQwen-VL\nRegular\n0.5397\n0.8144\n0.3842\n0.9116\n0.6684\nRAR\n0.6413\n0.8523\n0.4407\n0.9349\n0.7271\nmPLUG-Owl2\nRegular\n0.5206\n0.7955\n0.3672\n0.9209\n0.6560\nRAR\n0.5873\n0.8182\n0.4237\n0.9581\n0.7024\nResults on MMStar. In a more di\ufb03cult benchmark MMStar which rigorously assesses the multi-\nmodal pro\ufb01ciency, as depicted in Table 3, our method ARA secures a notable enhancement across\nall six subsets. For instance, LLaVA-1.5 realizes an average improvement of 8.8 points. Unlike POPE,\nwhich may include statistical bias, answering questions in MMStar necessitates reliance on im-\nage information. Hence, our retrieval augmentation method introducing external image and text\nknowledge signi\ufb01cantly bolsters model performance.\nResults on MMBench Hallucination Subset. This benchmark extends our hallucination eval-\nuation to incorporate the relation level in addition to the above object and attribute levels, pro-\nviding a more holistic assessment for reducing hallucinations. As evidenced in Table 4, bene\ufb01t-\ning from coarse-grained retrieval, our ARA manifests discernible enhancements in the object and\nrelation-level subsets, such as OL, SR, and ACR. For instance, our ARA model surpasses the vanilla\nLLaVA-1.5 with 5.08 percent points on object localization (OL). Concurrently, the integration of\n\ufb01ne-grained retrieval into our model facilitates superior performance in attribute-level subsets,\nnamely attribute recognition (ATR).\n5\nABLATION STUDY\nIn this section, we undertake a series of ablation studies with the aim of identifying the critical ele-\nments integral to constructing a resilient and e\ufb03cient retrieval-augmented large vision-language\nmodel.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n111:12\nQ et al.\nTable 5. Ablation studies on di\ufb00erent retrieving methods. The results are obtained on POPE MSCOCO Pop-\nular by LLaVA 1.5. For this study, we only conduct coarse-grained retrieval.\nMethods\nAccuracy\nPrecision\nRecall\nF1 Score\nT \u2192T\n86.67\n93.72\n78.60\n85.50\nT \u2192I\n51.60\n50.81\n99.87\n67.36\nI \u2192I\n86.93\n91.41\n81.53\n86.19\nI \u2192T\n86.67\n90.92\n81.47\n85.94\nDINOv2-Base\n86.07\n90.37\n80.73\n85.28\nCLIP-Base\n86.33\n90.79\n80.87\n85.54\nCLIP-Large\n86.93\n91.41\n81.53\n86.19\n5.1\nAblation of Coarse-to-Fine Retrieval\nIn this section, in order to explore the e\ufb00ects of di\ufb00erent factors on coarse-to-\ufb01ne retrieval in a\nsimple and e\ufb00ective way, we did not use the rerank method or active trigger strategy.\nWhich kind of retrieval is better? Due to the multimodal characteristics of the input, there are\nvarious retrieval methods at our disposal. As shown in Table 5, we attempt to use the input query\nand image to search for images and descriptions in the database. For this study, we only conduct\ncoarse-grained retrieval and we use the COCO dataset as the database which contains a caption\nfor each image. As a result, there are four retrieval methods available to us. As re\ufb02ected in the\nresults within this table, we adopt the input image as our primary retrieval modality for images in\nthe database throughout our series of experiments. Surprisingly, using the input query to search\nthe database for image leads to substantially poorer outcomes, achieving only 51.6% accuracy. This\nlower performance may stem from the fact that, despite the presence of relevant keywords, the\nimages retrieved this way tend to be surrounded by a signi\ufb01cant degree of noise.\nAdditionally, we investigate the in\ufb02uence of di\ufb00erent embedding models on the e\ufb03cacy of\nimage-to-image retrieval. In our comparison, we examine three models: CLIP-Base/Large [33] and\nDINOV2-Base [31]. Among them, we notice that CLIP-base performs better than DINO (86.33 vs\n86.07) and this may be related to their di\ufb00erent training methods. Moreover, when comparing\nCLIP models of various sizes, we observe that the larger-scale CLIP models contribute to more\npronounced improvements in performance. Therefore, in the context of this study, CLIP-Large is\nselected as the image embedding model for retrieval purposes.\nHow to augment the LVLM? After retrieving, we can obtain retrieved images and paired cap-\ntions. In this section, we explore how to augment the LVLM with this external information. For\na fair comparison, in this study, we \ufb01x the number of retrieved pairs to 3 for both coarse-grained\nand \ufb01ne-grained retrieval. As demonstrated in Table 6, when using coarse-grained retrieval, we\nobserve that the image and caption pair perform worse than the single caption (86.93 vs 87.43).\nHowever, the opposite results are observed in \ufb01ne-grained retrieval and the Fine (I+T) achieves\n86.53 compared with 86.20 of Fine (T). These results imply that retrieval augmentation is sensitive\nto speci\ufb01c con\ufb01gurations. In this experiment, we do not carefully compare with Coarse (I) or Fine\n(I) as we have found that the LVLMs we use in our experiments are still quite weak in multi-image\nreasoning.\nIn Section 3.5, we propose a joint coarse-grained and \ufb01ne-grained decoding, designed to op-\ntimally utilize the two retrieval mechanisms. In Table 6, we also investigate the e\ufb00ectiveness of\nprobabilistic-level fusion denoted as Coarse & Fine and instance-level fusion labeled as Coarse+Fine.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\nAlleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation\n111:13\nTable 6. Performance on the LLaVA-1.5. In this paper, we compare di\ufb00erent kinds of retrieval schemes and\nregular decoding. \u201cI\" indicates augmentation with only retrieved texts and \u201cI+T\" means enhancing with both\ntexts and images. The experiments are performed on the popular part of POPE MSCOCO dataset. Coarse\nand Fine indicate coarse-grained and fine-grained retrieval, separately. Coarse+Fine means the instance-level\nfusion, while Coarse & Fine corresponds to the probabilistic-level fusion.\nMethod\nAccuracy\nPrecision\nRecall\nF1\nRegular\n85.53\n95.02\n75.0\n83.83\nCoarse (T)\n86.43\n93.03\n80.93\n86.56\nCoarse (I+T)\n86.93\n91.41\n81.53\n86.19\nFine (T)\n86.20\n89.46\n82.07\n85.61\nFine (I+T)\n86.53\n93.08\n78.93\n85.43\nCoarse+Fine (T)\n86.27\n89.59\n82.07\n85.66\nCoarse+Fine (I+T)\n86.77\n91.88\n80.67\n85.91\nCoarse & Fine (T)\n87.47\n92.07\n82.00\n86.74\nCoarse & Fine (I+T)\n87.23\n92.47\n81.07\n86.39\nFig. 3. Performance of three LVLMs corresponding to di\ufb00erent numbers of retrieval pairs. The performance\ntrends of accuracy and F1 score are the same. The experiments are performed on the popular part of the\nPOPE MSCOCO dataset.\nAs shown from the results, we observe that Coarse+Fine even performs than Coarse retrieval, sig-\nnifying that simply integrating the coarse-grained and \ufb01ne-grained retrieved results in a prompt\nfails to enhance performance, given the distinct granularity of the information retrieved through\nthese two approaches. Instead, the probabilistic-level fusion Coarse & Fine achieves better accu-\nracy than both two kinds of retrieval. Thus, in our paper, we use probabilistic-level fusion for the\nfollowing experiments. It is worth noting that Coarse & Fine (T) performs better than Coarse &\nFine (I+T) for LLaVA-1.5 but Coarse & Fine (I+T) achieves better for Qwen-VL (we will describe in\nFigure 3 later). This result further validates the necessity of carefully selecting retrieval con\ufb01gura-\ntions to obtain the most favorable experimental results.\nHow much external information do we need? With the above retrieval, we can obtain multiple\nimage-caption pairs from the database. This research delves into the e\ufb03cacy of varying retrieved\npair con\ufb01gurations. Considering that di\ufb00erent LVLMs may be sensitive to retrieval con\ufb01gurations,\nour study incorporates both (T) and (I+T) con\ufb01gurations for a thorough evaluation. The analysis\nis constrained to examining between one to \ufb01ve retrieved pairs due to the input token limitations\ninherent in contemporary LVLMs. As depicted in Figure 3, LLaVA (T) reaches peak accuracy with\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n111:14\nQ et al.\nImage\nIs there a \ndining table \nin the image?\nQuestion\nA group of people are sitting in a swimming \npool, enjoying a meal together.\na hotel pool with lawn \nchairs and umbrellas.\na large group of people \nin a swimming pool.\nfruit and a bowl of bread \nsit on a table.\na group of people \nswimming in a pool.\na man sipping a \nwine glass at a bar.\nRetrieved Result\nImage\nCaption\n1\n2\n3\n4\n5\n0.83\n0.94\n0.35\n0.87\n0.26\nCalculate \nSimilarity \nRetrieval\nCaptioning\n2\n1\n4\n3\n5\nTop-K\nRerank\nFig. 4. The detailed reranking process in our method. With reranking, the retrieved five pairs are sorted in a\nnew order. This example is from the POPE benchmark and the LVLM used here is LLaVA 1.5.\nTable 7. Ablation studies on di\ufb00erent reranking methods. The results are obtained on POPE MSCOCO Pop-\nular by LLaVA-1.5. In this experiment, we incorporate a reranking strategy on top of the coarse-grained\nretrieval. Thus, the result of the first line (None) aligns with the results in Table 5.\nMethods\nAccuracy\nPrecision\nRecall\nF1 Score\nNone\n86.93\n91.41\n81.53\n86.19\n\ud458-Reciprocal [54]\n87.03\n91.36\n81.8\n86.32\nVSR [23]\n86.97\n91.29\n81.73\n86.25\nRAR\n87.17\n91.57\n81.87\n86.45\nthree retrieved texts, whereas mPLUG (T) begins to saturate when there are four retrieval texts.\nConversely, QwenVL (I+T) demonstrates improved outcomes with a greater number of retrieval\npairs; however, it still lags considerably behind the performance achieved by the other two LVLM\nmodels.\n5.2\nThe e\ufb00ectiveness of Reranking Method\nTo substantiate the e\ufb03cacy and indispensability of reranking, we begin by providing a case study\nwithin the POPE benchmark. As depicted in Figure 4, in instances where an image portrays peo-\nple in a pool, leveraging CLIP for image retrieval may inadvertently introduce irrelevant content\ndue to its predominant emphasis on visual congruence. For instance, it might retrieve an incon-\ngruent image captioned \u201cfruit and a bowl of bread sit on the table.\" Therefore, an adept reranking\nmethodology can pro\ufb01ciently \ufb01lter and organize the retrieved images in a more relevant manner.\nIn this particular scenario, our reranking strategy aligns the input image caption with those of\nthe retrieved images. By assessing these similarity metrics, we are able to reorganize the pool\nof retrieved images into a more accurately ranked series, thereby enhancing the precision of the\nretrieval process.\nIn order to further quantify the e\ufb00ect of reranking and compare di\ufb00erent reranking methods, we\nconduct experiments in Table 7. The result reveals that the omission of re-ranking precipitates a\ndecline in accuracy, dropping from 87.17% to 86.93% This suggests that re-ranking e\ufb00ectively miti-\ngates noise, particularly when employing the top three re-ordered retrieval pairs. For comparative\npurposes, we executed two alternative methodologies. The k-Reciprocal approach [54], a prevalent\no\ufb04ine person re-identi\ufb01cation method, re-ranks images using the k-Reciprocal nearest neighbor\nalgorithm. While this method o\ufb00ers a swifter execution by circumventing the captioning process\nof input images, its e\ufb03cacy slightly underperforms our strategy. Additionally, we examined the\nemployment of the external tool VSR [23], which determines the entailment between the caption\nof the input image and the retrieved images. As shown in Table 7, this method gains only a slight\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\nAlleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation\n111:15\nConfidence-Aware\nQuery-Aware\nImage-Aware\nLLaVA\nQwenVL\nmPLUG\nFig. 5. Di\ufb00erent strategies for triggering retrieval on the popular part of POPE MSCOCO. The horizontal\naxis represents the di\ufb00iculty threshold, the lef vertical axis represents the proportion of queries that trigger\nretrieval out of all queries, and the right vertical axis shows the values of F1 score and accuracy. From top to\nbotom, the three lines respectively represent: confidence-aware active retrieval, query-aware active retrieval,\nand image-aware active retrieval.\nTable 8. The performance comparison between our method RAR and VCD in POPE MSCOCO benchmark.\nVCD is the state-of-the-art method published in CVPR24. The results of VCD-sampling are from their paper.\nFor a fair comparison with our method, we rerun VCD with greedy decoding.\nSetting\nDecoding\nAccuracy\u2191\nPrecision\nRecall\nF1 Score\u2191\nRandom\nVCD-greedy [18]\n85.37\n93.24\n76.27\n83.90\nVCD-sampling [18]\n87.73\n91.42\n83.28\n87.16\nRAR\n89.43\n95.96\n82.33\n88.63\nPopular\nVCD-greedy [18]\n83.30\n88.75\n76.27\n82.04\nVCD-sampling [18]\n85.38\n86.92\n83.28\n85.06\nRAR\n87.47\n92.07\n82.00\n86.74\nAdversarial\nVCD-greedy [18]\n80.00\n82.37\n76.33\n79.24\nVCD-sampling [18]\n80.88\n79.45\n83.29\n81.33\nRAR\n84.58\n87.17\n80.73\n83.92\nincrease in accuracy (86.93 vs 86.97). This may be due to the visual entailment model not being\nsu\ufb03ciently versatile.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n111:16\nQ et al.\nTable 9. Text generation quality comparison. Results on the MS-COCO dataset.\nModel\nDecoding\nB@4\nMETEOR\nROUGE-L\nCIDEr\nSPICE\nAve.\nLLaVA-1.5\nRegular\n22.3\n28.0\n50.9\n75.3\n22.1\n39.7\nRAR\n25.8\n28.3\n53.2\n94.2\n22.2\n44.7\nQwen-VL\nRegular\n25.3\n25.5\n50.3\n96.4\n19.8\n43.4\nRAR\n40.8\n29.8\n60.5\n137.2\n23.4\n58.3\nmPLUG-Owl2\nRegular\n28.4\n27.3\n54.1\n96.5\n19.0\n45.1\nRAR\n32.1\n28.5\n56.3\n103.7\n20.8\n48.3\n5.3\nAblation of Active Triggering Retrieval\nAs excessive retrieval activations may lead to undue time expenditure and unnecessary retrieval,\nin this paper, retrieval is activated using a di\ufb03culty metric. In this ablation study, we compare\nthree kinds of active retrieval methods, including con\ufb01dence-aware active retrieval, query-aware\nactive retrieval, and image-aware active retrieval. As shown in Figure 5, we have drawn the propor-\ntion of queries that need to be retrieved to achieve peak performance. Firstly, it is quite intuitive,\nthe \ufb02uctuations of con\ufb01dence-aware active retrieval among di\ufb00erent models are very signi\ufb01cant.\nFor mPLUG-Owl2, we even need to retrieve all the queries, which indicates that con\ufb01dence-aware\nactive retrieval is not a good indicator of whether to perform a retrieval. For both query-aware\nactive retrieval and image-aware active retrieval, the LVLMs can better determine a trigger thresh-\nold. Considering that query-aware active retrieval is more concise and the process of adding noise\nis omitted, we use query-aware active retrieval in all our experiments. It is worth noting that the\nvery slight decline in the model\u2019s performance after continuing to use retrieval beyond the trigger\nthreshold is due to the fact that the model itself has a high level of con\ufb01dence, but the retrieval\nhas introduced some redundant noise information, which we cannot \ufb01lter out through reranking.\n5.4\nComparison with State-of-the-Art Method\nIn this paper, we augment LVLMs with external knowledge to mitigate hallucinations. To further\ndemonstrate the e\ufb00ectiveness of our method, we compare it with VCD [18] and the results are\ndemonstrated in Table 8. VCD is a state-of-the-art training-free method that explores intra-model\nknowledge with visual contrastive decoding. From the results, we can notice that our method\nsigni\ufb01cantly surpasses VCD in all three settings. For instance, in popular settings, our method\ngains a 4.06 percent improvement. It is worth noting that VCD performs comparably with the\nvanilla LLaVA-1.5 when using greedy decoding. This phenomenon is also observed in [7]. Instead,\nour method signi\ufb01cantly surpasses the vanilla LLaVA-1.5 model as presented in Tabe 1. This result\nfurther veri\ufb01es the robust improvement of introducing external information when applying our\nARA method.\n5.5\nText Generation Qality Evaluation\nIn this section, we evaluated the e\ufb00ectiveness of our method in improving text generation quality\non the MS-COCO validation set1. As shown in Table 9, all models LLaVA, Qwen-VL, and mPLUG\nexhibited signi\ufb01cant improvements after applying our method, with an average improvement of\n7.7% across the three models. Notably, the improvement was most pronounced for Qwen-VL, with\nan average increase of approximately 14.9%.\n1We use the o\ufb03cial COCO evaluation package.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\nAlleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation\n111:17\nIs there a mouse\nin the image?\nLLaVA:  No, there \nis no mouse in the \nimage.\nARA: Yes, there is a \nwhite mouse near the \nkeyboard.\nRetrieved Results\nAnswer\nCaption: two \nmonitors with a \nkeyboard and \nmouse on a desk.\nCaption: two imac\ncomputers are on a \ndesk with a mouse\nand keyboard.\nCaption: a turned \non  computer \nsitting on top of a \ndesk.\nCaption: a white \ncomputer mouse.\nCaption: white \nmouse on grey \nmousepad.\nCaption: white \nwireless mouse\nin front of tape.\nCoarse\nQuery\nFine\nIs there a laptop\nin the image?\nFine\nCoarse\nCaption: a \nlaptop and some \ncomputer screens \non a desk.\nCaption: office \ncubicle with a \ncomputer and a \nlaptop.\nCaption: a wooden \ntable topped with a \nlaptop computer and \na desktop computer.\nLLaVA:  No, there \nis a computer in the \nimage.\nARA: Yes, there is a \nblack laptop in the \ncorner.\nCaption: a silver \nlaptop that is \nswitched on.\nCaption: laptop\nis visible on a \ndesk.\nCaption: open \nblack laptop \nsitting on floor.\nIs there a clock\nin the image?\nFine\nCoarse\nCaption: a  \npicture of the big \nben clock tower \nin london.\nCaption: a large \ntower with a \nclock at the top.\nCaption: an old \nclock tower is \nseen next to a \ngrassy area.\nLLaVA:  No, there \nis only a tower in the \nimage.\nARA: Yes, there is a \nclock on the top of \nthe tower.\nCaption: black \nnumbers on \nclock face.\nCaption: clock\nin tower to tell \ntime to people \noutside.\nCaption: black \nand white clock.\nFine\nCoarse\nCaption: a \nslender vase filled \nwith four red \nflowers.\nCaption: a clear \nvase has red and \nyellow flowers.\nCaption: a group of \nred flowers with \nlong green stems in \na glass vase.\nLLaVA:  Yes, there \nare green flowers in \nthe image.\nARA: No, there are\nonly red flowers in\nthe image.\nCaption: a vase \nfilled with flowers.\nCaption: tan \ncurtains behind table \nand flower vases\nCaption: 10 \nroses in a vase.\nAre there any \ngreen flowers in \nthe image?\nFine\nCoarse\nCaption: three \ndogs running \ntogether in a field \nof grass.\nCaption: a dog \nreturning an \norange ball to his \nowner.\nCaption: four dogs \nare running  and \nbathing in the ocean.\nLLaVA:  Yes, there \nare three dogs in the \nimage.\nARA: No, there are\nfour dogs running on\ngrass.\ng\nCaption: there are \ntwo dogs in the \npicture.\nCaption: two dogs \nplayfully running.\nCaption: two \ndogs on grass.\nAre there only \nthree dogs appear \nin this image?\nFig. 6. Examples from POPE benchmark and MME. We present both coarse-grained and fine-grained re-\ntrieval for each instance.\n5.6\nQalitative Results\nTo qualitatively verify the e\ufb00ectiveness of our ARA method on downstream tasks, we presented\n\ufb01ve examples from the POPE MSCOCO dataset and MME benchmark. As shown in Figure 6, we\npresent three image-caption pairs for both coarse-grained retrieval and \ufb01ne-grained retrieval. In\nthe above three examples of determining the existence of objects, \ufb01ne-grained retrieval helps to\nincrease the model\u2019s existence information about speci\ufb01c objects. For the last two examples, the\ncoarse-grained retrieval contributed e\ufb00ectively to capturing the overall information of the images.\nThus, with both kinds of retrieval, our RAR model can e\ufb00ectively increase the accuracy compared\nwith vanilla LLaVA-1.5.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n111:18\nQ et al.\n6\nCONCLUSION\nIn this paper, we propose a novel framework to augment LVLMs with external knowledge by in-\ntroducing an Active Retrieval-Augmented large vision-language model (ARA) for mitigating hal-\nlucinations. Speci\ufb01cally, our proposed ARA is grounded in three critical dimensions for mitigat-\ning hallucinations in LVLMs, including coarse-to-\ufb01ne retrieval for dissecting the retrieval targets,\npinpointing the most e\ufb00ective retrieval methods, and triggering the retrieval process properly.\nOur \ufb01ndings indicate that by utilizing \ufb01tting retrieval mechanisms and timing the retrieval judi-\nciously, we can e\ufb00ectively mitigate the hallucination problem. Our extensive experiments across\nfour benchmarks and three widely-used LVLM con\ufb01rm ARA\u2019s e\ufb03cacy in reducing hallucination.\nREFERENCES\n[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al.\n2023. Qwen technical report. arXiv preprint arXiv:2309.16609 (2023).\n[2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. 2023. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond.\n(2023).\n[3] Jingwen Chen, Yingwei Pan, Yehao Li, Ting Yao, Hongyang Chao, and Tao Mei. 2023. Retrieval augmented convolu-\ntional encoder-decoder networks for video captioning. ACM Transactions on Multimedia Computing, Communications\nand Applications 19, 1s (2023), 1\u201324.\n[4] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas\nChandra, Yunyang Xiong, and Mohamed Elhoseiny. 2023. Minigpt-v2: large language model as a uni\ufb01ed interface for\nvision-language multi-task learning. arXiv preprint arXiv:2310.09478 (2023).\n[5] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao,\nDahua Lin, et al. 2024. Are We on the Right Way for Evaluating Large Vision-Language Models? arXiv preprint\narXiv:2403.20330 (2024).\n[6] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W Cohen. 2022. Re-imagen: Retrieval-augmented text-to-\nimage generator. arXiv preprint arXiv:2209.14491 (2022).\n[7] Zhaorun Chen, Zhuokai Zhao, Hongyin Luo, Huaxiu Yao, Bo Li, and Jiawei Zhou. 2024. HALC: Object Hallucination\nReduction via Adaptive Focal-Contrast Decoding. arXiv preprint arXiv:2403.00425 (2024).\n[8] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N\nFung, and Steven Hoi. 2024. Instructblip: Towards general-purpose vision-language models with instruction tuning.\nAdvances in Neural Information Processing Systems 36 (2024).\n[9] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022.\nA survey on in-context learning. arXiv preprint arXiv:2301.00234 (2022).\n[10] Anisha Gunjal, Jihan Yin, and Erhan Bas. 2024. Detecting and preventing hallucinations in large vision language\nmodels. In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence, Vol. 38. 18135\u201318143.\n[11] Jiaxian Guo, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Boyang Li, Dacheng Tao, and Steven CH Hoi. 2022.\nFrom images to textual prompts: Zero-shot vqa with frozen large language models. arXiv preprint arXiv:2212.10846\n(2022).\n[12] Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and\nNenghai Yu. 2023. Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty\nand retrospection-allocation. arXiv preprint arXiv:2311.17911 (2023).\n[13] Drew A Hudson and Christopher D Manning. 2019. Gqa: A new dataset for real-world visual reasoning and com-\npositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition.\n6700\u20136709.\n[14] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand\nJoulin, Sebastian Riedel, and Edouard Grave. 2022. Atlas: Few-shot learning with retrieval augmented language models.\narXiv preprint arXiv:2208.03299 (2022).\n[15] Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and\nGraham Neubig. 2023. Active retrieval augmented generation. arXiv preprint arXiv:2305.06983 (2023).\n[16] Vladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau\nYih. 2020. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906 (2020).\n[17] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A Shamma, et al. 2017. Visual genome: Connecting language and vision using crowdsourced\ndense image annotations. International journal of computer vision 123 (2017), 32\u201373.\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\nAlleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation\n111:19\n[18] Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. 2023.\nMiti-\ngating object hallucinations in large vision-language models through visual contrastive decoding. arXiv preprint\narXiv:2311.16922 (2023).\n[19] Chunyuan Li, Cli\ufb00Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung\nPoon, and Jianfeng Gao. 2024. Llava-med: Training a large language-and-vision assistant for biomedicine in one day.\nAdvances in Neural Information Processing Systems 36 (2024).\n[20] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023. Evaluating object hallucination\nin large vision-language models. arXiv preprint arXiv:2305.10355 (2023).\n[21] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence\nZitnick. 2014. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13. Springer, 740\u2013755.\n[22] Daizong Liu, Mingyu Yang, Xiaoye Qu, Pan Zhou, Wei Hu, and Yu Cheng. 2024. A Survey of Attacks on Large\nVision-Language Models: Resources, Advances, and Future Trends. arXiv preprint arXiv:2407.07403 (2024).\n[23] Fangyu Liu, Guy Edward Toh Emerson, and Nigel Collier. 2023. Visual Spatial Reasoning. Transactions of the Associ-\nation for Computational Linguistics (2023).\n[24] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. 2023. Aligning large multi-modal\nmodel with robust instruction tuning. arXiv preprint arXiv:2306.14565 (2023).\n[25] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. 2023. Mitigating hallucination in large\nmulti-modal models via robust instruction tuning. In The Twelfth International Conference on Learning Representations.\n[26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024. Visual instruction tuning. Advances in neural\ninformation processing systems 36 (2024).\n[27] Kangcheng Liu, Xinhu Zheng, Chaoqun Wang, Hesheng Wang, Ming Liu, and Kai Tang. 2024. Online Robot Naviga-\ntion and and Manipulation with Distilled Vision-Language Models. arXiv preprint arXiv:2401.17083 (2024).\n[28] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\nZhu, et al. 2023. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv\npreprint arXiv:2303.05499 (2023).\n[29] Yuanzhan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Con-\nghui He, Ziwei Liu, Kai Chen, and Dahua Lin. 2023. MMBench: Is Your Multi-modal Model an All-around Player?\nArXiv abs/2307.06281 (2023). https://api.semanticscholar.org/CorpusID:259837088\n[30] Zhenyi Lu, Jie Tian, Wei Wei, Xiaoye Qu, Yu Cheng, Dangyang Chen, et al. 2024. Mitigating Boundary Ambiguity\nand Inherent Bias for Text Classi\ufb01cation in the Era of Large Language Models. arXiv preprint arXiv:2406.07001 (2024).\n[31] Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. 2023. Dinov2: Learning robust visual features without\nsupervision. arXiv preprint arXiv:2304.07193 (2023).\n[32] Archiki Prasad, Elias Stengel-Eskin, and Mohit Bansal. 2023. Rephrase, augment, reason: Visual grounding of ques-\ntions for vision-language models. arXiv preprint arXiv:2310.05861 (2023).\n[33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda\nAskell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision.\nIn International conference on machine learning. PMLR, 8748\u20138763.\n[34] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.\n2023. In-context retrieval-augmented language models. Transactions of the Association for Computational Linguistics\n11 (2023), 1316\u20131331.\n[35] Rita Ramos, Bruno Martins, and Desmond Elliott. 2023. LMCap: Few-shot Multilingual Image Captioning by Retrieval\nAugmented Language Model Prompting. arXiv preprint arXiv:2305.19821 (2023).\n[36] Rita Ramos, Bruno Martins, Desmond Elliott, and Yova Kementchedjhieva. 2023. Smallcap: lightweight image caption-\ning prompted with retrieval augmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition. 2840\u20132849.\n[37] Sara Sarto, Marcella Cornia, Lorenzo Baraldi, Alessandro Nicolosi, and Rita Cucchiara. 2024.\nTowards Retrieval-\nAugmented Architectures for Image Captioning. ACM Transactions on Multimedia Computing, Communications and\nApplications (2024).\n[38] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. 2022. A-okvqa:\nA benchmark for visual question answering using world knowledge. In European Conference on Computer Vision.\nSpringer, 146\u2013162.\n[39] Dhruv Shah, B\u0142a\u017cej Osi\u0144ski, Sergey Levine, et al. 2023. Lm-nav: Robotic navigation with large pre-trained models of\nlanguage, vision, and action. In Conference on robot learning. PMLR, 492\u2013504.\n[40] Zhaochen Su, Juntao Li, Jun Zhang, Tong Zhu, Xiaoye Qu, Pan Zhou, Yan Bowen, Yu Cheng, et al. 2024. Living in the\nMoment: Can Large Language Models Grasp Co-Temporal Reasoning? arXiv preprint arXiv:2406.09072 (2024).\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n\n111:20\nQ et al.\n[41] Zhaochen Su, Jun Zhang, Tong Zhu, Xiaoye Qu, Juntao Li, Min Zhang, and Yu Cheng. 2024. Timo: Towards Better\nTemporal Reasoning for Language Models. arXiv preprint arXiv:2406.14192 (2024).\n[42] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-\nXiong Wang, Yiming Yang, et al. 2023. Aligning large multimodal models with factually augmented rlhf. arXiv preprint\narXiv:2309.14525 (2023).\n[43] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and \ufb01ne-tuned chat models.\narXiv preprint arXiv:2307.09288 (2023).\n[44] Lei Wang, Jiabang He, Shenshen Li, Ning Liu, and Ee-Peng Lim. 2024.\nMitigating \ufb01ne-grained hallucination by\n\ufb01ne-tuning large vision-language models with caption rewrites. In International Conference on Multimedia Modeling.\nSpringer, 32\u201345.\n[45] Sheng Wang, Zihao Zhao, Xi Ouyang, Qian Wang, and Dinggang Shen. 2023. Chatcad: Interactive computer-aided\ndiagnosis on medical image using large language models. arXiv preprint arXiv:2302.07257 (2023).\n[46] Yue Wang, Tianfan Fu, Yinlong Xu, Zihan Ma, Hongxia Xu, Bang Du, Yingzhou Lu, Honghao Gao, Jian Wu, and Jintai\nChen. 2024. TWIN-GPT: Digital Twins for Clinical Trials via Large Language Model. ACM Transactions on Multimedia\nComputing, Communications and Applications (2024).\n[47] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. 2023. mplug-\nowl2: Revolutionizing multi-modal large language model with modality collaboration. arXiv preprint arXiv:2311.04257\n(2023).\n[48] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. 2023. A survey on multimodal\nlarge language models. arXiv preprint arXiv:2306.13549 (2023).\n[49] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and En-\nhong Chen. 2023.\nWoodpecker: Hallucination correction for multimodal large language models.\narXiv preprint\narXiv:2310.16045 (2023).\n[50] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng,\nMaosong Sun, et al. 2023. Rlhf-v: Towards trustworthy mllms via behavior alignment from \ufb01ne-grained correctional\nhuman feedback. arXiv preprint arXiv:2312.00849 (2023).\n[51] Zequn Zeng, Yan Xie, Hao Zhang, Chiyu Chen, Zhengjue Wang, and Bo Chen. 2024. MeaCap: Memory-Augmented\nZero-shot Image Captioning. arXiv preprint arXiv:2403.03715 (2024).\n[52] Bohan Zhai, Shijia Yang, Xiangchen Zhao, Chenfeng Xu, Sheng Shen, Dongdi Zhao, Kurt Keutzer, Manling Li, Tan\nYan, and Xiangjun Fan. 2023. Halle-switch: Rethinking and controlling object existence hallucinations in large vision\nlanguage models for detailed caption. arXiv preprint arXiv:2310.01779 (2023).\n[53] Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He. 2023. Beyond hallucinations:\nEnhancing lvlms through hallucination-aware direct preference optimization. arXiv preprint arXiv:2311.16839 (2023).\n[54] Zhun Zhong, Liang Zheng, Donglin Cao, and Shaozi Li. 2017. Re-ranking person re-identi\ufb01cation with k-reciprocal\nencoding. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1318\u20131327.\n[55] Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao.\n2023. Analyzing and mitigating object hallucination in large vision-language models. arXiv preprint arXiv:2310.00754\n(2023).\n[56] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv preprint arXiv:2304.10592 (2023).\n[57] Lanyun Zhu, Deyi Ji, Tianrun Chen, Peng Xu, Jieping Ye, and Jun Liu. 2024. IBD: Alleviating Hallucinations in Large\nVision-Language Models via Image-Biased Decoding. arXiv preprint arXiv:2402.18476 (2024).\n[58] Linhai Zhuo, Yuqian Fu, Jingjing Chen, Yixin Cao, and Yu-Gang Jiang. 2024. Uni\ufb01ed View Empirical Study for Large\nPretrained Model on Cross-Domain Few-Shot Learning. ACM Transactions on Multimedia Computing, Communica-\ntions and Applications (2024).\nJ. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.\n"}