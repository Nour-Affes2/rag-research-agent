{"metadata": {"pdf_filename": "2403.03187v1__Reliable Adaptable and Attributable Language Models with Retrieval.pdf", "source": "arXiv"}, "text": "Reliable, Adaptable, and Attributable Language Models with Retrieval\nAkari Asai 1 2 Zexuan Zhong 3 Danqi Chen 3 Pang Wei Koh 1\nLuke Zettlemoyer 1 2 Hannaneh Hajishirzi 1 4 Wen-tau Yih 2\nAbstract\nParametric language models (LMs), which are\ntrained on vast amounts of web data, exhibit re-\nmarkable flexibility and capability. However, they\nstill face practical challenges such as hallucina-\ntions, difficulty in adapting to new data distribu-\ntions, and a lack of verifiability. In this position\npaper, we advocate for retrieval-augmented LMs\nto replace parametric LMs as the next genera-\ntion of LMs. By incorporating large-scale datas-\ntores during inference, retrieval-augmented LMs\ncan be more reliable, adaptable, and attributable.\nDespite their potential, retrieval-augmented LMs\nhave yet to be widely adopted due to several ob-\nstacles: specifically, current retrieval-augmented\nLMs struggle to leverage helpful text beyond\nknowledge-intensive tasks such as question an-\nswering, have limited interaction between re-\ntrieval and LM components, and lack the infras-\ntructure for scaling. To address these, we pro-\npose a roadmap for developing general-purpose\nretrieval-augmented LMs. This involves a recon-\nsideration of datastores and retrievers, the explo-\nration of pipelines with improved retriever-LM\ninteraction, and significant investment in infras-\ntructure for efficient training and inference.\n1. Introduction\nLarge language models (LMs) such as GPT-4 (Black et al.,\n2022) have shown impressive abilities in a range of natural\nlanguage processing (NLP) tasks. Such parametric LMs\nencapsulate rich natural language understanding abilities\nand a wealth of world knowledge in their parameters, ac-\nquired via massive pre-training on large-scale web corpora\n(Figure 1, top). However, they still suffer from several funda-\nmental weaknesses including W1: the prevalence of factual\nerrors (Min et al., 2023a; Mishra et al., 2024), W2: the dif-\n1University of Washington\n2Meta AI\n3Princeton Univer-\nsity 4Allen Institute for AI. Correspondence to: Akari Asai\n<akari@cs.washington.edu>.\nPreprint. Work in progress.\nParametric LMs\nInput:   List the top five US states with the highest per-capita GDP, in order.\nTraining \ncorpus\nTop five states are: \n1.\nDistrict of Columbia \n(DC)\n2.\nNew York\n3.\nMassachusetts\n4.\nCalifornia\n5.\nConnecticut\n: Pre-trained  on large-scale pre-training data\nLM\nDatastore\nRetriever\nTop five states are: \n1.\nDC\n2.\nNew York\n3.\nMassachusett\n4.\nWashington\n5.\nCalifornia\n: Incorporate data at inference \nFactual inaccuracies  \nDifficulty of verification\nLarge model size\nReduced factual errors\nInput\nExpensive costs to adapt\nDifficulty of data opt-out\nInput\n2022 per capita GDP: DC \n(192k), NY (79k), MA (77k), \nWA (74k), CA (73k)\nBetter attributions \nAdaptivity & \ncustomizability \nParameter efficiency \nFlexible data opt-in/out\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\nRetrieval-augmented LMs\nLM\nFigure 1. Parametric LMs (top) internalize large-scale text data\nin their parameters via massive pre-training, while retrieval-\naugmented LMs (bottom) incorporate text retrieved from a massive\ndatastore at test time.\nficulty of verification (Bohnet et al., 2022), W3: difficulty\nof opting out certain sequences with concerns (Henderson\net al., 2023), W4: computationally expensive costs for adap-\ntations (Longpre et al., 2023), and W5: prohibitively large\nmodel size (Kandpal et al., 2022a). Moreover, merely scal-\ning up the model has been insufficient to overcome such\nlimitations (Mallen et al., 2023) or even exacerbates the\nchallenges (Carlini et al., 2021).\nThis position paper advocates for retrieval-augmented LMs\nto supersede parametric LMs as the next generation of LMs\n(Figure 1, bottom), addressing many of the aforementioned\nweaknesses. Unlike parametric LMs\u2014which use large-scale\ntext data only during training\u2014retrieval-augmented LMs\nleverage an external large-scale collection of documents\n(datastore) at inference by selecting relevant documents\nfrom the datastore (Asai et al., 2023a). Retrieval-augmented\nLMs can W1: largely reduce factual errors (Mallen et al.,\n2023), W2: provide better attributions (Gao et al., 2023a),\nW3: enabling flexible opt-in and out of sequences (Min\net al., 2024). By adding or removing data from their datas-\ntores, retrieval-augmented LMs can W4: easily adapt to new\ndistributions (Khandelwal et al., 2020). Lifting the burden\nof memorizing everything in parameters makes them W5:\n1\narXiv:2403.03187v1  [cs.CL]  5 Mar 2024\n\nReliable, Adaptable, and Attributable Language Models with Retrieval\nmore parameter-efficient (Izacard et al., 2023).\nDespite their considerable potential to significantly improve\nreliability, adaptability, and attributability, their broader\nadoption beyond specific knowledge-intensive tasks (e.g.,\nquestion answering or QA; Chen et al. 2017) is currently\nlimited. We argue that through fundamental advancements\nin architecture, training methodologies, and infrastructure\nfor retrieval-augmented LMs, they can demonstrate sub-\nstantial efficacy across diverse domains. We urge the re-\nsearch community to intensify efforts aimed at overcoming\nthose inherent limitations for their widespread adoption.\nTo facilitate future research, we identify several significant\nchallenges. First, existing approaches primarily leverage\ncontext with high semantic or lexical similarity to the in-\nput (C1), struggling when helpful text is absent in com-\nmon datastores or does not align with conventional rele-\nvance definitions (BehnamGhader et al., 2023; Asai et al.,\n2023b). Second, prepending the retrieved text to the in-\nput of off-the-shelf LMs, which has been widely used re-\ncently, leads to shallow interactions between the retrieval\nand LM components (C2). This often results in unsupported\ngenerations (Gao et al., 2023a), susceptibility to irrelevant\ntext (Yoran et al., 2024), and challenges in handling infor-\nmation from multiple pieces of text (Borgeaud et al., 2022).\nFurthermore, unlike rapid progress for efficient training and\ninference of parametric LMs (Zhao et al., 2023b; Dao et al.,\n2022), there are limited studies and open-sourced efforts to\nenhance the training and inference efficiency of retrieval-\naugmented LMs at scale (C3).\nWe conclude this paper with a roadmap to advance retrieval-\naugmented LMs to foster wider adoption. First, addressing\nthe challenge of finding helpful text for diverse tasks (C1),\nit is important to reconsider the notion of relevance and\nadvance our understanding of what constitutes an effec-\ntive datastore\u2014specifically, exploring the types of infor-\nmation that should be retrieved from various datastores to\nenhance the performance in broader tasks. Then, we sug-\ngest approaches to ensure deeper interactions between the\ntwo components, including architecture, pre-training, and\npost-training adaptations (C2), rather than focusing on sup-\nplementary enhancement of existing parametric LMs. For\nchallenges of scaling (C3), we call for more open-sourced\nand interdisciplinary efforts across hardware, systems, and\nalgorithms to develop infrastructures for training and infer-\nence (e.g., scaling datastore to trillion tokens). By pursuing\nthese avenues, we anticipate unlocking the full capabilities\nof retrieval-augmented LMs and expanding their applica-\ntions across a spectrum of tasks and domains.\n2. How Far Can We Go with Parametric LMs?\nWe first assess the limitations of parametric LMs. Despite\nrapid progress in this area, we argue that parametric LMs\nhave many practical limitations, which in turn pose signifi-\ncant challenges to building reliable intelligent systems.\nDefinition.\nA parametric LM (Figure 1, top) consists of\na set of parameters \u03b8. Given input sequences from a large-\nscale text dataset Dtrain, learnable parameters \u03b8 are trained\nto predict the probabilities of future or masked tokens. Dur-\ning test time, for an input sequence x, the trained \u03b8 predicts\nthe outputs: y = f\u03b8(x), without accessing any external data\nbeyond that of the task at hand.\n2.1. Weaknesses of Parametric LMs\nMounting evidence highlights significant limitations in para-\nmetric LMs. Many such challenges arise from the strategy\nof attempting to store all knowledge within the parameters,\nwhich scaling alone may not adequately address.\nW1: Factual inaccuracies.\nAttempting to memorize all\nthe learned knowledge within the parameters can lead to\nfactual inaccuracies, which are often called hallucinations.\nSeveral recent papers report that even state-of-the-art LMs\nsuch as ChatGPT exhibit hallucinations in the majority of\ntheir outputs (Min et al., 2023a; Mishra et al., 2024). Mallen\net al. (2023); Kandpal et al. (2022a) show that they particu-\nlarly struggle with long-tail knowledge\u2014factual knowledge\nthat is less represented during pre-training\u2014and that scaling\nonly yields minor improvements. Gudibande et al. (2024)\nfind that increasing synthetic labeled data during instruction\ntuning may not improve the factuality of model outputs.\nW2: Difficulty of verifications.\nNot only have LMs\nshown a propensity for hallucinations in their generations,\nbut it is also difficult for practitioners to fact-check their out-\nputs due to a lack of clear attributions or provenance. The\noutputs of powerful LMs are often lengthy, assertive, and\nplausible (Min et al., 2023a), which makes post-hoc attri-\nbutions or factual verification to be challenging and largely\nunsolved tasks (Mishra et al., 2024; Yue et al., 2023).\nW3: Difficulty of opting out certain sequences from the\ndatasets.\nManaging the vast volume of pre-training data\nposes a considerable challenge in identifying and filtering\nout training instances with potential privacy (Brown et al.,\n2022) or copyright-protected data (Lee et al., 2024). Re-\ncent work studies intensive red teaming and safety tuning\nefforts (Touvron et al., 2023b; Perez et al., 2022), unlearn-\ning (Jang et al., 2023) or iterative pre-training of models on\ncorpora after removing certain data (Kandpal et al., 2022b).\nYet, the absence of proper attributions further complicates\nthese endeavors, as tracing back to and eliminating specific\ntraining instances becomes non-trivial (Grosse et al., 2023).\nW4: Computationally expensive costs to adapt.\nAdapt-\ning parametric LMs trained on static unlabeled text (i.e.,\n2\n\nReliable, Adaptable, and Attributable Language Models with Retrieval\ntext collected at a certain timestamp from the web) re-\nquires continuous training or computationally expensive\npost-adaptation to new data distributions. For instance, their\nparametric knowledge can quickly become obsolete (Long-\npre et al., 2023). While several approaches propose to locate\nand edit certain outdated knowledge (De Cao et al., 2021)\nor conduct efficient continued training (Jin et al., 2022) to\nkeep up with the world, these approaches require additional\ncomputationally expensive learning processes. LMs trained\non widely adopted pre-training corpora often perform well\non general-purpose domains such as news articles (Dodge\net al., 2021), but struggle on expert domains (Taylor et al.,\n2022). Prior work demonstrates the effectiveness of contin-\nued pre-training (Azerbayev et al., 2024; Chen et al., 2023b)\nor instruction tuning (Singhal et al., 2023), albeit at a consid-\nerable computational cost and possibilities of catastrophic\nforgetting (Li et al., 2022).\nW5: Prohibitively large model size.\nNumerous studies\nshowcase the positive impact of model scaling on task per-\nformance (Chowdhery et al., 2022; Wei et al., 2022), and\nthe ability to recall factual knowledge memorized from the\ntraining data (Carlini et al., 2023; Mallen et al., 2023; Kand-\npal et al., 2022a). This trend has prompted the community\nto focus on boosting the model size in pursuit of better\nperformance, at the cost of significant computational chal-\nlenges and environmental concerns (Strubell et al., 2019;\nWeidinger et al., 2022). Despite efforts to enhance effi-\nciency, hosting these massive models, which often exceed a\nhundred billion parameters, remains impractical for many\nindustry or academic groups (Schwartz et al., 2019).\n3. How Can Retrieval-Augmented LMs\nAddress These Issues?\nIn this section, we discuss how retrieval-augmented LMs\ncan alleviate the aforementioned issues in parametric LMs.\nDefinition.\nA retrieval-augmented LM (Figure 1, bottom;\ndetailed in Figure 2) typically consists of two key compo-\nnents: a retriever R and a parametric LM \u03b8. The retriever\nbuilds a search index I1 based on documents in the datastore\nD. During inference time, given an input sequence x, the\nretriever finds relevant text z2 from the inference datastore,\nleveraging an index I: z = fR,I(x). Subsequently, the\nLM \u03b8 uses both the original prompt and the retrieved text to\npredict the output y: y = f\u03b8(x, z).\n1In term-based retrieval systems such as BM25 (Robertson &\nZaragoza, 2009) that count the occurrences of words in documents\nin the datastore, the index I is a weighted bag-of-words vector,\nwhile in more recent trainable neural retrieval systems such as\nDPR (Karpukhin et al., 2020), the index is a collection of float\nembeddings encoded by an encoder LM.\n2There are different granularities for relevant text z (e.g., text\nchunks, tokens, phrases). See Section 4.1.1 for more details.\nOrigins, progress, and recent shift.\nThe concept of\nretrieval augmentation has been extensively explored across\nvarious machine learning domains (Tian et al., 2019). In\nNLP, earlier efforts have been applied to specific tasks such\nas QA and machine translation. Chen et al. (2017) introduce\nDrQA, which combines a term-based information retrieval\n(IR) system with a neural QA model to answer knowledge-\nintensive questions. While IR and such task LMs were ini-\ntially studied separately, several work explores more organic\ncombinations of retrieval and LM by pre-training the two\ncomponents jointly or sequentially, including REALM (Guu\net al., 2020), RAG (Lewis et al., 2020a), RETRO (Borgeaud\net al., 2022), etc.\nSuch earlier work designed special architectures and training\nobjectives for the retrieval-augmented LM. Most recently,\nthere has been a shift of view of retrieval-augmented LMs\u2014\ninstead of training retrieval-augmented LMs from scratch,\nsome work supplementary integrate retrieval on top of ex-\nisting powerful parametric LMs (e.g., GPT-3; Black et al.\n2022) without any additional training. Such methods\u2014often\nreferred to simply as Retrieval-Augmented Generation or\nRAG\u2014concatenate the original input sequence x with re-\ntrieved text z when prompting, yielding significant improve-\nments over the base parametric LMs on certain knowledge-\nintensive tasks (Ram et al., 2023; Shi et al., 2023c). Many\nrecent studies explore advanced prompting methods with\nretrieval components (Yao et al., 2023; Press et al., 2023)\nor develop pipelines for further improvements (Gao et al.,\n2023b). RAG has been integrated into real-world applica-\ntions such as LLM search systems.3\n3.1. Effectiveness of Retrieval-Augmented LMs\nWe now review some empirical findings from prior studies\nsuggesting their effectiveness in addressing the weaknesses\nof parametric LMs discussed in Section 2.1.\nW1: Reduced factual errors in long-tail knowledge.\nRe-\ncent studies show that retrieval-augmented LMs can allevi-\nate the shortcomings of parametric memorization by explic-\nitly capturing long-tail knowledge (Mallen et al., 2023). As\na result, retrieval-augmented LMs can minimize hallucina-\ntions and improve the factuality of generated outputs (Lewis\net al., 2020b; Izacard et al., 2023; Ram et al., 2023; Shi\net al., 2023c; Asai et al., 2024; Min et al., 2023b).\nW2: Better attributions.\nRetrieval-augmented LMs pro-\nvide retrieved results z used during inference, which can\nhelp practitioners inspect the correctness of model outputs\nmanually (Liu et al., 2023) or automatically (Mishra et al.,\n2024). Another way for verification is post-hoc attribution\u2014\ngiven the model output y, retrieving documents that support\ny. Yet, prior work finds that retrieval-augmented LMs using\n3https://bard.google.com/chat\n3\n\nReliable, Adaptable, and Attributable Language Models with Retrieval\nDatastore\nLM \u03b8\nInput  x\nWhat are the differences between \nllamas and alpacas? \nD1: The llama (Lama glama) \nis a domesticated South \nAmerican camelid \nT1: Llamas\nDocument \nTokens, phrases \nT3: The differences\nIncorporation of z\nInput augmentations \nRetriever \nIntermediate fusion\nD1\nInput  x +\nInput  x\nOutput interpolation\nInput  x\nD2D1\nGranularity of z\nD2: The alpaca is a species \nof South American camelid \nmammal. It is similar to \nllama.\nD2\nFrequency for retrieval\nOne time\nRetrieved \nText\nz\nAdaptive \nEvery  token \nk\nT1\nT2\n0\n1\nN\nT2: The alpaca\nT3\nFigure 2. Taxonomy of architectures of retrieval-augmented LMs.\nevidence during inference provide more accurate attribu-\ntions than such post-hoc attributions (Gao et al., 2023a;\nMalaviya et al., 2023)\nW3: Enabling flexible opt-in of sequences.\nRetrieval-\naugmented LMs offer some effective solutions to concerns\nrelated to massive training data through improved attribu-\ntions and adaptable datastore updates. Enhanced attribu-\ntions enable practitioners to exclude specific sequences from\nthe datastore, mitigating the risk of generating them verba-\ntim (Carlini et al., 2021). Additionally, integrating datas-\ntores during inference only still allows retrieval-augmented\nLMs to maintain performance across domains not included\nin their training data (Min et al., 2024).\nW4: Adaptability and customizability.\nThe separation\nand the interchangeability of knowledge sources for the\ndatastore enables better customization to specific domains,\napplications, and time stamps, without the need for addi-\ntional training (Khandelwal et al., 2020; Min et al., 2024).\nRecent work has shown that retrieval augmentation can even\noutperform LMs fine-tuned on the downstream domain data\non QA (Ovadia et al., 2023; Gupta et al., 2024). Such ef-\nfectiveness for domain adaptation has also been reported in\nnon-knowledge-intensive tasks, including machine transla-\ntion (Shi et al., 2022; Min et al., 2024; Khandelwal et al.,\n2021; Zhong et al., 2022). Updating the datastore with up-to-\ndate knowledge also bypasses the issue of data obsoleteness\nof parametric LMs (Izacard et al., 2023; Zhong et al., 2023;\nMitchell et al., 2022; Kasai et al., 2023).\nW5: Parameter efficiency.\nBy lifting the burden of mem-\norizing all knowledge in the model parameters, retrieval-\naugmented LMs often show strong parameter efficiency\u2014\nretrieval-augmented LMs with much fewer LM parame-\nters can outperform larger, more powerful parametric LMs.\nFor example, on knowledge-intensive tasks such as QA,\nretrieval-augmented LMs surpass parametric LMs with or-\nders of magnitude more parameters by a large margin (Izac-\nard et al., 2023; Min et al., 2023b; Mallen et al., 2023).\n4. Why Haven\u2019t Retrieval-Augmented LMs\nBeen Widely Adopted?\nDespite showing some empirical promise, the adoption of\nretrieval-augmented LMs remains limited compared to para-\nmetric LMs. To understand the obstacles hindering the\nwidespread adoption, we provide a brief review of existing\nretrieval-augmented LMs under our unified taxonomy for\narchitectures (Figure 2), training, and datastores, as summa-\nrized in Table 1.\n4.1. Current State of Retrieval-Augmented LMs\n4.1.1. ARCHITECTURE\nRetrieval-augmented LMs have diverse architectures. Our\ntaxonomy defines architecture based on three axes (Table 1\nleft): what the unit of retrieved text z is (granularity of z),\nhow z is incorporated (incorporation of z), and how often z\nis retrieved (frequency of retrieval).\nHere, we classify approaches based on how they incorporate\nthe retrieved text z (the Incorporation column in Table 1),\nEssentially, retrieval-augmented LMs\u2019 architectures can be\nclassified into the following three groups: 1) input augmen-\ntation, 2) intermediate fusion, and 3) output interpola-\ntion. Refer to Figure 2 for a taxonomy of these architectures.\nFor a more comprehensive review of the architecture, includ-\ning aspects such as the granularity of retrieval and retrieval\nfrequency, refer to Appendix B.1. In essence, input augmen-\ntation and intermediate fusion typically involve retrieving\ntext chunks and processing them with parametric LMs. On\n4\n\nReliable, Adaptable, and Attributable Language Models with Retrieval\nTable 1. Diverse retrieval-augmented LMs based on our architecture and training taxonomies. Full references of the papers are as follows:\nDrQA (Chen et al., 2017), REALM (Guu et al., 2020), RAG (Lewis et al., 2020b), ATLAS (Izacard et al., 2023), RALM (Ram et al.,\n2023), REPLUG (Shi et al., 2023c), Active Retriever (Jiang et al., 2023), Self-RAG (Asai et al., 2024), RETRO (Borgeaud et al.,\n2022), InstructRetro (Wang et al., 2023a), kNN LM (Khandelwal et al., 2020), TRIME (Zhong et al., 2022), NPM (Min et al., 2023b),\nCopyGenerator (Lan et al., 2023), SPALM (Yogatama et al., 2021), Adaptive kNN (Drozdov et al., 2022). \u2217indicates that approaches\ncombining off-the-shelf models without any task-specific training.\nGranularity\nIncorporation\nFrequency\nTraining\nData order\nDrQA\nChunks\nInput\nOne-time\nIndependent\nO(109)\nREALM, RAG, ATLAS\nChunks\nInput\nOne-time\nJoint\nO(109)\nRALM, REPLUG\nChunks\nInput\nEvery k tokens, One-time\nIndependent\u2217\nO(109)\nActive-Retriever, Self-RAG\nChunks\nInput\nAdaptive\nIndependent\u2217, Sequential\nO(109)\nRETRO, InstructRetro\nChunks\nIntermediate\nEvery k tokens\nSequential\nO(1012)\nkNN LM, TRIME\nTokens\nOutput\nEvery token\nIndependent\u2217, Joint\nO(109)\nNPM, Copy Generator\nPhrases\nOutput\nEvery phrase\nJoint\nO(109)\nSPALM, Adaptive kNN\nTokens\nOutput\nAdaptive\nIndependent\u2217, Joint\nO(109)\nthe other hand, output interpolation directly retrieves suc-\ncessive tokens or phrases, resulting in a much larger index.\nUnlike traditional approaches, which involve retrieving only\nonce (One-time) such as DRQA, recent studies have high-\nlighted the effectiveness of retrieval over specific token inter-\nvals (Every k tokens; Ram et al. 2023) or adaptively (Asai\net al., 2024; Jiang et al., 2023; Drozdov et al., 2022).\nInput augmentation. Input augmentation augments the\noriginal input x with retrieved results z in the input space\nof the LM \u03b8 and runs a standard LM inference. As in the pi-\noneering work from Chen et al. (2017), input augmentation\nenables flexible plug-ins of different models for retrieval\nand LM components. Many widely adopted models, in-\ncluding those that augment powerful LMs with off-the-shelf\nretrievers, mostly belong in this category (Yao et al., 2023;\nShi et al., 2024). One notable bottleneck to this approach\nis redundancy and inefficiency; encoding many documents\ntogether in the input space leads to context length window\nlimitations and increases inference costs exponentially (Xu\net al., 2024). While some work such as FiD (Izacard et al.,\n2023) explores parallel encoding to overcome such ineffi-\nciencies, it still encodes repeatedly for each input x.\nIntermediate fusion. To integrate retrieved results in a\nmore scalable manner, RETRO (Borgeaud et al., 2022) in-\ntroduces a new attention mechanism, which takes many\npre-encoded text chunks independent of query x and si-\nmultaneously incorporates them in intermediate spaces.\nRETRO++ (Wang et al., 2023b) and InstructRetro (Wang\net al., 2023a) demonstrate the effectiveness of this method\non top of larger, decoder-only LMs. However, a drawback\nof intermediate fusion is the need for extensive architecture\nmodification and pre-training of LMs for the new encoding\nblocks, potentially limiting widespread adoption.\nOutput interpolation.\nBoth input augmentation and in-\ntermediate fusion require the LM to generate continuations\nfrom their vocabularies. In contrast, kNN LM (Khandel-\nwal et al., 2020) interpolates a parametric LM token dis-\ntribution with a retrieved token distribution, without the\nneed for additional training. Some work extends this di-\nrection by designing new training objectives (Zhong et al.,\n2022) or completely replacing parametric distributions with\na non-parametric distribution over each phrase in the data-\nstore (Min et al., 2023b; Lan et al., 2023).b While these\napproaches frequently demonstrate their efficacy compared\nto input augmentation in language modeling (Min et al.,\n2024), they require a considerably larger index than the other\ntwo architectures. This is due to the necessity of generat-\ning embeddings for all tokens in the datastore, presenting\nscalability challenges.\n4.1.2. TRAINING\nRetrieval-augmented LMs consist of three main compo-\nnents: the index I, the retriever R (i.e., a model that gener-\nates encoding of input and documents), and the LM \u03b8. How\nto efficiently and simultaneously update them to optimize\nthe whole pipeline remains a challenging question. Cur-\nrently, there are two paradigms: independent or sequential\ntraining and joint training (Table 1 Training).\nIndependent or sequential training.\nIndependent train-\ning involves the separate development of a retriever and LM\nwith no direct interactions during training. This includes\nmethods such as kNN LM, or recently, RAG applied to off-\nthe-shelf LMs and retrieval systems. This allows practition-\ners to leverage existing training pipelines and objectives to\nenhance the individual components. There has been rich lit-\nerature in the area of IR on how to build reliable and efficient\nIR systems. Classical term-based retrieval systems, such\nas TF-IDF or BM25 (Robertson & Zaragoza, 2009), have\nbeen widely used. More recently, neural retrieval systems,\nsuch as DPR (Karpukhin et al., 2020) or ColBERT (Khat-\ntab & Zaharia, 2020), have shown superior performance.\n5\n\nReliable, Adaptable, and Attributable Language Models with Retrieval\nExtensive pre-training of retrieval models further improved\nsuch models (Izacard et al., 2022; Ni et al., 2022; Lin et al.,\n2023). For a comprehensive review of retrieval systems, we\ndirect readers to prior surveys (Zhao et al., 2023a).\nYet, independent training is often sub-optimal for the whole\nretrieval-augmented LM pipeline; for instance, LMs trained\nwithout retrieval could become easily distracted by irrele-\nvant preceding context (Shi et al., 2023a). To alleviate this\nissue, sequential training trains either the retriever or LM\nfirst, and then trains the other subsequently using signals\nfrom the first trained component. Many studies train the\nLM component with a powerful pre-trained retriever e.g.,\nDPR, search engines, or frozen pre-trained encoders (Izac-\nard & Grave, 2021a; Nakano et al., 2021; Borgeaud et al.,\n2022), or conversely, train the retriever with signals from\nthe LM (Shi et al., 2023c; Izacard & Grave, 2021b).\nJoint training. Joint training simultaneously trains the LM\nand retrieval components to further optimize their interac-\ntions and the end-to-end retrieval-augmented LM pipeline.\nA notable challenge in joint training is the substantial com-\nputational overhead incurred by updating both the retriever\nmodel and the resulting index during training. It is im-\npractical to repeatedly generate embeddings for millions\nor billions of documents in the datastore at each time step.\nThere are two approaches to achieve this under reasonable\nresource requirements: updating the datastore with updated\nparameters asynchronously or using an in-batch approxi-\nmation to a full datastore. Asynchronous updating is a\ntechnique that allows the index to grow stale over a fixed\nnumber of training steps before the update, aiming to use\nthe full corpus during training (Izacard et al., 2023), as in\ninference time.\nThere is a tradeoff between the update\nfrequency and computational overhead (Guu et al., 2020):\nto obtain better performance, the index should be updated\nmore frequently. In-batch approximation builds a tempo-\nrary index on the fly using training samples from the same\nmini-batch, which serves as an approximation to the full\nindex during training (Zhong et al., 2022; de Jong et al.,\n2022; Min et al., 2023b; Lan et al., 2023). Designing train-\ning batches that can provide strong training signals requires\ncareful consideration.\n4.1.3. APPLICATIONS AND DATASTORES\nApplications.\nRetrieval-augmented LMs have proven ef-\nfective in various NLP tasks. Notably, their impact is more\npronounced on knowledge-intensive tasks (Guu et al., 2020;\nLewis et al., 2020a; Izacard et al., 2023). Several studies\nshowcase their efficacy in machine translation (Khandelwal\net al., 2020; Gu et al., 2018) as well as broader language un-\nderstanding tasks (Min et al., 2023b; Shi et al., 2022). There\nare also decoding methods that leverage post-hoc retrieval\naugmentations to produce more efficient or factual gener-\nations (He et al., 2023; Shi et al., 2023b), or knowledge\nediting capabilities (Zhong et al., 2023). A further overview\nof applications with details of adaptation methodologies is\nin Appendix B.2.\nDatastores. Designing and building a reliable datastore\nis a key challenge of retrieval-augmented LMs. The infer-\nence datastore D may not be necessarily equivalent to the\ntraining datastore and is task-dependent. Some works, such\nas NPM (Min et al., 2023b), leverage the same corpus as\nthe training data D = Dtrain on more general tasks, while\nfor certain downstream tasks, a smaller and general-domain\ncorpus is often used (e.g., Wikipedia). Conversely, curating\nhigh-quality, domain-focused corpora is important for some\ntasks, e.g., code generation (Hayati et al., 2018; Zhou et al.,\n2023). As Table 1 shows, most prior work use a datastore\nthat is on the order of O(109) tokens, with examples such\nas Wikipedia containing roughly a few billion tokens. No-\ntably, Wang et al. (2023a); Borgeaud et al. (2022) scale the\ndatastore to over one trillion tokens, showcasing a large\nperplexity reduction.\n4.2. Limitations of Current Retrieval-Augmented LMs\nWe next identify several core challenges inherent to existing\nretrieval-augmented LMs, as summarized in Table 2.\nC1: Limitations of retrievers and datastores.\nDespite\nthe success of retrieval-augmented LMs on knowledge-\nintensive tasks, their broader applications often result in\nrestricted success. For example, retrieval-augmented LMs\nonly yield marginal gains on reasoning tasks, which can be\nattributed to weaknesses in both the retrieval and LM com-\nponents (BehnamGhader et al., 2023; Lin et al., 2024). We\nhypothesize that this stems from a misalignment between\nconventional retrieval and LM training objectives, as well as\nthe used datastore. Consider answering a factual knowledge-\nbased question: a retriever can efficiently search documents\nakin to a query in Wikipedia, and an LM can subsequently\ncopy or paraphrase the retrieved information. However, the\ntypes of beneficial text vary significantly based on the task.\nExisting retrieval systems evaluate the relevance of docu-\nments primarily by assessing their high lexical or semantic\nsimilarities to the input. Yet, such \u201crelevant\u201d documents\noften do not help tasks in reasoning or general language\nunderstanding (Rubin et al., 2022). It is still unclear what\nmakes certain retrieved contexts more effective than others.\nThe heavy dependence on Wikipedia as a datastore (Sec-\ntion 4.1.3) could also limit its effectiveness, as real-world\napplications frequently encounter queries that may not find\ndirect answers in Wikipedia (Asai & Choi, 2021).\nC2: Limited interactions between retrievers and LMs.\nCommon approaches, such as RAG, often straightforwardly\nentail appending retrieved results to the input of pre-trained\n6\n\nReliable, Adaptable, and Attributable Language Models with Retrieval\nTable 2. Current status of retrieval-augmented LMs and future directions.\nCurrent State of Retrieval-Augmented LMs (\u00a74)\nRoadmap to Advance Retrieval-Augmented LMs (\u00a75)\nC1: Usage of R\n\u2717Semantic and lexical similarity only\n\u2713Beyond semantic and lexical similarity\nand D\n\u2717Single and general-domain corpora\n\u2713Datastores for wider applications\nC2: Interaction\n\u2717Limited interactions beyond input augmentations\n\u2713Architectures with deep LM-retriever interactions\nof R and \u03b8\n\u2717Lack of joint optimization from the end use\n\u2713Large-scale joint training techniques\nC3:\nInfrastruc-\ntures for scaling\n\u2717Lack of standardized libraries beyond RAG\n\u2713Standardized and open-sourced library for retrieval-\nbased LMs\n& adoptions\n\u2717Difficulty in large-scale training and inference\n\u2713Infrastructure for large-scale training and inference\nparametric LMs and adopting input augmentation (Sec-\ntion 4.1.1), due to its simplicity and effectiveness by lever-\naging state-of-the-art parametric LMs.\nHowever, these\nmethods lack close interactions between the retrieval and\nLM components throughout both training and inference.\nThis deficiency amplifies issues such as unsupported gen-\nerations (Gao et al., 2023a) or susceptibility to irrelevant\ncontext, as noted in Yoran et al. (2024); Shi et al. (2023a).\nMoreover, input augmentation increases the context length\nof LMs, leading to an exponential increase in inference\ncosts (Xu et al., 2024). This becomes particularly prob-\nlematic when downstream applications require systems to\nassimilate information from multiple documents (Fan et al.,\n2019). Extended context can also induce LMs to overlook\nsignificant portions of the input (Liu et al., 2023).\nC3: Lack of infrastructure specialized for retrieval-\nbased LMs.\nRelative to parametric LMs, the optimiza-\ntion of retrieval-augmented LM training procedures has\nbeen comparatively under-studied, from both methodolog-\nical and infrastructural standpoints. For instance, open-\nsourced software such as PyTorch FSDP4 or DeepSpeed5\nenable resource-efficient parametric LM pre-training via\ntechniques such as Fully Sharded Data Parallelism (Zhao\net al., 2023b) or Zero Redundancy Optimizers (Rasley et al.,\n2020), respectively. While retrieval-augmented LMs can\ncertainly leverage improvements made to their paramet-\nric components, what remains lacking are focused efforts\nthat address challenges unique to retrieval-augmented LMs.\nSynchronously updating large-scale indexes during training\nintroduces significant computational overhead, and how to\nefficiently update the index under normal computational\nenvironments remains challenging (Section 4.1.2).\nInference in retrieval-augmented LMs can also be sig-\nnificantly more expensive than in standard parametric\nLMs (Mallen et al., 2023), especially if the datastore is\nlarge (e.g., over one trillion tokens). As scaling pre-training\ndata leads to better parametric LMs, some studies empiri-\ncally show that scaling the datastoresis promising (Borgeaud\n4https://pytorch.org/docs/stable/fsdp.\nhtml\n5https://github.com/microsoft/DeepSpeed\net al., 2022). Yet, nearest neighbor searches over billions\nof embeddings without extensive tricks can consume hun-\ndreds of GPUs or prohibitively high RAM usage. Scaling\ncosts thus hinder prior efforts to use larger datastores (Sec-\ntion 4.1.3).\n5. How Can We Further Advance\nRetrieval-Augmented LMs?\nWe believe that the community needs to develop robust intel-\nligent systems based on retrieval-augmented LMs that sur-\npass fully parametric LMs. Here, we present a roadmap to\novercome the technical constraints associated with retrieval-\naugmented LMs discussed in Section 4.2.\n5.1. Rethinking Retrieval and the Datastore (C1)\nBeyond semantic and lexical similarity.\nExtending\nthe use of retrieval-augmented LMs beyond conventional\nknowledge-centric tasks necessitates the formulation of a\nnew definition for \u201crelevance\u201d of the input query and docu-\nments in the datastore. This is essential for excelling in tasks\nas in those tasks informative text may not exhibit semantic\nor lexical similarity to the input query. Recent works show\nthat few-shot in-context learning demonstrations (Su et al.,\n2023a) or even unlabeled text (Lyu et al., 2023) could boost\nmodel performance on reasoning or language understand-\ning tasks. Yet, what makes certain documents helpful (e.g.,\nunderlying reasoning patterns, or writing style) remains an\nopen question. Acquiring a better understanding of the char-\nacteristics of helpful documents could unlock the potential\nof retrieval-augmented LMs. Furthermore, built upon such\nunderstanding, we should build retrieval systems capable of\ncontextualized retrieval, rather than building task-specific re-\ntrieval pipelines: developing a versatile retriever that adjusts\nits search behavior based on diverse notions of similarity\nwith additional input.\nFor instance, instruction-tuned re-\ntrievers (Asai et al., 2023b; Su et al., 2023b) exemplify this\ndirection.\nReconsidering and improving the datastore.\nWhen it\ncomes to wider, general downstream applications, or con-\nversely more expert-domain tasks, over-reliance on a single,\n7\n\nReliable, Adaptable, and Attributable Language Models with Retrieval\ngeneral-domain corpus such as Wikipedia may hinder the\ncapability of retrieval-augmented LMs. As discussed in\nSection 4.1.3, the curation and composition of the datas-\ntore significantly impact the final performance. Yet, many\nopen questions exist regarding how to build and ensure high-\nquality and effective datastores. For instance, should we\nintroduce a quality filter to the documents in the datastore,\nas common practice in pre-training data processing (Black\net al., 2022)? How should we balance multiple domains in a\ndatastore (Shao et al., 2023)? Despite the abundance of liter-\nature on what constitutes good LM pre-training data (Long-\npre et al., 2023), there have been limited explorations so far\non what data ought to go into the datastore.\n5.2. Enhancing Retriever-LM Interactions (C2)\nNew architectures beyond input augmentation.\nAs dis-\ncussed, the input augmentation of powerful LMs (e.g., RAG)\ncomes with several limitations that could be addressed by\nmore specialized, integrated architectures, such as output\ninterpolation or intermediate fusion. While recent work\nshows the success of new architectures (Wang et al., 2023b;\nMin et al., 2023b; Lan et al., 2023), compared to massively\npre-trained parametric LMs, their training and model size\nare often smaller, due to high computational costs for pre-\ntraining. Furthermore, approaches that employ a smaller\ngranularity of retrieval (e.g., token level in Section 4.1.1)\npose significant challenges for scaling. We urge collabora-\ntive efforts for scalable, effective architecture designs and\npre-training\u2014While pre-training retrieval-augmented LMs\nis computationally expensive, we hope that we can address\nthat challenge through collaborative multi-institution efforts,\nas in several successful parametric LM pre-training (Work-\nshop et al., 2022; Groeneveld et al., 2024). Recently, Muen-\nnighoff et al. (2024) have introduced generative represen-\ntational instruction tuning to train a single model for both\nretrieval and generative tasks, which allows for significantly\nreducing the latency of RAG by caching representations.\nIncorporating retrieval during LM pre-training.\nOff-\nthe-shelf parametric LMs trained without retrieval compo-\nnents often struggle with leveraging additional context (Shi\net al., 2023a). Pre-training LMs with retrieval has proven to\nbe effective (Guu et al., 2020; Lewis et al., 2020a; Izacard\net al., 2023), but often requires significant additional train-\ning costs, or non-trivial modifications to the standard LM\narchitecture. Recently, Shi et al. (2024) shows that retriev-\ning similar text chunks and reordering pre-training corpora\ncan enhance LMs\u2019 abilities to reason over long sequences or\nperform retrieval augmentation for diverse tasks. These im-\nprovements do not require the modification of pre-training\npipelines or model architectures. As such, the exploration\nof methods to induce LMs to leverage retrieved context with\nminimal or no additional costs remains promising.\nFurther adaptation after pre-training.\nSignificant ar-\nchitecture modification or pre-training are efforts that re-\nquire massive computing. One promising avenue under\nresource-constrained environments is to explore adaptations\nof retrieval-augmented LMs after pre-training. For instance,\ndespite the rapid developments of versatile instruction-\nfollowing parametric LMs, the exploration of instruction-\nfollowing retrieval-augmented LMs (Lin et al., 2024; Luo\net al., 2023; Asai et al., 2024) or RLHF for retrieval-\naugmented LMs (Nakano et al., 2021; Bohnet et al.,\n2022) remains comparatively scarce. Augmenting existing\ninstruction-tuned LMs trained without retrieval can often\ncause suboptimal performance as the LMs are not explicitly\ntrained to use the retrieved context. Further investigation for\nbetter post-hoc adaptation recipes (e.g., instruction-tuning,\nRLHF) for retrieval-augmented LMs may unleash their ef-\nfectiveness across diverse downstream adaptations. Recent\nstudies demonstrate the promise of incorporating additional\ncomponents to filter out irrelevant context (Xu et al., 2024;\nYoran et al., 2024) or instructing an LM to learn to distin-\nguish (Asai et al., 2024). Exploring enhanced pipelines or\ninference-time algorithms could further improve reliability.\nEfficient end-to-end training of retrieval-augmented\nLMs.\nRetrieval errors often stand out as prominent issues\nin retrieval-augmented LMs (Asai & Choi, 2021; Yoran\net al., 2024). Rather than focusing on optimizing the LM\ncomponent in isolation, it is crucial to jointly optimize the\nretriever component. Some tasks have demonstrated suc-\ncess in updating only the input encoding component without\nmodifying the index after pre-training (Izacard et al., 2023;\nLin et al., 2024). Another alternative strategy involves in-\ntroducing additional components, such as reranking models,\nand training them in an end-to-end fashion with LMs. In\nmany downstream tasks, no supervised labels are available\nto train retrieval systems. Studying effective training strate-\ngies without supervision on the latent variable for positively\nretrieved context (Lee et al., 2019; Singh et al., 2021) is\nessential for enabling the training of retrieval-augmented\nLMs for a broader range of applications.\n5.3. Building Better Systems and Infrastructures for\nScaling and Adaptation (C3)\nScalable search for massive-scale datastores.\nWe be-\nlieve significant efforts and expertise from interdisciplinary\nareas, including systems and algorithms, will enable practi-\ntioners to leverage large-scale datasets. For instance, explor-\ning compression and quantization algorithms for billions of\ntext embeddings is an important area (Douze et al., 2024),\nas well as faster nearest neighbor search algorithms (Wang\net al., 2021). Open-sourced toolkits such as FAISS (John-\nson et al., 2017) could accelerate such progress. Another\nbottleneck of datastore-scaling is the storage requirements\nfor millions or billions of encoded documents, and how to\n8\n\nReliable, Adaptable, and Attributable Language Models with Retrieval\nefficiently load them during inference. Some recent works\npropose to significantly reduce the index size by storing the\nindex as binary vectors (Yamada et al., 2021; Cao et al.,\n2024). Besides algorithmic improvements and system de-\nvelopment, another promising avenue is the development of\nspecialized hardware for retrieval-augmented LMs. Com-\npared to parametric LMs, retrieval-augmented LMs may\nrequire fewer GPUs, while it is often CPU-heavy and re-\nquires fast access to the datastore. Collaborative efforts\nfrom hardware, systems, and algorithms to LM applications\ncould help us tackle these challenging problems.\nStandardization and open-source developments. There\nare several repositories such as LangChain,6 LlamaIndex,7\nand DSPy (Khattab et al., 2024)8 that enable practitioners\nto build RAG on top of existing retrievers, parametric LMs,\nand user-provided datastores. Yet, we still lack a standard-\nized implementation of retrieval-augmented LM pipelines\nand evaluation benchmarks that can flexibly accommodate a\nrange of architectures and training configurations (Sections\n4.1.1 and 4.1.2) beyond RAG. As open-sourced efforts have\nfacilitated the rapid progress of parametric LMs, we urge the\ncommunity to similarly build a standardized open-source\nimplementation for retrieval-augmented LMs.\n6. Conclusion\nThis paper advocates for retrieval-augmented LMs as the\nnext generation of LMs to build more reliable, adaptable,\nand attributable intelligent systems. Despite their notable\nadvantages over parametric LMs, their adoption remains\nlimited. This limitation may be attributed to the focus on a\nnarrow form of retrieval augmentation, which simply com-\nbines exiting retrieval models and LMs in post-hoc manners\nto supplement parametric LMs. We outline a roadmap for\nfundamentally advancing retrieval-augmented LMs in terms\nof architectures, training methodologies, and infrastructure.\nWe emphasize the importance of collaborative interdisci-\nplinary efforts to achieve these advancements.\nImpact Statements\nWe believe the adoption of retrieval-augmented LMs could\naddress those fundamental limitations inherent to parametric\nLMs. We hope that this position paper will inspire further\nexploration in these areas, and collaboratively foster the ad-\nvancement of retrieval-augmented LMs. However, concerns\nmay arise. The effectiveness of retrieval-augmented LMs\nin tasks beyond knowledge-intensive domains remains an\nopen question, necessitating thorough assessments. Further-\n6https://python.langchain.com/docs/get_\nstarted/introduction\n7https://www.llamaindex.ai/\n8https://github.com/stanfordnlp/dspy\nmore, retrieval-augmented LMs may not completely address\nissues such as hallucinations.\nAcknowledgements\nWe express our gratitude to Jacqueline He for her meticulous\nproofreading and for providing many writing suggestions for\nthe drafts. We thank Rulin Shao, Weijia Shi, Dan Friedman,\nTanya Goyal, Howard Yen, Dhruba Ghosh, Jiacheng Liu,\nand Niklas Muennighoff for their insightful feedback on our\ndraft, and Sewon Min for fruitful discussions in the early\nstages. This work was funded in part by NSF IIS-2044660\nand IIS-2239290 and gifts from AI2.\nReferences\nAsai, A. and Choi, E. Challenges in information-seeking\nQA: Unanswerable questions and paragraph retrieval. In\nProceedings of the 59th Annual Meeting of the Associa-\ntion for Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Processing,\n2021. URL https://aclanthology.org/2021.\nacl-long.118.\nAsai, A., Yu, X., Kasai, J., and Hajishirzi, H.\nOne\nquestion\nanswering\nmodel\nfor\nmany\nlanguages\nwith cross-lingual dense passage retrieval.\nIn Ad-\nvances in Neural Information Processing Systems,\n2021.\nURL https://proceedings.neurips.\ncc/paper_files/paper/2021/file/\n3df07fdae1ab273a967aaa1d355b8bb6-Paper.\npdf.\nAsai, A., Min, S., Zhong, Z., and Chen, D.\nRetrieval-\nbased language models and applications.\nIn Pro-\nceedings of the 61st Annual Meeting of the As-\nsociation for Computational Linguistics (Tutorial),\n2023a.\nURL https://aclanthology.org/\n2023.acl-tutorials.6.\nAsai, A., Schick, T., Lewis, P., Chen, X., Izacard,\nG., Riedel, S., Hajishirzi, H., and Yih, W.-t.\nTask-\naware retrieval with instructions.\nIn Findings of\nthe Association for Computational Linguistics: ACL\n2023, 2023b. URL https://aclanthology.org/\n2023.findings-acl.225.\nAsai, A., Wu, Z., Wang, Y., Sil, A., and Hajishirzi, H. Self-\nRAG: Learning to retrieve, generate, and critique through\nself-reflection. In The Twelfth International Conference\non Learning Representations, 2024. URL https://\nopenreview.net/forum?id=hSyW5go0v8.\nAzerbayev, Z., Schoelkopf, H., Paster, K., Santos, M. D.,\nMcAleer, S., Jiang, A. Q., Deng, J., Biderman, S.,\nand Welleck, S.\nLlemma: An open language model\n9\n\nReliable, Adaptable, and Attributable Language Models with Retrieval\nfor mathematics. In The Twelfth International Confer-\nence on Learning Representations, 2024. URL https:\n//openreview.net/forum?id=4WnqRR915j.\nBehnamGhader, P., Miret, S., and Reddy, S. Can retriever-\naugmented language models reason? the blame game\nbetween the retriever and the language model.\nIn\nBouamor, H., Pino, J., and Bali, K. (eds.), Find-\nings of the Association for Computational Linguistics:\nEMNLP 2023. Association for Computational Linguis-\ntics, 2023. URL https://aclanthology.org/\n2023.findings-emnlp.1036.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,\nL., Golding, L., He, H., Leahy, C., McDonell, K., Phang,\nJ., Pieler, M., Prashanth, U. S., Purohit, S., Reynolds, L.,\nTow, J., Wang, B., and Weinbach, S. GPT-NeoX-20B:\nAn open-source autoregressive language model. In Fan,\nA., Ilic, S., Wolf, T., and Gall\u00b4e, M. (eds.), Proceedings\nof BigScience Episode #5 \u2013 Workshop on Challenges\n& Perspectives in Creating Large Language Models,\n2022. URL https://aclanthology.org/2022.\nbigscience-1.9.\nBohnet, B., Tran, V. Q., Verga, P., Aharoni, R., Andor, D.,\nSoares, L. B., Eisenstein, J., Ganchev, K., Herzig, J.,\nHui, K., et al. Attributed question answering: Evalu-\nation and modeling for attributed large language mod-\nels.\narXiv preprint arXiv:2212.08037, 2022.\nURL\nhttps://arxiv.org/abs/2212.08037.\nBorgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford,\nE., Millican, K., Van Den Driessche, G. B., Lespiau, J.-\nB., Damoc, B., Clark, A., De Las Casas, D., Guy, A.,\nMenick, J., Ring, R., Hennigan, T., Huang, S., Maggiore,\nL., Jones, C., Cassirer, A., Brock, A., Paganini, M., Irving,\nG., Vinyals, O., Osindero, S., Simonyan, K., Rae, J.,\nElsen, E., and Sifre, L. Improving language models by\nretrieving from trillions of tokens. In Proceedings of\nthe 39th International Conference on Machine Learning,\n2022. URL https://proceedings.mlr.press/\nv162/borgeaud22a.html.\nBrown, H., Lee, K., Mireshghallah, F., Shokri, R., and\nTram`er, F. What does it mean for a language model\nto preserve privacy?\nIn Proceedings of the 2022\nACM Conference on Fairness, Accountability, and Trans-\nparency, 2022. URL https://dl.acm.org/doi/\nfullHtml/10.1145/3531146.3534642.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry,\nG., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger,\nG., Henighan, T., Child, R., Ramesh, A., Ziegler, D.,\nWu, J., Winter, C., Hesse, C., Chen, M., Sigler, E.,\nLitwin, M., Gray, S., Chess, B., Clark, J., Berner,\nC., McCandlish, S., Radford, A., Sutskever, I., and\nAmodei, D. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems,\n2020.\nURL https://proceedings.neurips.\ncc/paper_files/paper/2020/file/\n1457c0d6bfcb4967418bfb8ac142f64a-Paper.\npdf.\nCao, Q., Min, S., Wang, Y., and Hajishirzi, H. BTR: Binary\ntoken representations for efficient retrieval augmented\nlanguage models. In The Twelfth International Confer-\nence on Learning Representations, 2024. URL https:\n//openreview.net/forum?id=3TO3TtnOFl.\nCarlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T., Song, D.,\nErlingsson, U., et al.\nExtracting training data from\nlarge language models. In 30th USENIX Security Sym-\nposium, 2021.\nURL https://arxiv.org/abs/\n2012.07805.\nCarlini, N., Ippolito, D., Jagielski, M., Lee, K., Tramer, F.,\nand Zhang, C. Quantifying memorization across neural\nlanguage models. In The Eleventh International Confer-\nence on Learning Representations, 2023. URL https:\n//openreview.net/forum?id=TatRHT_1cK.\nChen, D., Fisch, A., Weston, J., and Bordes, A. Read-\ning Wikipedia to answer open-domain questions.\nIn\nProceedings of the 55th Annual Meeting of the Associa-\ntion for Computational Linguistics, 2017. URL https:\n//aclanthology.org/P17-1171.\nChen, T., Wang, H., Chen, S., Yu, W., Ma, K., Zhao,\nX., Yu, D., and Zhang, H. Dense X Retrieval: What\nretrieval granularity should we use?\narXiv preprint\narXiv:2312.06648, 2023a.\nURL https://arxiv.\norg/abs/2312.06648.\nChen, W., Hu, H., Chen, X., Verga, P., and Cohen, W.\nMuRAG: Multimodal retrieval-augmented generator for\nopen question answering over images and text. In Gold-\nberg, Y., Kozareva, Z., and Zhang, Y. (eds.), Proceed-\nings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, 2022. URL https://\naclanthology.org/2022.emnlp-main.375.\nChen, Z., Cano, A. H., Romanou, A., Bonnet, A., Matoba,\nK., Salvi, F., Pagliardini, M., Fan, S., K\u00a8opf, A., Mo-\nhtashami, A., et al. MEDITRON-70B: Scaling medical\npretraining for large language models. arXiv preprint\narXiv:2311.16079, 2023b.\nURL https://arxiv.\norg/abs/2311.16079.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\nGehrmann, S., et al. PaLM: Scaling language modeling\n10\n\nReliable, Adaptable, and Attributable Language Models with Retrieval\nwith pathways. arXiv preprint arXiv:2204.02311, 2022.\nURL https://arxiv.org/abs/2204.02311.\nDao, T., Fu, D., Ermon, S., Rudra, A., and R\u00b4e, C. Flashat-\ntention: Fast and memory-efficient exact attention with\nio-awareness. In Advances in Neural Information Pro-\ncessing Systems, 2022. URL https://openreview.\nnet/forum?id=H4DqfPSibmx.\nDe Cao, N., Aziz, W., and Titov, I.\nEditing fac-\ntual knowledge in language models.\nIn Moens,\nM.-F.,\nHuang,\nX.,\nSpecia,\nL.,\nand Yih,\nS. W.-\nt. (eds.), Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\n2021. URL https://aclanthology.org/2021.\nemnlp-main.522/.\nde Jong, M., Zemlyanskiy, Y., FitzGerald, N., Sha, F., and\nCohen, W. W. Mention memory: incorporating textual\nknowledge into transformers through entity mention atten-\ntion. In International Conference on Learning Represen-\ntations, 2022. URL https://openreview.net/\nforum?id=OY1A8ejQgEX.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Associa-\ntion for Computational Linguistics: Human Language\nTechnologies, Minneapolis, Minnesota, June 2019. As-\nsociation for Computational Linguistics. URL https:\n//aclanthology.org/N19-1423.\nDodge, J., Sap, M., Marasovi\u00b4c, A., Agnew, W., Ilharco,\nG., Groeneveld, D., Mitchell, M., and Gardner, M. Doc-\numenting large webtext corpora: A case study on the\ncolossal clean crawled corpus. In Moens, M.-F., Huang,\nX., Specia, L., and Yih, S. W.-t. (eds.), Proceedings of\nthe 2021 Conference on Empirical Methods in Natural\nLanguage Processing, 2021.\ndoi: 10.18653/v1/2021.\nemnlp-main.98.\nURL https://aclanthology.\norg/2021.emnlp-main.98.\nDouze, M., Guzhva, A., Deng, C., Johnson, J., Szilvasy, G.,\nMazar\u00b4e, P.-E., Lomeli, M., Hosseini, L., and J\u00b4egou, H.\nThe faiss library. arXiv preprint arXiv:2401.08281, 2024.\nURL https://arxiv.org/abs/1702.08734.\nDrozdov, A., Wang, S., Rahimi, R., McCallum, A., Za-\nmani, H., and Iyyer, M.\nYou can\u2019t pick your neigh-\nbors, or can you?\nwhen and how to rely on re-\ntrieval in the kNN-LM.\nIn Findings of the Associ-\nation for Computational Linguistics: EMNLP 2022,\n2022. URL https://aclanthology.org/2022.\nfindings-emnlp.218.\nDubois, Y., Li, X., Taori, R., Zhang, T., Gulrajani, I., Ba, J.,\nGuestrin, C., Liang, P., and Hashimoto, T. B. Alpacafarm:\nA simulation framework for methods that learn from hu-\nman feedback. arXiv preprint arXiv:2305.14387, 2023.\nURL https://arxiv.org/abs/2305.14387.\nFan, A., Jernite, Y., Perez, E., Grangier, D., Weston, J.,\nand Auli, M.\nELI5: Long form question answering.\nIn Proceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, 2019.\nURL\nhttps://aclanthology.org/P19-1346.\nGao, L., Biderman, S., Black, S., Golding, L., Hoppe, T.,\nFoster, C., Phang, J., He, H., Thite, A., Nabeshima, N.,\net al. The pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027, 2020.\nURL https://arxiv.org/abs/2101.00027.\nGao, T., Yen, H., Yu, J., and Chen, D. Enabling large\nlanguage models to generate text with citations. In Pro-\nceedings of the 2023 Conference on Empirical Methods in\nNatural Language Processing, 2023a. URL https://\naclanthology.org/2023.emnlp-main.398.\nGao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai,\nY., Sun, J., and Wang, H.\nRetrieval-augmented gen-\neration for large language models: A survey.\narXiv\npreprint arXiv:2312.10997, 2023b. URL https://\narxiv.org/abs/2312.10997.\nGroeneveld, D., Beltagy, I., Walsh, P., Bhagia, A., Kinney,\nR., Tafjord, O., Jha, A. H., Ivison, H., Magnusson, I.,\nWang, Y., et al. Olmo: Accelerating the science of lan-\nguage models. arXiv preprint arXiv:2402.00838, 2024.\nURL https://arxiv.org/abs/2402.00838.\nGrosse, R., Bae, J., Anil, C., Elhage, N., Tamkin, A., Tajdini,\nA., Steiner, B., Li, D., Durmus, E., Perez, E., et al. Study-\ning large language model generalization with influence\nfunctions. arXiv preprint arXiv:2308.03296, 2023. URL\nhttps://arxiv.org/abs/2308.03296.\nGu, J., Wang, Y., Cho, K., and Li, V. O. K. Search engine\nguided neural machine translation. In AAAI Conference\non Artificial Intelligence, 2018. URL https://api.\nsemanticscholar.org/CorpusID:19206366.\nGudibande, A., Wallace, E., Snell, C., Geng, X., Liu, H.,\nAbbeel, P., Levine, S., and Song, D. The false promise\nof imitating proprietary language models. In The Twelfth\nInternational Conference on Learning Representations,\n2024. URL https://openreview.net/forum?\nid=Kz3yckpCN5.\nGupta, A., Shirgaonkar, A., Balaguer, A. d. L., Silva,\nB., Holstein, D., Li, D., Marsman, J., Nunes, L. O.,\nRouzbahman, M., Sharp, M., et al. Rag vs fine-tuning:\n11\n\nReliable, Adaptable, and Attributable Language Models with Retrieval\nPipelines, tradeoffs, and a case study on agriculture.\narXiv preprint arXiv:2401.08406, 2024. URL https:\n//arxiv.org/abs/2401.08406.\nGuu, K., Lee, K., Tung, Z., Pasupat, P., and Chang,\nM. Retrieval augmented language model pre-training.\nIn International Conference on Machine Learning,\n2020. URL https://dl.acm.org/doi/pdf/10.\n5555/3524938.3525306.\nHayati, S. A., Olivier, R., Avvaru, P., Yin, P., Tomasic, A.,\nand Neubig, G. Retrieval-based neural code generation.\nIn Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, 2018. URL\nhttps://aclanthology.org/D18-1111.\nHe, Z., Zhong, Z., Cai, T., Lee, J. D., and He, D. Rest:\nRetrieval-based speculative decoding.\narXiv preprint\narXiv:2311.08252, 2023.\nURL https://arxiv.\norg/abs/2311.08252.\nHenderson, P., Li, X., Jurafsky, D., Hashimoto, T., Lemley,\nM. A., and Liang, P. Foundation models and fair use.\narXiv preprint arXiv:2303.15715, 2023. URL https:\n//arxiv.org/abs/2303.15715.\nIzacard, G. and Grave, E. Leveraging passage retrieval with\ngenerative models for open domain question answering.\nIn Proceedings of the 16th Conference of the European\nChapter of the Association for Computational Linguis-\ntics, 2021a. URL https://aclanthology.org/\n2021.eacl-main.74.\nIzacard, G. and Grave, E.\nDistilling knowledge from\nreader to retriever for question answering.\nIn In-\nternational Conference on Learning Representations,\n2021b. URL https://openreview.net/forum?\nid=NTEz-6wysdb.\nIzacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski,\nP., Joulin, A., and Grave, E. Unsupervised dense infor-\nmation retrieval with contrastive learning. Transactions\non Machine Learning Research, 2022. URL https:\n//openreview.net/forum?id=jKN1pXi7b0.\nIzacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni,\nF., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and\nGrave, E. Atlas: Few-shot learning with retrieval aug-\nmented language models. Journal of Machine Learning\nResearch, 2023. URL http://jmlr.org/papers/\nv24/23-0037.html.\nJang, J., Yoon, D., Yang, S., Cha, S., Lee, M., Logeswaran,\nL., and Seo, M. Knowledge unlearning for mitigating pri-\nvacy risks in language models. In Proceedings of the 61st\nAnnual Meeting of the Association for Computational\nLinguistics, 2023. URL https://aclanthology.\norg/2023.acl-long.805.\nJiang, Z., Xu, F., Gao, L., Sun, Z., Liu, Q., Dwivedi-Yu, J.,\nYang, Y., Callan, J., and Neubig, G. Active retrieval aug-\nmented generation. In Proceedings of the 2023 Confer-\nence on Empirical Methods in Natural Language Process-\ning, 2023.\nURL https://aclanthology.org/\n2023.emnlp-main.495.\nJin, X., Zhang, D., Zhu, H., Xiao, W., Li, S.-W., Wei, X.,\nArnold, A., and Ren, X. Lifelong pretraining: Continually\nadapting language models to emerging corpora. In Pro-\nceedings of the 2022 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, 2022. URL https://\naclanthology.org/2022.naacl-main.351.\nJohnson, J., Douze, M., and J\u00b4egou, H. Billion-scale similar-\nity search with gpus. arXiv preprint arXiv:1702.08734,\n2017.\nURL https://arxiv.org/abs/1702.\n08734.\nKandpal, N., Deng, H., Roberts, A., Wallace, E.,\nand Raffel, C.\nLarge language models struggle\nto learn long-tail knowledge.\nIn International\nConference on Machine Learning, 2022a.\nURL\nhttps://proceedings.mlr.press/v202/\nkandpal23a/kandpal23a.pdf.\nKandpal, N., Wallace, E., and Raffel, C. Deduplicating train-\ning data mitigates privacy risks in language models. In\nInternational Conference on Machine Learning, 2022b.\nKarpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L.,\nEdunov, S., Chen, D., and Yih, W.-t. Dense passage\nretrieval for open-domain question answering. In Pro-\nceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing, 2020. URL https://\naclanthology.org/2020.emnlp-main.550.\nKasai, J., Sakaguchi, K., Takahashi, Y., Bras, R. L., Asai,\nA., Yu, X., Radev, D., Smith, N. A., Choi, Y., and Inui,\nK. Realtime qa: What\u2019s the answer right now? In Thirty-\nseventh Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track, 2023. URL\nhttps://arxiv.org/abs/2207.13332.\nKhandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and\nLewis, M. Generalization through memorization: Nearest\nneighbor language models. In International Conference\non Learning Representations, 2020. URL https://\nopenreview.net/forum?id=HklBjCEKvH.\nKhandelwal, U., Fan, A., Jurafsky, D., Zettlemoyer, L., and\nLewis, M. Nearest neighbor machine translation. In Inter-\nnational Conference on Learning Representations, 2021.\nURL https://arxiv.org/abs/2010.00710.\n12\n\nReliable, Adaptable, and Attributable Language Models with Retrieval\nKhattab, O. and Zaharia, M. Colbert: Efficient and ef-\nfective passage search via contextualized late interac-\ntion over bert.\nIn Proceedings of the 43rd Interna-\ntional ACM SIGIR conference on research and devel-\nopment in Information Retrieval, 2020. URL https:\n//doi.org/10.1145/3397271.3401075.\nKhattab, O., Singhvi, A., Maheshwari, P., Zhang, Z., San-\nthanam, K., Vardhamanan, S., Haq, S., Sharma, A., Joshi,\nT. T., Moazam, H., et al. DSPy: Compiling declara-\ntive language model calls into state-of-the-art pipelines.\nIn The Twelfth International Conference on Learning\nRepresentations, 2024. URL https://openreview.\nnet/forum?id=sY5N0zY5Od.\nKwiatkowski, T., Palomaki, J., Redfield, O., Collins, M.,\nParikh, A., Alberti, C., Epstein, D., Polosukhin, I., De-\nvlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey,\nM., Chang, M.-W., Dai, A. M., Uszkoreit, J., Le, Q.,\nand Petrov, S.\nNatural questions: A benchmark for\nquestion answering research. Transactions of the As-\nsociation for Computational Linguistics, 2019.\nURL\nhttps://aclanthology.org/Q19-1026.\nKwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,\nC. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient\nmemory management for large language model serving\nwith pagedattention. In Proceedings of the ACM SIGOPS\n29th Symposium on Operating Systems Principles, 2023.\nURL https://arxiv.org/abs/2309.06180.\nLan, T., Cai, D., Wang, Y., Huang, H., and Mao, X.-L. Copy\nis all you need. In The Eleventh International Conference\non Learning Representations, 2023. URL https://\nopenreview.net/forum?id=CROlOA9Nd8C.\nLee, K., Chang, M.-W., and Toutanova, K. Latent retrieval\nfor weakly supervised open domain question answering.\nIn Proceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, 2019.\nURL\nhttps://aclanthology.org/P19-1612.\nLee, K., Cooper, A. F., and Grimmelmann, J. Talkin\u201dbout\nai generation: Copyright and the generative-ai supply\nchain. Forthcoming, Journal of the Copyright Societ,\n2024. URL https://papers.ssrn.com/sol3/\npapers.cfm?abstract_id=4523551.\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mo-\nhamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L.\nBART: Denoising sequence-to-sequence pre-training for\nnatural language generation, translation, and comprehen-\nsion. 2020a. URL https://aclanthology.org/\n2020.acl-main.703.\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin,\nV., Goyal, N., K\u00a8uttler, H., Lewis, M., Yih, W.-t.,\nRockt\u00a8aschel, T., Riedel, S., and Kiela, D.\nRetrieval-\naugmented generation for knowledge-intensive nlp tasks.\nIn Advances in Neural Information Processing Sys-\ntems,\n2020b.\nURL\nhttps://proceedings.\nneurips.cc/paper/2020/file/\n6b493230205f780e1bc26945df7481e5-Paper.\npdf.\nLi, D., Chen, Z., Cho, E., Hao, J., Liu, X., Xing, F., Guo, C.,\nand Liu, Y. Overcoming catastrophic forgetting during do-\nmain adaptation of seq2seq language generation. In Pro-\nceedings of the 2022 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, 2022. URL https://\naclanthology.org/2022.naacl-main.398.\nLin, S.-C., Asai, A., Li, M., Oguz, B., Lin, J., Mehdad,\nY., Yih, W.-t., and Chen, X.\nHow to train your\ndragon: Diverse augmentation towards generalizable\ndense retrieval.\nIn Bouamor, H., Pino, J., and Bali,\nK. (eds.), Findings of the Association for Computa-\ntional Linguistics: EMNLP 2023, pp. 6385\u20136400, Singa-\npore, December 2023. Association for Computational\nLinguistics.\ndoi:\n10.18653/v1/2023.findings-emnlp.\n423. URL https://aclanthology.org/2023.\nfindings-emnlp.423.\nLin, X. V., Chen, X., Chen, M., Shi, W., Lomeli, M.,\nJames, R., Rodriguez, P., Kahn, J., Szilvasy, G., Lewis,\nM., Zettlemoyer, L., and Yih, S. RA-DIT: Retrieval-\naugmented dual instruction tuning. In The Twelfth In-\nternational Conference on Learning Representations,\n2024. URL https://openreview.net/forum?\nid=22OTbutug9.\nLiu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua,\nM., Petroni, F., and Liang, P. Lost in the middle: How\nlanguage models use long contexts. Transactions of the\nAssociation for Computational Linguistics, 2023. URL\nhttps://arxiv.org/abs/2307.03172.\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V.\nRoBERTa: A robustly optimized BERT pretraining ap-\nproach, 2020. URL https://openreview.net/\nforum?id=SyxS0T4tvS.\nLongpre, S., Yauney, G., Reif, E., Lee, K., Roberts, A.,\nZoph, B., Zhou, D., Wei, J., Robinson, K., Mimno, D.,\net al. A pretrainer\u2019s guide to training data: Measuring the\neffects of data age, domain coverage, quality, & toxicity.\narXiv preprint arXiv:2305.13169, 2023. URL https:\n//arxiv.org/abs/2305.13169.\nLuo, H., Chuang, Y.-S., Gong, Y., Zhang, T., Kim, Y.,\nWu, X., Fox, D., Meng, H., and Glass, J. Sail: Search-\naugmented instruction learning. In Findings of the As-\n13\n\nReliable, Adaptable, and Attributable Language Models with Retrieval\nsociation for Computational Linguistics: EMNLP 2023,\n2023. URL https://aclanthology.org/2023.\nfindings-emnlp.242.\nLyu, X., Min, S., Beltagy, I., Zettlemoyer, L., and Hajishirzi,\nH. Z-ICL: Zero-shot in-context learning with pseudo-\ndemonstrations. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics,\n2023. URL https://aclanthology.org/2023.\nacl-long.129.\nMalaviya, C., Lee, S., Chen, S., Sieber, E., Yatskar,\nM., and Roth, D.\nExpertqa:\nExpert-curated ques-\ntions and attributed answers. ArXiv, abs/2309.07852,\n2023.\nURL https://api.semanticscholar.\norg/CorpusID:261823130.\nMallen, A., Asai, A., Zhong, V., Das, R., Khashabi, D.,\nand Hajishirzi, H.\nWhen not to trust language mod-\nels: Investigating effectiveness of parametric and non-\nparametric memories. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics,\n2023. URL https://aclanthology.org/2023.\nacl-long.546.\nMin, S., Krishna, K., Lyu, X., Lewis, M., Yih, W.-t.,\nKoh, P. W., Iyyer, M., Zettlemoyer, L., and Hajishirzi,\nH. Factscore: Fine-grained atomic evaluation of factual\nprecision in long form text generation. arXiv preprint\narXiv:2305.14251, 2023a.\nURL https://arxiv.\norg/abs/2305.14251.\nMin, S., Shi, W., Lewis, M., Chen, X., Yih, W.-t., Ha-\njishirzi, H., and Zettlemoyer, L. Nonparametric masked\nlanguage modeling.\nIn Findings of the Association\nfor Computational Linguistics:\nACL 2023, Toronto,\nCanada, 2023b. Association for Computational Linguis-\ntics. URL https://aclanthology.org/2023.\nfindings-acl.132.\nMin, S., Gururangan, S., Wallace, E., Hajishirzi, H., Smith,\nN. A., and Zettlemoyer, L. SILO language models: Isolat-\ning legal risk in a nonparametric datastore. In The Twelfth\nInternational Conference on Learning Representations,\n2024. URL https://openreview.net/forum?\nid=ruk0nyQPec.\nMishra, A., Asai, A., Wang, Y., Balachandran, V., Neu-\nbig, G., Tsvetkov, Y., and Hajishirzi, H. Fine-grained\nhallucinations detections. arXiv preprint, 2024. URL\nhttps://arxiv.org/abs/2401.06855.\nMitchell, E., Lin, C., Bosselut, A., Manning, C. D., and Finn,\nC. Memory-based model editing at scale. In Proceed-\nings of the 39th International Conference on Machine\nLearning, 2022.\nURL https://proceedings.\nmlr.press/v162/mitchell22a.html.\nMuennighoff, N., Su, H., Wang, L., Yang, N., Wei, F., Yu, T.,\nSingh, A., and Kiela, D. Generative representational in-\nstruction tuning. arXiv preprint arXiv:2402.09906, 2024.\nURL https://arxiv.org/abs/2402.09906.\nNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,\nC., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., et al.\nWebgpt: Browser-assisted question-answering with hu-\nman feedback. arXiv preprint arXiv:2112.09332, 2021.\nURL https://arxiv.org/abs/2112.09332.\nNi, J., Qu, C., Lu, J., Dai, Z., Hernandez Abrego, G., Ma, J.,\nZhao, V., Luan, Y., Hall, K., Chang, M.-W., and Yang, Y.\nLarge dual encoders are generalizable retrievers. In Pro-\nceedings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, 2022. URL https://\naclanthology.org/2022.emnlp-main.669.\nNie,\nE.,\nLiang,\nS.,\nSchmid,\nH.,\nand Sch\u00a8utze,\nH.\nCross-lingual retrieval augmented prompt for low-\nresource languages.\nIn Findings of the Associ-\nation for Computational Linguistics:\nACL 2023,\n2023. URL https://aclanthology.org/2023.\nfindings-acl.528.\nOpenAI.\nGpt-4 technical report.\narXiv preprint\narXiv:2303.08774, 2023.\nURL https://arxiv.\norg/abs/2303.08774.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Gray, A.,\nSchulman, J., Hilton, J., Kelton, F., Miller, L., Simens,\nM., Askell, A., Welinder, P., Christiano, P., Leike, J.,\nand Lowe, R. Training language models to follow in-\nstructions with human feedback. In Advances in Neural\nInformation Processing Systems, 2022. URL https:\n//openreview.net/forum?id=TG8KACxEON.\nOvadia, O., Brief, M., Mishaeli, M., and Elisha, O. Fine-\ntuning or retrieval? comparing knowledge injection in\nllms.\narXiv preprint arXiv:2312.05934, 2023.\nURL\nhttps://arxiv.org/abs/2312.05934.\nPerez, E., Huang, S., Song, F., Cai, T., Ring, R., Aslanides,\nJ., Glaese, A., McAleese, N., and Irving, G.\nRed\nteaming language models with language models. arXiv\npreprint arXiv:2202.03286, 2022.\nURL https://\narxiv.org/abs/2202.03286.\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,\nC., Lee, K., and Zettlemoyer, L. Deep contextualized\nword representations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technolo-\ngies, 2018. URL https://aclanthology.org/\nN18-1202.\n14\n\nReliable, Adaptable, and Attributable Language Models with Retrieval\nPress, O., Zhang, M., Min, S., Schmidt, L., Smith, N., and\nLewis, M. Measuring and narrowing the composition-\nality gap in language models. In Findings of the Asso-\nciation for Computational Linguistics: EMNLP 2023,\n2023. URL https://aclanthology.org/2023.\nfindings-emnlp.378.\nRadford, A., Narasimhan, K., Salimans, T., Sutskever, I.,\net al. Improving language understanding by generative\npre-training. 2018. URL https://openai.com/\nresearch/language-unsupervised.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\nSutskever, I. Language models are unsupervised mul-\ntitask learners, 2019. URL https://openai.com/\nresearch/better-language-models.\nRae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoff-\nmann, J., Song, F., Aslanides, J., Henderson, S., Ring,\nR., Young, S., Rutherford, E., Hennigan, T., Menick,\nJ., Cassirer, A., Powell, R., van den Driessche, G.,\nHendricks, L. A., Rauh, M., Huang, P.-S., Glaese, A.,\nWelbl, J., Dathathri, S., Huang, S., Uesato, J., Mel-\nlor, J. F. J., Higgins, I., Creswell, A., McAleese, N.,\nWu, A., Elsen, E., Jayakumar, S. M., Buchatskaya, E.,\nBudden, D., Sutherland, E., Simonyan, K., Paganini,\nM., Sifre, L., Martens, L., Li, X. L., Kuncoro, A., Ne-\nmatzadeh, A., Gribovskaya, E., Donato, D., Lazaridou,\nA., Mensch, A., Lespiau, J.-B., Tsimpoukelli, M., Grig-\norev, N. K., Fritz, D., Sottiaux, T., Pajarskas, M., Pohlen,\nT., Gong, Z., Toyama, D., de Masson d\u2019Autume, C.,\nLi, Y., Terzi, T., Mikulik, V., Babuschkin, I., Clark, A.,\nde Las Casas, D., Guy, A., Jones, C., Bradbury, J., John-\nson, M. G., Hechtman, B. A., Weidinger, L., Gabriel,\nI., Isaac, W. S., Lockhart, E., Osindero, S., Rimell, L.,\nDyer, C., Vinyals, O., Ayoub, K. W., Stanway, J., Ben-\nnett, L. L., Hassabis, D., Kavukcuoglu, K., and Irving,\nG. Scaling language models: Methods, analysis & in-\nsights from training gopher.\nArXiv, abs/2112.11446,\n2021.\nURL https://api.semanticscholar.\norg/CorpusID:245353475.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Explor-\ning the limits of transfer learning with a unified text-\nto-text transformer. Journal of Machine Learning Re-\nsearch, 2020. URL http://jmlr.org/papers/\nv21/20-074.html.\nRam, O., Levine, Y., Dalmedigos, I., Muhlgay, D., Shashua,\nA., Leyton-Brown, K., and Shoham, Y.\nIn-context\nretrieval-augmented language models. Transactions of the\nAssociation for Computational Linguistics, 2023. URL\nhttps://arxiv.org/abs/2302.00083.\nRasley, J., Rajbhandari, S., Ruwase, O., and He, Y.\nDeepspeed: System optimizations enable training deep\nlearning models with over 100 billion parameters. In\nProceedings of the 26th ACM SIGKDD International\nConference on Knowledge Discovery & Data Min-\ning, 2020.\nURL https://doi.org/10.1145/\n3394486.3406703.\nRobertson,\nS. E. and Zaragoza,\nH.\nThe proba-\nbilistic relevance framework:\nBm25 and beyond.\nFoundations and Trends in Information Retrieval,\n2009.\nURL https://api.semanticscholar.\norg/CorpusID:207178704.\nRubin, O. and Berant, J. Long-range language modeling\nwith self-retrieval.\narXiv preprint arXiv:2306.13421,\n2023.\nURL https://arxiv.org/abs/2306.\n13421.\nRubin, O., Herzig, J., and Berant, J. Learning to retrieve\nprompts for in-context learning. In Carpuat, M., de Marn-\neffe, M.-C., and Meza Ruiz, I. V. (eds.), Proceedings\nof the 2022 Conference of the North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies, 2022. URL https://\naclanthology.org/2022.naacl-main.191.\nSchwartz, R., Dodge, J., Smith, N. A., and Etzioni, O. Green\nai; 2019. arXiv preprint arXiv:1907.10597, 2019. URL\nhttps://arxiv.org/abs/1907.10597.\nShao, R., Min, S., Zettlemoyer, L., and Koh, P. W. Retrieval-\nbased language models using a multi-domain datastore.\nIn NeurIPS 2023 Workshop on Distribution Shifts: New\nFrontiers with Foundation Models, 2023. URL https:\n//openreview.net/forum?id=5ck1WQ4yW4.\nShi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi,\nE. H., Sch\u00a8arli, N., and Zhou, D. Large language models\ncan be easily distracted by irrelevant context. In Proceed-\nings of the 40th International Conference on Machine\nLearning, 2023a.\nURL https://proceedings.\nmlr.press/v202/shi23a.html.\nShi, W., Michael, J., Gururangan, S., and Zettlemoyer, L.\nNearest neighbor zero-shot inference. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natural\nLanguage Processing, 2022. URL https://arxiv.\norg/abs/2205.13792.\nShi, W., Han, X., Lewis, M., Tsvetkov, Y., Zettlemoyer,\nL., and Yih, S. W.-t. Trusting your evidence: Halluci-\nnate less with context-aware decoding. arXiv preprint\narXiv:2305.14739, 2023b.\nURL https://arxiv.\norg/abs/2305.14739.\nShi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis,\nM., Zettlemoyer, L., and Yih, W.-t. REPLUG: Retrieval-\naugmented black-box language models. arXiv preprint\n15\n\nReliable, Adaptable, and Attributable Language Models with Retrieval\narXiv:2301.12652, 2023c.\nURL https://arxiv.\norg/abs/2301.12652.\nShi, W., Min, S., Lomeli, M., Zhou, C., Li, M., Lin, V.,\nSmith, N. A., Zettlemoyer, L., Yih, S., and Lewis, M.\nIn-context pretraining: Language modeling beyond doc-\nument boundaries. In The Twelfth International Confer-\nence on Learning Representations, 2024. URL https:\n//openreview.net/forum?id=LXVswInHOo.\nShuster, K., Poff, S., Chen, M., Kiela, D., and We-\nston, J.\nRetrieval augmentation reduces hallucina-\ntion in conversation.\nIn Findings of the Associa-\ntion for Computational Linguistics:\nEMNLP 2021,\n2021. URL https://aclanthology.org/2021.\nfindings-emnlp.320.pdf.\nSingh, D., Reddy, S., Hamilton, W., Dyer, C., and\nYogatama, D. End-to-end training of multi-document\nreader and retriever for open-domain question answering.\nIn Advances in Neural Information Processing Systems,\n2021.\nURL https://proceedings.neurips.\ncc/paper_files/paper/2021/file/\nda3fde159d754a2555eaa198d2d105b2-Paper.\npdf.\nSinghal, K., Azizi, S., Tu, T., Mahdavi, S. S., Wei, J., Chung,\nH. W., Scales, N., Tanwani, A., Cole-Lewis, H., Pfohl, S.,\net al. Large language models encode clinical knowledge.\nNature, 2023. URL https://www.nature.com/\narticles/s41586-023-06291-2.\nStrubell, E., Ganesh, A., and McCallum, A. Energy and\npolicy considerations for deep learning in NLP.\nIn\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, Florence, Italy,\n2019. Association for Computational Linguistics. URL\nhttps://aclanthology.org/P19-1355.\nSu, H., Kasai, J., Wu, C. H., Shi, W., Wang, T., Xin,\nJ., Zhang, R., Ostendorf, M., Zettlemoyer, L., Smith,\nN. A., and Yu, T. Selective annotation makes language\nmodels better few-shot learners.\nIn The Eleventh In-\nternational Conference on Learning Representations,\n2023a. URL https://openreview.net/forum?\nid=qY1hlv7gwg.\nSu, H., Shi, W., Kasai, J., Wang, Y., Hu, Y., Osten-\ndorf, M., Yih, W.-t., Smith, N. A., Zettlemoyer, L.,\nand Yu, T.\nOne embedder, any task:\nInstruction-\nfinetuned text embeddings. In Findings of the Associ-\nation for Computational Linguistics: ACL 2023, Toronto,\nCanada, 2023b. Association for Computational Linguis-\ntics. URL https://aclanthology.org/2023.\nfindings-acl.71.\nTaylor, R., Kardas, M., Cucurull, G., Scialom, T., Hartshorn,\nA., Saravia, E., Poulton, A., Kerkez, V., and Stojnic,\nR.\nGalactica: A large language model for science.\narXiv preprint arXiv:2211.09085, 2022. URL https:\n//arxiv.org/abs/2211.09085.\nThorne, J., Vlachos, A., Christodoulopoulos, C., and Mit-\ntal, A.\nFEVER: a large-scale dataset for fact extrac-\ntion and VERification.\nIn Proceedings of the 2018\nConference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), 2018. URL\nhttps://aclanthology.org/N18-1074.\nTian, Y., Luo, A., Sun, X., Ellis, K., Freeman, W. T.,\nTenenbaum, J. B., and Wu, J. Learning to infer and\nexecute 3d shape programs. In International Confer-\nence on Learning Representations, 2019. URL https:\n//openreview.net/forum?id=rylNH20qFQ.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\nAzhar, F., et al. Llama: Open and efficient foundation lan-\nguage models. arXiv preprint arXiv:2302.13971, 2023a.\nURL https://arxiv.org/abs/2302.13971.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A.,\nBabaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhos-\nale, S., et al. Llama 2: Open foundation and fine-tuned\nchat models. arXiv preprint arXiv:2307.09288, 2023b.\nURL https://arxiv.org/abs/2307.09288.\nVaswani,\nA.,\nShazeer,\nN.,\nParmar,\nN.,\nUszkoreit,\nJ., Jones, L., Gomez, A. N., Kaiser, L. u., and\nPolosukhin, I.\nAttention is all you need.\nIn Ad-\nvances in Neural Information Processing Systems,\n2017.\nURL https://proceedings.neurips.\ncc/paper_files/paper/2017/file/\n3f5ee243547dee91fbd053c1c4a845aa-Paper.\npdf.\nWang, B., Ping, W., McAfee, L., Xu, P., Li, B., Shoeybi,\nM., and Catanzaro, B.\nInstructretro: Instruction tun-\ning post retrieval-augmented pretraining. arXiv preprint\narXiv:2310.07713, 2023a.\nURL https://arxiv.\norg/abs/2310.07713.\nWang, B., Ping, W., Xu, P., McAfee, L., Liu, Z., Shoeybi,\nM., Dong, Y., Kuchaiev, O., Li, B., Xiao, C., Anand-\nkumar, A., and Catanzaro, B.\nShall we pretrain au-\ntoregressive language models with retrieval? a compre-\nhensive study. In Proceedings of the 2023 Conference\non Empirical Methods in Natural Language Process-\ning, 2023b. URL https://aclanthology.org/\n2023.emnlp-main.482.\n16\n\nReliable, Adaptable, and Attributable Language Models with Retrieval\nWang, M., Xu, X., Yue, Q., and Wang, Y.\nA compre-\nhensive survey and experimental comparison of graph-\nbased approximate nearest neighbor search. Proceed-\nings of the VLDB Endowment, 2021.\nURL https:\n//doi.org/10.14778/3476249.3476255.\nWang, Y., Ivison, H., Dasigi, P., Hessel, J., Khot, T., Chandu,\nK., Wadden, D., MacMillan, K., Smith, N. A., Belt-\nagy, I., and Hajishirzi, H.\nHow far can camels go?\nexploring the state of instruction tuning on open re-\nsources. In Thirty-seventh Conference on Neural Informa-\ntion Processing Systems Datasets and Benchmarks Track,\n2023c. URL https://openreview.net/forum?\nid=w4zZNC4ZaV.\nWang, Z., Nie, W., Qiao, Z., Xiao, C., Baraniuk, R., and\nAnandkumar, A. Retrieval-based controllable molecule\ngeneration. In The Eleventh International Conference\non Learning Representations, 2023d. URL https://\nopenreview.net/forum?id=vDFA1tpuLvk.\nWei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B.,\nBorgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Met-\nzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang,\nP., Dean, J., and Fedus, W. Emergent abilities of large\nlanguage models. Transactions on Machine Learning\nResearch, 2022. ISSN 2835-8856. URL https://\nopenreview.net/forum?id=yzkSU5zdwD. Sur-\nvey Certification.\nWeidinger, L., Uesato, J., Rauh, M., Griffin, C., Huang,\nP.-S., Mellor, J., Glaese, A., Cheng, M., Balle, B.,\nKasirzadeh, A., et al.\nTaxonomy of risks posed by\nlanguage models.\nIn Proceedings of the 2022 ACM\nConference on Fairness, Accountability, and Trans-\nparency, 2022. URL https://dl.acm.org/doi/\n10.1145/3531146.3533088.\nWelleck, S., Liu, J., Lu, X., Hajishirzi, H., and Choi, Y.\nNaturalprover: Grounded mathematical proof generation\nwith language models. Advances in Neural Information\nProcessing Systems, 35:4913\u20134927, 2022.\nWorkshop, B., Scao, T. L., Fan, A., Akiki, C., Pavlick,\nE., Ili\u00b4c, S., Hesslow, D., Castagn\u00b4e, R., Luccioni, A. S.,\nYvon, F., et al.\nBloom:\nA 176b-parameter open-\naccess multilingual language model.\narXiv preprint\narXiv:2211.05100, 2022.\nURL https://arxiv.\norg/abs/2211.05100.\nWu, Y., Rabe, M. N., Hutchins, D., and Szegedy, C.\nMemorizing transformers. In International Conference\non Learning Representations, 2022. URL https://\nopenreview.net/forum?id=TrjbxzRcnf-.\nXu, F., Shi, W., and Choi, E. RECOMP: Improving retrieval-\naugmented LMs with context compression and selective\naugmentation. In The Twelfth International Conference\non Learning Representations, 2024. URL https://\nopenreview.net/forum?id=mlJLVigNHp.\nYamada, I., Asai, A., and Hajishirzi, H. Efficient passage\nretrieval with hashing for open-domain question answer-\ning. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Pro-\ncessing, Online, 2021. Association for Computational\nLinguistics. URL https://aclanthology.org/\n2021.acl-short.123.\nYang, K., Swope, A. M., Gu, A., Chalamala, R., Song,\nP., Yu, S., Godil, S., Prenger, R., and Anandkumar, A.\nLeandojo: Theorem proving with retrieval-augmented\nlanguage models. In Advances in neural information pro-\ncessing systems, 2023. URL https://arxiv.org/\nabs/2306.15626.\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan,\nK. R., and Cao, Y.\nReAct: Synergizing reasoning\nand acting in language models.\nIn The Eleventh In-\nternational Conference on Learning Representations,\n2023. URL https://openreview.net/forum?\nid=WE_vluYUL-X.\nYasunaga, M., Aghajanyan, A., Shi, W., James, R.,\nLeskovec, J., Liang, P., Lewis, M., Zettlemoyer, L., and\nYih, W.-t. Retrieval-augmented multimodal language\nmodeling. arXiv preprint arXiv:2211.12561, 2022. URL\nhttps://arxiv.org/abs/2211.12561.\nYogatama, D., de Masson d\u2019Autume, C., and Kong, L.\nAdaptive Semiparametric Language Models. Transac-\ntions of the Association for Computational Linguistics,\n2021. URL https://doi.org/10.1162/tacl_\na_00371.\nYoran, O., Wolfson, T., Ram, O., and Berant, J.\nMak-\ning retrieval-augmented language models robust to ir-\nrelevant context. In The Twelfth International Confer-\nence on Learning Representations, 2024. URL https:\n//openreview.net/forum?id=ZS4m74kZpH.\nYu, W., Zhu, C., Zhang, Z., Wang, S., Zhang, Z., Fang,\nY., and Jiang, M. Retrieval augmentation for common-\nsense reasoning: A unified approach. In Proceedings\nof the 2022 Conference on Empirical Methods in Nat-\nural Language Processing, 2022.\nURL https://\naclanthology.org/2022.emnlp-main.294.\nYue, X., Wang, B., Chen, Z., Zhang, K., Su, Y.,\nand Sun, H.\nAutomatic evaluation of attribution by\nlarge language models.\nIn Findings of the Associ-\nation for Computational Linguistics: EMNLP 2023,\n2023. URL https://aclanthology.org/2023.\nfindings-emnlp.307.\n17\n\nReliable, Adaptable, and Attributable Language Models with Retrieval\nZha, L., Cui, Y., Lin, L.-H., Kwon, M., Arenas, M. G., Zeng,\nA., Xia, F., and Sadigh, D. Distilling and retrieving gener-\nalizable knowledge for robot manipulation via language\ncorrections.\narXiv preprint arXiv:2311.10678, 2023.\nURL https://arxiv.org/abs/2311.10678.\nZhao, W. X., Liu, J., Ren, R., and Wen, J.-R. Dense text\nretrieval based on pretrained language models: A sur-\nvey. 2023a. URL https://doi.org/10.1145/\n3637870.\nZhao, Y., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu,\nM., Wright, L., Shojanazeri, H., Ott, M., Shleifer, S.,\nDesmaison, A., Balioglu, C., Damania, P., Nguyen, B.,\nChauhan, G., Hao, Y., Mathews, A., and Li, S.\nPy-\ntorch fsdp: Experiences on scaling fully sharded data\nparallel. Proc. VLDB Endow., 2023b. URL https:\n//doi.org/10.14778/3611540.3611569.\nZhong, Z., Lei, T., and Chen, D. Training language models\nwith memory augmentation. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language\nProcessing, 2022. URL https://arxiv.org/abs/\n2205.12674.\nZhong, Z., Wu, Z., Manning, C., Potts, C., and Chen,\nD.\nMQuAKE: Assessing knowledge editing in lan-\nguage models via multi-hop questions.\nIn Proceed-\nings of the 2023 Conference on Empirical Methods in\nNatural Language Processing, 2023. URL https://\naclanthology.org/2023.emnlp-main.971.\nZhou, S., Alon, U., Xu, F. F., Jiang, Z., and Neubig, G.\nDocprompting: Generating code by retrieving the docs.\nIn The Eleventh International Conference on Learning\nRepresentations, 2023. URL https://openreview.\nnet/forum?id=ZTCxT2t2Ru.\n18\n\nReliable, Adaptable, and Attributable Language Models with Retrieval\nA. Progress of Parametric LMs\nThe rise of parametric LMs.\nPre-training to develop better parametric representations of text has been recently extensively\nstudied. BERT (Devlin et al., 2019) is considered to be the first pre-trained LM trained on large-scale text, built upon prior\ngreat success on pre-trained contextualized representations (ELMo; Peters et al. 2018). BERT is an encoder-only, masked\nLM that is trained to fill in blanks (masked tokens) during pre-training, similar to several widely used pre-trained models such\nas RoBERTa (Liu et al., 2020). BART (Lewis et al., 2020a) or T5 (Raffel et al., 2020), on the other hand, are encoder-decoder\nmodels that are trained in both masked and autoregressive manners. GPT (Radford et al., 2018) and GPT-2 (Radford et al.,\n2019) are decoder-only, autoregressive LMs that predict continuations of input tokens. Recent research highlights the\nadvantages of expanding both the parameter count of models and the scale of pre-training datasets (Rae et al., 2021). Many\nproprietary LLMs such as 175B GPT-3 (Black et al., 2022), GPT-4 (OpenAI, 2023) or publicly released checkpoints such as\nLlama 1 (Touvron et al., 2023a) and Llama 2 (Touvron et al., 2023b), which training a smaller number of parameters on\ntrillions of tokens, have shown strong performance across various tasks.\nVersatile, instruction-following systems.\nStarting with GPT-3 (Brown et al., 2020), large parametric LMs have demon-\nstrated an emergent ability known as in-context learning\u2014the ability to adapt to new tasks through few-shot prompting\nwithout needing any updates to its parameters. Further studies demonstrate the impact of large-scale supervised training\nacross varied input-output pairs, as well as subsequent refinements using reinforcement learning with human feedback\n(RLHF), resulting in powerful instruction-following models (Ouyang et al., 2022; Wang et al., 2023c; Dubois et al., 2023).\nInfrastructure for scalability and efficiency.\nThe necessity of training and hosting massive parametric LMs has motivated\nactive interdisciplinary research and open-source developments to reduce the computational costs and time of training and\ninference. For instance, open-sourced software such as PyTorch FSDP9 or DeepSpeed10 enable more resource-efficient\nparametric LM pre-training via techniques such as Fully Sharded Data Parallelism (Zhao et al., 2023b) or Zero Redundancy\nOptimizers (Rasley et al., 2020), respectively. FlashAttention (Dao et al., 2022) accelerates training and long-context\nprocessing. Intensive ongoing research addresses the challenges of high inference costs of massive parametric LMs;\nmemory-efficient inference algorithms such as PagedAttnetion (Kwon et al., 2023) used in vllm11 are proposed to speed up\nthe inference of billion-scale parametric LMs.\nB. Detailed Taxonomy of Retrieval-augmented LMs\nB.1. Architectures\nWe introduce a taxonomy of architectures of retrieval-based LMs. Our taxonomy (Figure 2) is based on three axes: (1) the\ngranularity of retrieval (what to retrieve), (2) the incorporation method (how to use retrieval), and the (3) frequency of\nretrieval (when to retrieve). This taxonomy extends the summarized taxonomy in Section 4.1.1.\nB.1.1. GRANULARITY OF RETRIEVAL\nWe specify the retrieval granularity as follows: text chunks or smaller granularity such as tokens, phrases, or entities. While\nit has shown to be effective, text chunks often contain more information than necessary, resulting in redundancy.\nText chunks. The retrieval of text chunks, such as 100-word paragraphs, is a prevalent strategy in widely used retrieval-\naugmented LMs such as REALM, RAG, and RETRO. To implement this, a large-scale corpus D is segmented into text\nchunks based on the number of tokens or predefined structures like section headers or paragraphs. Retrieved chunks are\ntypically integrated into input space or intermediate layers, which we discuss in detail in the following section, while recent\nwork shows that the choice of length significantly affects performance (Chen et al., 2023a). LMs are expected to predict\noutput token probability distributions by jointly leveraging their original knowledge in parameters and retrieved text chunks.\nTokens and phrases. Several work explores much smaller units such as tokens (Khandelwal et al., 2020) or phrases (Min\net al., 2023b). Given the input prompt x, such token or phrase retrieval-augmented LMs directly search possible next tokens\nfrom the datastore by matching the input prompt and similar prefixes in the datastore, instead of making the LM read and\ngenerate from the vocabulary. Token or phrase retrieval can often result in a much larger index size compared to text chunk\n9https://pytorch.org/docs/stable/fsdp.html\n10https://github.com/microsoft/DeepSpeed\n11https://github.com/vllm-project/vllm\n19\n\nReliable, Adaptable, and Attributable Language Models with Retrieval\nretrieval given the same size of datastore (i.e., the number of embeddings is by default equal to the number of tokens in the\ndatastore).\nB.1.2. INCORPORATION METHOD\nAnother important axis is how the retrieved information. Essentially, retrieval-augmented LMs\u2019 architectures can be\nclassified into the following three groups: 1) input augmentation, 2) intermediate fusion, and ) output interpolation.\nInput augmentation. Input augmentation simply augments the original input x with retrieved results z in the input space of\nthe LM \u03b8 and runs a standard LM inference. As in the pioneering work by Chen et al. (2017), input augmentation enables\nflexible plug-ins of different models for retrieval and LM components. For instance, ATLAS (Izacard et al., 2023) and\nREALM pre-trains LMs jointly with the retriever, while some recent work leverage off-the-shelf pre-trained LMs and\nretrievers (Ram et al., 2023; Shi et al., 2023c). One notable bottleneck is its redundancy and inefficiency; encoding many\ndocuments together in input space faces context length window limitations and increases inference costs exponentially (Xu\net al., 2024). While some work such as FiD (Izacard et al., 2023) explores parallel encoding to overcome such inefficiencies,\nstill the same documents need to be encoded repeatedly for each input x.\nIntermediate fusion. To integrate retrieved results in a more scalable manner, RETRO (Borgeaud et al., 2022) introduces\na new attention mechanism called chunked cross attention (CCA). CCA takes many pre-encoded text chunks, which are\nindependent of query x unlike input augmentation, simultaneously in intermediate spaces by adding a new block between\nstandard cross attention and feed-forward network in Transformer (Vaswani et al., 2017). Recently, RETRO++ (Wang\net al., 2023b) and InstructRetro (Wang et al., 2023a) incorporated CCA into powerful autoregressive LMs. However, a\ndrawback of intermediate fusion is the need for architecture modification and pre-training of LMs for the new encoding\nblocks, potentially limiting widespread adoption. Several studies focus on similar architectures for retrieving information\nfrom long-context input (Wu et al., 2022; Rubin & Berant, 2023).\nOutput interpolation. The two incorporation methods described above still let LMs generate continuations from their\nvocabularies, which often results in unsupported or unattributed generations (Liu et al., 2023; Gao et al., 2023a; Bohnet\net al., 2022). Instead, some models directly manipulate output token distributions. kNN LM interpolates the original LM\u2019s\nsoftmax token distributions with retrieved token distribution without additional training. Some work extends this direction\nby designing new training objectives (Zhong et al., 2022) or completely replacing a nonparametric distribution over every\nphrase in a reference corpus (Min et al., 2023b; Lan et al., 2023).\nB.1.3. FREQUENCY OF RETRIEVAL\nAnother significant design choice in retrieval-augmented LMs is the frequency of retrieval. In essence, opting for more\nfrequent retrieval tends to enhance performance, but comes at the expense of increased computational overhead. Retrieving\nonce before generating given input x has been widely used such as REALM or DrQA, often in input space incorporation\narchitectures. kNN LM, on the other hand, retrieves at every token, or some work retrieves every k token to maintain the\nrelevance between the target sequence and retrieved context (Ram et al., 2023). Several recent papers introduce methods\nthat make LMs adaptively decide when to retrieve (Jiang et al., 2023; Asai et al., 2024).\nB.2. Applications and Datastore\nThis section briefly reviews the applications of retrieval-augmented LMs and used datastores.\nApplications.\nRetrieval-augmented LMs are shown to be effective across a range of NLP tasks, including discriminative\nand generative tasks. The majority of prior work is often evaluated on knowledge-intensive tasks, such as open-domain\nQA (Kwiatkowski et al., 2019), fact verification (Thorne et al., 2018) and knowledge-grounding dialogue (Shuster et al.,\n2021). For such tasks, Wikipedia is often used as the sole knowledge source, while some recent work directly combines LMs\nwith commercial search engine APIs. For non-knowledge-intensive tasks, the usage of training instances (labeled data) as the\ndatastore has been widely explored, demonstrating effectiveness on tasks like machine translation (Khandelwal et al., 2021;\nZhong et al., 2022). Some recent works such as kNN-Prompt (Shi et al., 2022) or NPM (Min et al., 2023b) leverage larger\npre-training corpora (e.g., the Pile; Gao et al. 2020) for more general language understanding tasks (e.g., sentiment analysis)\nor entity translations. Yu et al. (2022) build a new large-scale corpus consisting of 20 million commonsense documents\ncollection from both open-domain knowledge sources. Several works on code generations use similar codes (Hayati\net al., 2018) or documentation (Zhou et al., 2023) of APIs. Designing and building a reliable datastore is a key challenge\n20\n\nReliable, Adaptable, and Attributable Language Models with Retrieval\nin retrieval-augmented LMs. Across those papers, retrieval-augmented LMs have shown significant improvements over\nparametric LMs.\nFurthermore, retrieval-augmented LMs have been applied beyond general-domain, English text data. Several works explore\nretrieving from multilingual data (Asai et al., 2021; Nie et al., 2023) or multiple modalities (Yasunaga et al., 2022; Chen\net al., 2022)\u2014which includes underexplored modalities such as robot controls (Zha et al., 2023). While prior work often\nexplores retrieving from general-domain datastore such as Wikipedia, some recent work shows that retrieving from a targeted\ndatastore is largely helpful to solve more challenging expert domain tasks, such as theorem proving (Welleck et al., 2022;\nYang et al., 2023) or molecule generation (Wang et al., 2023d).\n21\n"}