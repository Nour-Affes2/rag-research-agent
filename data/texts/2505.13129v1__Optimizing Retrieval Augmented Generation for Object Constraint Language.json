{"metadata": {"pdf_filename": "2505.13129v1__Optimizing Retrieval Augmented Generation for Object Constraint Language.pdf", "source": "arXiv"}, "text": "Optimizing Retrieval Augmented Generation for Object\nConstraint Language\nKevin Chenhao Li1,*, Vahid Zolfaghari1, Nenad Petrovic1, Fengjunjie Pan1 and\nProf. Alois Knoll1\n1Technical University of Munich (TUM), Arcisstra\u00dfe 21 D-80333 Munich, Germany\nAbstract\nThe Object Constraint Language (OCL) is essential for defining precise constraints within Model-Based Systems\nEngineering (MBSE). However, manually writing OCL rules is complex and time-consuming. This study explores\nthe optimization of Retrieval-Augmented Generation (RAG) for automating OCL rule generation, focusing on the\nimpact of different retrieval strategies. We evaluate three retrieval approaches\u2014BM25 (lexical-based), BERT-based\n(semantic retrieval), and SPLADE (sparse-vector retrieval)\u2014analyzing their effectiveness in providing relevant\ncontext for a large language model.\nTo further assess our approach, we compare and benchmark our retrieval-optimized generation results\nagainst PathOCL, a state-of-the-art graph-based method. We directly compare BM25, BERT, and SPLADE retrieval\nmethods with PathOCL to understand how different retrieval methods perform for a unified evaluation framework.\nOur experimental results, focusing on retrieval-augmented generation, indicate that while retrieval can enhance\ngeneration accuracy, its effectiveness depends on the retrieval method and the number of retrieved chunks (k).\nBM25 underperforms the baseline, whereas semantic approaches (BERT and SPLADE) achieve better results,\nwith SPLADE performing best at lower k values. However, excessive retrieval with high k parameter can lead\nto retrieving irrelevant chunks which degrades model performance. Our findings highlight the importance of\noptimizing retrieval configurations to balance context relevance and output consistency. This research provides\ninsights into improving OCL rule generation using RAG and underscores the need for tailoring retrieval.\nKeywords\nRetrieval-Augmented Generation, Object Constraint Language, Model-Based Systems Engineering, Large Lan-\nguage Models, Information Retrieval\n1. Introduction\nObject Constraint Language [1] plays an important role in Model-Based Systems Engineering (MBSE)\nby enabling precise constraint definition within meta-models. OCL is used to ensure the integrity of\nsystem designs by specifying conditions that must be held within a model. It is widely used in Unified\nModeling Language (UML) [2] and Eclipse Modeling Framework (EMF) [3] by defining invariants,\npreconditions, postconditions, and derived attributes, which cannot be specified by the model itself and\nthereby enhancing model expressiveness.\nHowever, manually writing OCL rules is complex and time-consuming, requiring a deep understand-\ning of both the system model and OCL syntax. Natural language is often the starting point for defining\nsystem constraints, such as those given in the requirements and specifications of the system. This\nmakes an automated approach that translates natural language specifications into OCL rules highly\nattractive and could significantly improve efficiency and accessibility.\nRecent advances in Large Language Models (LLMs) have revolutionized automated code and rule\ngeneration. Models such as GPT-4 [4], DeepSeek [5], and Meta-Llama-3 [6] have demonstrated remark-\nable capabilities in understanding and generating structured text, including programming languages\nand domain-specific rule sets. These models leverage pre-trained knowledge from large text corpora,\nenabling them to generalize across different programming languages, formal notations, and syntactic\nstructures. In particular, LLMs have been thoroughly applied in natural language-to-code translation\nFirst Workshop on Large Language Models For Generative Software Engineering (LLM4SE 2025)\n*Corresponding author.\n$ kevinchenhao.li@tum.de (K. C. Li); v.zolfaghari@tum.de (V. Zolfaghari); nenad.petrovic@tum.de (N. Petrovic);\nf.pan@tum.de (F. Pan); k@tum.de (Prof. A. Knoll)\n\u00a9 2025 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).\n\n[7] [8], showing strong performance in converting natural language instructions into executable code\nin languages such as Python, Java, and SQL. Similarly, they can be adapted to translate natural language\nspecifications into OCL rules [9] [10], reducing the manual effort required by human engineers. How-\never, LLMs often struggle with domain-specific knowledge [11], especially when dealing with complex\nspecifications and extensive meta-models. A meta-model defines the structure and rules for how models\nare built in MBSE. It includes elements such as classes, associations, enumerations, and attributes. This\nis where RAG becomes essential. Large Language Models have a limited context window, making it\nchallenging to include large and complex meta-models entirely in a prompt. RAG allows relevant parts\nof the meta-model to be retrieved and injected dynamically, helping the LLM generate precise rules\neven when the whole meta-model does not fit into the context window.\nFigure 1: Example meta-model from [9]\nRetrieval-Augmented Generation is an approach that enhances LLMs by integrating external knowl-\nedge retrieval into the generation process [12] [13]. This idea originated from the question answering\ndomain [14]. Instead of relying solely on a model\u2019s pre-trained knowledge, RAG retrieves relevant\ninformation from an external knowledge base and incorporates it into the model\u2019s input before generat-\ning the final output. This method has shown promise in improving accuracy, reducing hallucinations\n[15], and ensuring that generated content aligns with the domain [13]. Since OCL rules are tightly\ncoupled with the underlying meta-model structure, a standard LLM may not have sufficient context\nto generate the correct rule. RAG allows us to retrieve relevant meta-model elements (e.g., classes,\nassociations, enumerations) from a retrievable knowledge base and include them in the input prompt.\nThe desired result is that RAG helps the LLM generate rules that adhere to proper OCL syntax and\nsemantics. However, optimizing retrieval strategies for OCL generation has not been extensively studied,\nparticularly in the context of balancing retrieval efficiency and generation accuracy.\nWhile fine-tuning can adapt LLMs to specific tasks, such as OCL rule generation, it is resource-\nintensive and may not generalize well across new or unseen meta-models. Recent research has increas-\ningly highlighted the advantages of Retrieval-Augmented Generation over fine-tuning for knowledge\ninjection in large language models. RAG consistently outperforms fine-tuning across multiple datasets,\neven when dealing with previously known and entirely new knowledge. Fine-tuning struggles with\nlearning new factual information and requires extensive training data [16]. Similarly, another study\n[17] found that RAG is more effective in handling less popular or low-frequency knowledge, including\ndomain-specific knowledge. The study emphasizes that while smaller language models may still benefit\nfrom fine-tuning, larger models gain little additional advantage. Additionally, fine-tuning remains\nresource-intensive. Combining RAG with fine-tuning can lead to further improvements to performance\nin specialized domains as shown in [18].\nCurrent research in OCL rule generation has focused on other approaches, such as fine-tuning [9].\n\nThis study uses RAG but does not evaluate the impact of it or experiment with different retrieval\nconfigurations. Other current work about OCL rule generation using LLMs include [10] and [19], which\neither fully inject the meta-model without retrieval or use a path-based approach. However, the impact\nof retrieval-based methods such as RAG on OCL rule generation remains underexplored. This gap in\nthe literature highlights the necessity of further investigating retrieval-based strategies to enhance the\naccuracy and efficiency of OCL rule generation. By evaluating different retrieval configurations and\nassessing their impact, this study aims to contribute a novel perspective to the field.\nThis study explores the optimization of RAG for OCL rule generation, focusing on different retrieval\napproaches to enhance model performance. We investigate traditional lexical-based retrieval (BM25)\n[20], semantic dense-vector retrieval using transformer-based models (BERT) [21], and semantic sparse-\nvector retrieval (SPLADE) [22]. BM25 is a keyword-matching method that scores documents based on\nterm frequency and inverse document frequency. BERT-based retrieval uses contextual embeddings to\ncapture semantic similarity between queries and documents. SPLADE, on the other hand, creates sparse\nrepresentations of text using learned term expansions, combining term matching and the ability to\nfind nearest neighbors. While these methods have been extensively studied in general NLP tasks, their\neffectiveness in a RAG approach to OCL rule generation remains unexplored. This study bridges that\ngap by systematically evaluating retrieval strategies and their impact on generation accuracy. We aim\nto identify optimal configurations for improving OCL constraint generation. We compare our optimal\nconfigurations against other state-of-the-art methods for generating OCL constraints. Our evaluation\nemploys quantitative metrics such as Cosine Similarity and Euclidean Distance to assess model output\nquality. This research contributes to the growing body of work in domain-adapted LLM applications\nand provides insights into improving automated OCL constraint generation.\nThe results demonstrate that while retrieval strategies can enhance generation quality, they must be\ncarefully tuned to avoid performance degradation due to excessive or irrelevant retrieved information.\nOur findings highlight the importance of selecting an appropriate retrieval method and the optimal\nnumber of retrieved chunks (\ud835\udc58) to maximize performance. This study provides insights into the impact\nof different retrieval techniques and lays the foundation for future improvements in automated OCL\nrule generation.\n2. Methodology\n2.1. Pipeline\nWe first give a brief overview of the entire pipeline in this section before going into detail for every step\nof the pipeline in the subsequent sections. Our pipeline takes as input a natural language specification\nof an OCL rule and the associated name of the meta-model that we want to generate the OCL rule for.\nWe use both parts of the input in the retrieval stage to find relevant chunks of the meta-model. These\nretrieved chunks are then incorporated into a prompt alongside the natural language specification and\ngiven to a Large Language Model. We then compare the output of the LLM with the actual OCL rule to\ndetermine the quality of our output. This workflow can be seen in Figure 2.\n2.2. Environment Setup\nThe experiment was carried out using the free version of Google Colab, using the T4 GPU for access to\ncomputing resources. Necessary dependencies were installed, including transformers, bitsandbytes,\nflash-attn, and pyngrok. In addition, a Hugging Face authentication token was configured to facilitate\nsecure access to the model repository and data set.\nThe Meta-Llama-3-8B-Instruct model was selected and loaded using the transformers library. The\nselection of LLaMA-3-8B-Instruct model was due to its performance, accessibility, and resource efficiency.\nIts 8 billion parameter size is significantly smaller than models like GPT-4 enabling us to run it without\nthe need to pay for API calls. The role of the model was set to \"system\" and we further limited the\nmaximum length of the output to 1024 tokens. Since we do not further train the pre-trained model and\n\nFigure 2: Retrieval Augmented Generation Pipeline\nare only interested in the evaluation of the output, we set the do_sample flag of the Meta-Llama-3-8B-\nInstruct model to false to disable random sampling and use greedy decoding to improve reproducibility.\nThis ensures that any observed variance in outputs is attributable to changes in retrieval context rather\nthan sampling noise.\nA web service was implemented using Flask, a lightweight Python web framework. An REST API\nwas created to handle incoming requests, process inputs, and generate responses from the LLM. The\nAPI was structured to receive user queries, pass them through the model, and return the generated text\noutput.\n2.3. Data set\nWe used the data set collected by Pan et al. [9] 1. Each sample in the data set consists of:\n\u2022 OCL rule\n\u2022 Natural language specification of the OCL rule\n\u2022 Name of meta-model\n\u2022 Textual description of meta-model given in PlantUML format\nThe dataset was preprocessed by segmenting the PlantUML strings so that each chunk contained\nonly a single class, enumeration, or association. This ensured that each chunk is semantically complete\nand is the smallest atomic unit that cannot be further divided without losing meaning. We implemented\nthe chunking via stop words, where a chunk is considered to end if we encounter either one of \"class\",\n\"enum\" or \"association\". During this pre-processing step certain characters like tabs and unnecessary\nformatting like line breaks were removed, which were present in the original data set. This resulted\nin a total of 3595 unique chunks over the entire dataset. Using the chunks we then built our external\nknowledge base, where for each meta-model, uniquely identified by its name, we have a collection of\nchunks representing the whole meta-model.\nTo evaluate retrieval impact, we filtered the dataset such that only hard samples were considered.\nHard samples were defined as instances where the number of chunks for the meta-model exceeded 50,\nmaking retrieval non-trivial and requiring a retriever to select a subset of chunks to use in the context\nof the prompt. From that filtered dataset we then randomly sampled 72 instances for our evaluation,\nwhich corresponds to one-fifth of the size of the original dataset.\n1This data set can be found at https://huggingface.co/datasets/fpan/text-to-ocl-from-ecore.\n\nListing 1: Prompt used in the pipeline\nYou are given a meta-model with information about classes, associations and their\nattributes.\nYou are also given a natural language specification.\nYour task is to generate an OCL (Object Constraint Language) constraint for this\nspecification\nand based on the meta-model.\nDo not provide any explanations or additional text.\nThe meta-model information is: {retrieved chunks}\nThe natural language specification is: {specification}\n2.4. Retrieval\nTo enhance the accuracy and relevance of generated responses, a RAG pipeline was integrated. We built\nthe external knowledge base from the textual PlantUML provided in the data set as described before.\nThe RAG pipeline retrieves relevant chunks from the knowledge base before passing them as context to\nthe language model based on the natural language specification. We applied two filtering conditions for\nrelevance: (a) chunks must belong to the same meta-model as the input sample, and (b) chunks must\nscore higher than others based on similarity. Both conditions must be satisfied to be selected.\nDifferent retrieval approaches were evaluated, including lexical-based approaches in BM25-based\nretrieval, and transformer-based retrieval models based on dense and sparse vectors such as BERT and\nSPLADE. For all retrieval models, we evaluated them using top-k retrieval, where the top-k chunks\nregarding the retrieval score with the natural language specification were given as context to the LLM.\nFor each retrieval model, we evaluated them with \ud835\udc58set to 10, 20, 30, 40, and 50. We also evaluated the\nintrinsic performance of the LLM regarding OCL rule generation using no retrieval.\nFor our BM25-based retriever, we tokenized the natural language specification and used the result as\nour query for the BM25 algorithm. For the transformer-based, approaches we compared the embeddings\nbetween the natural language specification and the chunks of the meta-model and selected the top-k\nmost similar chunks using cosine similarity. We used the cosine similarity implementation from the\nscikit-learn library.\n2.5. Generation\nTo generate the output we used a prompt that was slightly adapted from [9]. We added \"Do not\nprovide any explanations or additional text.\" to discourage the LLM from outputting lengthy responses\nthat negatively impact its performance regarding our automated metrics. The final prompt template is\nshown in Listing 1\n2.6. Evaluation\nVarious configurations of the RAG pipeline were tested to assess their impact on response quality. This\nincluded differing the parameters such as the number of retrieved chunks, and the embedding model\nselection.\nThe evaluation was conducted using automated quantitative metrics comparing the generated model\noutput to the actual OCL rule as given in the data set. We use cosine similarity and Euclidean distance as\nimplemented by the scikit-learn library and based on BERT embeddings as our evaluation metrics.\nThe choice of the BERT embedding model is due to its open-source nature. The proposed evaluation\nmethodology provides an efficient and scalable way to measure the output quality, allowing us to\ncompare a large number of retriever configurations against each other.\n\nTable 1\nBM25-based retriever\nMetric\nk = 10\nk = 20\nk = 30\nk = 40\nk = 50\nk = 0 (Baseline)\nMean CS\n0.9231\n0.9208\n0.9292\n0.9212\n0.9133\n0.9338\nVariance CS\n0.0028\n0.0023\n0.0021\n0.0042\n0.0064\n0.0022\nTrimmed Mean CS\n0.9366\n0.9321\n0.9403\n0.9364\n0.9348\n0.946\nMean ED\n5.434\n5.5972\n5.2179\n5.4504\n5.6474\n5.0682\nVariance ED\n3.5722\n3.2460\n3.2735\n4.6489\n6.4171\n3.2026\nTrimmed Mean ED\n5.0244\n5.2199\n4.8489\n5.0026\n5.0397\n4.6396\n3. Results\nWe will first present the results for each retrieval approach and then compare the results across different\nretrieval approaches. The results are based on the random subset of the filtered data set by Pan et al. [9].\nGiven that the best possible cosine similarity and the best possible Euclidean distance to the original\nOCL rule are 1 and 0 respectively, the y-axis is inverted when displaying Euclidean distances. We\nconsider variances close to 0 as more desirable, as they represent output and performance consistency.\nWe also examined the performance when disregarding the worst 10% of generated samples to determine\nthe generation output quality without extreme outliers such as hallucinations or outputs disregarding\nthe instruction not to explain the answer. Trimmed mean refers to the mean calculated after removing\nthe worst 10% of samples based on similarity score. The results are rounded to the 4th decimal place to\nimprove readability. We abbreviate Cosine Similarity with CS and Euclidean Distance with ED.\nOur results highlight the impact of different retrieval approaches on the performance of the RAG\npipeline for OCL rule generation. The baseline results as seen in Table 1, where no retrieval was applied\n(k=0), indicate that the language model alone achieves a relatively high cosine similarity (0.9338) but\nstill leaves room for improvement through the integration of retrieval strategies. This result indicates\nthat the intrinsic knowledge in the domain of OCL rules or at least the ability to generate similar rules\ngives us a strong baseline in regards to our semantic similarity metrics.\n3.1. Effectiveness of Different Retrieval Approaches\nWe evaluated BM25 as a lexical retriever using cosine similarity and Euclidean distance to measure\nperformance. The results are shown in Table 1 and compared with the no-retrieval baseline. Table 2\npresents the results for BERT-based retrieval, which uses dense semantic embeddings for chunk selection.\nSPLADE-based retrieval results are shown in Table 3. SPLADE uses sparse semantic representations,\noptimized for balancing lexical precision with semantic flexibility.\nComparing the retrieval methods, the BM25-based retriever (Table 1) exhibited a decline in perfor-\nmance compared to the baseline, particularly at higher values of \ud835\udc58, suggesting that lexical retrieval\nalone is insufficient for effective context selection. The best performance for BM25 was observed at\n\ud835\udc58= 30 with a mean cosine similarity of 0.9292, which still underperforms our baseline across all metrics.\nGenerally, the variance in cosine similarity and Euclidean distance for BM25-based retrieval was higher\nthan in other retrieval approaches, indicating inconsistent retrieval performance as seen in Fig. 4 and\nFig. 5. We have marked the best-performing model in each table by using bold numbers.\nLexical approaches rely heavily on exact matching. We hypothesize that this approach struggles\nbecause there is no guarantee that our natural language description uses the exact terms that are present\nin the relevant chunks. The results point to no context being better than misleading or incomplete\ncontext, which is supported by current research [23].\nPresented in Table 2, the BERT-based retrieval model demonstrated more stable performance across\ndifferent values of \ud835\udc58, achieving a mean euclidean distance of 5.0418 at \ud835\udc58= 50, which is slightly better\nthan the baseline. This suggests that semantic similarity-based retrieval can contribute positively to the\noverall generation quality. Additionally, the lower variance in cosine similarity and Euclidean distance\n\nTable 2\nBERT-based retriever\nMetric\nk = 10\nk = 20\nk = 30\nk = 40\nk = 50\nk = 0 (Baseline)\nMean CS\n0.9265\n0.9263\n0.9254\n0.9259\n0.9334\n0.9338\nVariance CS\n0.0045\n0.0036\n0.0064\n0.0050\n0.0017\n0.0022\nTrimmed Mean CS\n0.9435\n0.9415\n0.9437\n0.9438\n0.9425\n0.946\nMean ED\n5.2026\n5.2436\n5.163\n5.2477\n5.0418\n5.0682\nVariance ED\n4.8764\n4.3763\n6.2543\n5.1349\n3.2301\n3.2026\nTrimmed Mean ED\n4.6916\n4.7734\n4.6247\n4.7097\n4.7065\n4.6396\nTable 3\nSPLADE-based retriever\nMetric\nk = 10\nk = 20\nk = 30\nk = 40\nk = 50\nk = 0 (Baseline)\nMean CS\n0.9360\n0.9189\n0.9292\n0.9211\n0.9116\n0.9338\nVariance CS\n0.0016\n0.0049\n0.0042\n0.0068\n0.0100\n0.0022\nTrimmed Mean CS\n0.9453\n0.9370\n0.9461\n0.9423\n0.9408\n0.946\nMean ED\n4.9842\n5.4851\n5.1187\n5.3317\n5.5794\n5.0682\nVariance ED\n2.7181\n5.3273\n4.6761\n6.3177\n7.9934\n3.2026\nTrimmed Mean ED\n4.6473\n4.9433\n4.5949\n4.7300\n4.8053\n4.6396\nfor BERT-based retrieval, as illustrated in Fig. 6 and Fig. 7, included in the appendix, suggests a more\nconsistent performance across different samples. When looking at the performance with 10% of the\nworst samples removed, we observe that the model with \ud835\udc58= 50 is no longer the best-performing one.\nThis indicates that when disregarding consistency as defined by the ability to limit outliers, a slightly\nlower value for \ud835\udc58might provide better performance.\nThe SPLADE-based retriever produced mixed results, as shown in Table 3, showing relatively high\ncosine similarity at lower values of \ud835\udc58= 10 but experiencing a strong decline in performance as\n\ud835\udc58increased. Notably, SPLADE at \ud835\udc58= 10 outperformed all other retrieval approaches, including\nthe baseline and the best-performing BERT-based retriever (Fig. 3), suggesting that sparse-vector\nretrieval models may be particularly beneficial when selecting a limited number of relevant chunks.\nWe hypothesize that SPLADE can leverage exact matching and synonyms to find all relevant chunks\nquickly. The performance then degrades when we increase k as little to no additional relevant chunks\nare included. Similar to BERT-based retrieval, our SPLADE-based has a stronger performance for\n\ud835\udc58= 30 when disregarding outliers. Once again this observation is in part due to the advantage of\nhigher consistency across outputs for \ud835\udc58= 10, but suggests that a different value for \ud835\udc58might be more\nbeneficial when removing outliers. While the absolute improvement in mean cosine similarity appears\nnumerically small, the difference can be semantically meaningful for domain-specific tasks like this one.\n3.2. Impact of k\nWe also analyzed how varying the number of retrieved chunks as determined by the parameter \ud835\udc58affects\nmodel performance. Interestingly, increasing \ud835\udc58does not always lead to improved results. For the BM25\nand SPLADE retrievers, performance fluctuated as \ud835\udc58increased, suggesting that excessive retrieval\nmay introduce irrelevant or redundant information, as shown in Table 1 and Table 3. We speculate\nthat excessive retrieval (higher \ud835\udc58) can introduce noise, leading to lower similarity scores. Conversely,\nBERT-based retrieval demonstrated relatively stable performance across different k values, with its best\nperformance occurring at \ud835\udc58= 50, as seen in Table 2. Our results suggest that optimizing the retrieval\nstep by carefully selecting an appropriate \ud835\udc58is crucial to maximizing the benefits of retrieval-augmented\ngeneration in the OCL domain, and blindly increasing \ud835\udc58can degrade the model\u2019s effectiveness.\nFurthermore, the analysis of variance across both evaluation metrics highlights the importance of\n\nFigure 3: Comparison of Retriever Models\nretrieval stability. Models with lower variance in cosine similarity (such as the BERT-based approach at\n\ud835\udc58= 50) tend to be more reliable in producing overall high-quality outputs, illustrated in Fig 6 and 7.\nInversely, higher variance in the BM25-based and SPLADE-based retrieval approaches at almost all \ud835\udc58\nvalues suggests inconsistency, potentially due to the inclusion of less relevant chunks in the retrieved\ncontext, as illustrated in Fig. 4, 7, 8, and 9. These results indicate that changing the retriever model and\nparameter \ud835\udc58can have a positive impact on hallucination and reducing outliers, but cannot fully remove\nthem. Depending on the needs and goals of a potential end user, a trade-off between model consistency\nand model output quality in a large percentage of cases needs to be considered.\n3.3. Comparison with PathOCL\nPathOCL is a novel path-based prompt augmentation method proposed by Abukhalaf et al. [19]. The\napproach constructs a graph based on the PlantUML of the meta-model, where each class is represented\nas a node of the graph and associations are directed edges. The direction of the graph is dependent on the\ntype of the association and its direction in the meta-model. PathOCL extracts all simple paths through\nthe graph and ranks them based on their similarity to the natural language specification using either\nJaccard or cosine similarity. For a natural language specification of an OCL constraint the approach\nextracts the UML elements using POS-tagging and then ranks all simple paths in the graph based on\neither the jaccard similarity or cosine similarity between the extracted elements and the node names\nalong the path. The most relevant paths are then included in the prompt to help the LLM generate the\ncorrect OCL constraint.\nAlthough their approach aims to retrieve relevant classes in the face of limited context size, the\ndataset they use to evaluate their approach only consists of 15 UML models, where the largest model\nonly contains 11 classes and 10 associations. Furthermore the PlantUML files of their dataset did not\ncontain interfaces, enum, composition relations or aggregation relations. We evaluated the PathOCL\nmethod on the larger and harder dataset provided by [9]. One issue that was raised is the runtime on\n\nTable 4\nPerformance on subset with maximum length for meta-models\nMetric\nPathOCL (Jaccard k=1)\nBaseline\nSPLADE k=10\nMean CS\n0.9251\n0.9280\n0.9328\nMean ED\n5.3356\n5.2775\n4.9725\nTable 5\nPathOCL performance with Jaccard Similarity\nMetric\nJaccard k=1\nJaccard k=3\nJaccard k=5\nMean CS\n0.9251\n0.9066\n0.8986\nMean ED\n5.3356\n5.8805\n6.2185\nTable 6\nPathOCL performance with Cosine Similarity\nMetric\nCosine k=1\nCosine k=3\nCosine k=5\nMean CS\n0.9204\n0.8957\n0.9030\nMean ED\n5.4709\n6.1700\n6.0020\nextremely large meta-models. Given the length of some of the meta-models, having upwards of 100\nclasses and 300 associations makes it almost impossible to perform the method as outlined, as it uses a\nbrute-force approach to compute all simple paths in the graph, which has a runtime complexity of at\nleast \ud835\udc42(\ud835\udc5b!).\nWe thus decided to do the opposite as mentioned in the methodology section and filter our dataset by\nonly considering samples where the number of chunks in the meta-model is less than 100 and randomly\nsampled 72 instances. We evaluated PathOCL under both Jaccard and cosine similarity configurations\nwith different values of \ud835\udc58= (1, 3, 5). As shown in Table 4, our SPLADE-based method at \ud835\udc58= 10\noutperformed PathOCL across both cosine similarity and Euclidean distance. Additional comparative\nPathOCL results for varying \ud835\udc58values and similarity measures are presented in Tables 5 and 6. Our\nresults indicate that sparse-vector retrieval provides a significant increase in performance against the\nno retrieval baseline. Counterintuitively the PathOCL approach is outperformed by the baseline as seen\nin Table 4. These results suggest that semantic retrieval strategies scale better on complex datasets.\n3.4. Limitations and Future Work\nWhile our study demonstrates the possible benefits of retrieval-based approaches, it is not without\nlimitations. First, our evaluation was conducted on a relatively small filtered subset of the dataset,\nwhich may not generalize to all OCL rule generation scenarios. While these metrics provide an\ninitial assessment of textual similarity and closeness, they do not capture functional correctness. To\ncomplement our automated metrics, future work could add validation of whether generated OCL rules\nconform to formal OCL syntax, human expert review, and in-depth error analysis.\nMoreover, while we evaluated diverse retrieval approaches in our experiments, further research\nis needed to explore more advanced retrieval techniques, such as hybrid approaches like multi-stage\nretrieval. Fine-tuning retrieval models specifically for OCL constraints may also yield additional\nperformance gains over our base models. Another underexplored way to improve the generation of\nOCL rules based on natural language specifications could be refining the chunking strategy to ensure\nthat retrieved information is both concise and semantically rich, for example by grouping chunks that\nare semantically connected. Instead of just retrieving meta-model chunks, it could also be beneficial to\nretrieve appropriate best practices and OCL examples, to leverage in context learning.\n\n4. Conclusion\nOur study investigated the impact of different retrieval strategies on the performance of a Retrieval-\nAugmented Generation pipeline for generating OCL rules. We evaluated three retrieval methods, BM25,\nBERT-based retrieval, and SPLADE-based retrieval, analyzing their effectiveness in providing relevant\ncontext for a large language model.\nOur findings indicate that while retrieval can enhance generation accuracy, its effectiveness is highly\ndependent on the retrieval method and the number of retrieved chunks \ud835\udc58. BM25-based retrieval\nunderperformed the baseline, likely due to its reliance on exact term matching, which may not always\nalign with natural language specifications. In contrast, semantic retrieval approaches such as BERT and\nSPLADE provided better performance, with SPLADE achieving the best results at lower \ud835\udc58values but\ndegrading at higher \ud835\udc58values due to the inclusion of less relevant context.\nA key takeaway is that blindly increasing \ud835\udc58does not always yield better results. Instead, an optimal\nretriever-dependent balance must be struck to avoid retrieval-induced noise while ensuring sufficient\ncontext for the generation model. Additionally, we observed that retrieval approaches with lower\nvariance in performance provide more reliable and overall better results, which may be preferable in\npractical applications where consistency is crucial.\nAdditionally, our comparison with the PathOCL method highlights that our RAG-based approach,\nparticularly SPLADE with \ud835\udc58= 10, outperforms graph-based path selection, especially on larger and\nmore complex meta-models.\nAcknowledgments\nThis research was funded by the Federal Ministry of Education and Research of Germany (BMBF) as\npart of the CeCaS project, FKZ: 16ME0800K.\nDeclaration on Generative AI\nDuring the preparation of this work, the author(s) used Grammarly in order to: Grammar and spelling\ncheck. After using these tool(s)/service(s), the author(s) reviewed and edited the content as needed and\ntake(s) full responsibility for the publication\u2019s content.\n\nReferences\n[1] Object Management Group, OCL 2.4 Specification Overview, 2014. URL: https://www.omg.org/\nspec/OCL/2.4/About-OCL, accessed: April 3, 2025.\n[2] D. Pilone, N. Pitman, UML 2.0 in a Nutshell, O\u2019Reilly Media, Inc., 2005.\n[3] D. Steinberg, F. Budinsky, E. Merks, M. Paternostro, EMF: Eclipse Modeling Framework, Pearson\nEducation, 2008.\n[4] OpenAI, et al., Gpt-4 technical report (2024). http://arxiv.org/abs/2303.08774.\n[5] DeepSeek-AI, et al., Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement\nlearning (2025). http://arxiv.org/abs/2501.12948.\n[6] A. Grattafiori, et al., The llama 3 herd of models (2024). http://arxiv.org/abs/2407.21783.\n[7] D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. K. Li, F. Luo, Y. Xiong,\nW. Liang, Deepseek-coder: When the large language model meets programming \u2013 the rise of code\nintelligence (2024). http://arxiv.org/abs/2401.14196.\n[8] A. Ni, P. Yin, Y. Zhao, M. Riddell, T. Feng, R. Shen, S. Yin, Y. Liu, S. Yavuz, C. Xiong, S. Joty,\nY. Zhou, D. Radev, A. Cohan, L2ceval: Evaluating language-to-code generation capabilities of\nlarge language models (2023). http://arxiv.org/abs/2309.17446.\n[9] F. Pan, V. Zolfaghari, L. Wen, N. Petrovic, J. Lin, A. Knoll, Generative AI for OCL Constraint\nGeneration: Dataset Collection and LLM Fine-tuning, 2024. https://ieeexplore.ieee.org/document/\n10741141/.\n[10] S. Abukhalaf, M. Hamdaqa, F. Khomh, On codex prompt engineering for ocl generation: An\nempirical study, 2023. https://ieeexplore.ieee.org/document/10173990.\n[11] X. Gu, M. Chen, Y. Lin, Y. Hu, H. Zhang, C. Wan, Z. Wei, Y. Xu, J. Wang, On the effectiveness of\nlarge language models in domain-specific code generation, ACM Trans. Softw. Eng. Methodol. 34\n(2025) 78:1\u201378:22.\n[12] P. Zhao, H. Zhang, Q. Yu, Z. Wang, Y. Geng, F. Fu, L. Yang, W. Zhang, J. Jiang, B. Cui, Retrieval-\naugmented generation for ai-generated content: A survey (2024). http://arxiv.org/abs/2402.19473.\n[13] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, M. Wang, H. Wang, Retrieval-augmented\ngeneration for large language models: A survey (2024). http://arxiv.org/abs/2312.10997.\n[14] S. Sharma, D. S. Yoon, F. Dernoncourt, D. Sultania, K. Bagga, M. Zhang, T. Bui, V. Kotte, Retrieval\naugmented generation for domain-specific question answering (2024). http://arxiv.org/abs/2404.\n14760.\n[15] J. Li, Y. Yuan, Z. Zhang, Enhancing llm factual accuracy with rag to counter hallucinations: A\ncase study on domain-specific queries in private knowledge-bases (2024). http://arxiv.org/abs/2403.\n10446.\n[16] O. Ovadia, M. Brief, M. Mishaeli, O. Elisha, Fine-tuning or retrieval? comparing knowledge\ninjection in llms (2024). http://arxiv.org/abs/2312.05934.\n[17] H. Soudani, E. Kanoulas, F. Hasibi, Fine tuning vs. retrieval augmented generation for less popular\nknowledge, 2024. https://dl.acm.org/doi/10.1145/3673791.3698415.\n[18] A. Balaguer, V. Benara, R. L. d. F. Cunha, R. d. M. E. Filho, T. Hendry, D. Holstein, J. Marsman,\nN. Mecklenburg, S. Malvar, L. O. Nunes, R. Padilha, M. Sharp, B. Silva, S. Sharma, V. Aski, R. Chandra,\nRag vs fine-tuning: Pipelines, tradeoffs, and a case study on agriculture (2024). http://arxiv.org/\nabs/2401.08406.\n[19] S. Abukhalaf, M. Hamdaqa, F. Khomh, Pathocl: Path-based prompt augmentation for ocl generation\nwith gpt-4 (2024). https://zenodo.org/doi/10.5281/zenodo.10841785.\n[20] S. E. Robertson, S. Walker, S. Jones, M. M. Hancock-Beaulieu, M. Gatford, Okapi at TREC 3 (1994).\n[21] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, BERT: Pre-training of Deep Bidirectional Trans-\nformers for Language Understanding (2019). http://arxiv.org/abs/1810.04805.\n[22] T. Formal, B. Piwowarski, S. Clinchant, SPLADE: Sparse Lexical and Expansion Model for First\nStage Ranking, 2021. https://dl.acm.org/doi/10.1145/3404835.3463098.\n[23] D. Li, A. S. Rawat, M. Zaheer, X. Wang, M. Lukasik, A. Veit, F. Yu, S. Kumar, Large Language\nModels with Controllable Working Memory, 2023. https://aclanthology.org/2023.findings-acl.112/.\n\nA. Plots\nFigure 4: Boxplot of Cosine Similarities BM25\nFigure 5: Boxplot of Euclidean Distances BM25\n\nFigure 6: Boxplot of Cosine Similarities BERT\nFigure 7: Boxplot of Euclidean Distances BERT\n\nFigure 8: Boxplot of Cosine Similarities SPLADE\nFigure 9: Boxplot of Euclidean Distances SPLADE\n"}