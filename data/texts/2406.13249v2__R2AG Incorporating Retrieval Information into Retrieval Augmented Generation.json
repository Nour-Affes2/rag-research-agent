{"metadata": {"pdf_filename": "2406.13249v2__R2AG Incorporating Retrieval Information into Retrieval Augmented Generation.pdf", "source": "arXiv"}, "text": "R2AG: Incorporating Retrieval Information into Retrieval Augmented\nGeneration\nFuda Ye1, Shuangyin Li1,*, Yongqi Zhang2, Lei Chen2,3\n1School of Computer Science, South China Normal University\n2The Hong Kong University of Science and Technology (Guangzhou)\n3The Hong Kong University of Science and Technology\nfudayip@m.scnu.edu.cn, shuangyinli@scnu.edu.cn, yongqizhang@hkust-gz.edu.cn, leichen@cse.ust.hk\nAbstract\nRetrieval augmented generation (RAG) has\nbeen applied in many scenarios to augment\nlarge language models (LLMs) with external\ndocuments provided by retrievers. However,\na semantic gap exists between LLMs and\nretrievers due to differences in their training\nobjectives and architectures. This misalign-\nment forces LLMs to passively accept the\ndocuments provided by the retrievers, leading\nto incomprehension in the generation process,\nwhere the LLMs are burdened with the task of\ndistinguishing these documents using their in-\nherent knowledge. This paper proposes R2AG,\na novel enhanced RAG framework to fill this\ngap by incorporating Retrieval information into\nRetrieval Augmented Generation. Specifically,\nR2AG utilizes the nuanced features from the\nretrievers and employs a R2-Former to capture\nretrieval information. Then, a retrieval-aware\nprompting strategy is designed to integrate re-\ntrieval information into LLMs\u2019 generation. No-\ntably, R2AG suits low-source scenarios where\nLLMs and retrievers are frozen. Extensive ex-\nperiments across five datasets validate the effec-\ntiveness, robustness, and efficiency of R2AG.\nOur analysis reveals that retrieval information\nserves as an anchor to aid LLMs in the gener-\nation process, thereby filling the semantic gap.\n1\nIntroduction\nRetrieval augmented generation (RAG) (Lewis\net al., 2020) significantly enhances the capabilities\nof large language models (LLMs) by integrating\nexternal, non-parametric knowledge provided by\nretrievers. In RAG framework, the retriever lo-\ncates and looks up useful documents based on a\ngiven query, and then the LLM interacts with these\nretrieved results to generate a response. The coordi-\nnation of retrieval and generation achieves impres-\nsive performance without additional training. Espe-\ncially in domain-specific and knowledge-intensive\n*Corresponding author. The source code is available at\nhttps://github.com/yefd/RRAG.git.\nRAG\nR2AG\nRetriever\nLLM\nCombine\nQuery &\nDocuments\nQuery\nTop-k\nDocuments\nR2-Former\n/\nRetriever\nLLM\nCombine\nQuery &\nDocuments\nQuery\nTop-k\nDocuments\nRetrieval-aware\nPrompting\nSemantic Gap\nFigure 1: A comparison between RAG and R2AG.\nR2AG employs a trainable R2-Former to bridge the\nsemantic gap between retrievers and LLMs. Optionally,\nLLMs can be fine-tuned to understand the retrieval in-\nformation further.\ntasks, RAG offers real-time knowledge with high\ninterpretability to LLMs, effectively mitigating the\nhallucination problem (Mallen et al., 2023).\nHowever, there exists a semantic gap between re-\ntrievers and LLMs due to their vastly different train-\ning objectives and architectures (BehnamGhader\net al., 2022). Specifically, retrievers, typically en-\ncoder architecture, are designed to retrieve the most\nrelevant documents for a query (Zhu et al., 2023b).\nConversely, LLMs, generally decoder architecture,\nare expected to answer questions based on their\ninherent knowledge or given documents. How-\never, the interaction between retrievers and LLMs\nin RAG primarily relies on simple text concatena-\ntion (BehnamGhader et al., 2022). This poor com-\nmunication strategy will lead to several challenges\nfor LLMs. Externally, it is hard for LLMs to uti-\nlize more information from retrievers in separate\nprocesses. In RAG, the retrieved documents that\nonly preserve sequential relationships are unidirec-\ntionally delivered to LLMs, and LLMs do not fully\nunderstand why retrievers provide the documents.\narXiv:2406.13249v2  [cs.CL]  30 Oct 2024\n\nParticularly, low-quality documents inevitably ap-\npear in retrieved results (Barnett et al., 2024), but\nLLMs have to accept this noise passively. Inter-\nnally, it is hard for LLMs to handle all of the re-\ntrieved documents with their inherent knowledge.\nLLMs must process all the results and assess which\ndocuments are important, impacting their ability\nto generate accurate answers (Wu et al., 2024).\nMoreover, LLMs face the lost-in-middle problem\nin overly long documents (Liu et al., 2023), leading\nto further misunderstanding.\nUnfortunately, existing enhanced RAG methods,\nincluding pre-processing approaches (Izacard et al.,\n2022; Yan et al., 2024; Asai et al., 2023; Ke et al.,\n2024) and compression-based approaches (Yan\net al., 2024; Xu et al., 2023; Jiang et al., 2023),\ndo not recognize this semantic gap between retriev-\ners and LLMs. They remain to treat retrieval and\ngeneration as separate processes and directly add\nprocessed or compressed documents into the inputs\nfor LLMs. These strategies ignore the semantic\nconnections necessary for deeper comprehension,\nwhich may lead to potentially misleading LLMs\neven with perfect retrievers.\nTo address these challenges, it is essential to\nbridge the semantic gap between retrievers and\nLLMs. As previously mentioned, retrievers can\nprovide high-quality semantic representations that\ncan be beneficial for catching nuanced differences\namong documents (Zhao et al., 2022). Thus, our in-\ntuition is to exploit these semantic representations\nas additional knowledge, empower LLMs to gain a\ndeeper comprehension of the retrieved documents,\nand thereby generate more accurate responses.\nThis paper proposes a cost-effective enhanced\nRAG framework to incorporate Retrieval informa-\ntion into Retrieval Argumented Generation (named\nR2AG), enhancing LLMs\u2019 perception of the key\ninformation among retrieved documents. Specif-\nically, R2AG adopts an input processing pipeline\nthat transforms semantic representations from a\nretriever into unified retrieval features. Then, a\ntrainable R2-Former is employed to capture es-\nsential retrieval information. As shown in Fig-\nure 1, R2-Former is a pluggable and lightweight\nmodel placed between the retriever and the LLM.\nFinally, through a retrieval-aware prompting strat-\negy, the LLM receives additional embeddings that\ncontain retrieval information. This strategy aligns\nthe knowledge from retrievers with LLMs without\nchanging the content and order of retrieved docu-\nments, thereby relieving information loss. R2AG\noffers the flexibility to fine-tune R2-Former alone\nor both with LLMs. Thus, in R2AG framework,\nboth retrievers and LLMs can be frozen to save\ncomputational costs, making R2AG suitable for\nscenarios with limited resources. Overall, our con-\ntributions are summarized as follows:\n\u2022 We propose R2AG, an enhanced RAG frame-\nwork, to incorporate retrieval information\ninto retrieval augmented generation. Notably,\nR2AG is compatible with low-source scenar-\nios where retrievers and LLMs are frozen.\n\u2022 We design a lightweight model, R2-Former,\nto bridge the semantic gap between retrievers\nand LLMs. R2-Former can be seamlessly in-\ntegrated into existing RAG frameworks using\nopen-source LLMs.\n\u2022 We introduce a retrieval-aware prompting\nstrategy to inject retrieval information into the\ninput embeddings, enhancing LLMs\u2019 ability\nto understand relationships among documents\nwithout much increase in complexity.\nExperimental results demonstrate the superior per-\nformance and robustness of R2AG in various sce-\nnarios. Our analysis shows that R2AG increases\nlatency by only 0.8% during inference. Further-\nmore, it demonstrates that retrieval information\nanchors LLMs to understand retrieved documents\nand enhances their generation capabilities.\n2\nRelated Works\n2.1\nRetrieval Augmented Generation\nDespite being trained on vast corpora, LLMs still\nstruggle with hallucinations and updated knowl-\nedge in knowledge-sensitive tasks (Zhao et al.,\n2023). RAG (Lewis et al., 2020) is regarded as an\nefficient solution to these issues by combining a re-\ntrieval component with LLMs. In detail, documents\ngathered by retrievers are bound with the original\nquery and placed into the inputs of LLMs to pro-\nduce final responses. RAG allows LLMs to access\nvast, up-to-date data in a flexible way, leading to\nbetter performance. Benefiting from the progress\nof multi-modal alignment techniques (Li et al.,\n2023b; Zhu et al., 2023a), the idea of RAG has\nbeen extended to various domains with modality-\nspecific retrievers, including audios (Koizumi et al.,\n2020), images (Yasunaga et al., 2023), knowledge\ngraphs (He et al., 2024), and so on. Despite its\nrapid growth, RAG suffers several limitations, such\n\nas sensitivity to retrieval results, increased com-\nplexity, and a semantic gap between retrievers and\nLLMs (Kandpal et al., 2022; Zhao et al., 2024).\n2.2\nEnhanced RAG\nRecent works develop many enhanced approaches\nbased on the standard RAG framework. To directly\nimprove the effectiveness of RAG, REPLUG (Shi\net al., 2023) and Atlas (Izacard et al., 2022) lever-\nage the LLM to provide a supervisory signal for\ntraining a better retriever. However, the noise will\ninevitably appear in retrieval results (Barnett et al.,\n2024).\nRecent studies focus on pre-processing\nthe retrieved documents before providing them\nto LLMs. Techniques such as truncation and se-\nlection are effective methods to enhance the qual-\nity of ranking lists without modifying the content\nof documents (Gao et al., 2023; Xu et al., 2024).\nCRAG (Yan et al., 2024) trains a lightweight re-\ntrieval evaluator to exclude irrelevant documents.\nBGM (Ke et al., 2024) is proposed to meet the\npreference of LLMs by training a bridge model to\nre-rank and select the documents. Some studies\naim to train small LMs to compress the retrieval\ndocuments, thus decreasing complexity or reducing\nnoise. Jiang et al. (2023) propose LongLLMLin-\ngua to detect and remove unimportant tokens. RE-\nCOMP (Xu et al., 2023) adopts two compressors\nto select and summarize the retrieved documents.\nHowever, the pre-processing methods introduce ad-\nditional computational costs during inference and\nmay lead to the loss of essential information.\nNotably, the above methods target providing\nhigher-quality retrieval results to LLMs and ac-\ntually treat retrieval and generation as two dis-\ntinct processes. This separation fails to bridge the\nsemantic gap between retrievers and LLMs fully.\nSome approaches (Deng et al., 2023; Sachan et al.,\n2021) enhance LLM comprehension abilities by in-\ncorporating documents into latent representations.\nHowever, these methods are typically designed for\nencoder-decoder LLMs, and constrain their suit-\nability for prevailing decoder-only LLMs. While\njoint modeling methods (Glass et al., 2022; Izac-\nard et al., 2024) benefit from the joint optimiza-\ntion of LLMs and retrievers, they need extra train-\ning to align semantic spaces, which may hamper\nthe generality of LLMs (Zhao et al., 2024). Com-\npared with these joint modeling methods, a key\ndifference is that R2AG offers a cost-effective and\nnon-destructive manner to bridge the semantic gap\nbetween LLMs and retrievers.\n3\nR2AG\n3.1\nProblem Formulation and Overview\nRAG involves the task that aims to prompt an\nLLM to generate answers based on a query\nand documents returned by a retriever.\nFor-\nmally, given a query q and a list of documents\nD={d1, d2, \u00b7 \u00b7 \u00b7 , dk} in preference order ranked by\nthe retriever fR, the LLM, a generator fG, is ex-\npected to generate the output \u02c6y. The pipeline can\nbe expressed as:\n\u02c6y = fG (P (q, D)) ,\n(1)\nwhere P is a predefined prompt template. It shows\nthe retrievers and LLMs are couple in a simplistic\nprompt-based method, which will lead to miscom-\nmunication and the semantic gap.\nFigure 2 illustrates the overall framework of\nR2AG. Initially, given a query and retrieved docu-\nments, R2AG processes representations modeled\nby a retriever into unified-format features. These\nlist-wise features consider nuanced relationships\nboth between the query and documents and among\nthe documents themselves. Then, a R2-Former is\ndesigned to capture retrieval information for LLM\nusage. It allows unified features to interact with\neach other via self-attention mechanism, enabling it\nto understand complex dependencies. To integrate\nretrieval information into the LLM\u2019s generation\nprocess, R2AG adopts a retrieval-aware prompting\nstrategy to insert the retrieval information into the\nLLM\u2019s input embedding space without causing in-\nformation loss or increasing much complexity. Be-\nsides, R2AG is flexible to be applied in low-source\nscenarios where LLMs are frozen.\n3.2\nRetrieval Feature Extraction\nBefore generation, it is necessary to obtain high-\nquality retrieval features. In R2AG, we first get\nsemantic representations from the retriever fR. For-\nmally, a query q and document d are encoded into\nrepresentations as xq=fR(q) and xd=fR(d), re-\nspectively. However, these representations can not\nbe directly used because a single representation\ncan not capture interactive features for LLM\u2019s gen-\neration. Moreover, to suit various retrievers, it is\nintuitive to transform representations in different\nspaces into unified format features.\nInspired by works in retrieval downstream\ntasks (Ma et al., 2022; Ye and Li, 2024), we align\nthese representations into retrieval features by com-\nputing relevance, precedent similarity, and neigh-\n\nQuery &\nDocuments\nQuery\u00a0<R>\u00a0Document1, ... , <R>\u00a0Documentk\nCombine\nR2-Former\nMLP\nLookup\nTraining Objective\nRetrieval-aware Prompting\n...\nFrozen\nNot Frozen\nFeature Extraction\nInput Embedding\nTransformer Encoder\nPE\ninput1\ninputk\ninput2\nR2-Former\n...\nLLM\n/\n\u00a0Query\u00a0\nEmb\nRetrieval Info1\nEmb\nDocument1\nEmb\nRetrieval Infok\nEmb\nDocumentk\nEmb\n...\nRetriever\nQueryDocumenti\nPrecedent\nNeighbor\ninputi\nFigure 2: An illustration of R2AG. The R2-Former is designed to extract retrieval features, acting as an information\nbottleneck between retrievers and LLMs. Through the retrieval-aware prompting strategy, the retrieval information\nserves as an anchor to guide LLMs during generation. \u201cEmb\u201d is short for embedding, \u201cPE\u201d stands for positional\nembeddings, and \u201c<R>\u201d denotes the placeholder for retrieval information.\nbor similarity scores. Specifically, these scores are\ncalculated by a similarity function such as dot prod-\nuct or cosine similarity. The relevance score ri is\nbetween the query and the i-th document and is\nalso used to sort the documents. The precedent and\nneighbor similarity scores are computed between\nthe i-th document representation and its precedent-\nweighted and adjacent representations, respectively.\nDetailed formulations are provided in Appendix A.\nFinally, three features are concatenated as input:\ninputi={ri, \u03b3i, \u03b6i}, representing relevance, prece-\ndent similarity, and neighbor similarity. Then, the\nfeature list {inputi}k\ni=1 is then fed into R2-Former\nto further exploit retrieval information.\n3.3\nR2-Former\nInspired by Li et al. (2023b), we propose the R2-\nFormer as the trainable module that bridges be-\ntween retrievers and LLMs.\nAs shown in the\nright side of Figure 2, R2-Former is a pluggable\nTransformer-based model that accepts list-wise fea-\ntures as inputs and outputs retrieval information.\nTo better comprehend list-wise features from re-\ntrievers, we employ an input embedding layer to\nlinearly transform input features into a higher di-\nmension space. Positional embeddings are then\nadded before attention encoding to maintain se-\nquence awareness. Then, a Transformer (Vaswani\net al., 2017) encoder is utilized to exploit the input\nsequences, which uses a self-attention mask where\neach position\u2019s feature can attend to other positions.\nFormally, for an input list {inputi}k\ni=1, the process\nis formulated by:\nH = fatt\nh\nf\u2192h1\n\u0010\n{inputi}k\ni=1\n\u0011\n+p\ni\n,\n(2)\nwhere fatt is the Transformer encoder with h1 hid-\nden dimension, f\u2192h1 is a linear mapping layer, and\np \u2208Rk\u00d7h1 represents trainable positional embed-\ndings. The output embeddings H \u2208Rk\u00d7h1 thus\ncontain the deeper retrieval information and will be\ndelivered to the LLM\u2019s generation.\n3.4\nRetrieval-Aware Prompting\nIn the generation process, it is crucial for the LLM\nto utilize the retrieval information effectively. As\nshown in the upper part of Figure 2, we introduce a\nretrieval-aware prompting strategy that injects the\nretrieval information extracted by R2-Former into\nthe LLM\u2019s generation process.\nFirst, we employ a projection layer to linearly\ntransform the retrieval information into the same\ndimension as the token embedding layer of the\nLLM. Formally, this is represented as:\nER = f\u2192h2(H) = {eR\ni }k\ni=1,\n(3)\nwhere f\u2192h2 is a linear projection layer via an MLP\nlayer, and h2 is the dimension of LLM\u2019s token\nembedding layer.\nThen, we tokenize the query and documents us-\ning LLM\u2019s tokenizer and convert them into embed-\ndings. For example, a document d is tokenized into\ntd={td\nj}nd\nj=1, where td\nj is the j-th token in the docu-\nment, nd is the number of tokens in the document d.\nAnd the token embeddings can be transformed by a\nlookup in the token embedding layer. The process\ncan be expressed as:\nEd = femb\n\u0010\ntd\u0011\n= {ed\nj}nd\nj=1,\n(4)\nwhere femb is the token embedding layer of the\nLLM, and Ed \u2208Rnd\u00d7h2 is the embeddings of\n\ndocument d. A similar process is applied to obtain\nthe query embeddings Eq = {eq\nj}nq\nj=1, where nq is\nthe number of query tokens.\nFor nuanced analysis of each document, the cor-\nresponding retrieval information embeddings are\nthen prepended to the front of each document\u2019s\nembeddings. They are external knowledge and\nfunction as an anchor, guiding the LLM to focus\non useful documents. The final input embeddings\ncan be arranged as:\nE = [eq\n1, \u00b7 \u00b7 \u00b7 , eq\nnq\n|\n{z\n}\nquery\n, eR\n1 , ed1\n1 , \u00b7 \u00b7 \u00b7 , ed1\nnd1\n|\n{z\n}\ndocument1\n, \u00b7 \u00b7 \u00b7 , eR\nk , edk\n1\n, \u00b7 \u00b7 \u00b7 , edk\nndk\n|\n{z\n}\ndocumentk\n],\n(5)\nwhere eR\ni denotes the retrieval information embed-\nding for the i-th document. In this way, the re-\ntrieval information of corresponding document can\nbe well mixed, reducing the burden of the LLM\nto process all documents. Finally, we can get the\nresponses by:\n\u02c6y = fG(E),\n(6)\nwhere \u02c6y represents the LLM-generated results. No-\ntably, this part simplifies the instruction prompt,\nand detailed descriptions and prompt templates can\nbe found in Appendix B.\n3.5\nTraining Strategy\nAs the interdependence of retrieval and generation,\nwe integrate R2-Former training and LLM align-\nment into one stage. The joint training allows R2-\nFormer to better understand list-wise features from\nthe retriever, ensuring retrieval information can be\ndeeply interpreted by the LLM.\nFor R2-Former training, we perform a query-\ndocument matching (QDM) task that enforces R2-\nFormer to learn the relevance relationships from\nlist-wise features. In detail, it is a binary classi-\nfication task that asks to model each document\u2019s\nrelevance to the query. The formula for prediction\nis as follows:\n\u02c6s = f\u21921(H) = {\u02c6si}k\ni=1,\n(7)\nwhere f\u21921 is a binary classification head that\noutputs the relevance predictions \u02c6s. Supporting\ns={si}k\ni=1 are the ground-truth labels for docu-\nments, we use cross-entropy as the loss function,\ndefined as:\nLQDM(s,\u02c6s) = \u2212\nk\nX\ni=1\nsi log(\u02c6si)+(1\u2212si) log(1\u2212\u02c6si).\n(8)\nFor LLM alignment, we utilize the language\nmodeling (LM) task, which involves learning to\ngenerate subsequent tokens based on the preceding\ncontext and retrieval information. The language\nmodeling loss LLM aims to maximize the log-\nlikelihood of the tokens, rewarding the LLM for\npredicting subsequent words correctly.\nThe joint training involves instruction fine-\ntuning with a linear combination of QDM and LM\ntasks. The final loss is expressed as:\nL = LQDM+LLM.\n(9)\nNotably, R2AG offers the flexibility to train the\nR2-Former solely while freezing the LLM or to\ntrain both together for a deeper understanding of\nretrieval information. The decision represents a\ntrade-off between lower computational costs and\nhigher accuracy in real-world scenarios.\n4\nExperiments\n4.1\nDatasets and Metrics\nWe evaluate R2AG on five datasets:\nNatural\nQuestions (NQ) (Kwiatkowski et al., 2019), Hot-\npotQA (Yang et al., 2018), MuSiQue (Trivedi\net al., 2021), 2WikiMultiHopQA (2Wiki) (Ho et al.,\n2020), and DuReader (He et al., 2018). For NQ\ndataset, we utilize NQ-10, NQ-20, and NQ-30\ndatasets built by Liu et al. (2023), which contain 10,\n20, and 30 total documents, respectively. DuReader\nis a multiple documents QA version built by Bai\net al. (2023b). Detailed introduction and statistics\nare shown in Appendix C.\nFollowing Mallen et al. (2023); Liu et al. (2023),\nwe adopt accuracy (Acc) as the evaluation met-\nric for NQ datasets. Following Bai et al. (2023b),\nwe adopt accuracy (Acc) and F1 score as evalua-\ntion metrics for HotpotQA, MuSiQue, and 2Wiki\ndatasets. For DuReader dataset, we measure per-\nformance by F1 score and Rouge (Lin, 2004).\n4.2\nBaselines\nTo fully evaluate R2AG, we compared two types of\nmethods: standard RAG using various LLMs, and\nenhanced RAG using the same foundation LLM.\nFirst, we evaluate standard RAG baselines\nwhere LLMs generate responses given the query\nprepended with retrieved documents. For English\ndatasets, we use several open-source LLMs, includ-\ning LLaMA27B, LLaMA213B, LLaMA38B (Tou-\nvron et al., 2023), and LongChat1.57B (Li et al.,\n2023a). Besides, we adopt ChatGPT (Ouyang et al.,\n2022) and GPT4 (Achiam et al., 2023) as baselines\nof closed-source LLMs. For the Chinese dataset,\n\nMethods\nNQ-10\nNQ-20\nNQ-30\nHotpotQA\nMuSiQue\n2Wiki\nAcc\nAcc\nAcc\nAcc\nF1\nAcc\nF1\nAcc\nF1\nFrozen LLMs\nLLaMA27B\n0.3898\n-\n-\n0.2630\n0.0852\n0.0546\n0.0241\n0.1205\n0.0634\nLongChat1.57B\n0.6045\n0.5782\n0.5198\n0.5424\n0.3231\n0.2808\n0.1276\n0.3882\n0.2253\nLLaMA38B\n0.5141\n0.4991\n0.5311\n0.5901\n0.2056\n0.2427\n0.0891\n0.4723\n0.1952\nLLaMA213B\n0.7684\n-\n-\n0.3788\n0.1000\n0.0909\n0.0446\n0.2405\n0.0898\nChatGPT\n0.6886\n0.6761\n0.6347\n0.6557\n0.6518\n0.3376\n0.3321\n-\n-\nGPT4\n0.7759\n0.7514\n0.7514\n0.7673\n0.6026\n0.4853\n0.3270\n-\n-\nCoT\n0.4482\n0.6026\n0.5631\n0.2365\n0.1028\n0.0626\n0.0412\n0.1627\n0.0969\nRECOMP\n0.0169\n0.2222\n0.1977\n0.2388\n0.0265\n0.0830\n0.0156\n0.2666\n0.0329\nCRAG\n0.3974\n0.6441\n0.6347\n0.1194\n0.0360\n0.0262\n0.0047\n0.0768\n0.0422\nLongLLMLingua\n0.3635\n-\n-\n0.4174\n0.1178\n0.1939\n0.0477\n0.2374\n0.0888\nR2AG\n0.6930\n0.7062\n0.6704\n0.6675\n0.3605\n0.1864\n0.1687\n0.3342\n0.3452\nFine-tuned LLMs\nSelf-RAG\n0.1883\n-\n-\n0.2475\n0.1236\n0.0701\n0.0378\n0.2611\n0.1389\nRAFT\n0.7514\n0.8041\n0.7307\n0.7349\n0.3172\n0.2529\n0.1502\n0.7555\n0.4869\nR2AG+RAFT\n0.8192\n0.8060\n0.7458\n0.7351\n0.3056\n0.2295\n0.1533\n0.7444\n0.6351\nTable 1: Main results on four English datasets. All enhanced RAG methods utilize the same foundation LLMs,\nwith results marked in gray background indicating the performance of these foundation LLMs. Results in gray\nrepresent the performance of closed-source LLMs. Results in bold and results in underlined mean the best and\nsecond-best performance among current classified methods.\nMethods\nDuReader\nF1\nRouge\nFrozen LLMs\nLongChat1.57B\n0.0914\n0.1181\nQwen1.50.5B\n0.1395\n0.1656\nQwen1.51.8B\n0.1533\n0.1570\nInternLM21.8B\n0.1330\n0.1391\nR2AG\n0.1510\n0.1663\nFine-tuned LLMs\nRAFT\n0.2423\n0.2740\nR2AG+RAFT\n0.2507\n0.2734\nTable 2: Performance comparison on DuReader dataset.\nwe employ Qwen1.50.5B, Qwen1.51.8B (Bai et al.,\n2023a) and InternLM21.8B (Cai et al., 2024).\nSecondly, we experiment with several meth-\nods that can enhance RAG, including CoT (Wei\net al., 2022),\nRECOMP (Xu et al., 2023),\nCRAG (Yan et al., 2024), Self-RAG (Asai et al.,\n2023), LongLLMLingua (Jiang et al., 2023), and\nRAFT (Zhang et al., 2024). For NQ-10, HotpotQA,\nMuSiQue, and 2Wiki datasets, we use LLaMA27B\nas the foundation LLM for enhanced RAG methods,\nwhich has a maximum context length of 4k tokens.\nFor NQ-20 and NQ-30 datasets, LongChat1.57B\nis selected as the foundation LLM, which extends\nthe context window to 32k tokens. For DuReader\ndataset, Qwen1.50.5B is the foundation LLM, also\nwith a maximum context length of 32k tokens.\nThese methods were categorized into two groups\n\u2013 frozen and fine-tuned \u2013 based on whether they\nrequire training the LLMs.\nThe implementation details are in Appendix D.\n4.3\nMain Results\nTable 1 and Table 2 provide the main results. We\ncan obtain the following conclusions:\n(1) Compared with foundation LLMs using stan-\ndard RAG, R2AG can significantly increase perfor-\nmance. Even in multi-hot datasets, R2AG improves\nLLMs\u2019 ability for complex reasoning. In DuReader\ndataset, with a token length of 16k, R2AG remains\neffective, demonstrating its robustness and effi-\nciency in handling extensive text outputs. These re-\nsults indicate that R2AG effectively enables LLMs\nto better understand the retrieval information and\n\nMethods\nNQ-10\nNQ-20\nLLaMA27B\nLongChat1.57B\nR2AG\n0.6930\n0.7062\nw/o r\n0.6761 (\u21932.45%)\n0.6798 (\u21933.73%)\nw/o \u03b3\n0.6723 (\u21932.99%)\n0.6930 (\u21931.87%)\nw/o \u03b6\n0.6252 (\u21939.78%)\n0.6855 (\u21932.93%)\nw/o LQDM\n0.6441 (\u21937.07%)\n0.7043 (\u21930.27%)\nTable 3: Ablation studies on NQ-10 and NQ-20 datasets.\nGT\nTop1 Top2 Top3 Top4 Top5 Top6 Top7 Top8 Top9 Top10\n0.4\n0.5\n0.6\n0.7\n0.8\nMetric\nLearnable Tokens\nLLaMA27B\nMean\nFigure 3: Performance of learnable tokens across dif-\nferent document counts on NQ-10 dataset. \u201cGT\u201d means\nonly retaining ground-true documents.\nboosts their capabilities in handling provided doc-\numents. (2) Compared with other LLMs using\nstandard RAG, R2AG generally achieves better per-\nformance except for closed-source LLMs. GPT4\nshows superior results in most datasets, establish-\ning it as a strong baseline. Notably, R2AG ex-\ncels ChatGPT in NQ and HotpotQA datasets. Us-\ning LLaMA27B as the foundational LLM, R2AG\ncompetes well with LLaMA38B and LLaMA213B\nacross most metrics. (3) It is clear that R2AG\nsignificantly surpasses other enhanced RAG meth-\nods in most results, underscoring the importance\nof incorporating retrieval information. Although\nCRAG has a good result in NQ datasets, its perfor-\nmance significantly declines in multi-hop datasets.\nThat is because CRAG\u2019s simplistic approach of fil-\ntering out documents irrelevant to the query can\nomit crucial connections needed for understanding\ncomplex queries. Additionally, our method outper-\nforms compression-based methods (RECOMP and\nLongLLMLingua). Our case studies reveal their\npoor performance is mainly because the coordi-\nnation between the compressors and LLMs tends\nto result in substantial information loss and even\nsevere hallucinations. (4) RAFT can significantly\nimprove the performance. When combined with\nR2AG, the results are the best overall, suggesting\nthat a deeper understanding acquired through train-\ning benefits generation capabilities.\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nRetrieval Metric\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nGeneration Metric\n0.401\n0.356\n0.292\n0.296\n0.298\n0.303\nBGE-Ranker\nBERT\nContriever\nOpenAIsmall\nOpenAIlarge\nBERT-FT\nR2AG\nLLaMA27B\nFigure 4: Performance comparison of R2AG with vari-\nous retrievers on NQ-10 dataset.\nHotpotQA\n2Wiki\nMuSiQue\n0.0\n0.2\n0.4\n0.6\nAcc\n4.51%\n8.14%\n38.95%\nAcc\nF1\n0.0\n0.1\n0.2\n0.3\n0.4\nF1\n24.05%\n1.68%\n3.08%\nFigure 5: Performance of R2AG7B and R2AG13B.\nDarker parts mean the difference values of R2AG13B.\n4.4\nAblation Studies\nTo demonstrate the effectiveness of R2AG, we\ncreate four variants.\nSpecifically, we remove\nthree retrieval features r, \u03b3, \u03b6, individually. For\nR2-Former, we remove the QDM loss LQDM. We\nconduct the ablation studies on the NQ-10 and NQ-\n20 datasets, using LLaMA27B and LongChat1.57B\nas foundation LLMs with results shown in Table 3.\nWe can obtain the following observations: First,\nthe performance decreases without any of the three\nretrieval features, underscoring their effectiveness.\nThe results reveal that utilizing additional retrieval\nfeatures can help LLMs disentangle irrelevant\ndocuments. Secondly, the performance decreases\nwithout the QDM loss, showing that the query-\ndocument matching task is indeed beneficial for\nexploiting retrieval information.\nTo explore the effectiveness of the retrieval-\naware prompting strategy, we design an experi-\nment on NQ-10 dataset with various top-k retrieved\ndocuments where the retrieval information is set\nas learnable tokens. This means R2AG only uses\nthese soft prompts without additional features when\ntraining and inference. From the results shown in\nFigure 3, we can find that: (1) When retrieval re-\nsults are largely relevant, with few or no redundant\ndocuments, learnable tokens do not aid the LLM\nand may instead become redundant information\nfor the generation. (2) As the number of docu-\nments increases, it is natural to observe a decline\n\n48\ndocument1\n193\ndocument2\n(relevant)\n326\ndocument3\n486\ndocument4\n635\n32\n28\n24\n20\n16\n12\n8\n4\nLayer\n63\neR\n1\ndocument1\n214\neR\n2\ndocument2\n(relevant)\n353\neR\n3\ndocument3\n519\neR\n4\ndocument4\n674\neR\n5\n32\n28\n24\n20\n16\n12\n8\n4\nLayer\nFigure 6: Heatmaps of self-attention distribution of the last token, broken out by token position (X-axis) and layer\n(Y-axis). Each attention layer comprises 8 heads, and the attention weights are the mean of all the heads. Darker\nyellow means higher attention weights. eR\ni is the retrieval information embedding for i-th document.\nperformance. Surprisingly, learnable tokens sig-\nnificantly enhance the performance of the LLM.\nThese findings demonstrate that the retrieval-aware\nprompting strategy effectively assists LLMs in pro-\ncessing multiple documents, especially when those\ndocuments include irrelevant information.\n4.5\nDiscussions\nThe Impact of Performance of Retrievers and\nLLMs.\nAs mentioned in Section 1, the quality\nof retrieved documents can heavily influence the\nperformance of LLMs in RAG. From the main re-\nsults, R2AG achieves improvements even when\nthe retrieval performance is poor, as observed\nin MuSiQue and DuReader datasets.\nFurther-\nmore, we conduct experiments on NQ-10 dataset\nwith five non-trained retrievers, specifically BGE-\nReranker (Xiao et al., 2023), BERT (Devlin et al.,\n2019), Contriever (Izacard et al., 2022), and Ope-\nnAI Embedding models (small and large) (Nee-\nlakantan et al., 2022), with 1024, 768, 768, 1536,\nand 3072 dimensions, respectively. Note that Ope-\nnAI Embedding models are closed-source. From\nthe results presented in Figure 4, we easily observe\nthat a stronger retriever leads to better performance,\nboth standard RAG and R2AG. Importantly, R2AG\nsignificantly enhances the effectiveness of LLMs,\neven when the retrieval performance is poor.\nWe\nconduct\nexperiments\non\nHotpotQA,\nMuSiQue, and 2Wiki datasets using LLaMA213B\nas the foundation LLM. Results shown in Figure 5\nindicate that R2AG13B outperforms R2AG7B,\nparticularly in the accuracy metric.\nSpecially,\nthere is a decline performance in F1 scores for\nHotpotQA and MuSiQue datasets. We find this\nprimarily because larger LLMs usually tend to\noutput longer answers with explanations (the\naverage response token count in HotpotQA dataset\nfor R2AG7B is 37.44, compared to 49.71 for\nR2AG13B). This tendency also can be observed\nfrom the results of ChatGPT and GPT4.\nThese results reveal that both a stronger LLM\nand a more effective retriever lead to better perfor-\nmance, validating that R2AG is a genetic method\nthat can be efficiently applied in various scenarios.\nThe Effect of Retrieval Information.\nFor a\ndeeper and more intuitive exploration of how re-\ntrieval information improves LLMs\u2019 generation,\nwe present a visualization of the self-attention dis-\ntribution in R2AG compared with standard RAG.\nIn detail, we analyze a case in NQ-10 dataset in\nwhich the foundation LLM is LLaMA27B. We ex-\ntract the self-attention weights in different layers\nfrom LLM\u2019s outputs and visualize the last token\u2019s\nattention distribution for other tokens. The relevant\ndocument is ranked in position 2 in our selected\ncase, while the 1st document is potentially confus-\ning. For a clear illustration, we select attention\ndistribution for tokens in top-4 documents. From\nFigure 6, it is evident that the retrieval informa-\ntion receives higher attention scores even in deeper\nlayers, and the relevant document can get more at-\ntention within 1-4 layers. That means the retrieval\ninformation effectively acts as an anchor, guiding\nthe LLM to focus on useful documents.\n\n5\nConclusion and Future Work\nThis paper proposed a novel enhanced RAG frame-\nwork named R2AG to bridge the semantic gap be-\ntween the retrievers and LLMs. By incorporating\nretrieval information from retrievers into LLMs\u2019\ngeneration process, R2AG captures a comprehen-\nsive understanding of retrieved documents. Experi-\nmental results show that R2AG outperforms other\ncompetitors. In addition, the robustness and effec-\ntiveness of R2AG are further confirmed by detailed\nanalysis. In future work, more retrieval features\ncould be applied to R2AG framework.\nLimitations\nThe following are the limitations associated with\nR2AG: First, R2AG depends on the semantic rep-\nresentations modeled by encoder-based retrievers.\nThe suitability of other types of retrievers, such as\nsparse and cross-encoder retrievers, requires further\nexploration. Secondly, as mentioned in Section 4.5,\nR2AG relies on the ability of the foundation LLM,\nand more powerful closed-source LLMs may not be\ncompatible with R2AG. Thirdly, there may be other\ninformative features besides the three retrieval fea-\ntures - relevance, precedent similarity, and neighbor\nsimilarity scores. Lastly, R2AG is evaluated on five\ndatasets, of which relevant documents are provided.\nHowever, situations where no relevant documents\nare available need to be considered. R2AG may\nbenefit from integrating techniques like self-RAG\nto better handle such situations.\nEthics Statement\nLLMs can generate incorrect and potentially harm-\nful answers. Our proposed method aims to alle-\nviate this issue by providing LLMs with retrieved\ndocuments and retrieval information, thereby en-\nhancing LLMs\u2019 capability of generation. In the\ndevelopment and execution of our work, we strictly\nadhered to ethical guidelines established by the\nbroader academic and open-source community. All\nthe datasets and models used in this work are pub-\nlicly available. No conflicts of interest exist for any\nof the authors involved in this work.\nAcknowledgments\nThis work was supported by Major Program\nof National Language Committee (WT145-39),\nNatural\nScience\nFoundation\nof\nGuangdong\n(2023A1515012073) and National Natural Science\nFoundation of China (No. 62006083).\nReferences\nOpenAI Josh Achiam, Steven Adler, Sandhini Agarwal,\nLama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, and et al. 2023. Gpt-4 technical\nreport.\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\nHannaneh Hajishirzi. 2023. Self-rag: Learning to\nretrieve, generate, and critique through self-reflection.\nArXiv, abs/2310.11511.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,\nXiaodong Deng, Yang Fan, Wenhang Ge, Yu Han, Fei\nHuang, Binyuan Hui, Luo Ji, Mei Li, and et al. 2023a.\nQwen technical report. ArXiv, abs/2309.16609.\nYushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai\nTang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Ao-\nhan Zeng, Lei Hou, and et al. 2023b. Longbench:\nA bilingual, multitask benchmark for long context\nunderstanding. ArXiv, abs/2308.14508.\nScott Barnett, Stefanus Kurniawan, Srikanth Thudumu,\nZach Brannelly, and Mohamed Abdelrazek. 2024.\nSeven failure points when engineering a retrieval aug-\nmented generation system. ArXiv, abs/2401.05856.\nParishad BehnamGhader, Santiago Miret, and Siva\nReddy. 2022.\nCan retriever-augmented language\nmodels reason? the blame game between the retriever\nand the language model. ArXiv, abs/2212.09146.\nZheng Cai, Maosong Cao, Haojiong Chen, Kai Chen,\nKeyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi\nChen, Pei Chu, Xiao wen Dong, and et al. 2024.\nInternlm2 technical report. ArXiv, abs/2403.17297.\nJingcheng Deng, Liang Pang, Huawei Shen, and Xueqi\nCheng. 2023.\nRegaVAE: A retrieval-augmented\nGaussian mixture variational auto-encoder for lan-\nguage modeling. In Findings of the Association for\nComputational Linguistics: EMNLP 2023, pages\n2500\u20132510, Singapore. Association for Computa-\ntional Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171\u20134186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,\nJinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo,\nMeng Wang, and Haofen Wang. 2023. Retrieval-\naugmented generation for large language models: A\nsurvey. ArXiv, abs/2312.10997.\nMichael Glass, Gaetano Rossiello, Md Faisal Mahbub\nChowdhury, Ankita Naik, Pengshan Cai, and Alfio\nGliozzo. 2022. Re2G: Retrieve, rerank, generate.\nIn Proceedings of the 2022 Conference of the North\n\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 2701\u20132715, Seattle, United States. Association\nfor Computational Linguistics.\nWei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao,\nXinyan Xiao, Yuan Liu, Yizhong Wang, and et al.\n2018. DuReader: a Chinese machine reading compre-\nhension dataset from real-world applications. pages\n37\u201346.\nXiaoxin He, Yijun Tian, Yifei Sun, N. Chawla, Thomas\nLaurent, Yann LeCun, Xavier Bresson, and Bryan\nHooi. 2024. G-retriever: Retrieval-augmented gen-\neration for textual graph understanding and question\nanswering. ArXiv, abs/2402.07630.\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\nand Akiko Aizawa. 2020. Constructing a multi-hop\nQA dataset for comprehensive evaluation of reason-\ning steps. pages 6609\u20136625.\nJ. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu\nChen. 2021. Lora: Low-rank adaptation of large\nlanguage models. ArXiv, abs/2106.09685.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\nHosseini, Fabio Petroni, Timo Schick, and et al. 2024.\nAtlas: few-shot learning with retrieval augmented\nlanguage models. J. Mach. Learn. Res., 24(1).\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\nHosseini, Fabio Petroni, Timo Schick, Jane A. Yu,\nArmand Joulin, Sebastian Riedel, and Edouard Grave.\n2022. Atlas: Few-shot learning with retrieval aug-\nmented language models. ArXiv, abs/2208.03299.\nHuiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng\nLi, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023.\nLongllmlingua: Accelerating and enhancing llms\nin long context scenarios via prompt compression.\nArXiv, abs/2310.06839.\nNikhil Kandpal, H. Deng, Adam Roberts, Eric Wallace,\nand Colin Raffel. 2022. Large language models strug-\ngle to learn long-tail knowledge. In International\nConference on Machine Learning.\nZixuan Ke, Weize Kong, Cheng Li, Mingyang Zhang,\nQiaozhu Mei, and Michael Bendersky. 2024. Bridg-\ning the preference gap between retrievers and llms.\nArXiv, abs/2401.06954.\nDiederik P. Kingma and Jimmy Ba. 2014.\nAdam:\nA method for stochastic optimization.\nCoRR,\nabs/1412.6980.\nYuma Koizumi, Yasunori Ohishi, Daisuke Niizumi,\nDaiki Takeuchi, and Masahiro Yasuda. 2020. Au-\ndio captioning using pre-trained large-scale language\nmodel guided by audio-based similar caption re-\ntrieval. ArXiv, abs/2012.07331.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur P. Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, and et al. 2019.\nNatural questions: A benchmark for question answer-\ning research. Transactions of the Association for\nComputational Linguistics, 7:453\u2013466.\nPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Kuttler, Mike Lewis, and et al. 2020. Retrieval-\naugmented generation for knowledge-intensive nlp\ntasks. ArXiv, abs/2005.11401.\nDacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lian-\nmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe\nMa, and Hao Zhang. 2023a. How long can open-\nsource LLMs truly promise on context length?\nDongxu Li, Junnan Li, Hung Le, Guangsen Wang, Sil-\nvio Savarese, and Steven C. H. Hoi. 2022. Lavis:\nA library for language-vision intelligence. ArXiv,\nabs/2209.09019.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven C. H.\nHoi. 2023b. Blip-2: Bootstrapping language-image\npre-training with frozen image encoders and large\nlanguage models. In International Conference on\nMachine Learning.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74\u201381, Barcelona, Spain.\nAssociation for Computational Linguistics.\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-\njape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. 2023. Lost in the middle: How language mod-\nels use long contexts. Transactions of the Association\nfor Computational Linguistics, 12:157\u2013173.\nYixiao Ma, Qingyao Ai, Yueyue Wu, Yunqiu Shao,\nYiqun Liu, M. Zhang, and Shaoping Ma. 2022. In-\ncorporating retrieval information into the truncation\nof ranking lists for better legal search. Proceedings\nof the 45th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\nDaniel Khashabi, and Hannaneh Hajishirzi. 2023.\nWhen not to trust language models: Investigating\neffectiveness of parametric and non-parametric mem-\nories. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 9802\u20139822, Toronto,\nCanada. Association for Computational Linguistics.\nArvind Neelakantan, Tao Xu, Raul Puri, Alec Rad-\nford, Jesse Michael Han, Jerry Tworek, Qiming Yuan,\nNikolas A. Tezak, Jong Wook Kim, and et al. 2022.\nText and code embeddings by contrastive pre-training.\nArXiv, abs/2201.10005.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, and et al. 2022. Training language\nmodels to follow instructions with human feedback.\nArXiv, abs/2203.02155.\n\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, and et al. 2019. Pytorch: An\nimperative style, high-performance deep learning li-\nbrary. ArXiv, abs/1912.01703.\nNils Reimers and Iryna Gurevych. 2019.\nSentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982\u20133992, Hong Kong, China. Association for Com-\nputational Linguistics.\nDevendra Singh Sachan, Siva Reddy, William L. Hamil-\nton, Chris Dyer, and Dani Yogatama. 2021. End-to-\nend training of multi-document reader and retriever\nfor open-domain question answering. In Advances in\nNeural Information Processing Systems.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon\nSeo, Rich James, Mike Lewis, Luke Zettlemoyer, and\nWen tau Yih. 2023. Replug: Retrieval-augmented\nblack-box language models. ArXiv, abs/2301.12652.\nHugo Touvron, Louis Martin, Kevin R. Stone, Peter\nAlbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, and et al. 2023. Llama 2:\nOpen foundation and fine-tuned chat models. ArXiv,\nabs/2307.09288.\nH. Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2021.\nMusique: Multi-\nhop questions via single-hop question composition.\nTransactions of the Association for Computational\nLinguistics, 10:539\u2013554.\nAshish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Neural Information Processing Systems.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and\nDenny Zhou. 2022. Chain of thought prompting\nelicits reasoning in large language models. ArXiv,\nabs/2201.11903.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, and et al. 2020. Transformers:\nState-of-the-art natural language processing. pages\n38\u201345.\nSiye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai\nZhang, and Yanghua Xiao. 2024. How easily do\nirrelevant inputs skew the responses of large language\nmodels? ArXiv, abs/2404.03302.\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\nMuennighoff. 2023. C-pack: Packaged resources\nto advance general chinese embedding.\nArXiv,\nabs/2309.07597.\nFangyuan Xu, Weijia Shi, and Eunsol Choi. 2023.\nRecomp: Improving retrieval-augmented lms with\ncompression and selective augmentation.\nArXiv,\nabs/2310.04408.\nShicheng Xu, Liang Pang, Jun Xu, Huawei Shen, and\nXueqi Cheng. 2024. List-aware reranking-truncation\njoint model for search and retrieval-augmented gen-\neration. In The Web Conference.\nShi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling.\n2024.\nCorrective retrieval augmented generation.\nArXiv, abs/2401.15884.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W. Cohen, Ruslan Salakhutdinov, and\nChristopher D. Manning. 2018. Hotpotqa: A dataset\nfor diverse, explainable multi-hop question answer-\ning. In Conference on Empirical Methods in Natural\nLanguage Processing.\nMichihiro Yasunaga, Armen Aghajanyan, Weijia Shi,\nRich James, Jure Leskovec, Percy Liang, Mike Lewis,\nLuke Zettlemoyer, and Wen tau Yih. 2023. Retrieval-\naugmented multimodal language modeling. ArXiv,\nabs/2211.12561.\nFuda Ye and Shuangyin Li. 2024. Milecut: A multi-\nview truncation framework for legal case retrieval.\nIn Proceedings of the ACM Web Conference 2024,\nWWW \u201924, page 1341\u20131349, New York, NY, USA.\nAssociation for Computing Machinery.\nTianjun Zhang, Shishir G. Patil, Naman Jain, Sheng\nShen, Matei A. Zaharia, Ion Stoica, and Joseph E.\nGonzalez. 2024. Raft: Adapting language model to\ndomain specific rag. ArXiv, abs/2403.10131.\nPenghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren\nWang, Yunteng Geng, Fangcheng Fu, Ling Yang,\nWentao Zhang, and Bin Cui. 2024.\nRetrieval-\naugmented generation for ai-generated content: A\nsurvey. ArXiv, abs/2402.19473.\nWayne Xin Zhao, Jing Liu, Ruiyang Ren, and Ji rong\nWen. 2022. Dense text retrieval based on pretrained\nlanguage models: A survey. ACM Transactions on\nInformation Systems.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, and et al. 2023. A survey of\nlarge language models. ArXiv, abs/2303.18223.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny. 2023a. Minigpt-4: Enhancing\nvision-language understanding with advanced large\nlanguage models. ArXiv, abs/2304.10592.\nYutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu,\nWenhan Liu, Chenlong Deng, Zhicheng Dou, and\nJi rong Wen. 2023b. Large language models for infor-\nmation retrieval: A survey. ArXiv, abs/2308.07107.\n\nDatasets\nLanguage\n# Query\n# Train/Test\n# Tokens\n# Rel/Docs\nMAP\nNQ-10\nEnglish\n2655\n2124/531\n\u223c2k\n1/10\n0.9602\nNQ-20\nEnglish\n2655\n2124/531\n\u223c4k\n1/20\n0.9287\nNQ-30\nEnglish\n2655\n2124/531\n\u223c6k\n1/30\n0.9215\nHotpotQA\nEnglish\n97852\n90447/7405\n\u223c2k\n2.36/10\n0.9138\nMuSiQue\nEnglish\n22355\n19938/2417\n\u223c3k\n2.37/20\n0.5726\n2Wiki\nEnglish\n180030\n167454/12576\n\u223c2k\n2.42/10\n0.9637\nDuReader\nChinese\n200\n160/40\n\u223c16k\n1.82/20\n0.7169\nTable 4: Statistics of datasets. \u201c# Rel/Docs\u201d denotes the number of relevant documents and the total number of\ndocuments for each query. \u201cMAP\u201d represents the Mean Average Precision, a common retrieval metric.\nA\nRetrieval Feature Extraction Details\nFormally, the relevance between the query and the\ni-th document is calculated as:\nri = sim\n\u0010\nxq, xd\ni\n\u0011\n,\n(10)\nwhere sim is a similarity function such as dot prod-\nuct or cosine similarity, xq and xd\ni are representa-\ntions of query and i-th document, respectively.\nThe precedent similarity computes the simi-\nlarity score between case representation and its\nprecedent-weighted representations in the ranking\nlist as follows:\n\u03b3i=sim\n\uf8eb\n\uf8edxd\ni ,\ni\u22121\nX\nj=1\nwj \u00b7 xd\nj\n\uf8f6\n\uf8f8, wj=\nexp(rj)\nPk\n\u2113=1 exp(r\u2113)\n,\n(11)\nwhere \u03b3i is the precedent similarity between i-th\ndocument and its precedents in the ranking list, and\nri is relevance between the query and i-th docu-\nment.\nNeighbor similarity represents the average simi-\nlarity of i-th document to its adjacent documents.\nSpecifically, the neighbor similarity of a case in the\nranking list is given by:\n\u03b6i =\n(sim(xd\n1, xd\n2),\ni = 1\n[sim(xd\ni\u22121, xd\ni ) + sim(xd\ni , xd\ni+1)]/2,\ni \u2208[2, k)\nsim(xd\nk\u22121, xd\nk),\ni = k\n,\n(12)\nwhere \u03b6i represents the average similarity of i-th\ndocument to its adjacent documents. Such that we\ncan get the list-wise features among documents.\nB\nPrompt Templates\nIn R2AG, retrieval information, we append k spe-\ncial tokens (\u201c<R>\u201d) in front of each document to\nfacilitate the incorporation of retrieval information.\nThese tokens do not carry meaningful semantics\nbut serve as placeholders for the retrieval informa-\ntion within the prompt. This special token facili-\ntates the integration of retrieval information into\nthe generation process.\nTable 5 shows the prompt templates for R2AG\nand other baselines.\nThe prompt templates of\nDuReader dataset can be found in our source code.\nC\nDataset Introduction\nWe conduct evaluations on five datasets, including:\nNatural Questions (NQ)\n(Kwiatkowski et al.,\n2019) is developed from Google Search and con-\ntains questions coupled with human-annotated an-\nswers extracted from Wikipedia.\nFurther, Liu\net al. (2023) collect k\u22121 distractor documents from\nWikipedia that do not contain the answers, where\nk is the total document number for each question.\nThis dataset has three versions: NQ-10, NQ-20,\nand NQ-30, with total document numbers of 10,\n20, and 30, respectively.\nHotpotQA\n(Yang et al., 2018) is a well-known\nmulti-hop question answering dataset based on\nWikipedia. This dataset involves questions requir-\ning finding and reasoning over multiple supporting\nfacts from 10 documents. There are two reasoning\ntypes of questions: bridging and comparison.\nMuSiQue\n(Trivedi et al., 2021) has questions\nthat involve 2-4 hops and six types of reasoning\nchains. The dataset is constructed through a bot-\ntom\u2013up process by carefully selecting and compos-\ning single-hop questions. The final answer to each\nquestion in the distractor setting is extracted from\n20 documents.\n2WikiMultiHopQA (2Wiki)\n(Ho et al., 2020)\nconsists of up to 5-hop questions, each associated\nwith 10 documents. Unlike HotpotQA, this dataset\nneeds to evaluate the interpretability of models not\nonly with supporting evidence but also with entity-\nrelation tuples.\n\nDuReader\n(He et al., 2018) is a Chinese dataset\ndeveloped based on Baidu Search and Baidu Zhi-\ndao. To adapt it for assessing long context ability,\nfor each question, Bai et al. (2023b) arbitrarily se-\nlect several documents from the total corpus as\ndistractors until each question is associated with 20\ncandidate documents.\nThe ground truth labels are provided in original\ndatasets. Detailed statistics can be found in Table 4.\nD\nImplementation Details\nUnlike some works (Li et al., 2023b; Zhu et al.,\n2023a) built on LAVIS (Li et al., 2022), we com-\npletely implement R2AG on PyTorch (Paszke et al.,\n2019) and Transformers (Wolf et al., 2020) libraries\nfor easy usage.\nFor the retrieval task, we utilize the Sentence-\nTransformer (Reimers and Gurevych, 2019) to fine-\ntune a BERT (Devlin et al., 2019) model as the re-\ntriever, which is a siamese dual encoder with shared\nparameters. The models \u201cbert-base-uncased\u201d\nand \u201cbert-base-chinese\u201d are used for English\ndatasets and the Chinese dataset, respectively. All\nretrievers adopt default hyper-parameter settings\nwith 768 embedding dimensions. Cosine similarity\nis employed as the scoring function for retrieval\nand feature extraction. The retrieval performance\nacross datasets is shown in Table 4. Contrary to\nsome works (Liu et al., 2023; Jiang et al., 2023)\nthat artificially place ground truth documents in\nfixed positions, this paper considers that candidate\ndocuments are ranked by the retriever to simulate\nreal-world scenarios.\nFor R2-Former, we determine the learning rate\nas 2e-4 and dropout as 0.1. The number of attention\nheads and hidden size in Transformer encoder are\n4 and 256, respectively. Adam (Kingma and Ba,\n2014) is adopted as the optimization algorithm.\nFor LLMs, all methods use default settings and\nadopt greedy decoding for fair comparison. The\nChatGPT version is \u201cgpt-3.5-turbo-0125\u201d with\na 16k context window size, and the GPT4 version is\n\u201cgpt-4-turbo-2024-04-09\u201d with a 128k context\nwindow size. In CRAG, the retrieval evaluator only\ntriggered {Correct, Ambiguous} actions to next\nknowledge refinement process as there are at least\none relevant document in retrieval results. In the\nRAFT method, we employ LoRA (Hu et al., 2021)\nto effectively fine-tune LLMs, with LoRA rank set\nat 16, alpha at 32, and dropout at 0.1.\nMethods\nPrompts\nRAG\nWrite a high-quality answer for the given\nquestion using only the provided search\nresults (some of which might be irrelevant).\nOnly give me the answer and do not output\nany other words.\n[1]{#d1}\n[2]{#d2}\n...\n[k]{#dk}\nOnly give me the answer and do not output\nany other words.\nQuestion: {#q}\nAnswer:\nCoT\nWrite a high-quality answer for the given\nquestion using only the provided search\nresults (some of which might be irrelevant).\nOnly give me the answer and do not output\nany other words.\n[1]{#d1}\n[2]{#d2}\n...\n[k]{#dk}\nOnly give me the answer and do not output\nany other words.\nQuestion: {#q}\nLet\u2019s think it step by step.\nComps\nWrite a high-quality answer for the given\nquestion using only the provided search\nresults (some of which might be irrelevant).\nOnly give me the answer and do not output\nany other words.\n{#Compressed documents}\nOnly give me the answer and do not output\nany other words.\nQuestion: {#q}\nAnswer:\nR2AG\nWrite a high-quality answer for the given\nquestion using only the provided search\nresults (some of which might be irrelevant).\nOnly give me the answer and do not output\nany other words. The similarity\ninformation is provided in front of search\nresults.\n[1]similarity: <R>{#d1}\n[2]similarity: <R>{#d2}\n...\n[k]similarity: <R>{#dk}\nOnly give me the answer and do not output\nany other words.\nQuestion: {#q}\nAnswer:\nTable 5:\nPrompt templates of different methods.\n\u201cComps\u201d means compression-based methods, including\nRECOMP and LongLLMLingua. \u201c<R>\u201d is the place-\nholder for retrieval information.\n"}