{"metadata": {"pdf_filename": "2305.17331v1__Augmentation-Adapted Retriever Improves Generalization of Language Models as Gen.pdf", "source": "arXiv"}, "text": "Augmentation-Adapted Retriever Improves Generalization of Language\nModels as Generic Plug-In\nZichun Yu1\nChenyan Xiong2\nShi Yu1\nZhiyuan Liu13\n1Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China\n2Microsoft Research, Redmond, USA\n3Beijing National Research Center for Information Science and Technology, Beijing, China\n{yuzc19, yus21}@mails.tsinghua.edu.cn; chenyan.xiong@microsoft.com\nliuzy@tsinghua.edu.cn\nAbstract\nRetrieval augmentation can aid language\nmodels (LMs) in knowledge-intensive tasks\nby supplying them with external information.\nPrior works on retrieval augmentation usually\njointly fine-tune the retriever and the LM,\nmaking them closely coupled. In this paper, we\nexplore the scheme of generic retrieval plug-in:\nthe retriever is to assist target LMs that may\nnot be known beforehand or are unable to\nbe fine-tuned together.\nTo retrieve useful\ndocuments for unseen target LMs, we propose\naugmentation-adapted retriever (AAR), which\nlearns LM\u2019s preferences obtained from a\nknown source LM. Experiments on the MMLU\nand PopQA datasets demonstrate that our AAR\ntrained with a small source LM is able to signif-\nicantly improve the zero-shot generalization of\nlarger target LMs ranging from 250M Flan-T5\nto 175B InstructGPT. Further analysis indicates\nthat the preferences of different LMs overlap,\nenabling AAR trained with a single source\nLM to serve as a generic plug-in for various\ntarget LMs.\nOur code is open-sourced at\nhttps://github.com/OpenMatch/Augmentation-\nAdapted-Retriever.\n1\nIntroduction\nLarge language models (LMs) that possess bil-\nlions of parameters are able to capture a signif-\nicant amount of human knowledge, leading to\nconsistent improvements on various downstream\ntasks (Brown et al., 2020; Kaplan et al., 2020;\nRoberts et al., 2020). However, the undeniable\ndrawback of large LMs lies in their high compu-\ntational cost, which negatively impacts their effi-\nciency (Strubell et al., 2019; Bender et al., 2021).\nFurthermore, the knowledge memorized from pre-\ntraining and the implicit reasoning process of LMs\ncan be inaccurate and intractable sometimes, hin-\ndering their applications on knowledge-intensive\ntasks (Guu et al., 2020; Lewis et al., 2020; Mallen\net al., 2022; Wei et al., 2022).\nFlan-T5Base\n(250M)\nFlan-T5Large\n(780M)\nFlan-T5XL\n(3B)\nInstructGPT\n(175B)\n# Parameters\n35\n40\n45\n50\n55\n60\n65\nMMLU Accuracy\nStandalone LM\nLM w/ Few-Shot Prompting\nLM w/ Adaptive Retrieval\nLM w/ AAR (Ours)\nFigure 1: Performance of LM w/ AAR (Ours).\nInstead of leveraging the knowledge and rea-\nsoning abilities embedded within the parameters\nof the LMs, retrieval augmentation (Guu et al.,\n2020; Lewis et al., 2020; Borgeaud et al., 2022)\nenhances the LM with a retriever that can retrieve\nknowledge from an external corpus. On the other\nhand, prior retrieval augmentation methods (Izac-\nard and Grave, 2021a; Izacard et al., 2022) necessi-\ntate fine-tuning the backbone LM to adjust to the\nretriever and tackle specific downstream tasks. This\nkind of fine-tuning can be expensive when more\nand more unique demands emerge (Maronikolakis\nand Sch\u00fctze, 2021). More importantly, many top-\ntier LMs can only be accessed through black-box\nAPIs (Ouyang et al., 2022; OpenAI, 2023). These\nAPIs allow users to submit queries and receive re-\nsponses but typically do not support fine-tuning.\nIn this paper, we introduce Augmentation-\nAdapted Retriever (AAR) to assist black-box LMs\nwith downstream tasks as generic plug-in. To re-\ntrieve valuable documents for many unseen LMs,\nwe propose to leverage a small source LM to pro-\nvide LM-preferred signals for retriever\u2019s training.\nThe retriever after training (i.e., AAR) can be di-\nrectly utilized to assist a large target LM by plug-\nging in the retrieved documents.\nSpecifically, we choose a small encoder-decoder\nLM as the source LM and utilize its fusion-\narXiv:2305.17331v1  [cs.CL]  27 May 2023\n\nin-decoder attention scores (Izacard and Grave,\n2021a) to annotate LM-preferred documents. The\nLM-preferred documents are then combined with\nhuman-preferred documents to form the positive\ndocument set. Negative documents are mined by\nthe retriever itself using the ANCE (Xiong et al.,\n2021) technique. After fine-tuning the retriever\nwith LM\u2019s preferences, it can directly assist unseen\ntarget LMs in the zero-shot task generalization.\nWe evaluate AAR on a multi-task language\nunderstanding dataset MMLU (Hendrycks et al.,\n2021) and an entity-centric question answering\ndataset PopQA (Mallen et al., 2022). For the tar-\nget LMs, we choose Flan-T5 (Chung et al., 2022)\nseries as our backbone for encoder-decoder LMs\nand InstructGPT (Ouyang et al., 2022) as our back-\nbone for decoder-only LMs. Figure 1 shows that\nassisted with a generic AAR, LMs of different sizes\nand architectures can consistently outperform the\nstandalone LMs; the performance of smaller LMs\ncan sometimes surpass the standalone counterparts\nof significantly larger sizes (e.g., Flan-T5Large w/\nAAR outperforms standalone Flan-T5XL by 0.6%).\nAAR also demonstrates advantages over other aug-\nmentation approaches such as few-shot prompting\nand adaptive retrieval (Mallen et al., 2022).\nFurther analysis reveals that the preferences ob-\ntained from different-sized source LMs are similar,\nand LMs with near capacities tend to yield closer\npreferred document sets. As a result, our AAR\nmodel trained from a small source LM can be con-\nsidered as a generic plug-in to enhance the zero-\nshot generalization of a significantly larger target\nLM. We also discover that the documents preferred\nby LMs can provide assistance to the model from\nalternative perspectives, rather than relying solely\non the full information favored by search users.\n2\nRelated Work\nRetrieval Augmentation. Augmenting LMs with\nretrieved information from external memories has\nshown effective on diverse knowledge-intensive\ntasks (Guu et al., 2020).\nPrior works explore\nnovel ways to train the whole retriever-LM sys-\ntem in an end-to-end fashion, using retrieval-\naugmented sequence log-likelihood (Lewis et al.,\n2020; Borgeaud et al., 2022), fusion-in-decoder\nattention distillation (Izacard and Grave, 2021a;\nIzacard et al., 2022), or knowledge graph (Ju et al.,\n2022). To decouple the retriever from LM, Rubin\net al. (2022) train an independent prompt retriever\nfor in-context learning, and Lin et al. (2022) only\nfine-tune the LM via the retrieved data that is simi-\nlar to few-shot unsupervised samples.\nRecent researches adopt zero-shot retrieval aug-\nmentation that does not fine-tune the LM on In-\nstructGPT (Ouyang et al., 2022). It can benefit\nentity-centric question answering (Mallen et al.,\n2022), chain-of-thought reasoning (He et al., 2022),\nand multi-hop question answering (Khattab et al.,\n2022). Parallel work (Shi et al., 2023) uses LM\nlikelihood to train the retriever for satisfying black-\nbox LM\u2019s preferences, and they adopt GPT-3\nCurie (Brown et al., 2020) to provide the super-\nvision signals. In this work, we devise the retriever\nthat can be used as a generic plug-in to assist a\nvariety of unseen LMs.\nZero-shot Learning and Reasoning.\nLarge-\nscale unsupervised pre-trained LMs like GPT-\n3 (Brown et al., 2020), GPT-4 (OpenAI, 2023),\nand PaLM (Chowdhery et al., 2022) are able to\nperform zero-shot learning on many downstream\ntasks with a task description provided at inference\ntime. Instruction-finetuned LMs (Sanh et al., 2022;\nChung et al., 2022; Ouyang et al., 2022), which\nare pre-trained on multiple supervised tasks using\nhuman instructions, also also exhibit robust zero-\nshot learning capabilities. Yu et al. (2023) pro-\npose a new scheme of zero-shot reasoning, which\nfirst prompts large LMs to generate relevant docu-\nments and then perform reading comprehension\non the generated contents. Recently, there has\nbeen a growing trend of utilizing plug-and-play\nknowledge injection to enhance the zero-shot per-\nformance of LMs, which is achieved through map-\nping network (Zhang et al., 2023) or document\nencoding (Xiao et al., 2023). Our work improves\nthe zero-shot generalization of LMs by utilizing the\nretrieved information. We demonstrate that identi-\nfying LMs\u2019 preferences to train the retriever can in\nturn bring additional evidence texts for LMs.\n3\nMethod\nIn this section, we first introduce the preliminaries\nof the dense retrieval and the retrieval-augmented\nLM (\u00a7 3.1), then propose our augmentation-\nadapted retriever (\u00a7 3.2).\n3.1\nPreliminaries\nRetrieval-augmented LM (Guu et al., 2020; Lewis\net al., 2020) is a type of LM that leverages external\ninformation to improve its performance. It retrieves\n\nrelevant documents from a corpus using a retriever,\nand then utilizes the documents to enhance its lan-\nguage generation capabilities.\nThe objective of the retriever is to find an aug-\nmentation document set Da from a corpus C that\nhelps the LM handle a given query q. Previous\nresearches (Karpukhin et al., 2020; Xiong et al.,\n2021) concentrate primarily on the dense retrieval\nsystem that searches in the dense vector space since\ndense retrieval usually performs more accurately\nand efficiently than sparse one.\nA dense retrieval model first represents q and\nthe document d into an embedding space using a\npre-trained encoder g,\nq = g(q); d = g(d), d \u2208C,\n(1)\nand match their embeddings by dot product func-\ntion f, which supports fast approximate nearest\nneighbor search (ANN) (Andr\u00e9 et al., 2016; John-\nson et al., 2021). We then define Da that contains\ntop-N retrieved documents as:\nDa = {da\n1 . . . da\nN} = ANNN\nf(q,\u25e6).\n(2)\nFor the LM backbones, the decoder-only and\nthe encoder-decoder models are the two primary\nchoices of the retrieval-augmented LMs (Izacard\nand Grave, 2021b; Yu et al., 2023).\nGiven a decoder-only LM like GPT-3 (Brown\net al., 2020), the LM input can be a simple concate-\nnation of the query and all the augmentation docu-\nments {da\n1 . . . da\nN}. Then, the LM will generate the\nanswer based on the inputs auto-regressively.\nFor an encoder-decoder LM like T5 (Raffel et al.,\n2020), taking simple concatenation as the encoder\ninput may still be effective. However, this method\nmay not scale to a large volume of documents due\nto the quadratic self-attention computation associ-\nated with the number of documents. To aggregate\nmultiple documents more efficiently, Izacard and\nGrave (2021b) propose the fusion-in-decoder (FiD)\nmechanism, which soon becomes the mainstream\nin the development of encoder-decoder retrieval-\naugmented LMs. It first encodes each concatena-\ntion of the (da\ni , q) pair separately and then lets the\ndecoder attend to all parts:\nFiD(q) = Dec(Enc(da\n1\u2295q) . . . Enc(da\nN \u2295q)). (3)\nIn this way, the encoder computes self-attention\nover one document at a time so that the compu-\ntational cost can grow linearly with the number\nof documents. Furthermore, FiD cross-attention\nis found effective in estimating the relative im-\nportance of the augmentation documents from\nNegatives\nANCE Sampling\nPositives\nGround Truth \nTop-K FiDAtt\n\u00a0\n\u00a0\n\nPre-Trained Retriever\nQ + D1\n...\nEnc\nEnc\nDec\nEnc\n...\nSource LM\nFusion-in-Decoder\nRetrieve\nN Docs\nSource Task\nQ + D2\nQ + DN\nAugmentation-Adapted Retriever\nTarget LMs Target Tasks\nGeneric \nPlug-In\nFigure 2: Illustration of augmentation-adapted retriever.\nthe LM\u2019s perspective (Izacard and Grave, 2021a).\nTherefore, soft FiD distillation (Izacard and Grave,\n2021a; Izacard et al., 2022; Shi et al., 2023), which\nminimizes the KL-divergence between retrieval\nlikelihood and LM likelihood, is often used to train\nthe retriever and the LM end-to-end.\n3.2\nAugmentation-adapted Retriever\nDue to the emerging real-world demands and\nthe limitations of black-box APIs, fine-tuning\nretrieval-augmented LM for each possible down-\nstream task can be infeasible. Hence, we intro-\nduce Augmentation-Adapted Retriever (AAR) as a\ngeneric plug-in for black-box LMs. As illustrated\nin Figure 2, AAR can learn the preferences of LMs\nwithout the need for fine-tuning them.\nSpecifically, we utilize an encoder-decoder LM\nas source LM (Ls) to provide LM-preferred signals\non a source task (Ts) for fine-tuning a pre-trained\nretriever. Then, we plug the fine-tuned retriever\ninto unseen target LM (Lt) on a set of target tasks\n(Tt) non-intersecting with Ts.\nOur training method starts from a source task Ts,\nwhere we aggregate the source LM Ls\u2019s average\nFiD cross-attention (FiDAtt) scores Sa\ni correspond-\ning to document da\ni from the first decoder token\nover all the layers, all the heads and all the input\ntokens t of da\ni \u2295q:\nSa\ni =\n1\nln \u2217hn \u2217tn\nX\nlayers\nX\nheads\nX\nt\u2208da\ni \u2295q\nFiDAtt(FiD(q)). (4)\nwhere ln, hn, tn are the numbers of the layers, the\nheads and the input tokens.\nTo make the training process more robust, we uti-\nlize the FiDAtt scores to annotate the LM-preferred\npositive documents in a discrete way:\nDa+ = Dh+ \u222aTop-KSa\ni ,Da,\n(5)\n\nwhere Dh+ is the human-preferred positive doc-\nument set (i.e., ground truth) on Ts. Top-KSa\ni ,Da\nmeans the documents with the top-k average Fi-\nDAtt scores Sa\ni in the retrieved document set Da.\nThen, we sample hard negatives following\nANCE (Xiong et al., 2021) and formulate the train-\ning loss L of the retriever as:\nD\u2212= ANNM\nf(q,\u25e6)\\Da+,\n(6)\nL =\nX\nq\nX\nd+\u2208Da+\nX\nd\u2212\u2208D\u2212\nl(f(q, d+), f(q, d\u2212)),\n(7)\nwhere M is the hyperparameter of the negative\nsampling depth and l is the standard cross entropy\nloss. After fine-tuning the retriever, we directly use\nit to augment unseen target LM Lt on each task\nfrom target task set Tt.\n4\nExperimental Methodologies\nIn this section, we discuss our main experimental\nsetup. More details can be found in Appendix A.\n4.1\nTarget Tasks\nFollowing prior works (Chung et al., 2022; Mallen\net al., 2022), we choose MMLU (Hendrycks et al.,\n2021) and PopQA (Mallen et al., 2022) as target\ntasks Tt.\nMMLU is a multitask language understanding\ndataset, which includes 57 multi-choice question\nanswering subtasks. These subtasks can be gen-\nerally classified into four categories: humanities,\nsocial sciences, STEM, and other. We average the\naccuracy of the subtasks in each category to ob-\ntain the final score. We report the accuracy of the\nevaluation set in our main experiments.\nPopQA is an entity-centric question answering\ndataset that mainly concentrates on long-tail ques-\ntions. We report the accuracy of the test set in our\nmain experiments.\n4.2\nOur Method\nRetrievers. We adopt two widely used retriev-\ners to initialize AAR: ANCE initialized from\nT5Base (Raffel et al., 2020; Ge et al., 2023) and\nContriever (Izacard et al., 2021) initialized from\nBERTBase (Devlin et al., 2019). Both of them have\nbeen fine-tuned on MS MARCO (Bajaj et al., 2016)\npreviously. For the retrieval corpus, we choose the\nMS MARCO (Bajaj et al., 2016) for MMLU and\nthe KILT-Wikipedia (Petroni et al.) for PopQA.\nLanguage Models. We adopt Flan-T5 (Chung\net al., 2022) series as our backbone for encoder-\ndecoder LMs and InstructGPT1 (Ouyang et al.,\n2022) as our backbone for decoder-only LMs.\nThese models have been multi-task instruction-\nfinetuned and are widely utilized for assessing zero-\nshot generalization (Zhou et al., 2023).\nImplementation Details.\nWe utilize the MS\nMARCO (Bajaj et al., 2016) as our source task\nTs since it is the common choice to train the re-\ntriever (Xin et al., 2022). This dataset consists\nof high-quality questions that require real-world\nknowledge to answer, which aligns strongly with\nour target tasks Tt and possesses no overlap with\nthem. Considering the implementation efficiency,\nwe take the Flan-T5Base as the source LM Ls and\ntreat the larger model as the target LM Lt. We di-\nrectly set the total document number N = 10, LM-\npreferred document number K = 2, and the nega-\ntive mining depth M = 100 in the augmentation-\nadapted training. We run all experiments on a sin-\ngle A100 GPU (40G).\n4.3\nBaselines\nZero-shot Setting. We compare our method with\nthe state-of-the-art zero-shot baselines. Standalone\nLMs, including Flan-T5 (Chung et al., 2022), In-\nstructGPT (Ouyang et al., 2022), GAL (Taylor\net al., 2022) and OPT-IML-Max (Iyer et al., 2022),\nare prompted by a natural language instruction that\ndescribes the desired task and question. Adaptive\nretrieval (Mallen et al., 2022) selectively utilizes\nnon-parametric memory (retrieval augmentation)\nand parametric memory (the knowledge obtained\nfrom pre-training) based on questions\u2019 popularity.\nIn our main experiment, we select the optimal com-\nbination in their paper, which consists of Contriever\nas the non-parametric memory and GenRead (Yu\net al., 2023) as the parametric memory.\nFew-shot Setting. We also include the results of\nprevious few-shot models for reference. Flan-T5,\nInstructGPT, Chinchilla (Hoffmann et al., 2022)\nand OPT-IML-Max adopt few-shot demonstrations,\nwhich provide the LMs with a limited number of\ntask examples. This enables the models to gener-\nalize from these examples and generate accurate\nresponses (Gao et al., 2021). Atlas (Izacard et al.,\n2022) is a state-of-the-art retrieval-augmented LM,\nwhich jointly pre-trains the retriever with the LM\n1We use the GPT-3text-davinci-002 December 2022 version.\n\nSettings\nMethods\n# Parameters\nMMLU\nPopQA\nAll\nHum.\nSoc. Sci.\nSTEM\nOther\nAll\nBase Setting: T5 Base Size\nFew-shot\nFlan-T5Base (Chung et al., 2022)\n250M\n35.8\n39.6\n39.8\n26.3\n41.2\n8.0\nZero-shot\nFlan-T5Base\n250M\n36.1\n40.4\n39.8\n27.0\n40.6\n8.8\nFlan-T5Base w/ AR (Mallen et al., 2022)\n250M\n42.8\n43.5\n44.0\n35.8\n50.0\n29.4\nFlan-T5Base w/ AARContriever (Ours)\n250M\n44.4\n44.7\n47.7\n35.8\n52.2\n31.9\nFlan-T5Base w/ AARANCE (Ours)\n250M\n44.8\n42.2\n46.4\n39.0\n53.2\n37.7\nLarge Setting: T5 Large Size\nFew-shot\nAtlasLarge FT (Izacard et al., 2022)\n770M\n38.9\n37.3\n41.7\n32.3\n44.9\nn.a.\nFlan-T5Large\n780M\n45.1\n47.7\n53.5\n34.4\n49.2\n9.3\nZero-shot\nFlan-T5Large\n780M\n44.8\n46.3\n51.4\n34.8\n50.6\n7.2\nFlan-T5Large w/ AR\n780M\n49.8\n50.0\n55.6\n38.4\n59.5\n29.6\nFlan-T5Large w/ AARContriever (Ours)\n780M\n51.8\n50.8\n59.7\n39.4\n61.8\n33.4\nFlan-T5Large w/ AARANCE (Ours)\n780M\n50.4\n48.0\n58.1\n39.3\n60.2\n39.3\nXL Setting: T5 XL Size\nFew-shot\nAtlasXL FT\n3B\n42.3\n40.0\n46.8\n35.0\n48.1\nn.a.\nFlan-T5XL\n3B\n51.6\n55.0\n61.1\n36.8\n59.5\n11.1\nZero-shot\nFlan-T5XL\n3B\n51.2\n55.5\n57.4\n38.1\n58.7\n11.3\nFlan-T5XL w/ AR\n3B\n55.5\n56.7\n64.5\n43.0\n62.6\n33.7\nFlan-T5XL w/ AARContriever (Ours)\n3B\n56.7\n57.7\n65.4\n43.6\n65.1\n31.5\nFlan-T5XL w/ AARANCE (Ours)\n3B\n56.2\n59.4\n64.8\n41.5\n64.9\n38.0\nGiant Setting: Over 70B Size\nFew-shot\nChinchilla (Hoffmann et al., 2022)\n70B\n67.5\n63.6\n79.3\n55.0\n73.9\nn.a.\nOPT-IML-Max (Iyer et al., 2022)\n175B\n47.1\nn.a.\nn.a.\nn.a.\nn.a.\nn.a.\nInstructGPT (Ouyang et al., 2022)\n175B\n60.5\n62.0\n71.8\n44.3\n70.1\n35.2\nZero-shot\nGAL (Taylor et al., 2022)\n120B\n52.6\nn.a.\nn.a.\nn.a.\nn.a.\nn.a.\nOPT-IML-Max\n175B\n49.1\nn.a.\nn.a.\nn.a.\nn.a.\nn.a.\nInstructGPT\n175B\n60.2\n65.7\n68.0\n46.1\n66.5\n34.7\nInstructGPT w/ AR\n175B\n60.5\n62.2\n71.3\n44.7\n69.7\n43.3\nInstructGPT w/ AARContriever (Ours)\n175B\n61.5\n64.5\n73.1\n45.0\n69.9\n43.9\nInstructGPT w/ AARANCE (Ours)\n175B\n62.2\n62.0\n72.0\n49.2\n70.7\n52.0\nTable 1: Our main results on MMLU and PopQA dataset. We group the methods mainly by the parameters. Our\nLs is Flan-T5Base. AARContriever: AAR initialized from Contriever; AARANCE: AAR initialized from ANCE; FT:\nfine-tuning; AR: adaptive retrieval. Unspecified methods represent direct prompting. The score marked as bold\nmeans the best performance among the models in the zero-shot setting.\n2.5\n5.0\n7.5\nTraining FLOPs 1e21\n30\n35\n40\n45\n50\n55\n60\nMMLU Accuracy\nAARANCE\nLs=Flan-T5Base, Lt=Flan-T5Base\nAARANCE\nLs=Flan-T5Base, Lt=Flan-T5Large\nAARANCE\nLs=Flan-T5Base, Lt=Flan-T5XL\nAARANCE\nLs=Lt=Flan-T5Large\nAARANCE\nLs=Lt=Flan-T5XL\nAtlasLarge\nAtlasXL\nFigure 3: Training FLOPs of retrieval augmentation\nmethods.\nusing unsupervised data and fine-tunes the retriever\nvia the attention distillation on few-shot data.\n5\nEvaluation Results\nIn this section, we discuss our main results on\nMMLU and PopQA datasets (\u00a7 5.1) and conduct\ncomprehensive studies about how (\u00a7 5.2, \u00a7 5.3,\n\u00a7 5.4) and when (\u00a7 5.5, \u00a7 5.6) AAR helps.\n5.1\nOverall Performance\nTable 1 demonstrates that, with the assistance of a\ngeneric AAR, target LMs of different sizes and\narchitectures can significantly outperform their\nstandalone baselines in the zero-shot setting. No-\ntably, AAR even improves powerful InstructGPT\nby 2% on MMLU and by nearly 20% on PopQA.\nWe hypothesize that the PopQA dataset mainly\ncomprises long-tail questions and thus necessitates\nmore augmentation information to attain high accu-\nracy. AAR outperforms other augmentation meth-\nods like few-shot prompting and adaptive retrieval,\nas they may not offer as extensive evidence text as\nAAR does.\nMeanwhile, AAR is a highly efficient augmenta-\ntion approach since it only relies on a small source\n\n250M 780M\n3B\n175B\n# Parameters\n40\n45\n50\n55\n60\n65\n70\n75\nMMLU Accuracy\nANCE\nAARANCE\nContriever\nAARContriever\n(a) Pre-trained Retrievers.\n250M 780M\n3B\n175B\n# Parameters\n40\n45\n50\n55\n60\n65\n70\n75\nMMLU Accuracy\nHuman\nLs=Flan-T5Base LM\nLs=Flan-T5Base\nLs=Flan-T5Large\n(b) Positive docs selection.\nFigure 4: AAR\u2019s performance when (a) using differ-\nent pre-trained retrievers and (b) trained with different\npositive documents, using Flan-T5Base (250M), Flan-\nT5Large (780M), Flan-T5XL (3B), InstructGPT (175B)\nas Lt. The retriever in (b) is initialized from ANCE.\nLM Flan-T5Base (250M) to provide training signals\nand can generalize well to target LMs of larger ca-\npacities. Figure 3 illustrates that solely setting the\nsource LM as the target LM (represented by the in-\nverted triangles) does not significantly enhance the\nMMLU accuracy. However, it may triple the train-\ning budget required. Only using a small source LM\nis able to outperform the powerful Atlas by large\nmargins with fewer training FLOPs.\n5.2\nAblation Study\nIn this experiment, we conduct the ablation study of\naugmentation-adapted training and analyze model\nbehaviors during the training process.\nFigure 4a illustrates that augmentation-adapted\ntraining can bring additional improvements com-\npared to the pre-trained retrievers.\nIn general,\nANCE benefits more from augmentation-adapted\ntraining than Contriever. This may be due to the\nfact that Contriever has been already intensively\npre-trained on massive data augmentations as well\nas MS MARCO whereas ANCE is trained only on\nMS MARCO. We provide exact numbers in Table 7\nand PopQA results in Figure 8, which yield similar\nobservations as MMLU.\nIn Figure 4b, we compare retrievers trained with\ndifferent positive documents, including human-\npreferred documents annotated by search users (the\nblue bar), LM-preferred documents obtained by\nthe source LM (the orange bar), and their combi-\nnations (the green bar and the red bar). Since the\nretriever has been pre-trained on user-annotated\nMS MARCO, simply using human-preferred docu-\nments to train it may be meaningless and therefore\nperforms the worst among all approaches. Only\nusing LM-preferred documents demonstrates no-\ntable gains over only using human-preferred doc-\n0 10K\n30K\n50K\n70K\nTraining step\n1.0\n1.1\n1.2\n1.3\n1.4\nLoss\n28\n30\n32\n34\n36\nMS MARCO MRR@10\nTrain Loss\nEval Loss\nMS MARCO\n(a) Retriever\u2019s performance.\n0 10K\n30K\n50K\n70K\nTraining step\n42\n43\n44\n45\n46\n47\nMMLU Accuracy\nMMLU\n28.0\n28.4\n28.8\n29.2\n29.6\n30.0\nMSMARCO QA Rouge-L\nMSMARCO QA\n(b) Lt\u2019s performance.\nFigure 5: AAR\u2019s training process. (a) exhibits the re-\ntriever\u2019s (ANCE) performance on MS MARCO. (b)\npresents the Lt\u2019s (Flan-T5Base) performance on MS-\nMARCO QA and MMLU.\numents, and merging both human-preferred and\nLM-preferred documents (our main setup) further\nenhances the retriever\u2019s performance. Finally, us-\ning Flan-T5Base as source LM yields better results\ncompared to using Flan-T5Large when the target\nLMs are relatively small. However, as the target\nLM\u2019s size increases, both approaches achieve com-\nparable performance. Hence, our choice to utilize\na small source LM in the augmentation-adapted\ntraining is reasonable and effective.\nFigure 5a and Figure 5b plot the retriever\u2019s and\nLM\u2019s performance during augmentation-adapted\ntraining, respectively. At the beginning of the train-\ning, the retriever\u2019s MRR@10 on the MS MARCO\ndrops dramatically, indicating a large distribution\ngap between human-preferred and LM-preferred\ndocuments. As the retriever\u2019s train and dev loss\ncontinually decline, the retrieval-augmented LM\ngradually performs better on MSMARCO QA and\neventually, on MMLU. This result implies that LMs\non different task may share common preferences,\nmaking AAR generalize well from single source\ntask to heterogeneous target tasks.\n5.3\nAnalysis of LM-preferred Documents\nWe highlight the necessity of adapting existing re-\ntrievers to LMs by comparing the preferred docu-\nments between search users and LMs. In general,\nwe discover that LM-preferred documents can as-\nsist LM from alternative perspectives rather than\nthe full information favored by search users.\nFirst, we define the set overlap O between two\npositive documents set D+\n1 and D+\n2 as:\nO = D+\n1 \u2229D+\n2\nD+\n1 \u222aD+\n2\n.\n(8)\nAs illustrated in Figure 6a, the set overlaps of the\npositive document sets annotated by human users\n\nQuestion\nHuman-preferred Document\nLM-preferred Document\nwhat happens if you miss\nyour cruise ship\nIf you do miss the ship, go into the\ncruise terminal and talk with the port\nagents, who are in contact with both\nshipboard and shoreside personnel.\nThey can help you decide the best way\nto meet your ...\nThe cruise line is not financially respon-\nsible for getting passengers to the next\nport if they miss the ship. Your travel\nto the subsequent port, or home, is on\nyour dime, as are any necessary hotel\nstays and meals...\nwhat is annexation?\nAnnexation is an activity in which two\nthings are joined together, usually with\na subordinate or lesser thing being at-\ntached to a larger thing. In strict legal\nterms, annexation simply involves...\nAnnexation (Latin ad, to, and nexus,\njoining) is the administrative action and\nconcept in international law relating to\nthe forcible transition of one state\u2019s ter-\nritory by another state. It is generally\nheld to be an illegal act...\nTable 2: Cases study on MSMARCO QA dataset. We show Top-1 document annotated by human users and FiDAtt\nscores. Red texts are the gold answer spans.\nBase Large XL Human\nBase\nLarge\nXL\nHuman\n100.0%\n60.6%\n55.2%\n13.2%\n60.6%\n100.0%\n69.2%\n13.3%\n55.2%\n69.2%\n100.0%\n13.1%\n13.2%\n13.3%\n13.1%\n100.0%\n0.2\n0.4\n0.6\n0.8\n1.0\nSet Overlap\n(a) Positive docs overlap.\n0\n10\n20\n30\n40\n50\n60\nMSMARCO QA Rouge-L\nStandalone LM\nHuman\nHuman (Ans-Deletion)\nLM\nLM (Ans-Deletion)\n(b) Answer-deletion test.\nFigure 6: Analysis of LM-preferred documents. (a)\nshows the overlaps of positive document sets, where\nused LMs are Flan-T5 series. (b) presents the answer-\ndeletion experiments on the MSMARCO QA dataset.\nThe retriever is initialized from ANCE.\n(Dh+) and LMs (Top-KSa\ni ,Da) are quite low (near\n13%), demonstrating their distinct tendencies in\nselecting valuable documents. On the contrary, the\noverlaps between different LMs are relatively high\n(over 55%). This evidence provides a strong ratio-\nnale for the generalization ability of AAR since\nLMs with different sizes tend to annotate simi-\nlar positive documents. Furthermore, LMs whose\nsizes are closer generally possess higher overlaps.\nThis implies a better generalization ability of the\nAAR to the LMs whose capacity is near the source\nLM. The findings further validate the results illus-\ntrated in Figure 4b.\nTo give an in-depth analysis of how human-\npreferred and LM-preferred documents differ, we\nshow two representative cases sampled from the\nMSMARCO QA in Table 2. We observe that the\nhuman-preferred document can always present the\ngold answer at the beginning of the text, while the\nLM-preferred document may not contain the ex-\nact answer. However, an LM-preferred document\n250M\n780M\n3B\n175B\n# Parameters\n40\n45\n50\n55\n60\n65\n70\n75\nMMLU Accuracy\nTART\nAARContriever (MSMARCO QA)\nAARContriever (KILT)\nAARANCE (MSMARCO QA)\nAARANCE (KILT)\nFigure 7:\nComparison between single-task (MS-\nMARCO QA) and multi-task (KILT) trained AAR.\nTART (Asai et al., 2022) is a multi-task instruction-\nfinetuned retriever that has not been finetuned with LM-\npreferred signals.\ncan (1) deliver a new perspective to answer the\ngiven question, e.g., \u201cthe cruise line\u2019s responsibil-\nity if you miss your cruise ship\u201d and (2) give a\nspecific explanation instead of an abstract defini-\ntion, e.g., \u201cforcible transition of one state\u2019s territory\nby another state\u201d, These characteristics differ from\nsearch users who want the full information and can\nfurther assist LMs in knowledge-based reasoning.\nWe further examine the unique characteristics\nof LM-preferred documents through the answer-\ndeletion test (i.e., deleting the exact answer span\nfrom the retrieved documents).\nAs shown in\nFigure 6b, the retriever trained by either human-\npreferred (i.e., human-preferred retriever) or LM-\npreferred documents (i.e., LM-preferred retriever)\ncan help LM answer the given question. Never-\ntheless, after the answer-deletion, the performance\nof LM with the human-preferred retriever declines\nmore significantly than with the LM-preferred re-\ntriever. Despite having fewer exact match answers\n(0.6% for LM-preferred documents vs. 13.0% for\n\nCorpora\nMMLU\nPopQA\nAll\nHum.\nSoc. Sci.\nSTEM\nOther\nAll\nMS MARCO\n44.8\n42.2\n46.4\n39.0\n53.2\n13.6\nKILT-Wikipedia\n42.6\n42.5\n45.9\n34.3\n50.5\n37.7\nStandalone LM\n36.1\n40.4\n39.8\n27.0\n40.6\n8.8\nTable 3: Ablation of the retrieval corpus, with Flan-\nT5Base as LM and AARANCE as retriever.\nhuman-preferred documents), LM-preferred docu-\nments provide helpful information from alternative\nperspectives. Therefore, adapting retrievers with\nLM-preferred documents can in turn make retrieval-\naugmented LM perform better.\n5.4\nMulti-task Training of AAR\nIn this section, we explore if the multi-task training\nof AAR can endow the retriever with better gener-\nalization to the target task. Specifically, we choose\nKILT (Petroni et al.) as our multi-task data source,\nwhich consists of 5 categories (Fact Checking, En-\ntity Linking, Slot Filling, Open Domain QA, and\nDialogue). We take one representative subtask per\ncategory to form a mixture of multiple source tasks.\nFigure 7 illustrates that ANCE trained with\nmulti-task KILT can consistently outperform the\nsingle-task MSMARCO QA, proving the bet-\nter generalization ability brought by multi-task\naugmentation-adapted training. It is possible that\nLMs may vary slightly in preferred documents for\ndifferent tasks and AAR can switch more smoothly\nto the target task with the help of multi-task train-\ning. Contriever does not benefit greatly from multi-\ntask training. We conjecture that this is because\nContriever has been pre-trained with multiple for-\nmats of data augmentations and thus generalizes\nbetter to new data distribution than ANCE. Inter-\nestingly, multi-task instruction-finetuned retriever\nTART (Asai et al., 2022) has an overall worse per-\nformance compared to AAR, highlighting the ben-\nefits of having LM-preferred documents during the\nmulti-task training. A more detailed analysis about\nthe selection of source tasks is in Appendix B.\n5.5\nEffect of Retrieval Corpus\nTable 3 demonstrates that regardless of the retrieval\ncorpus, AAR results in consistent and substantial\nperformance gains over the standalone LM.\nOn MMLU, using MS MARCO as the retrieval\ncorpus improves the LM more compared to KILT-\nWikipedia. We hypothesize that the retriever has\nbeen trained with MS MARCO corpus and thus\nholds better retrieval performance on it.\nSettings\nMethods\nMMLU\nPopQA\nAll\nAll\nFew-shot\nOPT (Zhang et al., 2022)\n26.0\n12.3\nGPT-neo (Black et al., 2021)\n28.7\n11.3\nZero-shot\nOPT\n22.7\n12.0\nGPT-neo\n25.3\n9.9\nOPT GenRead\n22.3\n12.2\nGPT-neo GenRead\n24.4\n11.9\nOPT w/ AARContriever (Ours)\n23.2\n29.1\nGPT-neo w/ AARContriever (Ours)\n25.2\n27.8\nOPT w/ AARANCE (Ours)\n23.7\n32.9\nGPT-neo w/ AARANCE (Ours)\n26.6\n30.1\nTable 4: Results of OPT and GPT-neo. We use their\n1.3B version. The score marked as bold means the best\nperformance in the zero-shot setting.\nOn PopQA, model performance will drop by\nlarge margins if we use MS MARCO as the re-\ntrieval corpus instead of KILT-Wikipedia. The pri-\nmary reason is that the PopQA dataset is sampled\nfrom Wikidata and designed for long-tail questions.\nPartial long-tail knowledge can be only found in\nKILT-Wikipedia (Mallen et al., 2022) while MS\nMARCO lacks the indispensable evidence that\nshould be utilized for answer prediction. For in-\nstance, given the question \u201cWho is the mother\nof Melissa Benn?\u201d, there is no document in MS\nMARCO containing the answer \u201cCaroline Benn\u201d.\nUnder such circumstances, aligning the retrieval\ncorpus with the data source can be necessary to\nleverage AAR\u2019s ability.\n5.6\nApplication Scenarios of AAR\nTo examine if AAR works for unseen LMs that\nlack zero-shot generalization ability, we also report\nthe results of OPT (Zhang et al., 2022) and GPT-\nneo (Black et al., 2021). These models may have\npoor zero-shot performance due to the lack of multi-\ntask instruction tuning.\nFrom Table 4, we find that our AAR improves\nboth LMs marginally on MMLU while achieving\nsignificant gains on PopQA. We conjecture that\nLMs can benefit more easily from retrieval augmen-\ntation on the knowledge-probing task like PopQA,\nwhere the answer span can be directly acquired\nfrom the retrieved documents. MMLU requires the\nLM to not only comprehend the retrieved pieces of\nevidence but also perform knowledge-based reason-\ning over them. OPT and GPT-neo may not possess\nsuch abilities in zero-shot scenarios.\nIn summary, although AAR perfectly fits the\nmulti-task instruction-finetuned LMs such as the\nFlan-T5 series and InstructGPT, it may not bring\nsignificant gains for LMs whose zero-shot perfor-\n\nmance is sometimes poor, especially on knowledge-\nbased reasoning. However, we believe that multi-\ntask instruction-finetuned models will be the foun-\ndation of future work due to their outstanding zero-\nshot generalization capabilities, ensuring the wide-\nranging application scenarios of AAR.\n6\nDiscussions\nLM-preferred Documents. Acquiring discrete\nfeedback signals from LMs is challenging as it re-\nquires superior labeling ability, which is not the de-\nsigned purpose of LMs. Inspired by ADist (Izacard\nand Grave, 2021a) and Atlas (Izacard et al., 2022),\nwe utilize the FiDAtt scores to select LM-preferred\ndocuments for the augmentation-adapted training.\nHowever, FiDAtt scores may not reflect the actual\ncontribution of each document faithfully since LM\nmay prefer attending to readable rather than in-\nformative documents. Furthermore, the quality of\nLM-preferred documents depends heavily on the\ninitial performance of the retrieval-augmented LM.\nParallel work (Shi et al., 2023) computes the KL\ndivergence between retrieval likelihood and LM\nlikelihood to train the retriever. Nevertheless, they\nrequire a larger source LM, Curie (6.7B), to pro-\nvide accurate LM likelihood signals. In the future,\nreinforcement learning could serve as an alterna-\ntive method to train the retriever, as it optimizes\nthe retriever by directly leveraging LM\u2019s signals\nwithout relying on the devised rule.\nGeneric Retrieval Plug-in.\nChatgpt-retrieval-\nplugin2 has recently gained attention in the NLP\ncommunity as a generic retrieval plug-in. It re-\ntrieves the most relevant document from users\u2019 data\nsources and tailor ChatGPT\u2019s response to meet their\nspecific needs. We believe that techniques such as\nAAR will enhance the ability of black-box Chat-\nGPT to generate more reasonable responses based\non the retrieved information, thereby promoting the\ndevelopment of human-centered LM design.\n7\nConclusion and Future Work\nThis paper introduces generic retrieval plug-in that\nutilizes a generic retriever to enhance target LMs\nthat may be unknown in advance or are unable\nto be fine-tuned jointly. Our proposed retriever,\nAAR, can directly support black-box LMs without\nrequiring any fine-tuning of the LMs. This is ac-\ncomplished by building the AAR\u2019s training data\n2https://github.com/openai/chatgpt-retrieval-plugin\nwith preferred documents from a small source LM\ntogether with the ground truth.\nEmpirical results on MMLU and PopQA demon-\nstrate that AAR-assisted LMs greatly outperform\nthe standalone ones in zero-shot scenarios, and\nAAR generalizes well to LMs of different sizes\nand structures. Analytical results reveal that LM-\npreferred and human-preferred documents comple-\nment each other; LM-preferred documents from\ndifferent LMs overlap significantly, and LMs with\nsimilar sizes tend to yield closer document sets.\nWe leave a more detailed explanation of how dif-\nferent LMs interact with augmentation documents\nand a more reasonable selection of LM-preferred\ndocuments for future work. We hope our work\nshed light on a path to a generic way of treating\nlarge LMs as black boxes and adapting retrievers\nto augment them.\nLimitations\nDue to the limitation of computational resources,\nwe have not evaluated the Flan-T5XXL whose num-\nber of parameters is 11B, and the OPT whose num-\nber of parameters is greater than 1.3B.\nSince OPT and GPT-neo perform poorly in the\nzero-shot setting and separating attention scores of\neach document in the input is tedious for decoder-\nonly models, we choose not to use them as source\nLMs. However, we prove that taking the encoder-\ndecoder model Flan-T5Base as our source LM is\nalso robust to augment decoder-only models. We\nwill explore new methods to annotate LM-preferred\ndocuments of decoder-only models based on their\ninherent signals.\nAcknowledgement\nZichun Yu, Shi Yu, and Zhiyuan Liu are supported\nby Institute Guo Qiang at Tsinghua University, Bei-\njing Academy of Artificial Intelligence (BAAI).\nAll authors proposed the original idea together.\nZichun Yu conducted the experiments. Zichun Yu,\nChenyan Xiong, Shi Yu, and Zhiyuan Liu wrote\nthe paper. Chenyan Xiong and Zhiyuan Liu pro-\nvided valuable suggestions for the research. We\nthank Suyu Ge for sharing the ANCE checkpoint\ninitialized from T5Base.\n\nReferences\nFabien Andr\u00e9, Anne-Marie Kermarrec, and Nicolas\nLe Scouarnec. 2016. Cache locality is not enough:\nHigh-performance nearest neighbor search with prod-\nuct quantization fast scan. In VLDB, page 12.\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen,\nGautier Izacard, Sebastian Riedel, Hannaneh Ha-\njishirzi, and Wen-tau Yih. 2022. Task-aware retrieval\nwith instructions. arXiv preprint arXiv:2211.09260.\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder, An-\ndrew McNamara, Bhaskar Mitra, Tri Nguyen, et al.\n2016. Ms marco: A human generated machine read-\ning comprehension dataset. In CoCo@NeurIPS.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of ACM FAccT, pages\n610\u2013623.\nSid Black, Gao Leo, Phil Wang, Connor Leahy, and\nStella Biderman. 2021. Gpt-neo: Large scale autore-\ngressive language modeling with mesh-tensorflow.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, Diego\nDe Las Casas, Aurelia Guy, Jacob Menick, Roman\nRing, Tom Hennigan, Saffron Huang, Loren Mag-\ngiore, Chris Jones, Albin Cassirer, Andy Brock,\nMichela Paganini, Geoffrey Irving, Oriol Vinyals,\nSimon Osindero, Karen Simonyan, Jack Rae, Erich\nElsen, and Laurent Sifre. 2022. Improving language\nmodels by retrieving from trillions of tokens. In\nICML, pages 2206\u20132240.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In NeurIPS, pages 1877\u20131901.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, Parker Schuh, and et al. 2022. Palm:\nScaling language modeling with pathways. arXiv\npreprint arXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Zhao,\nYanping Huang, Andrew Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V. Le, and Jason Wei.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL, pages 4171\u2013\n4186.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of ACL, pages 3816\u20133830.\nSuyu Ge, Chenyan Xiong, Corby Rosset, Arnold Over-\nwijk, Jiawei Han, and Paul Bennett. 2023. Augment-\ning zero-shot dense retrievers with plug-in mixture-\nof-memories. arXiv preprint arXiv:2302.03754.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training. In ICML,\npages 3929\u20133938.\nHangfeng He, Hongming Zhang, and Dan Roth. 2022.\nRethinking with retrieval: Faithful large language\nmodel inference. arXiv preprint arXiv:2301.00303.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2021. Measuring massive multitask language under-\nstanding. In ICLR.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks, Jo-\nhannes Welbl, Aidan Clark, Thomas Hennigan, Eric\nNoland, Katherine Millican, George van den Driess-\nche, Bogdan Damoc, Aurelia Guy, Simon Osindero,\nKar\u00e9n Simonyan, Erich Elsen, Oriol Vinyals, Jack\nRae, and Laurent Sifre. 2022. An empirical analysis\nof compute-optimal large language model training.\nIn NeurIPS, pages 30016\u201330030.\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,\nTodor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster,\nTianlu Wang, Qing Liu, Punit Singh Koura, Xian Li,\nBrian O\u2019Horo, Gabriel Pereyra, Jeff Wang, Christo-\npher Dewan, Asli Celikyilmaz, Luke Zettlemoyer,\nand Ves Stoyanov. 2022. Opt-iml: Scaling language\nmodel instruction meta learning through the lens of\ngeneralization. arXiv preprint arXiv:2212.12017.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\nbastian Riedel, Piotr Bojanowski, Armand Joulin,\nand Edouard Grave. 2021. Unsupervised dense infor-\nmation retrieval with contrastive learning. TMLR.\nGautier Izacard and Edouard Grave. 2021a. Distilling\nknowledge from reader to retriever for question an-\nswering. In ICLR.\nGautier Izacard and Edouard Grave. 2021b. Leveraging\npassage retrieval with generative models for open\ndomain question answering. In Proceedings of EACL,\npages 874\u2013880.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\n\nEdouard Grave. 2022. Few-shot Learning with Re-\ntrieval Augmented Language Models. arXiv preprint\narXiv:2208.03299.\nJeff Johnson, Matthijs Douze, and Herve Jegou. 2021.\nBillion-scale similarity search with gpus. IEEE TBD,\n7(3):535\u2013547.\nMingxuan Ju, Wenhao Yu, Tong Zhao, Chuxu Zhang,\nand Yanfang Ye. 2022. Grape: Knowledge graph\nenhanced passage reader for open-domain question\nanswering. In Findings of EMNLP.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models.\narXiv\npreprint arXiv:2001.08361.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020.\nDense passage retrieval for\nopen-domain question answering. In Proceedings\nof EMNLP, pages 6769\u20136781.\nOmar Khattab,\nKeshav Santhanam,\nXiang Lisa\nLi, David Hall, Percy Liang, Christopher Potts,\nand Matei Zaharia. 2022.\nDemonstrate-search-\npredict: Composing retrieval and language mod-\nels for knowledge-intensive nlp.\narXiv preprint\narXiv:2212.14024.\nPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rock-\nt\u00e4schel, Sebastian Riedel, and Douwe Kiela. 2020.\nRetrieval-augmented generation for knowledge-\nintensive NLP tasks. In NeurIPS, pages 9459\u20139474.\nBill Yuchen Lin, Kangmin Tan, Chris Miller, Beiwen\nTian, and Xiang Ren. 2022. Unsupervised cross-\ntask generalization via retrieval augmentation. In\nNeurIPS, pages 22003\u201322017.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi\nDas, Hannaneh Hajishirzi, and Daniel Khashabi.\n2022. When not to trust language models: Inves-\ntigating effectiveness and limitations of paramet-\nric and non-parametric memories. arXiv preprint\narXiv:2212.10511.\nAntonis Maronikolakis and Hinrich Sch\u00fctze. 2021. Mul-\ntidomain pretrained language models for green NLP.\nIn Proceedings of AdaptNLP, pages 1\u20138.\nOpenAI. 2023. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Gray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In NeurIPS, pages 27730\u201327744.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rockt\u00e4schel, and Sebastian\nRiedel. KILT: a benchmark for knowledge intensive\nlanguage tasks. In Proceedings of NAACL, pages\n2523\u20132544.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. JMLR, 21:140:1\u2013140:67.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the parame-\nters of a language model? In Proceedings of EMNLP,\npages 5418\u20135426.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2022. Learning to retrieve prompts for in-context\nlearning. In Proceedings of NAACL, pages 2655\u2013\n2671.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, and et al. 2022.\nMultitask prompted training enables zero-shot task\ngeneralization. In ICLR.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\njoon Seo, Rich James, Mike Lewis, Luke Zettle-\nmoyer, and Wen tau Yih. 2023. Replug: Retrieval-\naugmented black-box language models.\narXiv\npreprint arXiv:2301.12652.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of ACL, pages\n3645\u20133650.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas\nScialom, Anthony Hartshorn, Elvis Saravia, Andrew\nPoulton, Viktor Kerkez, and Robert Stojnic. 2022.\nGalactica: A large language model for science. arXiv\npreprint arXiv:2211.09085.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,\nand Denny Zhou. 2022. Chain-of-thought prompt-\ning elicits reasoning in large language models. In\nNeurIPS, pages 24824\u201324837.\nChaojun Xiao, Zhengyan Zhang, Xu Han, Chi-Min\nChan, Yankai Lin, Zhiyuan Liu, Xiangyang Li,\nZhonghua Li, Zhao Cao, and Maosong Sun. 2023.\nPlug-and-play document modules for pre-trained\nmodels. In Proceedings of ACL.\nJi Xin, Chenyan Xiong, Ashwin Srinivasan, Ankita\nSharma, Damien Jose, and Paul Bennett. 2022. Zero-\nshot dense retrieval with momentum adversarial do-\nmain invariant representations. In Findings of ACL,\npages 4008\u20134020.\n\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul N. Bennett, Junaid Ahmed, and\nArnold Overwijk. 2021. Approximate nearest neigh-\nbor negative contrastive learning for dense text re-\ntrieval. In ICLR.\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\nMingxuan Ju, Soumya Sanyal, Chenguang Zhu,\nMichael Zeng, and Meng Jiang. 2023.\nGenerate\nrather than retrieve: Large language models are\nstrong context generators. In ICLR.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open pre-\ntrained transformer language models. arXiv preprint\narXiv:2205.01068.\nZhengyan Zhang, Zhiyuan Zeng, Yankai Lin, Huadong\nWang, Deming Ye, Chaojun Xiao, Xu Han, Zhiyuan\nLiu, Peng Li, Maosong Sun, and Jie Zhou. 2023.\nPlug-and-play knowledge injection for pre-trained\nlanguage models. In Proceedings of ACL.\nCe Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu,\nGuangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan,\nLifang He, Hao Peng, Jianxin Li, Jia Wu, Ziwei Liu,\nPengtao Xie, Caiming Xiong, Jian Pei, Philip S. Yu,\nand Lichao Sun. 2023. A comprehensive survey on\npretrained foundation models: A history from bert to\nchatgpt. arXiv preprint arXiv:2302.09419.\n\nA\nExperimental Settings\nA.1\nTraining Hyperparameters\nWe take the ANCE initialized from T5Base3 (Xiong\net al., 2021; Ge et al., 2023) and Contriever4 (Izac-\nard et al., 2021)\u2019s hyperparameters in the\naugmentation-adapted training. Specifically, we fix\nbatch size as 8, learning rate as 5e-6, and epochs as\n6 for ANCE while taking batch size as 8, learning\nrate as 1e-5, and epochs as 3 for Contriever. We\nchoose their best checkpoints based on the perfor-\nmance of the development set. The information\nabout our source tasks and target tasks are listed in\nTable 6.\nA.2\nNumber of Augmentation Documents\nLMs of different sizes, facing various target tasks,\nmay require indefinite numbers of augmentation\ndocuments to achieve their best performance.\nFor MMLU, we analyze how the number of aug-\nmentation documents affects LMs\u2019 performance.\nAs illustrated in Figure 9, we discover that LMs of\nlarger capacity generally benefit more from more\naugmentation documents. A possible explanation\nis that larger LMs are more capable of integrating\ninformation from multiple documents and perform-\ning complicated reasoning based on them.\nFor PopQA, using 3 augmentation documents\nachieves the best performance across all LMs.\nA.3\nPrompt Templates\nThe prompt template for MMLU is:\nHere\u2019s a problem to solve: {question}\nAmong the 4 following options, which is\nthe correct answer?\n- A: {choice_A}\n- B: {choice_B}\n- C: {choice_C}\n- D: {choice_D}\nThe prompt template for PopQA is:\nQ: {question} A:\nB\nSelection of Source Task\nWe provide a detailed selection of the source tasks\nhere, using a variety of source and target tasks to an-\nalyze. MSMARCO QA, KILT-TriviaQA, and NQ\nbelong to Open Domain QA, while KILT-T-REx\nand zsRE belong to Slot Filling. MMLU belongs\nto Multi-task Language Understanding, which is\n3https://huggingface.co/OpenMatch/t5-ance\n4https://huggingface.co/facebook/contriever-msmarco\nTs\nTt\nMMLU\nNQ\nzsRE\nMSMARCO QA\n44.8\n46.7\n75.1\nKILT-TriviaQA\n43.6\n46.4\n74.9\nKILT-T-REx\n44.1\n45.9\n77.2\nTable 5: Relationship between the selection of source\ntask Ts and the performance of target task Tt. The\nmodel is Flan-T5Base w/ AARANCE. As NQ and zsRE\nare included in the Flan-T5 training data, we only report\ntheir F1 results here for reference.\ncloser to the Open Domain QA in terms of the task\nobjective. As shown in Table 5, when we align the\ncategory of the source task with the target task, the\nLM w/ AAR can generally achieve the best results.\nWe suppose that this is because LM may share sim-\nilar document preferences on the tasks from the\nsame dataset category, making AAR easier to gen-\neralize. Furthermore, taking MSMARCO QA as\nthe source task performs the best on MMLU. This\nvalidates the rationality to set Ts as MSMARCO\nQA in our main experimental settings.\nC\nAAR\u2019s Improvements on PopQA\n250M\n780M\n3B\n175B\n# Parameters\n25\n30\n35\n40\n45\n50\n55\nPopQA Accuracy\nANCE\nAARANCE\nContriever\nAARContriever\nFigure 8: AAR\u2019s improvements on PopQA, using Flan-\nT5Base (250M), Flan-T5Large (780M), Flan-T5XL (3B),\nInstructGPT (175B) as target LMs.\nD\nFine-tuning Results\nWe also report the fine-tuning results of Flan-\nT5Base and Flan-T5Large on MMLU auxiliary train-\ning data (Hendrycks et al., 2021) in Table 7. Due to\nthe limitation of the computational resources, we\ndo not include the fine-tuning result of Flan-T5XL.\nWe take batch size as 32, learning rate as 5e-5, and\nepochs as 3 in fine-tuning. In general, the LM that\nhas already been massively multi-task instruction-\nfinetuned, such as Flan-T5, improves little from\nfine-tuning on extra tasks but benefits greatly from\nour AAR. The results further validate the power of\nzero-shot retrieval augmentation.\n\n1 2 3 4 5 6 7 8 9 10\nNumber of documents\n36\n38\n40\n42\n44\nMMLU Accuracy\nStandalone LM\nLt=Flan-T5Base\n(a) Flan-T5Base w/ AARANCE.\n1 2 3 4 5 6 7 8 9 10\nNumber of documents\n44\n46\n48\n50\n52\nMMLU Accuracy\nStandalone LM\nLt=Flan-T5Large\n(b) Flan-T5Large w/ AARANCE.\n1 2 3 4 5 6 7 8 9 10\nNumber of documents\n50\n52\n54\n56\n58\nMMLU Accuracy\nStandalone LM\nLt=Flan-T5XL\n(c) Flan-T5XL w/ AARANCE.\nFigure 9: Relationship between LM\u2019s performance and the number of augmentation documents.\nCategory\nNumber\nTs\nMSMARCO QA\nOpen Domain QA\n148122\nKILT-FEVER\nFact Checking\n10444\nKILT-WNED\nEntity Linking\n3396\nKILT-T-REx\nSlot Filling\n5000\nKILT-TriviaQA\nOpen Domain QA\n5359\nKILT-Wizard of Wikipedia\nDialogue\n3054\nTt\nMMLU\nMulti-task Language Understanding\n1531\nPopQA\nOpen Domain QA\n14267\nTable 6: Configurations of our source tasks and target tasks.\nMethods\nMMLU\nAll\nHum.\nSoc. Sci.\nSTEM\nOther\nFlan-T5Base\n36.1\n40.4\n39.8\n27.0\n40.6\nFlan-T5Base Fine-tuning\n36.1\n38.9\n41.2\n27.9\n39.9\nFlan-T5Base w/ Contriever\n43.7\n44.4\n45.0\n36.4\n51.1\nFlan-T5Base w/ ANCE\n43.0\n44.2\n44.3\n34.5\n51.9\nFlan-T5Base w/ AARContriever (Ours)\n44.4\n44.7\n47.7\n35.8\n52.2\nFlan-T5Base w/ AARANCE (Ours)\n44.8\n42.2\n46.4\n39.0\n53.2\nFlan-T5Large\n45.1\n47.7\n53.5\n34.4\n49.2\nFlan-T5Large Fine-tuning\n45.3\n47.6\n54.1\n35.2\n48.7\nFlan-T5Large w/ Contriever\n50.7\n50.5\n56.4\n38.9\n61.1\nFlan-T5Large w/ ANCE\n49.2\n49.3\n56.7\n38.1\n57.2\nFlan-T5Large w/ AARContriever (Ours)\n51.8\n50.8\n59.7\n39.4\n61.8\nFlan-T5Large w/ AARANCE (Ours)\n50.4\n48.0\n58.1\n39.3\n60.2\nFlan-T5XL\n51.2\n55.5\n57.4\n38.1\n58.7\nFlan-T5XL w/ Contriever\n56.4\n57.3\n66.1\n43.9\n63.2\nFlan-T5XL w/ ANCE\n55.3\n55.9\n64.0\n41.5\n64.9\nFlan-T5XL w/ AARContriever (Ours)\n56.7\n57.7\n65.4\n43.6\n65.1\nFlan-T5XL w/ AARANCE (Ours)\n56.2\n59.4\n64.8\n41.5\n64.9\nInstructGPT\n60.2\n65.7\n68.0\n46.1\n66.5\nInstructGPT w/ Contriever\n60.5\n62.0\n71.8\n44.3\n70.1\nInstructGPT w/ ANCE\n61.6\n62.4\n73.4\n47.6\n68.6\nInstructGPT w/ AARContriever (Ours)\n61.5\n64.5\n73.1\n45.0\n69.9\nInstructGPT w/ AARANCE (Ours)\n62.2\n62.0\n72.0\n49.2\n70.7\nTable 7: Fine-tuning results on MMLU. We use the official auxiliary training data of MMLU to fine-tune the LM.\n"}