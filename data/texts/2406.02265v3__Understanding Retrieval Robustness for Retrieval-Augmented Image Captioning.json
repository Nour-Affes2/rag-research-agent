{"metadata": {"pdf_filename": "2406.02265v3__Understanding Retrieval Robustness for Retrieval-Augmented Image Captioning.pdf", "source": "arXiv"}, "text": "Understanding Retrieval Robustness for\nRetrieval-Augmented Image Captioning\nWenyan Li1, Jiaang Li1, Rita Ramos2, Raphael Tang3, Desmond Elliott1\n1Department of Computer Science, University of Copenhagen\n2INESC-ID, Instituto Superior Tecnico, University of Lisbon 3Comcast Applied AI\n{weli, jili, de}@di.ku.dk\nritaparadaramos@tecnico.ulisboa.pt\nraphael_tang@comcast.com\nAbstract\nRecent advances in retrieval-augmented mod-\nels for image captioning highlight the bene-\nfit of retrieving related captions for efficient,\nlightweight models with strong domain-transfer\ncapabilities. While these models demonstrate\nthe success of retrieval augmentation, retrieval\nmodels are still far from perfect in practice: the\nretrieved information can sometimes mislead\nthe model, resulting in incorrect generation and\nworse performance. In this paper, we analyze\nthe robustness of a retrieval-augmented caption-\ning model SMALLCAP. Our analysis shows\nthat the model is sensitive to tokens that appear\nin the majority of the retrieved captions, and\nthe input attribution shows that those tokens are\nlikely copied into the generated output. Given\nthese findings, we propose to train the model by\nsampling retrieved captions from more diverse\nsets. This decreases the chance that the model\nlearns to copy majority tokens, and improves\nboth in-domain and cross-domain performance.\n1\nIntroduction\nRecent retrieval-augmented image captioning mod-\nels have shown success in strong image caption-\ning performance while reducing model parame-\nters by retrieving related captions for a given im-\nage (Ramos et al., 2023b; Sarto et al., 2022; Yang\net al., 2023). These models use retrieved informa-\ntion as additional context besides the input image.\nHowever, similar to retrieval-augmented language\nmodels (Yoran et al., 2023), image captioning mod-\nels enhanced with retrieval can sometimes be mis-\nled by irrelevant information. For example, in Fig-\nure 1 the captioning model is misled by the token\n\u201celephant\u201d in the retrieved captions, and generates\ncaptions that do not match the given image.\nFor retrieval-augmented language models, Yoran\net al. (2023) have studied the cases where retrieval\nmisled the model prediction, and address this prob-\nlem with a retrieval-robust LLM by continuous\nan elephant is parked in \nfront of a building\nwith retrieval\nRetrieved captions\nwithout retrieval\nwith robust retrieval\na truck parked in front of a \nrestaurant\na white truck parked in front \nof a building\no a fake elephant is being driven on a \ntruck by a palace\no road sign for elephant & castle \nWalworth pointing to the right\no a sign in english and Chinese points \nout castle road\no a sign pointing to elephant &castle \nand walwort\nFigure 1: Comparison of generated image captions\nthat are predicted without retrieval, misled by retrieval,\nand predicted with a more retrieval-robust model. The\nretrieval-augmented model generates the token \u201cele-\nphant\u201d, which appears in 3/4 of the retrieved captions.\ntraining with synthetic data for question answer-\ning tasks. However, in their approach, the retrieval\nsystem returns only one passage at each step. Con-\nsidering that LLMs can be sensitive to the order of\nprompts (Lu et al., 2022), the robustness of using\nmultiple retrieved results has not been fully stud-\nied. Evaluating and improving the robustness of\nretrieval-augmented image captioning models re-\nmains under-explored, specifically when the model\nis augmented with multiple retrieved results.\nTo bridge this gap in the literature, we study the\nrobustness of the SMALLCAP retrieval-augmented\ncaptioning model (Ramos et al., 2023b). By the\ndefinition of retrieval robustness proposed in Yoran\net al. (2023), retrieved context should boost model\nperformance when relevant, and should not ad-\nversely affect it when irrelevant. We thoroughly\nexamine the robustness of the model with regards\nto the order of the retrieved captions, and the rel-\nevance of the retrieved content. We also present\na novel analysis of model behaviour based on ma-\njority voting, supported by input attribution and\narXiv:2406.02265v3  [cs.CV]  6 Aug 2024\n\nattention analyses to investigate how the retrieved\ntokens influence the model generation. And finally,\ninspired by Hoang et al. (2022), we propose to sam-\nple the retrieved captions from a larger list during\ntraining to prevent the model from overfitting to\nthe top relevant captions. Our evaluation shows im-\nproved model robustness and better out-of-domain\ngeneralization.\nThe main findings of this paper are: 1) We study\nthe robustness of an existing retrieval-augmented\ncaptioning model SMALLCAP and find it is not\nrobust to processing randomly retrieved content.\n2) We identify that tokens that frequently occur in\nthe retrieved captions, i.e. majority tokens, have\nhigh attribution scores with regard to the tokens\ngenerated by the model. This phenomenon sug-\ngests heightened sensitivity and copying. 3) Train-\ning with sampled retrieved captions from a larger\nlist instead of with fixed top-k relevant captions\nimproves model robustness, yielding better gener-\nalization and out-of-domain performance.1\n2\nRelated Work\nRobustness of retrieval-augmented models.\nRetrieval-augmented generation (RAG) involves\nenhancing the generation process by incorporat-\ning retrieved information from an external datas-\ntore as additional context to the input (Lewis et al.,\n2020). RAG models have shown to improve perfor-\nmance across a variety of NLP tasks (Mialon et al.,\n2023). However, RAG models can overly rely on\nretrieved information, resulting in inaccurate gen-\neration when the retrieved context is flawed (Yan\net al., 2024; Yoran et al., 2023).\nRecent efforts aim to enhance RAG model ro-\nbustness against misguided or hallucinated gener-\nations. One approach involves filtering retrieved\ncontent (Wang et al., 2023; Yoran et al., 2023; Ya-\nsunaga et al., 2023; Yan et al., 2024; Asai et al.,\n2023) by applying or training an additional eval-\nuator. Another direction focuses on improving\nrobustness during the training of the generation\nmodel itself. Specifically, for retrieval-augmented\nquestion answering with large language models,\nYoran et al. (2023) propose continued training\nwith a synthetic dataset that contains both rele-\nvant and irrelevant context, while Cuconasu et al.\n(2024) suggests incorporating irrelevant documents.\nIn retrieval-augmented translation, robustness is\n1We release the code at https://github.com/\nlyan62/RobustCap\nimproved through shuffling retrieved translations\n(Hoang et al., 2022), ensemble model decoding\n(Hao et al., 2023), and controlled interactions be-\ntween source and retrieved translations (Hoang\net al., 2023).\nRetrieval-augmented image captioning.\nImage\ncaptioning is the task that describes the visual\ncontents of an image in natural language (Xu\net al., 2015; Osman et al., 2023). Recent stud-\nies have integrated RAG into this field. Sarto et al.\n(2022) and Zhou and Long (2023) experimented\nwith retrieving similar or style-aware images be-\nfore generating captions. Li et al. (2023) intro-\nduced a lightweight image captioning model that\nutilizes retrieved concepts. More related to our\nwork, Ramos et al. (2023a) developed end-to-end\nencoder-decoder models that attend to both the im-\nage and retrieved caption embeddings.\nIn particular, the SMALLCAP model (Ramos\net al., 2023b), presenting retrieval augmentation\nin image captioning could reduce trainable param-\neters and adapt to out-of-domain settings. The\nmodel utilizes frozen unimodal models, incorporat-\ning a pre-trained encoder and decoder connected\nby trainable cross-attention layer.\nHowever, it still remains unclear how retrieved\ncaptions influence the generation of captions in\nretrieval-augmented image captioning, especially\nconcerning visual inputs. Additionally, the evalu-\nation and enhancement of the robustness of these\nmodels are still under-explored.\n3\nRobustness of Retrieval-Augmented\nImage Captioning\nTo evaluate the robustness of the SMALLCAP\nretrieval-augmented caption model (Ramos et al.,\n2023b), we conduct controlled experiments and ob-\nserve its resilience to changes in (1) the order of\nthe retrieved captions and (2) the content relevance\nof the retrieved captions.\n3.1\nRobustness Evaluation\nFor a given image, SMALLCAP is augmented with\na sequence of k retrieved captions that are com-\nbined into an input for the language model decoder:\n\u201cSimilar images show cap1, cap2, ..., capk. This im-\nage shows ...\u201d. The retrieved captions are obtained\nthrough image-to-text retrieval using CLIP embed-\ndings (Radford et al., 2021), and are sorted accord-\ning to their relevance, i.e., cosine similarity. From\nthe sorted retrieved captions, we retain the most\n\n1\n2\n3\n4\nk\n50\n60\n70\n80\n90\n100\n110\n120\nCIDEr\ntop-k\nlow-rank\nrandom\nno rag\nFigure 2: CIDEr evaluation of SMALLCAP on the\nCOCO validation set using the top-k, low(er)-ranked,\nrandomly retrieved captions, against a baseline without\nretrieval augmentation. Performance drops by up to\n50% when using randomly retrieved captions compared\nto baseline, suggesting that the model is not robust.\nsimilar captions as the retrieval list. In this regard,\nthe top-k retrieved candidates are the first k cap-\ntions in the list, and the low-ranked captions are\nthe last-k captions in the list. SMALLCAP uses the\ntop-k retrieved captions in the prompt by default.\nContext order.\nWhen prompting the model to\ngenerate a caption for a given image, we can change\nthe order of the retrieved captions by permuting\nor reversing them. We evaluate the effect of the\norder changes in two settings: one with a model\ntrained using the top-k retrieved captions (default),\nand another that is also trained with permuted or\nreversed retrieved captions.2\nContent relevance.\nTo evaluate how robust the\nmodel is towards noise in the retrieved captions,\nwe are curious to see how the model performs\nwhen (1) captions are randomly retrieved, i.e. likely\nto be irrelevant for the given image (2) only low-\nranked retrieved captions are available. Here the\nrandomly retrieved captions are those retrieved\nwith another image. For low-ranked captions, we\ntake the lowest-ranked k captions from the retrieval\nlist that consists of top seven relevant captions.\n3.2\nExperimental Setup\nIn the experiments, we set k = 4 as it has been\ndemonstrated as the optimal number of captions\n2For\nthe\nmodel\ntrained\nwith\ndefault\norder\u2014top\nfour captions, we use the pretrained checkpoints from\nHuggingFace:\nhttps://huggingface.co/Yova/\nSmallCap7M,\nhttps://huggingface.co/Yova/\nSmallCapOPT7M\nRetrieval Order\nLM Backbone\nTrain\nEval\nGPT-2\nOPT\ndefault\ndefault\n116.4\n120.3\npermute\n116.2\n120.1\nreverse\n115.8\n119.7\npermute\npermute\n117.2\n120.4\nreverse\nreverse\n116.4\n120.7\nTable 1: CIDEr evaluation on the COCO validation\nset with GPT-2 and OPT variants of SMALLCAP when\nmanipulating the order of the top-k retrieved captions.\nby Ramos et al. (2023b). We evaluate SMALLCAP\nmodels with both OPT-350M (Zhang et al., 2022)\nand GPT-2 (Radford et al., 2019) as the decoder\nmodels. For the image encoder, we use ResNet-\n50x64 (He et al., 2016) and CLIP-ViT-B/32 (Rad-\nford et al., 2021) as the retrieval encoder. We keep\nthe same model setting in the following sections\nunless stated otherwise.\nData and metrics\nWe first evaluate the robust-\nness of SMALLCAP on COCO validation set for\nin-domain evaluation. Then we evaluate on No-\nCaps (Agrawal et al., 2019), which contains In,\nNear and Out-of-domain data, and serves as a chal-\nlenging dataset designed to assess the generaliza-\ntion capabilities of models trained on COCO. For\nboth datasets we use the validation set experiment-\ning with different number of retrieved captions, i.e.\ndifferent k values. We report peformance using\nCIDEr score (Vedantam et al., 2015).\n3.3\nOrder Robust but Content Sensitive\nOrder robust.\nFrom the results in Table 1 and\nTable 2, we observe that SMALLCAP is indeed ro-\nbust to the order of the retrieved texts. Permuting\nthe order of the captions during training and evalu-\nation show 1 CIDEr point improvement for COCO\n(Lin et al., 2014) and 2 \u22123 CIDEr score increase\nfor NoCaps (Agrawal et al., 2019). This indicates\nthat if multiple captions are used for augmentation,\nthen permuting their order helps.\nContent sensitive.\nFigure 2 shows that when us-\ning randomly retrieved captions instead of the top-k\nmost relevant captions, performance drops drasti-\ncally compared to the no-retrieval baseline.3 This\n3Here the top and low ranked captions are obtained from a\nlist of top-seven captions retrieved captions ordered by their\ncosine similarity to the image embedding.\n\nRetrieval Order\nLM Backbone\nGPT-2\nOPT\nTrain\nEval\nIn\nNear\nOut\nIn\nNear\nOut\ndefault\ndefault\n80.1\n79.4\n69.6\n91.0\n84.4\n76.3\npermute\n81.6\n79.8\n68.5\n92.5\n84.5\n75.8\nreverse\n80.2\n79.3\n68.4\n92.0\n84.4\n76.6\npermute\npermute\n81.5\n79.7\n69.8\n94.2\n84.0\n79.4\nreverse\nreverse\n80.4\n80.1\n68.4\n92.5\n85.6\n75.9\nTable 2: Evaluation on NoCaps using CIDEr score with\nthe GPT-2 and OPT variants of SMALLCAP when ma-\nnipulating the order of the top-k retrieved captions.\nimplies that SMALLCAP lacks resilience to noise\nin the retrieved captions, and the irrelevant context\nhas the potential to mislead the model, resulting\nin inaccurate predictions. When prompting with\nlow-ranked retrieved captions, while performance\nslightly decreases, the retrieval-augmented model\nstill outperforms the one without retrieval.\n4\nMajority Tokens Explain Behavior\nTo better understand how each token of the re-\ntrieved content relates to the observed sensitivity\ndiscussed in the previous section, we hypothesize\nthat the model is driven by the presence of ma-\njority tokens. In other words, when the model is\nprompted with retrieved captions, we assume that\nthe predicted tokens are influenced by the tokens\nthat appear in the majority of the retrieved captions.\nTo test this assumption, we propose a majority vot-\ning analysis, followed by input attribution, and an\nattention analysis of the model behavior.\n4.1\nMajority Tokens\nWe first introduce the definition of majority tokens.\nLet R = [T1, \u00b7 \u00b7 \u00b7 , Tn] represent a retrieved caption\nR, which contains a sequence of n tokens. For a\ngiven image, we assume that a total of K retrieved\ncaptions are used in the model prompt: R1, R2,\n..., RK. For each token Ti in the set of unique\ntokens from the retrieved captions, we define Ti\nas a majority token (denoted as TM) if Ti appears\nin more than half of the retrieved captions4, i.e.,\nCTi > K\n2 where CTi is the number of retrieved\ncaptions that contains token Ti as in Equation 1:\n4Note that we remove the stop words in the retrieved\ncaptions when determining the majority tokens. The stop\nwords are filtered from the top-100 most frequent tokens in\nthe COCO dataset, where we manually remove meaningful\ntokens such as \u201cman\u201d, \u201ctwo\u201d from the list. Please see the\nAppendix A for the complete list.\nCTi =\nK\nX\nl=1\n1[Ti \u2208Rk]\n(1)\nFor a generated caption Y = [y1, \u00b7 \u00b7 \u00b7 , yn] in the\nevaluation data, we can calculate the majority-vote\nprobability PTM\u2208Y as the probability of the major-\nity token TM appearing in the generated caption.\nWe expect that the higher the value of PTM\u2208Y ,\nthe more likely it is that the model is generating\ncaptions based on the majority tokens.\n4.2\nExperimental Setup\nWe test our majority vote assumption with a con-\ntrolled experiment. Specifically, we analyze the\npredictions of the model in two settings, each pro-\nvided with K = 3 retrieved captions to ensure the\npresence of a majority token:\n2 Good 1 Bad (2G1B): The retrieval set contains\ntwo relevant captions and one irrelevant caption;\n2 Bad 1 Good (2B1G): The retrieval set contains\ntwo irrelevant captions and one relevant caption.\nThe assumption is that, if there is a majority vot-\ning behavior with respect to the retrieved captions,\nthe model will copy such majority tokens to the\nfinal output. The distinction will be clear in this\nsetting \u2014 in the setup 2B1G, if the model is robust\nto the retrieved context, the model will focus more\non the good caption instead of the majority tokens\nin the two bad captions.\nWe use the COCO evaluation set and the pre-\ntrained checkpoint with the OPT decoder of Ramos\net al. (2023b) for this analysis. Good captions are\nobtained using the top-two and top-one retrieved\ncaptions, respectively, for a given image. Bad cap-\ntions are obtained by retrieving one or two captions,\nrespectively, from a randomly selected image.\nResults.\nWe find that the probability of majority\nvote in the 2G1B setting is 86.47%. This high\nprobability suggests that the majority tokens in the\ngood captions could be being used to guide the\nmodel generation. In the 2B1G setting, the model\nis much less likely to generate majority tokens from\nthe bad captions, indicating some robustness in\nnot always following them. However, 20.84% of\nthe time, the model can still be misled by their\nappearance, resulting in the majority tokens being\ncopied into the model output.\n\nSimilar\nimages\nshow\na\nman\nholding\na\nblack\numbrella\nher\nding\ncattle\na\nyoung\nchild\nholding\nan\nanimal\nthemed\numbrella\na\nlittle\nboy\nholding\na\nwhite\numbrella\nover\na\nsmall\nanimal\na\nperson\nwith\na\numbrella\nand\nhas\na\nfew\nanimals\n.\nThis\nimage\nshows\na\nboy\nholding\nan\numbrella\nover\na\nherd\nof\ncattle\n.\na\nboy\nholding\nan\numbrella\nover\na\nherd\nof\ncattle\n.\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\nFigure 3: Input attribution for each generated token (y-axis). The brighter the color, the more greater the attribution\nfrom the input token. We observe high attribution scores to \u201cumbrella\u201d, \u201cboy\u201d, \u201ccattle\u201d, and \u201cover\u201d.\n4.3\nInput Attribution with Integrated\nGradients\nTo better understand the role of majority tokens in\nmodel generation, we use integrated gradients (Sun-\ndararajan et al., 2017) for input attribution analysis.\nThis enables us to examine the influence of each\nindividual token in the retrieved captions on the\nmodel prediction.\nAttribution visualization.\nFigure 3 shows an ex-\nample of an attribution visualization, where the\nattribution score of each input token (x-axis) is\ncomputed at each generation step (y-axis). Bright\ncolor cells correspond to high attribution to the\ninput token. High attribution scores to the same\ntokens seen in the retrieved captions may indicate\ncopying. Negative attribution scores are observed\nat contradicting tokens observed in the retrieved\ncaptions to the current generation. Negative scores\nare observed at token \u201cher\u201d when model is pre-\ndicting the token \u201cboy\u201d and at token \u201csmall\u201d when\npredicting \u201cherd\u201d. Additional input attribution vi-\nsualizations can be found in Appendix B.1.\nQuantitative analysis.\nWe also quantitatively an-\nalyze the impact of majority tokens by calculating\npairwise attribution scores between tokens in re-\ntrieved captions and those predicted by the model.\nHigher attribution values suggest greater sensitiv-\nity to the input token (Ancona et al., 2018). Fig-\nure 4 shows the distribution of the pairwise attribu-\ntion scores for the 2B1G setup. It is clear that the\nmodel is sensitive to the majority tokens, especially\nwhen the generated token exists in the retrieved\ncaptions. Such behavior indicates weak robustness:\nwe would not expect a robust model to be distracted\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\nAttribution score\nOT -> OT (abs)\nMT -> OT (abs)\nOT -> MT (abs)\nMT -> MT (abs)\nOT -> OT \nMT -> OT\nOT -> MT\nMT -> MT\nRetrieved tokens -> prediction\nAttribution score\nAbsolute\nOriginal\nFigure 4: Pairwise average attribution score between\nretrieved and generated tokens in the 2B1G setup. MT:\nmajority tokens in the retrieved captions. OT: all other\ntokens. The larger pairwise attribution values shows\nthat the majority tokens have a larger impact during\ngeneration than the other tokens in the retrieved captions.\nby the tokens from the two irrelevant retrieved sen-\ntences at inference time. To better visualize the\nimpact, we show distribution of original attribution\nvalues and the absolute values (Ancona et al., 2018)\nacross all evaluation samples.\n4.4\nAttention and Model Behavior\nFinally, we visualize the self-attention and cross-\nattention to locate the heads and layers in the\nSMALLCAP-OPT125M model that may contribute\nto the majority voting behaviour when generating\na caption. This is crucial because all interactions\nbetween captions (self-attention) and images (cross-\nattention) take place in this stage.\nDistribution of max attention occurrence.\nWe\npartition the text input prompt into five distinct\nsegments: begin of the sentence token (<BOS>),\n\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n5\n10\nProportion\nHEAD 2\nHEAD 7\nHEAD 11\n(c) \ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc5d\ud835\udc5c\ud835\udc5f\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b(max(\ud835\udc4b\ud835\udc34!\"#))\n0\n5\n10\n0\n0.2\n0.4\n0.6\n0.8\n0\n5\n10\n0\n5\n10\n<BOS>\nPrefix\nRetrieval\nSuffix\nGeneration\nCross Attention\nProportion\nHEAD 5\nHEAD 10\nHEAD 12\n(b) \ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc5d\ud835\udc5c\ud835\udc5f\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b(max(\ud835\udc4b\ud835\udc34$%&$))\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n5\n10\n<BOS>\nPrefix\nRetrieval\nSuffix\nGeneration\nSelf Attention\nProportion\nHEAD 2\nHEAD 6\nHEAD 12\n(a) \ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc5d\ud835\udc5c\ud835\udc5f\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b(max(\ud835\udc46\ud835\udc34$%&$))\n \n \n  \n \nCLS\nOther\nFigure 5: Statistics of all maximum attention scores\u2019\ndistribution across different layers and heads from self\nand cross attention. XA denotes cross attention, while\nSA signifies self-attention. img represents the distribu-\ntion of maximum attention scores across image patches,\nwhereas text pertains to the distribution of maximum\nattention scores across text tokens.\nprompt tokens before retrieved k captions (pre-\nfix), i.e. \u201cSimilar image shows\u201d, the retrieved cap-\ntions (retrieval) cap1, \u00b7 \u00b7 \u00b7 capk, prompt tokens be-\nfore generation (suffix), i.e. \u201cThis image show\u201d,\nand the generation itself. For image patches, we\nsegment them into two pieces \u2013 the CLS output em-\nbedding, and the set of patch output embeddings.\nLet Sn denote the sets of indices, where n =\n1, 2, . . . , 5 for five segments. For the text input,\neach segment Sn contains the indices of the to-\nkens in each segment. To track the occurrence of\nmax attention values in Sn, we define the indicator\nfunction 1[In(i, j)] as follows:\n1[In(i, j)] =\n(\n1\nif arg maxz Att(j, z)i \u2208Sn\n0\notherwise\n,\n(2)\nwhere arg maxz Att(j, z)i is the index of the input\nwith the maximum attention score for sample i.\nFor self-attention between the textual tokens,\nAtt(j, z) represents the attention score between\nthe jth generated token and the zth text context\ntoken, denoted as SAtext(j, z).\nFor cross-attention between the decoder and the\nimage representations, we report both a text-centric\nand an image-centric analysis. The text-centric\nanalysis XAtext(j, z) measures the attention be-\ntween the jth image patch and the zth text token,\nto identify which segments of the text have the\nhighest cross-attention scores in relation to the im-\nage. In the image-centric analysis XAimg(j, z),\nwe measure the attention between the jth gener-\nated token and the zth image patch. We now re-\ndefine the Sn notation to let S1 represent the CLS\noutput embedding, and S2 represent the set of im-\nage patch embeddings, respectively. This allows\nXAimg(j, z) to identify if the CLS patch embed-\nding receives the highest cross-attention scores in\nrelation to the generated tokens, or if it is the actual\nimage patch embeddings.\nFor each analysis, SAtext(j, z), XAtext(j, z),\nand XAimg(j, z), we calculate the proportion of\noccurrences of the maximum score in Sn by aver-\naging through all generated tokens for a dataset.\nSelf-attention.\nWe gather attention scores be-\ntween the generated tokens and context tokens, and\ncategorize the distribution of the maximum scores\ninto the five text segments (BOS, prefix, retrieval,\nsuffix, and generation).\nFigure 5(a) illustrates the changes in the distri-\nbution of maximum self-attention scores in each\nlayer of the decoder language model. Notably, at\nthe initial layers, a majority of attention heads ex-\nhibit heightened focus on retrieved captions or the\ncurrent context for generation. However, after the\nsecond layer, we observe an increased emphasis\non the beginning of sentence token (</s>). This\nbehavior is consistent with prior research on the\nattention mechanism of GPT-2 (Vig and Belinkov,\n2019). Figures 9a and 10a show the behaviour for\nall self-attention heads in for the GPT and OPT\nmodel variants, respectively.\nCross-attention.\nSimilar to the self-attention be-\nhaviour, we categorize the occurrence of the maxi-\nmum cross-attention to the five text segments. As\nshown in Figure 5(b), in most attention heads, the\ncross-attention attains its maximum value between\nthe image and the retrieved captions or between\nthe image and the generated tokens. Figures 9b\nand 10b show the text-centric analysis for all cross-\nattention heads for the GPT and OPT backbones.\nFinally, we inspect whether the model focuses on\nthe CLS patch or actual image patches. In Figure\n5(c), we observe that the model only pays max-\nimum attention to the image patches in the final\nlayers (the blue line). Figures 9c and 10c show the\nfull results for the image-centric analysis.\nOverall, these observations show that the model\nattends to both modalities during the caption gen-\neration process. However the lack of strong cross-\n\nattention to actual image patches suggests that the\nmodel is misled by text prompts, even when irrele-\nvant information is absent in the provided image.\n5\nImproving Robustness to Retrieval via\nSampling\nIn order to improve the robustness of the model to\npotentially noisy captions, we propose to randomly\nsample the captions from a larger retrieval list for a\ngiven image, instead of training with only the top-k\nretrieved captions. In this manner, the model can\nlearn from more diverse context that includes both\ntop- and lower-ranked captions.\n5.1\nExperimental Setup\nInspired by Hoang et al. (2022), we experiment\nwith two sampling methods during training to im-\nprove retrieval robustness.\nSample-k training.\nWe sample k captions ran-\ndomly from the top-N=7 retrieved captions during\ntraining5. Following Ramos et al. (2023b), we train\nSMALLCAP with the OPT-350M decoder on the\nCOCO captioning dataset (Chen et al., 2015) for\n10 epochs on a NVIDIA A100 40GB GPU with the\ndefault learning rate of 1e-4 and batch size of 64.\nWe experiment with k in the range of 1\u20134.\nControlled sample-k\ntraining (c-sample-k).\nAiming to train the model that better distinguishes\nirrelevant context, we design a controlled sampling\nprocess \u2014 selecting k\u22121 randomly from the larger\nlist while keeping the top relevant caption of the\nimage during training. We train the model with\nsame hyperparameters and dataset as sample-k.\n5.2\nEvaluation and Results\nIn addition to the COCO and NoCaps validation\nset, we evaluate the Out-domain performance of\nthe model using VizWiz caption dataset (Gurari\net al., 2020) and report CIDEr scores.\nSample-k training improves model robustness\nto random retrieved captions.\nAs shown in Ta-\nble 3, incorporating sampled retrieved captions into\ntraining consistently enhances performance across\nvarious k values. The improvement is particularly\nnotable when captions are randomly retrieved, sug-\ngesting the model is now better able to ignore irrele-\nvant context. If we compare across different values\nof k, sampling mitigates the model\u2019s sensitivity\n5We sample from the top-N=7 for alignment with the\nbaseline; see the Appendix for an ablation on varying N.\nCOCO Eval\nModel\nk\ntop-k\nlast-k\nrandom\ntop-k\n1\n115.1\n112.2\n73.2\nsample-k\n1\n116.0\n115.0\n98.9\ntop-k\n2\n116.8\n115.0\n67.4\nsample-k\n2\n117.4\n116.8\n84.6\ntop-k\n3\n118.3\n117.1\n71.8\nsample-k\n3\n118.5\n117.3\n77.6\ntop-k\n4\n120.1\n117.1\n70.1\nsample-k\n4\n119.2\n118.6\n73.1\nc-sample-k\n4\n119.3\n118.9\n72.6\nTable 3: CIDEr scores when training on the top-k,\nsample-k and c-sample-k captions. Training by sam-\npling the retrieved captions almost always outperforms\nSMALLCAP for all k values. It also reduces the gap\nbetween using top-relevant and low-ranked retrieved\ncaptions. Results are averaged over three seeds. Im-\nproved scores are in bold.\nto the number of retrieved captions, outperforming\ntop-k training. For instance, it achieves comparable\nperformance with a smaller k value than in the case\nof top-k training. Furthermore, the gap between\nusing the top-k vs. the last-k retrieved captions is\nreduced with sample-k training: the maximum gap\nis reduced from 3.0 to 1.0 CIDEr points, indicat-\ning increased model robustness, even with lower-\nranked retrieved captions. Figure 6 and 12 show\nqualitative examples of the improved robustness to\nrandomly retrieved examples.\nSampling improves cross-domain evaluation.\nWe also evaluate on VizWiz and NoCaps to mea-\nsure cross-domain performance (Table 4). This is a\nmore realistic setting where retrieved captions are\nout-of-domain and could be more noisy and less\nrelevant. The application of sampling improves\nacross all values of k for Vizwiz. On the NoCaps\ndataset, with the COCO datastore, sampling consis-\ntently improves near and out-domain performance,\nsuggesting increased robustness to noisy retrieval\ncontext. This is consistent with the benefits of sam-\npled training demonstrated in cross-domain ma-\nchine translation by Hoang et al. (2022). If we use\na larger datastore that incorporates internet-derived\ncaptions (+Web), this consistently improves in-\ndomain performance. Retrieval constraints are al-\nleviated for near and out-domain samples with the\nlarger datastore, where we see smaller gains with\n\n\u2022\na train with the numbers 60016 is heading down the tracks\n\u2022\na black and white photo of two people holding hands in a city on a rainy day\n\u2022\nthis youngster has a boogie board to ride the smaller waves\n\u2022\na wooden entertainment center containing a television set\n\u2022\na man posing with a surfboard on an elevator \n\u2022\na woman sitting on a bench next to a man in a hat\n\u2022\na greyhound dog lying on an unmade bed\n\u2022\na pink teddy bear and a brown teddy bear sitting on wooden rods\na person riding a horse on top of a beach\nSample-k\nTop-k\na person sitting on a bench on a beach\na close up of a fire hydrant on a sidewalk\na close up of a person on a sidewalk\nSample-k\nTop-k\nFigure 6: Qualitative examples of generated captions when randomly retrieving four captions for a given image\nusing a model trained with either the Sample-k or the Top-k method.\nVizWiz\nNoCaps\nNoCaps (+Web)\nModel\nk\nIn\nNear\nOut\nIn\nNear\nOut\ntop-k\n1\n31.3\n85.0\n74.3\n62.3\n84.1\n80.7\n81.5\nsample-k\n1\n32.3\n87.0\n75.7\n63.6\n87.8\n81.2\n77.5\ntop-k\n2\n33.7\n85.0\n74.3\n62.3\n90.5\n86.2\n89.5\nsample-k\n2\n34.0\n87.8\n77.4\n67.6\n90.6\n85.3\n86.7\ntop-k\n3\n35.0\n87.4\n79.6\n68.3\n91.7\n88.3\n89.9\nsample-k\n3\n35.4\n88.7\n80.3\n69.4\n92.6\n88.0\n90.0\ntop-k\n4\n35.5\n87.4\n79.6\n68.3\n94.2\n89.4\n91.2\nsample-k\n4\n35.7\n89.7\n80.9\n71.1\n94.8\n89.5\n93.1\nc-sample-k\n4\n36.0\n90.1\n81.3\n71.5\n94.5\n90.0\n93.3\nTable 4: Training with sampled retrieval always outper-\nforms top-k retrieval for all values of k on the out-of-\ndomain VizWiz and NoCaps datasets. The gains are\nsmaller when using a larger datastore (+Web) but it\nstill improves out-domain performance when retrieving\nmore captions. Improved scores are in bold.\nsample-k. See qualitative examples in Figure 11 in\nAppendix C.\nControlled sampling further improves cross-\ndomain evaluation.\nFinally, on top of our best\nperforming sample-k model, controlled sample-k\nfurther improves performance for both NoCaps and\nVizWiz. This suggests that incorporating both top-\nrelevant and low-ranked captions during training\naids the model in distinguishing irrelevant context.\n6\nDiscussion\nMajority tokens are reliable hints during train-\ning.\nTo better understand why the model relies\non majority tokens during generation, we calcu-\nlate the probability that majority tokens in the re-\ntrieved captions overlap with the ground truth cap-\ntions (TM \u2208GT), and with the predicted tokens\n(TM \u2208Pred). Table 5 shows that in 88%\u201399% of\nthe training examples, the majority tokens in the re-\ntrieved captions are also present in the ground truth\ncaptions. This suggests that the model can develop\na bias towards majority tokens due to the fact that\nthey are so often present in the ground truth during\ntraining. This analysis also clarifies the decrease\nin the model\u2019s robustness as k increases when ran-\ndomly retrieving captions. This is because a higher\nk only adds noise without providing useful major-\nity tokens. The use of sampling during training\nexposes the model to more diverse context, which\nleads to a slightly increased level of selectivity.\nk=2\n3\n4\nTM \u2208GTtrain\n88.0\n97.5\n99.2\nTM \u2208GTval\n74.7\n86.5\n91.0\nTM \u2208Pred\n82.8\n93.4\n96.7\nTM \u2208Pred (sample-k)\n81.9\n93.3\n96.6\nTable 5: Percentage of samples in the COCO train and\nvalidation set where the majority token of the retrieved\ncaptions are present in the ground truth compared to the\npercentage of their presence in prediction.\nIn Figure 7, we show the variation in the distri-\nbution of majority tokens across various evaluation\ndatasets. When captions are randomly selected for\nthe COCO evaluation data, there are fewer major-\nity tokens in the retrieved captions. This presents\na challenge for the model in making use of the\nretrieved captions, which accounts for the perfor-\nmance decrease shown in Figure 1. For evaluation,\n\n0\n2\n4\n6\n8\n10\n12\nNumber of majority tokens in retrieved captions\n0\n500\n1000\n1500\n2000\n2500\nCount\nDataset\ncoco_top4\ncoco_random4\nvw_top4\nnocaps_top4\nFigure 7: Distribution of number of majority tokens\nin the retrieved captions for the COCO, VizWis, and\nNoCaps evaluation datasets. For the COCO dataset, we\nalso show the difference between retrieving the top-4\ncaptions against four randomly selected captions.\nwith the same value of k, the fewer the number\nof majority tokens in the retrieved captions, the\nharder it is for the model to \u201ccopy\u201d those tokens\nto the final output. In such scenarios, we obtain\nbigger improvements with the sample-k training.\n7\nConclusion and Future Work\nWe studied the robustness of the state-of-the-\nart retrieval-augmented image captioning model\nSMALLCAP and provide an through analysis and\nexplanation of how retrieved captions effect the fi-\nnal prediction. Our exploration shows that SMALL-\nCAP is robust to the order of the retrieved captions,\nbut it is sensitive to retrieval noise, which has im-\nplications for using retrieval-augmented models\nin new domains. With extensive input attribution\nanalysis, we show that such sensitivity is due to ma-\njority tokens in the retrieved captions. We demon-\nstrate a more retrieval robust model can be trained\nwith sampling methods during training. We expect\nthat our analysis can inspire better retrieval-robust\ncaptioning models in the field.\nIn the future, we will investigate whether the\nmajority voting behaviour is exploited in other\nretrieval-augmented captioning models. We hope\nto further explore if other techniques such as token-\ndropping or prefix-tuning would further improve\nretrieval robustness.\nEthics Statement\nWe acknowledge the potential risks of hallucination\nand biases introduced by retrieval augmentation in\ncaptioning models. Misleading tokens from the re-\ntrieved captions could cause the model to generate\ncaptions describing nonexistent entities or objects\nin images (Liu et al., 2024; Rohrbach et al., 2018).\nThis could have adverse effects, such as propagat-\ning systematic biases present in the datastore used\nfor retrieval (Foulds et al., 2024).\nDespite the exploration in our work, we acknowl-\nedge that no system is perfect, and undesirable bi-\nases may still be present with our methods. We em-\nphasize the need for continued research into tech-\nniques for identifying and mitigating hallucination\nand bias in retrieval-augmented models (Foulds\net al., 2024; Deng et al., 2024). We also stress the\nimportance of responsible deployment, with human\noversight and content moderation pipelines.\nAs researchers, we have an ethical obligation to\nbe transparent about the potential risks and limi-\ntations of our work. We welcome further scrutiny\nand discussion around these critical issues within\nthe research community.\nLimitations\nWe evaluate the robustness of a single retrieval-\naugmented image captioning model in this study.\nGiven variations in training process and model\nstructures, the observed model behavior may be\nspecific to our chosen model. Applying the same\nanalysis to other models would be useful for a\ndeeper understanding regarding explainability and\ninterpretation of retrieval augmented image cap-\ntioning models, which we leave for future work.\nFor all experiments in our study, we employ the\nsame CLIP-ViT-B/32 backbone as the image en-\ncoder. Investigating how model robustness varies\nwith different visual encoders would enhance the\nscope of our study.\nWhile training with sampling improves model ro-\nbustness, it is intuitive that introducing more noise\nduring training makes the task more challenging.\nIn all our experiments, we train the model for same\nnumber of epochs as SMALLCAP, therefore it is\nnot clear if the model would gain more robustness\nif trained longer. We are curious if there exists an\noptimal balance between training time and the level\nof noise exposure for achieving model robustness.\nAcknowledgments\nWe thank Lei Li and the CoAStal and LAMP\ngroups for feedback.\nWenyan Li is supported\nby the Lundbeck Foundation (BrainDrugs grant:\nR279-2018-1145) and a research grant (VIL53122)\n\nfrom VILLUM FONDEN. Jiaang Li is sup-\nported by Carlsberg Research Foundation (grant:\nCF221432). Rita Ramos is supported by the Por-\ntuguese Recovery and Resilience Plan through\nproject C645008882-00000055 (i.e., the Cen-\nter For Responsible AI), and also by Funda\u00e7\u00e3o\npara a Ci\u00eancia e Tecnologia (FCT), through\nthe project with reference UIDB/50021/2020\n(DOI:10.54499/UIDB/50021/2020) and the Ph.D.\nscholarship with reference 2020.06106.BD.\nReferences\nHarsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen,\nRishabh Jain, Mark Johnson, Dhruv Batra, Devi\nParikh, Stefan Lee, and Peter Anderson. 2019. no-\ncaps: novel object captioning at scale. In Proceed-\nings of the IEEE International Conference on Com-\nputer Vision.\nMarco Ancona, Enea Ceolini, Cengiz \u00d6ztireli, and\nMarkus Gross. 2018. Towards better understanding\nof gradient-based attribution methods for deep neural\nnetworks. In International Conference on Learning\nRepresentations.\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\nHannaneh Hajishirzi. 2023. Self-rag: Learning to\nretrieve, generate, and critique through self-reflection.\narXiv preprint arXiv:2310.11511.\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-\nishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and\nC Lawrence Zitnick. 2015. Microsoft coco captions:\nData collection and evaluation server. arXiv preprint\narXiv:1504.00325.\nFlorin Cuconasu, Giovanni Trappolini, Federico Sicil-\niano, Simone Filice, Cesare Campagnano, Yoelle\nMaarek, Nicola Tonellotto, and Fabrizio Silvestri.\n2024. The power of noise: Redefining retrieval for\nrag systems.\nAilin Deng, Zhirui Chen, and Bryan Hooi. 2024. Seeing\nis believing: Mitigating hallucination in large vision-\nlanguage models via clip-guided decoding. arXiv\npreprint arXiv:2402.15300.\nPhilip Feldman Foulds, R James, and Shimei Pan.\n2024.\nRagged edges: The double-edged sword\nof retrieval-augmented chatbots.\narXiv preprint\narXiv:2403.01193.\nDanna Gurari, Yinan Zhao, Meng Zhang, and Nilavra\nBhattacharya. 2020. Captioning images taken by\npeople who are blind.\nHongkun Hao, Guoping Huang, Lemao Liu, Zhirui\nZhang, Shuming Shi, and Rui Wang. 2023. Rethink-\ning translation memory augmented neural machine\ntranslation. In Findings of the Association for Com-\nputational Linguistics: ACL 2023.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. Proceedings of the IEEE conference on com-\nputer vision and pattern recognition.\nCuong Hoang, Devendra Sachan, Prashant Mathur,\nBrian Thompson, and Marcello Federico. 2022. Im-\nproving robustness of retrieval augmented translation\nvia shuffling of suggestions.\nCuong Hoang, Devendra Sachan, Prashant Mathur,\nBrian Thompson, and Marcello Federico. 2023. Im-\nproving retrieval augmented neural machine transla-\ntion by controlling source and fuzzy-match interac-\ntions. In Findings of the Association for Computa-\ntional Linguistics: EACL 2023.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rock-\nt\u00e4schel, Sebastian Riedel, and Douwe Kiela. 2020.\nRetrieval-augmented generation for knowledge-\nintensive NLP tasks. In Advances in Neural Infor-\nmation Processing Systems, volume 33, pages 9459\u2013\n9474.\nJiaxuan Li, Duc Minh Vo, Akihiro Sugimoto, and\nHideki Nakayama. 2023.\nEvcap:\nRetrieval-\naugmented image captioning with external visual-\nname memory for open-world comprehension. arXiv\npreprint arXiv:2311.15879.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,\nand C Lawrence Zitnick. 2014.\nMicrosoft coco:\nCommon objects in context. In Computer Vision\u2013\nECCV 2014: 13th European Conference, Zurich,\nSwitzerland, September 6-12, 2014, Proceedings,\nPart V 13. Springer.\nHanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen,\nXiutian Zhao, Ke Wang, Liping Hou, Rongjun Li,\nand Wei Peng. 2024.\nA survey on hallucination\nin large vision-language models.\narXiv preprint\narXiv:2402.00253.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2022. Fantastically ordered\nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers).\nGr\u00e9goire Mialon, Roberto Dess\u00ec, Maria Lomeli, Christo-\nforos Nalmpantis, Ram Pasunuru, Roberta Raileanu,\nBaptiste Rozi\u00e8re, Timo Schick, Jane Dwivedi-Yu,\nAsli Celikyilmaz, et al. 2023. Augmented language\nmodels: a survey. arXiv preprint arXiv:2302.07842.\nAsmaa AE Osman, Mohamed A Wahby Shalaby,\nMona M Soliman, and Khaled M Elsayed. 2023. A\nsurvey on attention-based models for image caption-\ning. International Journal of Advanced Computer\nScience and Applications, 14(2).\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\nsupervision. In Proceedings of the 38th International\nConference on Machine Learning, Proceedings of\nMachine Learning Research.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nRita Ramos, Desmond Elliott, and Bruno Martins.\n2023a. Retrieval-augmented image captioning. In\nProceedings of the 17th Conference of the European\nChapter of the Association for Computational Lin-\nguistics, pages 3666\u20133681, Dubrovnik, Croatia. As-\nsociation for Computational Linguistics.\nRita Ramos, Bruno Martins, Desmond Elliott, and Yova\nKementchedjhieva. 2023b. Smallcap: lightweight\nimage captioning prompted with retrieval augmenta-\ntion. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages\n2840\u20132849.\nAnna Rohrbach, Lisa Anne Hendricks, Kaylee Burns,\nTrevor Darrell, and Kate Saenko. 2018. Object hallu-\ncination in image captioning. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing. Association for Computational\nLinguistics.\nSara Sarto, Marcella Cornia, Lorenzo Baraldi, and Rita\nCucchiara. 2022. Retrieval-augmented transformer\nfor image captioning. In Proceedings of the 19th\nInternational Conference on Content-Based Multime-\ndia Indexing, CBMI \u201922.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.\nAxiomatic attribution for deep networks. In Proceed-\nings of the 34th International Conference on Machine\nLearning - Volume 70, ICML\u201917.\nRamakrishna Vedantam, C. Lawrence Zitnick, and Devi\nParikh. 2015. Cider: Consensus-based image de-\nscription evaluation. In CVPR.\nJesse Vig and Yonatan Belinkov. 2019.\nAnalyzing\nthe structure of attention in a transformer language\nmodel. In Proceedings of the 2019 ACL Workshop\nBlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP, pages 63\u201376, Florence, Italy. As-\nsociation for Computational Linguistics.\nZhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan\nParvez, and Graham Neubig. 2023. Learning to filter\ncontext for retrieval-augmented generation.\nKelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,\nAaron Courville, Ruslan Salakhudinov, Rich Zemel,\nand Yoshua Bengio. 2015. Show, attend and tell:\nNeural image caption generation with visual atten-\ntion. In International conference on machine learn-\ning, pages 2048\u20132057. PMLR.\nShi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling.\n2024.\nCorrective retrieval augmented generation.\narXiv preprint arXiv:2401.15884.\nZhuolin Yang, Wei Ping, Zihan Liu, Vijay Kor-\nthikanti, Weili Nie, De-An Huang, Linxi Fan, Zhid-\ning Yu, Shiyi Lan, Bo Li, et al. 2023.\nRe-vilm:\nRetrieval-augmented visual language model for zero\nand few-shot image captioning.\narXiv preprint\narXiv:2302.04858.\nMichihiro Yasunaga, Armen Aghajanyan, Weijia Shi,\nRich James, Jure Leskovec, Percy Liang, Mike Lewis,\nLuke Zettlemoyer, and Wen-tau Yih. 2023. Retrieval-\naugmented multimodal language modeling. In Pro-\nceedings of the 40th International Conference on\nMachine Learning, ICML\u201923.\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan\nBerant. 2023. Making retrieval-augmented language\nmodels robust to irrelevant context. arXiv preprint\narXiv:2310.01558.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open pre-\ntrained transformer language models.\nYucheng Zhou and Guodong Long. 2023. Style-aware\ncontrastive learning for multi-style image captioning.\nIn Findings of the Association for Computational\nLinguistics: EACL 2023.\n\nRetrieval Order\nSmallCap LM\nTraining\nEvaluation\nGPT-2\nOPT\ndefault\ndefault\n116.4/36.1\n120.3/37.1\npermute\n116.2/36.0\n120.1/37.0\nreverse\n115.8/36.0\n119.7/36.8\npermute\npermute\n117.2/36.4\n120.4/37.2\nreverse\nreverse\n116.4/36.1\n120.7/37.0\nTable 6: Results of manipulating the order of the top-\nk retrieved captions by either randomly permuting or\nreversing the list. We report CIDEr/BLEU4 scores on\nthe COCO validation set using either a GPT-2 or OPT\nbackbone in the SmallCap model.\nA\nMajority Tokens\nA.1\nStop words list\nIn this section, we present the stop words that were\nfiltered from the COCO dataset in the experiments\ndescribed in Section 4.2:\n[\u201cout\u201d, \u201csome\u201d, \u201cof\u201d, \u201cis\u201d, \u201cwhile\u201d, \u201care\u201d,\n\u201cwith\u201d, \u201cdown\u201d, \u201chas\u201d, \u201cover\u201d, \u201cthe\u201d, \u201cnext\u201d, \u201cup\u201d,\n\u201cnear\u201d, \u201cseveral\u201d, \u201cother\u201d, \u201cat\u201d, \u201ctop\u201d, \u201cfrom\u201d, \u201cin\u201d,\n\u201con\u201d, \u201ca\u201d, \u201cthere\u201d, \u201can\u201d, \u201cto\u201d, \u201cand\u201d, \u201cher\u201d, \u201cfront\u201d,\n\u201cby\u201d, \u201cfor\u201d, \u201chis\u201d, \u201cit\u201d]\nB\nMore Visualization\nB.1\nInput Attribution with Integrated\nGradients\nIn Figure 8, we show more attribution visualization\nfor the experiment setup 2B1G in Section 4 where\nhigh attribution scores are observed in the majority\ntokens and mislead the model to generate incorrect\ncaptions.\nB.2\nAttention\nIn Figure 9 and Figure 10, we depict the distri-\nbutions of both self-attention and cross-attention\nscores across various heads and layers for SMALL-\nCAP with GPT-2 and OPT decoder variants.\nC\nQualitative examples\nWe show more qualitative examples in Figure 11\nand Figure 12.\nD\nMore results\nOrder robustness evaluation\nIn Table 6 and Ta-\nble 7, we provide both CIDEr and BLEU4 scores\nfor order robustness evaluation (Section 3).\nModel\nRetrieval Order\nIn\nNear\nOut\nGPT2\ndefault\n80.1/37.9\n79.4/35.9\n69.6/25.3\npermute\n81.5/38.8\n79.7/36.6\n69.8/26.2\nreverse\n80.4/38.4\n80.1/36.3\n68.4/25.1\nOPT\ndefault\n91.0/27.1\n84.4/23.8\n76.3/15.0\npermute\n94.2/28.6\n84.0/25.0\n79.4/15.8\nreverse\n92.5/28.4\n85.6/25.3\n75.9/14.2\nTable 7: Complete results with both CIDEr/BLEU4 on\nthe NoCaps dataset when evaluated with different order\nof the top-four retrieved captions. The order applies to\nboth train and evaluation stage.\nNumber of retrieved captions for sample-k train-\ning\nWe experiment with different size of the re-\ntrieval candidate list from which we randomly se-\nlect captions for sample-k training (Table 8).\nCOCO\nSize\nk\nVizWiz\ntop-k\nlast-k\nrandom\n7\n4\n36.0\n119.2\n117.1\n71.0\n10\n4\n36.0\n119.3\n118.3\n67.6\n50\n4\n33.9\n118.1\n117.7\n81.2\nTable 8: CIDEr score when sampling from different\nsize of retrieval candidates. We see more improvements\non random k evaluation while almost keeping the same\nlevel of in-domain performance. With more noise in-\nvolved during training, we would expect a longer train-\ning time would yield more robust performance.\nPercentage of tokens that are likely to be copied\nIn Table 9 we show the percentage of tokens that\nare likely to be copied from retrieved captions av-\neraging through all samples in the validation set.\nMajority tokens takes more than half of the copied\ntokens.\nk=1\n2\n3\n4\nTR \u2208Pred\n49.1\n63.3\n69.8\n75.7\nTR \u2208Pred (sample-k)\n46.0\n61.5\n69.5\n74.0\nTM \u2208Pred\n-\n33.1\n45.7\n54.5\nTM \u2208Pred (sample-k)\n-\n32.5\n45.3\n53.3\nTable 9: Percentage of tokens in the predicted caption\nthat are likely copied from majority tokens in retrieved\ncaptions in the COCO validation set. TR represent to-\nkens in retrieved captions. TM represent the majority\ntokens in retrieved captions.\nComparison with other methods\nInspired\nby Yoran et al. (2023), we have considered inten-\ntionally including less relevant captions by includ-\ning one irrelevant caption, one low-ranked caption,\n\nFigure 8: Attribution visualization with few more examples. Here the model prediction is misled by the majority\ntokens in the 2B1G setting.\nand top-2 relevant captions instead of using top-4\nretrieved captions. However, in our preliminary\nexperiments, this strategy does not perform as well\nas the sampling approach, likely due to the high\nnoise level it introduced.\nCOCO Evaluation\nNoCaps Evaluation\nMethod\ntop-k\nlast-k\nrandom\nIn\nNear\nOut\ntop-4\n120.1\n117.1\n70.1\n87.4\n79.6\n68.3\nsample-4\n119.2\n118.6\n73.1\n89.7\n80.9\n71.1\nmixed-4\n119.2\n118.1\n66.7\n59.9\n57.9\n39.4\nTable 10: CIDEr on COCO and NoCaps.\n\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\nPrefix\nRetrieval\nSuffix\nGeneration\nHEAD 0\nHEAD 1\nHEAD 2\nHEAD 3\nHEAD 4\nHEAD 5\nHEAD 6\nHEAD 7\nHEAD 8\nHEAD 9\nHEAD 10\nHEAD 11\n(a) Self attention distribution.\n0\n5\n10\n0\n0.2\n0.4\n0.6\n0.8\n0\n5\n10\n0\n0.2\n0.4\n0.6\n0.8\n0\n5\n10\n0\n0.2\n0.4\n0.6\n0.8\n0\n5\n10\n0\n0.2\n0.4\n0.6\n0.8\n0\n5\n10\n0\n0.2\n0.4\n0.6\n0\n5\n10\n0\n0.2\n0.4\n0.6\n0.8\n0\n5\n10\n0\n0.2\n0.4\n0.6\n0\n5\n10\n0\n0.2\n0.4\n0.6\n0\n5\n10\n0\n0.5\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.2\n0.4\n0.6\n0\n5\n10\n0\n0.2\n0.4\n0.6\nPrefix\nRetrieval\nSuffix\nGeneration\nHEAD 0\nHEAD 1\nHEAD 2\nHEAD 3\nHEAD 4\nHEAD 5\nHEAD 6\nHEAD 7\nHEAD 8\nHEAD 9\nHEAD 10\nHEAD 11\n(b) Cross attention distribution. Distribution of max attention\nscores of the interaction between various part of text prompt\nand image patches.\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\nCLS\nOthers\nHEAD 0\nHEAD 1\nHEAD 2\nHEAD 3\nHEAD 4\nHEAD 5\nHEAD 6\nHEAD 7\nHEAD 8\nHEAD 9\nHEAD 10\nHEAD 11\n(c) Cross attention distribution. Distribution of max attention\nscores of the interaction between two type of image patches\n(cls, others) and all text tokens.\nFigure 9: Statistics of max attention scores in self and\ncross attentions from different different layers and heads\nwith SMALLCAP (GPT2 variant). Compute the propor-\ntion of each attention scores from self and cross attention\nbelongs to which parts.\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n<BOS>\nPrefix\nRetrieval\nSuffix\nGeneration\nHEAD 0\nHEAD 1\nHEAD 2\nHEAD 3\nHEAD 4\nHEAD 5\nHEAD 6\nHEAD 7\nHEAD 8\nHEAD 9\nHEAD 10\nHEAD 11\n(a) Self attention distribution.\n0\n5\n10\n0\n0.2\n0.4\n0.6\n0\n5\n10\n0\n0.2\n0.4\n0.6\n0\n5\n10\n0\n0.2\n0.4\n0\n5\n10\n0\n0.2\n0.4\n0.6\n0\n5\n10\n0\n0.2\n0.4\n0.6\n0\n5\n10\n0\n0.2\n0.4\n0\n5\n10\n0\n0.2\n0.4\n0.6\n0\n5\n10\n0\n0.2\n0.4\n0.6\n0\n5\n10\n0\n0.2\n0.4\n0.6\n0\n5\n10\n0\n0.2\n0.4\n0.6\n0.8\n0\n5\n10\n0\n0.2\n0.4\n0\n5\n10\n0\n0.2\n0.4\n0.6\n<BOS>\nPrefix\nRetrieval\nSuffix\nGeneration\nHEAD 0\nHEAD 1\nHEAD 2\nHEAD 3\nHEAD 4\nHEAD 5\nHEAD 6\nHEAD 7\nHEAD 8\nHEAD 9\nHEAD 10\nHEAD 11\n(b) Cross attention distribution. Distribution of max attention\nscores of the interaction between various part of text prompt\nand image patches.\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\n0\n5\n10\n0\n0.5\n1\nCLS\nOthers\nHEAD 0\nHEAD 1\nHEAD 2\nHEAD 3\nHEAD 4\nHEAD 5\nHEAD 6\nHEAD 7\nHEAD 8\nHEAD 9\nHEAD 10\nHEAD 11\n(c) Cross attention distribution. Distribution of max attention\nscores of the interaction between two type of image patches\n(cls, others) and all text tokens.\nFigure 10: Statistics of max attention scores in self and\ncross attentions from different different layers and heads\nwith SMALLCAP (OPT-125M variant). Compute the\nproportion of each attention scores from self and cross\nattention belongs to which parts.\n\n\u2022\na garage door leading out to a fancy car\n\u2022\na refrigerator up against a wall of a \nbuilding\n\u2022\na pickup truck is parked in front of a \ngarage\n\u2022\n\u2026\nTop-k\nSample-k\na white building that is \non the side of the road \na black and white photo of a \nbuilding with a white door \n\u2022\na bunch of butterfly's sitting on a basket\n\u2022\nan individual enjoying itself on a sunny day\n\u2022\nbutterflies feast on the nectar of fruit in a \nglass bowl\n\u2022\ntwo males one has a pink flower in his \nmouth\nTop-k\nSample-k\na butterfly that is sitting in \nthe grass near some flowers \na couple of flowers that are on \na field\n\u2022\na glass plate topped with \nstrawberries and cream\n\u2022\nthe strawberries are supposed \nto make this dessert look less \nfattening\n\u2022\n...\nTop-k\nSample-k\na table topped with two \nglasses of water and a \nglass of water \na couple of white plates \ntopped with ice cream \nFigure 11: Qualitative examples of generated captions on NoCaps out-domain samples where the captions retrieved\nfor the given image can be noisy and irrelevant. Here we retrieve four captions for each image.\na laptop computer \nsitting on top of a table\na couple of dogs are sitting \non a couch\n\u2022\nas two men browse the vegetables \nfrom above a dog browses below\n\u2022\nan older large green and yellow trash \ntruck driving down a busy street\n\u2022\n\u2026\nTop-k\na truck driving down a street \nnext to a sign\nan orange and yellow fire \nhydrant sitting on a street\nTop-k\n\u2022\nan orange placed on a fir tree branch\n\u2022\na display of different styles of analog\nclocks on a wall\n\u2022\n\u2026\nSample-k\nSample-k\n\u2022\nworld war ii vintage fighter plane \nparked in a museum \n\u2022\none zebra eating in a zoo like \nenvironment and another zebra \npartially in view \n\u2022\n\u2026\na white truck parked in \nfront of a building\na truck parked in front of a \nbuilding\nSample-k\nTop-k\n\u2022\na man in the air over a wave \nwith a surfboard\n\u2022\na tennis player in a red dress \ntaking a swing \n\u2022\na person wearing a red biker \nshirt stands next to his bike\n\u2022\n\u2026\na cat is sitting on top of a chair \na large black and white bear \nsitting on a wooden table\na man riding a snowboard down a \nsnow covered slope\na person standing in the snow with a dog\n\u2022\na dog watching the water flush down \na toilet\n\u2022\ntwo young boys standing and playing \nwith wii motes\n\u2022\na employee giving someone their \norder at an eating establishment\n\u2022\n\u2026\na cat is standing on the \nedge of a light \na black and white dog \nstanding on a toilet\n\u2022\na cat laying inside of a bathroom \nsink\n\u2022\na group of three giraffe standing \nnext to each other near a wall\n\u2022\nman sitting on seat with child and \nmultiple dogs nearby on floor\n\u2022\n\u2026\nTop-k\nSample-k\nSample-k\nTop-k\nSample-k\nTop-k\nFigure 12: More qualitative examples of generated captions when randomly retrieving four captions for a given\nimage.\n"}