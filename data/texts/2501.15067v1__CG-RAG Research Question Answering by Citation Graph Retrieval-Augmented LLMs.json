{"metadata": {"pdf_filename": "2501.15067v1__CG-RAG Research Question Answering by Citation Graph Retrieval-Augmented LLMs.pdf", "source": "arXiv"}, "text": "CG-RAG: Research Question Answering by Citation Graph\nRetrieval-Augmented LLMs\nYuntong Hu\nyuntong.hu@emory.edu\nEmory University\nAtlanta, GA, USA\nZhihan Lei\nzhihan.lei@emory.edu\nEmory University\nAtlanta, GA, USA\nZhongjie Dai\ndzj@tongji.edu.cn\nTongji University\nShanghai, China\nAllen Zhang\nazhang490@gatech.edu\nGeorgia Institute of Technology\nAtlanta, GA, USA\nAbhinav Angirekula\naa125@illinois.edu\nUniversity of Illinois\nUrbana-Champaign\nUrbana, IL, USA\nZheng Zhang\nzheng.zhang@emory.edu\nEmory University\nAtlanta, GA, USA\nLiang Zhao\nliang.zhao@emory.edu\nEmory University\nAtlanta, GA, USA\nABSTRACT\nResearch question answering requires accurate retrieval and con-\ntextual understanding of scientific literature. However, current\nRetrieval-Augmented Generation (RAG) methods often struggle\nto balance complex document relationships with precise informa-\ntion retrieval. In this paper, we introduce Contextualized Graph\nRetrieval-Augmented Generation (CG-RAG), a novel framework\nthat integrates sparse and dense retrieval signals within graph\nstructures to enhance retrieval efficiency and subsequently im-\nprove generation quality for research question answering. First,\nwe propose a contextual graph representation for citation graphs,\neffectively capturing both explicit and implicit connections within\nand across documents. Next, we introduce Lexical-Semantic Graph\nRetrieval (LeSeGR), which seamlessly integrates sparse and dense\nretrieval signals with graph encoding. It bridges the gap between\nlexical precision and semantic understanding in citation graph re-\ntrieval, demonstrating generalizability to existing graph retrieval\nand hybrid retrieval methods. Finally, we present a context-aware\ngeneration strategy that utilizes the retrieved graph-structured in-\nformation to generate precise and contextually enriched responses\nusing large language models (LLMs). Extensive experiments on\nresearch question answering benchmarks across multiple domains\ndemonstrate that our CG-RAG framework significantly outper-\nforms RAG methods combined with various state-of-the-art re-\ntrieval approaches, delivering superior retrieval accuracy and gen-\neration quality.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\n\u00a9 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-XXXX-X/18/06\nhttps://doi.org/XXXXXXX.XXXXXXX\nCCS CONCEPTS\n\u2022 Computing methodologies \u2192Natural language generation;\n\u2022 Information systems \u2192Retrieval models and ranking.\nKEYWORDS\nGraph retrieval-augmented generation, research question answer-\ning, hybrid retrieval, graph learning, citation graphs\nACM Reference Format:\nYuntong Hu, Zhihan Lei, Zhongjie Dai, Allen Zhang, Abhinav Angirekula,\nZheng Zhang, and Liang Zhao. 2018. CG-RAG: Research Question An-\nswering by Citation Graph Retrieval-Augmented LLMs. In Proceedings of\nMake sure to enter the correct conference title from your rights confirma-\ntion emai (Conference acronym \u2019XX). ACM, New York, NY, USA, 10 pages.\nhttps://doi.org/XXXXXXX.XXXXXXX\n1\nINTRODUCTION\nQuestion answering is a critical domain recently driven by ad-\nvancements in Large Language Models (LLMs). While LLMs ex-\nhibit exceptional capabilities in addressing commonsense questions\n[2, 13, 20], their pre-trained knowledge inevitably becomes outdated\nover time, rendering them insufficient for delivering timely, precise,\nand comprehensive answers, particularly for complex scientific\nand domain-specific questions. Additionally, the substantial cost of\ncontinuously fine-tuning and updating LLMs makes maintaining\ntheir relevance impractical.\nFor open-domain question answering, which requires relevant\ncontexts for precise answers, Retrieval-Augmented Generation\n(RAG) [21] offers a promising solution. By integrating external\nknowledge, RAG addresses the limitations of static pre-trained\nLLMs, enabling access to up-to-date, domain-specific information\nand thereby enhancing answer accuracy [34, 36, 42]. RAG comprises\ntwo components: a retriever, which identifies relevant information\nfrom the database based on the query, and a generator, which uti-\nlizes the retrieved information to construct responses. The quality\nof generation heavily depends on the effectiveness of the retrieval\narXiv:2501.15067v1  [cs.IR]  25 Jan 2025\n\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nYuntong et al.\nFigure 1: Illustration of retrieval-augmented research question answering using: (a) sparse retrieval based on lexical matches,\n(b) dense retrieval based on semantic relevance, and (c) contextualized retrieval leveraging graph context, i.e., interactions\nbetween documents. represents the dense embedding, where deeper colors indicate a higher semantic relevance to the question.\nBoxed text in red highlights the matched terms between questions and documents, while Boxed text in blue highlights the\nmatched terms between documents.\nprocess [45, 47]. Traditional retrieval methods primarily focus on\nlexical and semantic relevance to evaluate the relevance between\ndocuments and queries, utilizing sparse and dense retrieval sig-\nnals, respectively [25, 48]. For graph-based databases, however,\nthese methods fall short, as they fail to account for intricate inter-\ndocument relationships. For example, simple retrieval methods\noften overlook critical citation links in paper citation networks,\nwhich are essential for capturing nuanced connections between\ndocuments.\nUnlike typical QA tasks, which range from layman-level to\ndomain-expert-level based on well-curated documents or knowl-\nedge graphs, this paper focuses on more challenging QA at the\nresearch frontier, which can only be addressed by analyzing the\nbody of research papers, a task referred to as research question\nanswering. Research question answering necessitates considering\nconnections between documents through citation links to ensure\ncomprehensiveness [10, 24, 38]. In citation graphs, the relevant\npapers are connected not only semantically but also via citation\nlinks, encapsulating structured contextual information. Leveraging\nthis contextual information is essential for enhancing both retrieval\naccuracy and the quality of generated responses, as illustrated in\nFigure 1. Specifically, as shown in Figure 1(c), each paper requires\nan evaluation of lexical and semantic relevance, as well as its cita-\ntions to other relevant documents. For instance, a paper focused\non \"graph representation learning\" is theoretically correlated to\na query on \"node classification,\" but it cannot be retrieved using\nsparse or dense retrieval alone. When multi-hop citation links are\nconsidered, however, it becomes highly relevant. Notably, not all\ncitation-linked papers are pertinent to the query, requiring an ap-\nproach that integrates lexical and semantic relevance, structural\npatterns, and graph-based context to ensure accurate retrieval.\nTo tackle this problem, we propose Contextualized Graph\nRetrieval-Augmented Generation (CG-RAG), a novel framework\ntailored for research question answering. An important considera-\ntion of research question answering is that research paper content\nis highly heterogeneous: for instance, the information in the re-\nlated work, methodology, and experimental sections each serve\ndifferent purposes and answer distinct types of questions. There-\nfore, effective modeling requires breaking down documents into\nsemantic chunks\u2014coherent sections representing specific aspects\nof a research paper. To achieve this, we mathematically construct a\ncitation graph at the chunk level, where each chunk represents a\nsemantically meaningful module of the paper. These chunks form a\ngraph structure with intra-paper and inter-paper connections, cap-\nturing both internal coherence and external relationships. To rep-\nresent this, we introduce the Contextual Citation Graph, which\ndecomposes citation graphs into chunk-level granularity, enabling\nfine-grained relationship discovery.\nTo synergize lexical and semantic retrieval signals inside and\nacross networked documents in citation graphs, we propose the\nLexical-Semantic Graph Retrieval (LeSeGR) method. This ap-\nproach convolves over query-relevant subgraphs, where edges and\nnodes are characterized by lexical and semantic relevance scores.\nImportantly, we theoretically demonstrate that the existing post-\nretrieval paradigm is a special case of our approach. When docu-\nments are contextually linked, entangling retrieval signals through\ngraph structures enhances retrieval performance. Finally, the sub-\ngraph embeddings are used as input to LLMs for generating final\nanswers. Our experiments conducted on citation graphs from di-\nverse scientific domains demonstrate the superior retrieval and\ngeneration effectiveness of CG-RAG. Furthermore, our evaluations\n\nCG-RAG: Research Question Answering by Citation Graph Retrieval-Augmented LLMs\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nreveal that retrieval-augmented LLMs equipped with CG-RAG out-\nperform state-of-the-art retrieval strategies, significantly enhancing\nthe quality of LLM-generated responses.\nThe rest of the paper is organized as follows. Section 2 reviews re-\nlated work on retrieval-augmented generation. Section 3 highlights\nkey aspects of retrieval-augmented research question answering\nwithin the context of citation graphs. Section 4 introduces a novel\nformulation that extends beyond the current retrieval paradigm and\ndetails our proposed retrieval strategy and retrieval-augmented gen-\neration method. Section 5 presents experimental results, comparing\nour approach with RAG frameworks using various state-of-the-art\nretrieval methods for research question answering. Finally, Section\n6 concludes the paper with key insights.\n2\nRELATED WORK\n2.1\nInformation Retrieval\nInformation retrieval (IR) focuses on extracting relevant informa-\ntion from large corpora. Two primary retrieval techniques domi-\nnate the field: sparse retrieval and dense retrieval. Sparse retrieval\nmethods, such as TF-IDF [32] and BM25 [31], rely on term-based\nrepresentations to evaluate lexical matches between queries and\ndocuments. These approaches perform well in scenarios where\nexact term matching is essential, but they struggle with seman-\ntic meaning. In contrast, dense retrieval methods leverage pre-\ntrained language models such as BERT [5] to encode queries and\ndocuments as continuous, low-dimensional embeddings, captur-\ning semantic similarity through maximum inner product search\n(MIPS) [16, 30, 43, 44]. Dense retrieval effectively overcomes the\nlexical gap, retrieving semantically related results even when query\nterms differ from the document\u2019s terminology. Recently, hybrid\nretrieval techniques have also emerged to combine the strengths\nof sparse and dense methods while addressing their respective\nlimitations [25, 26, 29]. By integrating sparse and dense signals,\nthese approaches provide a robust solution for retrieving relevant\ninformation from long and complex documents.\n2.2\nRetrieval-Augmented Generation\nRetrieval-Augmented Generation (RAG) is a technique that inte-\ngrates external retrieval systems to enhance large language mod-\nels (LLMs) [9, 21, 22]. Unlike traditional LLMs that rely solely on\npre-trained knowledge, RAG leverages external sources during in-\nference, enabling more accurate and up-to-date responses. This\nmakes RAG particularly effective for specialized tasks, such as\nliterature-based question answering [27, 46]. Naturally, current lit-\nerature question-answering systems are predominantly built upon\nRAG frameworks [1, 10, 35, 39], relying mainly on dense retrieval\ncombined with LLMs. Literature data, however, is inherently graph-\nstructured, where topological information, such as hierarchical\nand citation relationships, plays a crucial role in the retrieval and\ngeneration processes. As such, most existing literature question\nanswering methods fail to incorporate this structural context ef-\nfectively. Recently, GraphRAG [6, 11, 12] was introduced to extend\nRAG to graph-related scenarios, offering the potential to capture\ncomplex interconnections in literature datasets by leveraging graph-\nbased relationships such as hierarchical and citation structures. In\nour study, we focus on leveraging graph context to enhance the\nretrieval and generation processes in literature question answering.\n3\nRESEARCH QUESTION ANSWERING\nCitation Graph.\nA citation Graph, G = (\ud835\udc49, \ud835\udc38, {\ud835\udc51\ud835\udc63}\ud835\udc63\u2208\ud835\udc49), consists\nof a node set \ud835\udc49and an edge set \ud835\udc38, where each node \ud835\udc63\u2208\ud835\udc49is\nassociated with natural language attributes in a paper. These papers,\nrepresented by D = {\ud835\udc51\ud835\udc63}\ud835\udc63\u2208\ud835\udc49, include textual information such as\nabstracts, sections, and other relevant content.\nResearch Question Answering.\nGiven a query \ud835\udc5eover the ci-\ntation graph G, the objective of research question answering is to\ngenerate an appropriate answer by leveraging the relevant infor-\nmation retrieved from G. Formally, this objective is defined as:\n\ud835\udc5d\ud835\udf03(\ud835\udc4c|\ud835\udc5e, G) = arg max\n\ud835\udf03\n\ud835\udc5b\n\u00d6\n\ud835\udc56=1\n\ud835\udc5d\ud835\udf03(\ud835\udc66\ud835\udc56|\ud835\udc66<\ud835\udc56,\ud835\udc5e, G),\n(1)\nwhere \ud835\udf03represents the parameters of the generative language\nmodel, \ud835\udc4c= {\ud835\udc66\ud835\udc56}\ud835\udc5b\n\ud835\udc56=1 is the generated answer sequence, and \ud835\udc66<\ud835\udc56\ndenotes the prefix tokens of the sequence up to position \ud835\udc56\u22121.\nThe quality of the generated answer is highly dependent on\nthe effectiveness of the retrieval process within the citation graph.\nLet \ud835\udc53\ud835\udc5cdenote the relevance scoring function for a retrieval system\n\ud835\udc5c. Recent advances in graph-based retrieval leverage structural\ninformation, formalized as:\n\ud835\udc53graph : \ud835\udc54(\ud835\udc53dense) \u2192R,\n(2)\nwhere \ud835\udc54(\u00b7) encodes topological information. These methods pri-\nmarily rely on dense representations, which often fail to capture\nsparse lexical matches\u2014essential in citation graphs for identifying\nexact cross-references and key terms critical to retrieval accuracy.\nThus, integrating sparse and dense retrieval is essential. Existing\nhybrid retrieval systems combine these signals via post-retrieval\nfusion: \ud835\udc53hybrid : \ud835\udc53sparse\n\u00c9 \ud835\udc53dense \u2192R, where \u00c9 denotes opera-\ntions such as score fusion. Treating documents as isolated entities,\nhowever, limits their effectiveness in structured databases such as\ncitation graphs. Designing a retrieval system that is both lexically\nand semantically aware in graph retrieval remains an unresolved\nchallenge.\n4\nMETHODOLOGY\n4.1\nOverview\nTo overcome these limitations, we propose Contextualized Graph\nRetrieval-Augmented Generation (CG-RAG), introducing a novel re-\ntrieval method called Lexical-Semantic Graph Retrieval (LeSeGR),\nwhich integrates discrete sparse signals and continuous dense sig-\nnals in a manner that respects the graph topology. Formally, the\nparadigm is defined as:\n\ud835\udc53entangled : \ud835\udc54(\ud835\udc53sparse\n\u00cc\n\ud835\udc53dense) \u2192R,\n(3)\nwhere \ud835\udc54(\u00b7) is a graph encoder that incorporates structured con-\ntext during retrieval, and \u00cb represents the entangled fusion of\nsparse and dense signals. The transition to \ud835\udc53entangled offers greater\ngenerality and capabilities, but requires addressing fundamental\nchallenges:\n\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nYuntong et al.\n\u2022 Section 4.2 introduces the contextual citation graph, which cap-\ntures chunk-level cross-relationships by surrounding each chunk\nwith its relevant context.\n\u2022 Section 4.3 introduces the Lexical-Semantic Graph Retrieval that\nintegrates sparse and dense signals within graph-based scenarios.\nWe prove that it encompasses existing hybrid retrieval meth-\nods based on post-retrieval approaches as a special case when\ngraph contextual information is absent and extends dense-signal-\nonly graph retrieval, highlighting its generalizability to current\nretrieval frameworks.\n\u2022 Section 4.4 introduces Contextualized Graph Retrieval-Augmented\nGeneration that leverages retrieved contextual subgraphs by\nLeSeGR to improve the quality of generated responses.\n4.2\nContextual Citation Graph\nResearch paper content is highly diverse, with sections like related\nwork, methodology, and experiments serving distinct purposes\nand answering different questions, necessitating the segmentation\nof documents into semantic chunks that capture specific aspects.\nGiven a citation graph G = (\ud835\udc49, \ud835\udc38, {\ud835\udc51\ud835\udc63}\ud835\udc63\u2208\ud835\udc49), each document \ud835\udc51\ud835\udc63is\ndecomposed into a set of chunks \ud835\udc36\ud835\udc63, with all chunks collectively\nforming C = \u00d0\n\ud835\udc63\u2208\ud835\udc49\ud835\udc36\ud835\udc63. Chunks may reference each other within\nthe same document (intra-connections) or across different docu-\nments (inter-connections). Intra-connections are explicit, such as a\nsection referencing earlier subsections within the same paper, while\ninter-connections occur when one paper cites another without ex-\nplicitly linking to specific chunks within it. Intra-document links\ntypically provide highly relevant context, whereas inter-document\nlinks offer supplementary but less significant information. To cap-\nture these interactions during retrieval, we propose the hierarchical\ncitation graph, modeling relationships both within and across docu-\nments. Formally, it is defined as \u00afG = ( \u00af\ud835\udc49, \u00af\ud835\udc38, C), where C = {\ud835\udc50\ud835\udc56}\ud835\udc56\u2208\u00af\ud835\udc49\nrepresents the set of chunks.\n4.2.1\nChunk Node. Each chunk \ud835\udc50\ud835\udc56\u2208C corresponds to a fixed-\nlength segment of text extracted from a document in the citation\ngraph G. Specifically, documents are divided into chunks with a\nmaximum token length of\ud835\udc59, such that a document with a total token\nlength of \ud835\udc3fis divided into \u2308\ud835\udc3f\n\ud835\udc59\u2309chunks. These chunks serve as the\nnodes in the hierarchical citation graph \u00afG.\n4.2.2\nChunk-Chunk Edge. The edges in \u00afG represent relationships\nbetween chunks, capturing both intra- and inter-document con-\nnections. Edge weights reflect the strength and nature of these\nrelationships.\nIntra-document Edges connect chunks within the same docu-\nment, preserving the logical flow and structural hierarchy of the text.\nWhen \ud835\udc50\ud835\udc56,\ud835\udc50\ud835\udc57\u2208\ud835\udc36\ud835\udc63, an edge is established if a structural dependency\nexists between them, such as adjacency or explicit cross-references.\nAdjacency refers to the logical connection between two consecutive\nchunks, where \ud835\udc50\ud835\udc57precedes \ud835\udc50\ud835\udc56, resulting in an edge \ud835\udc50\ud835\udc57\u2192\ud835\udc50\ud835\udc56. Explicit\ncross-references occur when \ud835\udc50\ud835\udc56refers to \ud835\udc50\ud835\udc57, establishing an edge\n\ud835\udc50\ud835\udc57\u2192\ud835\udc50\ud835\udc56.\nInter-document Edges, in contrast, connect chunks across differ-\nent documents. For a chunk \ud835\udc50\ud835\udc56\u2208\ud835\udc36\ud835\udc62, the Top-\ud835\udc5brelevant chunks in\n\ud835\udc36\ud835\udc63are linked to \ud835\udc50\ud835\udc56if (\ud835\udc63,\ud835\udc62) \u2208\ud835\udc38. The relevance \ud835\udc5f\ud835\udc56\ud835\udc57between \ud835\udc50\ud835\udc56and\n\ud835\udc50\ud835\udc57is computed using the relevance scoring functions \ud835\udc53sparse and\n\ud835\udc53dense: \ud835\udc53sparse(csparse\n\ud835\udc56\n, csparse\n\ud835\udc57\n) + \ud835\udc53dense(cdense\n\ud835\udc56\n, cdense\n\ud835\udc57\n), where csparse\n\ud835\udc56\nand cdense\n\ud835\udc56\nare the sparse and dense representations of chunk \ud835\udc50\ud835\udc56,\nrespectively, and similarly for \ud835\udc50\ud835\udc57.\n4.3\nLexical-Semantic Graph Retrieval (LeSeGR)\nGiven a hierarchical citation graph \u00afG = ( \u00af\ud835\udc49, \u00af\ud835\udc38, C,\ud835\udc64), the represen-\ntation of each chunk \ud835\udc50\ud835\udc56\u2208C is designed to combine the advantages\nof sparse lexical vectors and dense semantic vectors. Additionally,\nif the contexts around \ud835\udc50\ud835\udc56contain relevant information, the repre-\nsentation incorporates contributions from these contextual chunks.\nThis forms the basis of an entangled representation, integrating\nboth sparse-dense fusion and graph contextual information.\nAlgorithm 1 Lexical-Semantic Graph Retrieval.\nRequire: Citation graph G = (\ud835\udc49, \ud835\udc38, {\ud835\udc51\ud835\udc63}\ud835\udc63\u2208\ud835\udc49), Query \ud835\udc5e\nEnsure: A list of contextual subgraphs { \u00afG}\n1: \u22b2Initialize graph structures and representations.\n2: if cached contextual graph \u00afG exists then\n3:\nLoad \u00afG = ( \u00af\ud835\udc49, \u00af\ud835\udc38, C)\n4: else\n5:\nGenerate \u00afG = ( \u00af\ud835\udc49, \u00af\ud835\udc38, C) from G\n6:\nCache \u00afG for future use\n7: end if\n8: \u22b2Initialize sparse and dense representations for the query and\nall chunks.\n9: csparse\n\ud835\udc56\n, cdense\n\ud835\udc56\n\u2190sparse and dense encoders for \u2200\ud835\udc50\ud835\udc56\u2208C\n10: qsparse, qdense \u2190sparse and dense encoders for \ud835\udc5e\n11: \u22b2Compute initial query relevance for all chunks.\n12: for each chunk \ud835\udc50\ud835\udc56\u2208C do\n13:\n\ud835\udeff\ud835\udc5e\ud835\udc56\u2190\ud835\udc53sparseqsparse, csparse\n\ud835\udc56\n)\n\u22b2Sparse Signal\n14:\n\ud835\udefc\ud835\udc56\ud835\udc57\u2190MLP\ud835\udf19(cdense\n\ud835\udc56\n\u2296cdense\n\ud835\udc57\n)\n\u22b2Dense Signal\n15: end for\n16: \u22b2Perform message passing through the graph.\n17: for each layer \ud835\udc58= 1, . . . , \ud835\udc3edo\n18:\nfor each chunk \ud835\udc50\ud835\udc56\u2208C do\n19:\n\u22b2Compute messages from neighbors and itself.\n20:\nm(\ud835\udc58)\n\ud835\udc57\n\u2190MSG(\ud835\udc58) \u0000\ud835\udeff\ud835\udc5e\ud835\udc57\u00b7 \ud835\udefc\ud835\udc56\ud835\udc57\u00b7 h(\ud835\udc58)\n\ud835\udc57\n\u0001\u2200\ud835\udc57\u2208{\ud835\udc56} \u222aN (\ud835\udc56)\n21:\n\u22b2Aggregate messages.\n22:\nh(\ud835\udc58+1)\n\ud835\udc56\n\u2190AGG(\ud835\udc58) \u0000{m(\ud835\udc58)\n\ud835\udc57\n| \ud835\udc57\u2208{\ud835\udc56} \u222aN (\ud835\udc56)}\u0001\n23:\nend for\n24: end for\n25: \u22b2Compute relevance scores with entangled representations.\n26: for each chunk \ud835\udc50\ud835\udc56\u2208C do\n27:\n\ud835\udc60(\ud835\udc5e,\ud835\udc50\ud835\udc56) \u2190\ud835\udc53dense(qdense, h(\ud835\udc3e)\n\ud835\udc56\n)\n28: end for\n29: \u22b2Select top-relevant chunks and construct subgraphs.\n30: \ud835\udc46( \u00afG;\ud835\udc5e) \u2190Top-\ud835\udc41contextual subgraphs based on \ud835\udc60(\ud835\udc5e,\ud835\udc50\ud835\udc56)\n31: \u22b2Return the retrieved subgraphs.\n32: return \ud835\udc46( \u00afG;\ud835\udc5e)\nThe entangled representation of \ud835\udc50\ud835\udc56is obtained through a graph\nencoder, such as a GNN, which aggregates information based on\nthe contextual graph:\nH\ud835\udc56= \ud835\udc54\u03a6({h(0)\n\ud835\udc57\n,\ud835\udeff\ud835\udc5e\ud835\udc57, \ud835\udefc\ud835\udc56\ud835\udc57| \ud835\udc57\u2208{\ud835\udc56} \u222aN (\ud835\udc56)}),\n(4)\n\nCG-RAG: Research Question Answering by Citation Graph Retrieval-Augmented LLMs\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nwhere \u03a6 denotes the parameters of the graph encoder, and h(0)\n\ud835\udc57\n=\ncdense\n\ud835\udc57\nis the initial dense representation of \ud835\udc50\ud835\udc57. The terms \ud835\udeff\ud835\udc5e\ud835\udc57and\n\ud835\udefc\ud835\udc56\ud835\udc57control the message passing, ensuring that only relevant infor-\nmation is propagated. Specifically, the message contribution from a\nneighboring chunk \ud835\udc50\ud835\udc57to \ud835\udc50\ud835\udc56and the subsequent update are defined\nas:\nm(\ud835\udc58)\n\ud835\udc57\n= MSG(\ud835\udc58) \u0010\n\ud835\udeff\ud835\udc5e\ud835\udc57\u00b7 \ud835\udefc\ud835\udc56\ud835\udc57\u00b7 h(\ud835\udc58)\n\ud835\udc57\n\u0011\n,\n(5)\nh(\ud835\udc58+1)\n\ud835\udc56\n= AGG(\ud835\udc58) \u0010\n{m(\ud835\udc58)\n\ud835\udc57\n| \ud835\udc57\u2208{\ud835\udc56} \u222aN (\ud835\udc56)}\n\u0011\n,\n(6)\nwhere h(\ud835\udc58)\n\ud835\udc57\nis the representation of \ud835\udc50\ud835\udc57at layer \ud835\udc58. The term \ud835\udeff\ud835\udc5e\ud835\udc57eval-\nuates the relevance between the query and the context, while \ud835\udefc\ud835\udc56\ud835\udc57\nmeasures the relevance between the central chunk \ud835\udc50\ud835\udc56and its neigh-\nboring chunks \ud835\udc50\ud835\udc57. Specifically, \ud835\udeff\ud835\udc5e\ud835\udc56, the sparse relevance between a\nquery \ud835\udc5eand a chunk \ud835\udc50\ud835\udc56, is computed as:\n\ud835\udeff\ud835\udc5e\ud835\udc56= \ud835\udc53sparse(qsparse\n\ud835\udc56\n, csparse\n\ud835\udc56\n).\n(7)\nwhere \ud835\udc53sparse indicates a relevance scoring function used in sparse\nretrieval such as cosine similarity and dot product. This incorpo-\nrates sparse relevance into the dense embedding during message\npassing. To model the dense interaction between \ud835\udc50\ud835\udc56and its neigh-\nboring chunks \ud835\udc50\ud835\udc57\u2208N (\ud835\udc56), \ud835\udefc\ud835\udc56\ud835\udc57is calculated as:\n\ud835\udefc\ud835\udc56\ud835\udc57= MLP\ud835\udf19(cdense\n\ud835\udc56\n\u2296cdense\n\ud835\udc57\n),\n(8)\nwhere \ud835\udefc\ud835\udc56\ud835\udc56is defined as 1, \u2296represents element-wise subtraction\nto compute the feature difference, and MLP\ud835\udf19parameterized by \ud835\udf19,\nadaptively assesses the relevance between chunks.\nThis entangled representation framework integrates sparse and\ndense features while leveraging structural information from the\ngraph context, ensuring that each chunk\u2019s representation reflects\nboth its intrinsic relevance and its contextual relationships.\nGiven a query \ud835\udc5eover the chunk set C of a citation graph G, the\nrelevance scoring function \ud835\udc53entangled(\ud835\udc5e,\ud835\udc50\ud835\udc56) : Q \u00d7 C \u2192R is:\n\ud835\udc60(\ud835\udc5e,\ud835\udc50\ud835\udc56) = \ud835\udc53dense(qdense, H\ud835\udc56),\n(9)\nwhere \ud835\udc53dense indicates a relevance scoring function used in dense\nretrieval, qdense represents the dense vector of the query, and H\ud835\udc56\ndenotes the entangled representation of the chunk \ud835\udc50\ud835\udc56. The overall\nalgorithm is depicted in Algorithm 1.\nProposition 4.1 (LeSeGR Generality). Post-retrieval methods\nwith the metric \ud835\udc53hybrid : \ud835\udc53sparse\n\u00c9 \ud835\udc53dense \u2192R, represent a special\ncase of the proposed Lexical-Semantic Graph Retrieval (LeSeGR). This\nholds when no additional relevant contextual information exists for\nany chunk in the citation graph, reducing LeSeGR to existing hybrid\nretrieval used post-retrieval fusion.\nProof. The entangled representation of a chunk \ud835\udc50\ud835\udc56is given by:\nH\ud835\udc56= AGG({h(\ud835\udc58)\n\ud835\udc57\n| \ud835\udc57\u2208{\ud835\udc56} \u222aN (\ud835\udc56)}) = AGG(\ud835\udf06\ud835\udc56m(\ud835\udc58)\n\ud835\udc56\n, \ud835\udf06Nm(\ud835\udc58)\nN ),\n(10)\nwhere m(\ud835\udc58)\n\ud835\udc56\nand m(\ud835\udc58)\nN denote messages from the central chunk \ud835\udc50\ud835\udc56\nand its neighboring chunks at layer \ud835\udc58, respectively. The weights \ud835\udf06\ud835\udc56\nand \ud835\udf06N are typically equal and defined as\n1\n|N(\ud835\udc56) |+1.\nWhen the AGG function is defined as either mean or sum, both\nof which satisfy the distributive law, the relevance score \ud835\udc60(\ud835\udc5e,\ud835\udc50\ud835\udc56), as\nFigure 2: Overview of Contextualized Graph Retrieval-\nAugmented Generation.\ndefined in Equation 9, can be expanded as:\n\ud835\udc60(\ud835\udc5e,\ud835\udc50\ud835\udc56) = \ud835\udc53dense(qdense, AGG(\ud835\udf06\ud835\udc56m(\ud835\udc58)\n\ud835\udc56\n, \ud835\udf06Nm(\ud835\udc58)\nN ))\n(11)\n\u221d\ud835\udf06\ud835\udc56\ud835\udc53dense(qdense, m(\ud835\udc58)\n\ud835\udc56\n) + \ud835\udf06N\ud835\udc53dense(qdense, m(\ud835\udc58)\nN ).\n(12)\nWhen no relevant neighbors exist (i.e., N (\ud835\udc56) = \u2205), the second term\nvanishes, leaving:\n\ud835\udc60(\ud835\udc5e,\ud835\udc50\ud835\udc56) \u221d\ud835\udf06\ud835\udc56\ud835\udc53dense(qdense, m(\ud835\udc58)\n\ud835\udc56\n) = \ud835\udf06\ud835\udc56\ud835\udeff\ud835\udc5e\ud835\udc56\ud835\udc53dense(qdense, h(\ud835\udc58)\n\ud835\udc56\n). (13)\nSubstituting h(\ud835\udc58)\n\ud835\udc56\n= cdense\n\ud835\udc56\n, the relevance score simplifies to:\n\ud835\udc60(\ud835\udc5e,\ud835\udc50\ud835\udc56) \u221d\ud835\udeff\ud835\udc5e\ud835\udc56\ud835\udc53dense(qdense, cdense\n\ud835\udc56\n).\n(14)\nTaking the logarithm for further analysis:\nlog(\ud835\udc60(\ud835\udc5e,\ud835\udc50\ud835\udc56)) \u221dlog(\ud835\udeff\ud835\udc5e\ud835\udc56) + log(\ud835\udc53dense(qdense, cdense\n\ud835\udc56\n)).\n(15)\nSince \ud835\udeff\ud835\udc5e\ud835\udc56= \ud835\udc53sparse(qsparse, csparse\n\ud835\udc56\n), this becomes:\nlog(\ud835\udc60(\ud835\udc5e,\ud835\udc50\ud835\udc56)) \u221dlog(\ud835\udc53sparse(qsparse, csparse\n\ud835\udc56\n))\n(16)\n+ log(\ud835\udc53dense(qdense, cdense\n\ud835\udc56\n)),\n(17)\nwhere the use of log(\u00b7) requires \ud835\udc53sparse and \ud835\udc53dense to be mapped\nto R+. This requirement can be fulfilled by applying appropriate\nactivation functions or transformations to transform them from R\ninto the non-negative domain. In this case, the relevance score is\nequivalent to the additive fusion of sparse and dense relevance, as\nused in post-retrieval hybrid methods. Hence, when no contextual\ninformation exists (N (\ud835\udc56) = \u2205), our graph-contextualized retrieval\nreduces to the post-retrieval fusion paradigm, demonstrating that\nthe latter is a specific instance of the former.\n\u25a1\nWhen relevant graph contexts are present, the entangled frame-\nwork dynamically propagates and aggregates sparse and dense sig-\nnals through structural relationships among neighboring chunks.\nThis enables the model to capture relational dependencies and\nmulti-hop connections in graphs, enhancing retrieval accuracy and\neffectively utilizing sparse and dense signals from neighbors.\n4.4\nContextualized Graph Retrieval-Augmented\nGeneration (CG-RAG)\nGiven a query \ud835\udc5eon a citation graph G, we use LeSeGR to retrieve\nthe top \ud835\udc41chunks that are most relevant to the question. These\nretrieved chunks, together with their contextual subgraph, are then\n\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nYuntong et al.\nused to generate the answer, as illustrated in Figure 2. Specifically,\nfor each selected chunk, its corresponding contextual subgraph \u00afG\ud835\udc56\nis retained for the generation phase, where \u00afG\ud835\udc56= \u00afG[{\ud835\udc56} \u222aN (\ud835\udc56)]\nrepresents the induced subgraph consisting of chunk \ud835\udc50\ud835\udc56and its\ndirect neighbors. Formally, the set of contextual subgraphs for the\nTop-\ud835\udc41chunks is defined as:\n\ud835\udc46( \u00afG;\ud835\udc5e) =\n\u00d8\n\ud835\udc50\ud835\udc56\u2208Top-\ud835\udc41(\ud835\udc5e,C)\n\u00afG\ud835\udc56,\n(18)\nwhere Top-\ud835\udc41(\ud835\udc5e,\ud835\udc36) represents the Top-\ud835\udc41chunks ranked by \ud835\udc60(\ud835\udc5e,\ud835\udc50\ud835\udc56).\nThis ensures the retrieval retains both the most relevant chunks\nand their graph context for downstream tasks.\nTo effectively utilize the contextual information of each retrieved\nchunk and adapt to various LLMs, including open-source models\nsuch as LLaMA and closed-source models such as ChatGPT, we\nfirst summarize the graph context and then concatenate this sum-\nmarized context with the central chunks to enhance generation.\nSpecifically, for each contextual subgraph \u00afG\ud835\udc56\u2208\ud835\udc46( \u00afG;\ud835\udc5e), we prompt\nthe LLM to summarize the contextual information surrounding \ud835\udc50\ud835\udc56.\nThe summarized context is concatenated with the query to form\nthe final input for generating the answer. The generation process\nover the citation graph G is formally defined as:\n\ud835\udc5d\ud835\udf03(\ud835\udc4c|\ud835\udc5e, G) = arg max\n\ud835\udf03\n\ud835\udc5b\n\u00d6\n\ud835\udc56=1\n\ud835\udc5d\ud835\udf03(\ud835\udc66\ud835\udc56|\ud835\udc66<\ud835\udc56,\ud835\udc4b\ud835\udc5e,\ud835\udc4bC),\n(19)\nwhere \ud835\udf03represents the LLM parameters, \ud835\udc4b\ud835\udc5e= TextEmbedder(\ud835\udc5e) is\nthe query embedding, and \ud835\udc4bC is the context embedding:\n\ud835\udc4bC = TextEmbedder\n\u0010\u0002\nSummarize(\ud835\udc5e, \u00afG\ud835\udc56)\n\u0003\n\u00afG\ud835\udc56\u2208\ud835\udc46( \u00afG;\ud835\udc5e)\n\u0011\n,\n(20)\nrepresenting the embeddings of the concatenated summarized con-\ntexts from the retrieved contextual subgraphs. The summarization\nis performed by the LLM itself, extracting relevant information\nfrom the contextual subgraph to aid in generating the final answer.\n5\nEXPERIMENT\nWe conduct experiments to evaluate the effectiveness (Section 5.2)\nand efficiency (Section 5.3) of Contextualized Graph RAG, along\nwith an analysis of the individual contributions of our technical\ndesigns (Section 5.4).\n5.1\nSettings\nDatasets.\nOur experiments utilize two datasets: PubMedQA-1k\nand PapersWithCodeQA. PubMedQA-1k is a publicly available\ndataset, introduced by Jin et al., and comprises 1,000 question-\nanswer pairs designed for PubMed literature 1, with human-labeled\ngold-standard retrieval and answer annotations. The original dataset,\nhowever, lacks citation information between papers, which we ad-\ndressed by extracting the references for each paper and constructing\na citation graph database with a total of 7,849 papers.\nThe PapersWithCodeQA dataset was collected from the Paper-\nsWithCode website2, which tracks research papers across various\ncomputer science fields. We used 84 leaderboards3 spanning diverse\ndomains, including Computer Vision, Natural Language Processing,\n1https://pubmed.ncbi.nlm.nih.gov/\n2https://paperswithcode.com/\n3https://paperswithcode.com/sota\nTable 1: Example question-answering pairs and correspond-\ning evaluation metrics.\nTrue or False:\nDo mitochondria play a role in remodelling lace\nplant leaves during programmed cell death? Yes,\nNo or Maybe.\nAnswer:\nYes\nMetrics:\nAccuracy (Acc), \ud835\udc391 Score.\nMultiple Choice:\nWhich model achieves state-of-the-art\u2217perfor-\nmance on the ADE20K dataset for semantic seg-\nmentation? (a) BEiT-3 (b) DINOv2 (c) ONE-PEACE\n(d) EVA\nAnswer:\n(c)\nMetrics:\nMean Reciprocal Rank (MRR), Hit@k.\nEssay:\nCould you provide an overview of the model devel-\nopment for semantic segmentation?\nAnswer:\n... Early models like FCN (Fully Convolutional Net-\nworks) laid the foundation by adapting classifi-\ncation networks for pixel-level predictions ... Re-\ncently, ONE-PEACE emerged as a state-of-the-art\nmodel ...\nMetrics:\nCoherence, Consistency, Relevance\n\u2217Within the citation graph we collected.\nMedical, and Graphs. For each leaderboard, we extracted the top 20\npapers\u2019 contents and references from arXiv4 to construct a graph\ndatabase. The LaTeX content of each paper was preserved as its\ntextual attributes. The dataset comprises 12,171 papers, from which\nwe also crafted 924 questions centered on leaderboard analysis,\nwith ground truth answers derived directly from the leaderboards.\nThese include 420 True/False questions, 420 multiple-choice ques-\ntions, and 84 generative questions. For generative questions, we\nfirst provide the LLMs with the most relevant contexts labeled by\nhumans and allow the LLMs to generate answers. We then evalu-\nate the quality of retrieval-augmented generation by replacing the\nhuman-selected contexts with those retrieved by different retrieval\nmethods.\nMetrics.\nTo comprehensively evaluate the performance of RAG\nsystems in retrieving relevant information and generating accurate,\ncontextually appropriate answers, we employ distinct metrics for\nretrieval and question answering. For retrieval, we use Hit@1 and\nHit@3, which measure the proportion of queries where the cor-\nrect chunk is ranked within the top-1 and top-3 retrieved results,\nrespectively.\nFor research question answering, example questions and used\nevaluation metrics are presented in Table 1. For multiple-choice\nquestions, we use Mean Reciprocal Rank (MRR) and Hit@k to assess\nranking quality. For True/False questions, Accuracy (Acc) and \ud835\udc391\nscore evaluate classification performance. For generative tasks, we\nleverage the UniEval model [49] to assess Coherence, Consistency,\nand Relevance, which evaluate logical flow, factual accuracy, and\ntopical alignment, respectively. All metrics adhere to the principle\nthat higher values indicate better model performance.\nImplementations.\nExperiments are conducted using two NVIDIA\nA10 GPUs, with Graph Transformer [33] serving as the graph en-\ncoder. The configuration includes two layers, each featuring four\n4https://arxiv.org/\n\nCG-RAG: Research Question Answering by Citation Graph Retrieval-Augmented LLMs\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nTable 2: Evaluation of the retrieval-augmented research question answering. The best performance is highlighted in BOLD,\nwhile the second-best performance is underlined. Performance of our methods is highlighted .\nPapersWithCodeQA\nPubMedQA\nCategory\nMethod\nAcc\n\ud835\udc391\nMRR\nHit@1\nCoherence\nConsistency\nRelevance\nAcc\n\ud835\udc391\nSparse\nBM25\n0.689\n0.617\n0.765\n0.736\n0.905\n0.858\n0.859\n0.662\n0.604\nDoc2Query\n0.705\n0.629\n0.748\n0.731\n0.914\n0.833\n0.852\n0.684\n0.614\nBGE-M3\n0.751\n0.648\n0.810\n0.787\n0.934\n0.876\n0.863\n0.722\n0.644\nDense\nMiniLM\n0.730\n0.644\n0.782\n0.758\n0.919\n0.872\n0.828\n0.712\n0.641\nLaBSE\n0.591\n0.552\n0.677\n0.643\n0.875\n0.545\n0.616\n0.403\n0.396\nmContriever\n0.523\n0.531\n0.647\n0.613\n0.863\n0.469\n0.438\n0.288\n0.271\nE5\n0.579\n0.560\n0.659\n0.628\n0.872\n0.521\n0.544\n0.363\n0.360\nSPAR\n0.611\n0.583\n0.643\n0.609\n0.886\n0.549\n0.537\n0.392\n0.384\nHybrid\nScore Fusion\n0.739\n0.656\n0.774\n0.749\n0.908\n0.891\n0.887\n0.674\n0.613\nColBERT\n0.769\n0.661\n0.827\n0.778\n0.927\n0.884\n0.874\n0.724\n0.642\nCLEAR\n0.618\n0.575\n0.667\n0.643\n0.894\n0.623\n0.685\n0.468\n0.456\nLeSeGR (Ours)\n0.835\n0.703\n0.884\n0.827\n0.956\n0.921\n0.914\n0.778\n0.685\nattention heads and a hidden dimension size of 1024. The maximum\nchunk length is set to 8,192 tokens. Training is conducted using\nCrossEntropy, with 10% of the samples labeled with gold-standard\nretrieval, and optimized using the AdamW optimizer [23]. The rele-\nvance scoring function for both dense and sparse representations is\ndot product. For generation, GPT-4 is employed through the Ope-\nnAI API, specifically leveraging the gpt-4o-2024-05-13 model\nversion.\nBaseline methods.\nTo evaluate the proposed graph-contextualized\nretrieval method, we benchmark it against several state-of-the-art\nbaselines renowned for their effectiveness in the retrieval phase\nacross various RAG systems. These retrieval techniques are catego-\nrized into three groups: sparse retrieval, dense retrieval, and hybrid\nretrieval.\n\u2022 Sparse Retrieval: BM25 [31], an advanced refinement of TF-IDF\n[32], enhances relevance scoring by incorporating probabilistic\nmodeling, term saturation, and document length normalization,\nproviding robust performance in keyword-based retrieval tasks.\nDoc2Query [28] improves sparse retrieval by generating synthetic\nqueries for documents using a pre-trained language model (PLM).\nBGE-M3 [3] utilizes a multi-vector architecture to create robust\nrepresentations, integrating contrastive learning and knowledge\ndistillation techniques.\n\u2022 Dense Retrieval: MiniLM [41] is a lightweight transformer\nmodel that uses deep self-attention distillation to create dense em-\nbeddings. LaBSE [7] is a bilingual embedding model, leveraging\ndual encoders and a large-scale parallel corpus to ensure semantic\nalignment across languages. mContriever [14] employs unsuper-\nvised contrastive learning to train dense retrievers, focusing on\nencoding diverse and nuanced contextual information. E5 [40]\noptimizes embeddings for text retrieval by integrating explicit\nsupervision from retrieval datasets and task-specific fine-tuning.\nSPAR [4] employs salient phrase representation learning to bridge\ndense and sparse retrieval, utilizing a dual encoder architecture\nthat explicitly models both phrase-level and document-level se-\nmantics.\n\u2022 Hybrid Retrieval: ScoreFusion [19] combines the output scores\nof sparse and dense retrieval models to produce a unified ranking.\nColBERT [17] introduces late interaction to compute pairwise\nterm similarities between query and document embeddings, en-\nabling efficient and fine-grained integration of sparse and dense\nsignals. CLEAR [8] employs a residual learning framework to\ncombine sparse and dense representations, ensuring complemen-\ntary signals are utilized for improved retrieval performance.\n5.2\nMain Results\nOur proposed Contextualized Graph Retrieval-Augmented Genera-\ntion with LeSeGR achieves state-of-the-art performance across all\ntasks and datasets, as shown in Table 2. For true/false questions,\nLeSeGR significantly surpasses sparse, dense, and hybrid baselines\nin both accuracy (Acc) and \ud835\udc391 scores, demonstrating its capability\nto effectively capture domain-specific terms and semantic nuances.\nHybrid methods such as ScoreFusion combine sparse and dense\nsignals but fail to achieve the deeper integration of retrieval sig-\nnals offered by LeSeGR. Through its graph-structured integration,\nLeSeGR dynamically propagates and entangles retrieval signals\nfrom contextual information, fully leveraging the relationships em-\nbedded in the graph structure. This advanced integration translates\nto superior performance, particularly in metrics such as MRR and\nHit@1.\nIn generative tasks, our method demonstrates significant im-\nprovement in Coherence, Consistency, and Relevance by leveraging\nits entangled sparse-dense representation and graph-based con-\ntextualization. Unlike hybrid baselines such as ColBERT, which\nemphasizes token-level interactions but overlooks graph-level rela-\ntionships, LeSeGR\u2019s graph encoder dynamically aggregates signals\nacross interconnected chunks. This enhances contextual under-\nstanding, enabling high-quality and contextually rich text gener-\nation. For instance, LeSeGR achieves a Coherence score of 0.956\non PapersWithCodeQA, outperforming ColBERT\u2019s 0.927. These\nresults underscore the unique strengths of LeSeGR in effectively\n\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nYuntong et al.\nTable 3: Evaluation of the retrieval effectiveness on the cita-\ntion graph of PubMed (PubMedQA). The best performance is\nhighlighted in BOLD, while the second-best performance is\nunderlined.\nCategory\nMethod\nHit@1\nHit@3\nSparse\nBM25\n0.835\n0.912\nDoc2Query\n0.832\n0.930\nBGE-M3\n0.915\n0.960\nDense\nMiniLM\n0.887\n0.945\nLaBSE\n0.305\n0.471\nmContriever\n0.472\n0.496\nE5\n0.231\n0.355\nSPAR\n0.256\n0.385\nHybrid\nScore Fusion\n0.829\n0.925\nColBERT\n0.913\n0.968\nCLEAR\n0.470\n0.612\nLeSeGR (Ours)\n0.961\n0.987\nbridging sparse and dense retrieval with graph-based contextu-\nalization, thereby advancing the retrieval-augmented generation\nprocess to new levels of effectiveness.\nTable 3 demonstrates that LeSeGR significantly outperforms all\nbaselines in the retrieval phase, including sparse, dense, and hybrid\napproaches. Our method achieves superior retrieval accuracy by\neffectively entangling sparse and dense signals within the graph\nstructure, allowing contextual information to enhance relevance\nscoring. Unlike post-retrieval fusion methods, such as ScoreFusion,\nwhich combine sparse and dense signals after separate retrieval\nprocesses, our approach dynamically integrates these signals dur-\ning retrieval, leading to more coherent and effective utilization of\nboth lexical and semantic information. Furthermore, while Col-\nBERT performs token-level interactions for fine-grained relevance,\nit operates at the query-document level without fully leveraging\nthe structural relationships present in citation graphs. In contrast,\nour method extends relevance computation to the graph structure,\npropagating and aggregating signals across related chunks to cap-\nture multi-hop and relational dependencies. This deeper integration\nof graph context and entangled sparse-dense signals enables our\nmethod to outperform ColBERT, achieving the highest Hit@1 and\nHit@3 scores.\n5.3\nEfficiency Analysis\nAs shown in Table 4, LeSeGR demonstrates competitive retrieval\nefficiency on the citation graph of arXiv (PapersWithCodeQA). It\nstrikes a balance between memory usage and latency, leveraging\nGPU computation effectively. LeSeGR achieves faster query latency\n(403.94 ms) compared to ColBERT (561.91 ms) while maintaining\na moderate GPU memory footprint (1,921 MB). In contrast, Score-\nFusion exhibits high CPU memory usage (5,655 MB) and slower\nquery speeds, whereas LeSeGR optimizes GPU utilization by inte-\ngrating both sparse and dense retrieval signals into the message\npassing process of the graph encoder. Additionally, LeSeGR outper-\nforms CLEAR in query speed while maintaining similar memory\nusage. These results highlight LeSeGR\u2019s efficiency and scalability\nfor large-scale graph-based retrieval tasks without compromising\neffectiveness.\nTable 4: Evaluation of the retrieval efficiency on the citation\ngraph of arXiv (PapersWithCodeQA).\nMethod\nCPU & GPU Memory\nIndexing & Query Latency\n(MB)\n(MB)\n(ms)\n(ms)\nScore Fusion\n5,655\n770\n43.94\n1,580.14\nColBERT\n0\n12,674\n12.40\n561.91\nCLEAR\n0\n1,538\n205.36\n16.07\nLeSeGR\n0\n1,921\n19.22\n403.94\n5.4\nAblation Studies\nThe ablation studies performed for each influential factor in LeSeGR\nis shown in Table 5. In this experiment, we evaluate LeSeGR on the\ncitation graph of PubMed, which contains 7,849 papers. Our main\nobservations are as follows:\nTable 5: Ablation studies on PubMedQA. The default settings\nof LeSeGR are marked with *.\nFactor\nSetting\nHit@1\nHit@3\nGraph\nEncoder\nGAT\n0.939\n0.968\nGCN\n0.955\n0.976\nGraph Transformer\u2217\n0.961\n0.987\nTop-\ud835\udc5b\nContext\n2\n0.931\n0.949\n8\n0.950\n0.984\n4\u2217\n0.961\n0.987\nSparse\nSignal\nTF-IDF\n0.903\n0.962\nBM25\n0.919\n0.962\nDoc2Query\n0.926\n0.964\nBGE-M3\u2217\n0.961\n0.987\nDense\nSignal\nE5\n0.676\n0.765\nmContriever\n0.838\n0.883\nMiniLM\u2217\n0.961\n0.987\n\u2022 Graph Encoder.\nAmong Graph Attention Networks (GAT)\n[37], Graph Convolutional Networks (GCN) [18], and Graph\nTransformer [33], Graph Transformer achieves the highest Hit@1\nand Hit@3 scores of 0.961 and 0.987, respectively. Notably, regard-\nless of which graph encoder is employed, our LeSeGR method\nstill consistently achieves the best retrieval performance when\ncompared to the baselines in Table 3. This underscores LeSeGR\u2019s\nsuperior ability to model complex relationships and effectively\naggregate contextual information for retrieval.\n\u2022 Top-\ud835\udc5bContext.\nWe further assess three configurations for the\nnumber of contexts connected via inter-document edges, i.e.,\nTop-\ud835\udc5b. The \ud835\udc5b= 4 setting achieves the best results, striking a bal-\nance between sufficient contextual inclusion and noise reduction.\nSmaller values, such as \ud835\udc5b= 2, restrict the scope of context, while\nlarger values, such as \ud835\udc5b= 8, may introduce irrelevant informa-\ntion, diluting the positive impact of relevant context and reducing\nretrieval effectiveness. It is anticipated that if the chunk length\n\nCG-RAG: Research Question Answering by Citation Graph Retrieval-Augmented LLMs\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nis sufficiently large, retaining only the top-1 relevant chunk will\nsuffice.\n\u2022 Retrieval Signals.\nWe compare four sparse representation\nmethods. BGE-M3 outperforms other sparse encoders, achieving\nthe highest scores, as its ability to integrate lexical and semantic\nfeatures is critical for domain-specific term matching. TF-IDF and\nBM25, while strong in lexical precision, lack semantic adaptability.\nIn addition, we also compare three dense representation methods,\nwith MiniLM delivering the best performance. MiniLM\u2019s compact\nrepresentation effectively captures semantic nuances, making\nit better suited for diverse queries and documents. As a whole,\nhowever, combining two expressive retrieval models with our\nLeSeGR framework results in stronger retrieval performance.\nNotably, the performance of LeSeGR appears to be primarily\nconstrained by the quality of the dense retrieval signal.\n6\nCONCLUSION\nIn this work, we introduce Lexical-Semantic Graph Retrieval (LeSeGR),\na novel framework that integrates sparse, dense, and graph-structured\nretrieval signals for complex and structured database. Based on\nLeSeGR, we present Contextualized Graph Retrieval-Augmented\nGeneration (CG-RAG) for research question answering. By leverag-\ning a contextual citation graph, our approach effectively captures\nintra- and inter-document relationships, enabling a dynamic prop-\nagation of contextual information through an entangled hybrid\nretrieval paradigm. This paradigm bridges lexical precision and se-\nmantic understanding while generalizing to existing retrieval meth-\nods. Furthermore, CG-RAG incorporates a graph-aware generation\nstrategy, enhancing the contextual richness of generated responses.\nExtensive experiments across multiple citation networks demon-\nstrate the superior performance of CG-RAG based on LeSeGR,\nachieving state-of-the-art results in retrieval metrics such as Hit@1\nand generation metrics such as Coherence and Relevance. Our find-\nings underscore the effectiveness of graph-contextualized represen-\ntations in advancing the capabilities of retrieval-augmented gen-\neration for citation graphs, setting a new benchmark for retrieval-\naugmented research question answering.\nREFERENCES\n[1] S\u00f6ren Auer, Allard Oelen, Muhammad Haris, Markus Stocker, Jennifer D\u2019Souza,\nKheir Eddine Farfar, Lars Vogt, Manuel Prinz, Vitalis Wiens, and Mohamad Yaser\nJaradeh. 2020. Improving access to scientific literature with knowledge graphs.\nBibliothek Forschung und Praxis 44, 3 (2020), 516\u2013529.\n[2] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao\nChen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2024. A survey on\nevaluation of large language models. ACM Transactions on Intelligent Systems\nand Technology 15, 3 (2024), 1\u201345.\n[3] Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024.\nBge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text\nembeddings through self-knowledge distillation. arXiv preprint arXiv:2402.03216\n(2024).\n[4] Xilun Chen, Kushal Lakhotia, Barlas O\u011fuz, Anchit Gupta, Patrick Lewis, Stan\nPeshterliev, Yashar Mehdad, Sonal Gupta, and Wen-tau Yih. 2021. Salient phrase\naware dense retrieval: can a dense retriever imitate a sparse one? arXiv preprint\narXiv:2110.06918 (2021).\n[5] Jacob Devlin. 2018. Bert: Pre-training of deep bidirectional transformers for\nlanguage understanding. arXiv preprint arXiv:1810.04805 (2018).\n[6] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva\nMody, Steven Truitt, and Jonathan Larson. 2024. From local to global: A graph\nrag approach to query-focused summarization. arXiv preprint arXiv:2404.16130\n(2024).\n[7] Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei\nWang. 2020. Language-agnostic BERT sentence embedding. arXiv preprint\narXiv:2007.01852 (2020).\n[8] Luyu Gao, Zhuyun Dai, Tongfei Chen, Zhen Fan, Benjamin Van Durme, and\nJamie Callan. 2021. Complement lexical retrieval model with semantic residual\nembeddings. In Advances in Information Retrieval: 43rd European Conference on\nIR Research, ECIR 2021, Virtual Event, March 28\u2013April 1, 2021, Proceedings, Part I\n43. Springer, 146\u2013160.\n[9] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai,\nJiawei Sun, and Haofen Wang. 2023. Retrieval-augmented generation for large\nlanguage models: A survey. arXiv preprint arXiv:2312.10997 (2023).\n[10] Hamed Babaei Giglou, Tilahun Abedissa Taffa, Rana Abdullah, Aida Usmanova,\nRicardo Usbeck, Jennifer D\u2019Souza, and S\u00f6ren Auer. 2024. Scholarly Question\nAnswering using Large Language Models in the NFDI4DataScience Gateway.\narXiv preprint arXiv:2406.07257 (2024).\n[11] Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V Chawla, Thomas Laurent, Yann\nLeCun, Xavier Bresson, and Bryan Hooi. 2024. G-retriever: Retrieval-augmented\ngeneration for textual graph understanding and question answering. arXiv\npreprint arXiv:2402.07630 (2024).\n[12] Yuntong Hu, Zhihan Lei, Zheng Zhang, Bo Pan, Chen Ling, and Liang Zhao. 2024.\nGRAG: Graph Retrieval-Augmented Generation. arXiv preprint arXiv:2405.16506\n(2024).\n[13] Yuntong Hu, Zheng Zhang, and Liang Zhao. 2023. Beyond Text: A Deep Dive into\nLarge Language Models\u2019 Ability on Understanding Graph Data. arXiv preprint\narXiv:2310.04944 (2023).\n[14] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo-\njanowski, Armand Joulin, and Edouard Grave. 2021. Unsupervised dense in-\nformation retrieval with contrastive learning. arXiv preprint arXiv:2112.09118\n(2021).\n[15] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, and Xinghua Lu.\n2019. Pubmedqa: A dataset for biomedical research question answering. arXiv\npreprint arXiv:1909.06146 (2019).\n[16] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey\nEdunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-\nDomain Question Answering. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP). 6769\u20136781.\n[17] Omar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage\nsearch via contextualized late interaction over bert. In Proceedings of the 43rd\nInternational ACM SIGIR conference on research and development in Information\nRetrieval. 39\u201348.\n[18] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph\nconvolutional networks. arXiv preprint arXiv:1609.02907 (2016).\n[19] Saar Kuzi, Mingyang Zhang, Cheng Li, Michael Bendersky, and Marc Najork. 2020.\nLeveraging semantic and lexical matching to improve the recall of document\nretrieval systems: A hybrid approach. arXiv preprint arXiv:2010.01195 (2020).\n[20] Jens Lehmann, Antonello Meloni, Enrico Motta, Francesco Osborne, Diego Re-\nforgiato Recupero, Angelo Antonio Salatino, and Sahar Vahdati. 2024. Large\nLanguage Models for Scientific Question Answering: An Extensive Analysis of\nthe SciQA Benchmark. In European Semantic Web Conference. Springer, 199\u2013217.\n[21] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nNaman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel,\net al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks.\nAdvances in Neural Information Processing Systems 33 (2020), 9459\u20139474.\n[22] Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu. 2022. A survey on\nretrieval-augmented text generation. arXiv preprint arXiv:2202.01110 (2022).\n[23] I Loshchilov. 2017. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101 (2017).\n[24] Alejandro Lozano, Scott L Fleming, Chia-Chun Chiang, and Nigam Shah. 2023.\nClinfo. ai: An open-source retrieval-augmented large language model system for\nanswering medical questions using scientific literature. In PACIFIC SYMPOSIUM\nON BIOCOMPUTING 2024. World Scientific, 8\u201323.\n[25] Yi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. 2021. Sparse,\ndense, and attentional representations for text retrieval. Transactions of the\nAssociation for Computational Linguistics 9 (2021), 329\u2013345.\n[26] Priyanka Mandikal and Raymond Mooney. 2024.\nSparse Meets Dense: A\nHybrid Approach to Enhance Scientific Document Retrieval. arXiv preprint\narXiv:2401.04055 (2024).\n[27] Kurnia Muludi, Kaira Milani Fitria, Joko Triloka, et al. 2024. Retrieval-Augmented\nGeneration Approach: Document Question Answering using Large Language\nModel. International Journal of Advanced Computer Science & Applications 15, 3\n(2024).\n[28] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document\nexpansion by query prediction. arXiv preprint arXiv:1904.08375 (2019).\n[29] V\u00edt Novotn`y and Michal Stef\u00e1nik. 2022. Combining Sparse and Dense Information\nRetrieval.. In CLEF (Working Notes). 104\u2013118.\n[30] N Reimers. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-\nNetworks. arXiv preprint arXiv:1908.10084 (2019).\n[31] Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance\nframework: BM25 and beyond. Foundations and Trends\u00ae in Information Retrieval\n3, 4 (2009), 333\u2013389.\n\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nYuntong et al.\n[32] Gerard Salton, Anita Wong, and Chung-Shu Yang. 1975. A vector space model\nfor automatic indexing. Commun. ACM 18, 11 (1975), 613\u2013620.\n[33] Yunsheng Shi, Zhengjie Huang, Shikun Feng, Hui Zhong, Wenjin Wang, and Yu\nSun. 2020. Masked label prediction: Unified message passing model for semi-\nsupervised classification. arXiv preprint arXiv:2009.03509 (2020).\n[34] Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kalu-\narachchi, Rajib Rana, and Suranga Nanayakkara. 2023. Improving the domain\nadaptation of retrieval augmented generation (RAG) models for open domain\nquestion answering. Transactions of the Association for Computational Linguistics\n11 (2023), 1\u201317.\n[35] Markus Stocker, Allard Oelen, Mohamad Yaser Jaradeh, Muhammad Haris,\nOmar Arab Oghli, Golsa Heidari, Hassan Hussein, Anna-Lena Lorenz, Salomon\nKabenamualu, Kheir Eddine Farfar, et al. 2023. FAIR scientific information with\nthe open research knowledge graph. FAIR Connect 1, 1 (2023), 19\u201321.\n[36] Tilahun Abedissa Taffa and Ricardo Usbeck. 2023. Leveraging LLMs in Scholarly\nKnowledge Graph Question Answering.. In QALD/SemREC@ ISWC.\n[37] Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro\nLio, and Yoshua Bengio. 2017.\nGraph attention networks.\narXiv preprint\narXiv:1710.10903 (2017).\n[38] Chengrui Wang, Qingqing Long, Xiao Meng, Xunxin Cai, Chengjun Wu, Zhen\nMeng, Xuezhi Wang, and Yuanchun Zhou. 2024. BioRAG: A RAG-LLM Frame-\nwork for Biological Question Reasoning. arXiv preprint arXiv:2408.01107 (2024).\n[39] Haiwen Wang, Le Zhou, Weinan Zhang, and Xinbing Wang. 2021. LiteratureQA:\nA Qestion Answering Corpus with Graph Knowledge on Academic Literature. In\nProceedings of the 30th ACM International Conference on Information & Knowledge\nManagement. 4623\u20134632.\n[40] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang,\nRangan Majumder, and Furu Wei. 2022. Text embeddings by weakly-supervised\ncontrastive pre-training. arXiv preprint arXiv:2212.03533 (2022).\n[41] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou.\n2020. Minilm: Deep self-attention distillation for task-agnostic compression of\npre-trained transformers. Advances in Neural Information Processing Systems 33\n(2020), 5776\u20135788.\n[42] Yuhao Wang, Ruiyang Ren, Junyi Li, Wayne Xin Zhao, Jing Liu, and Ji-Rong\nWen. 2024. REAR: A Relevance-Aware Retrieval-Augmented Framework for\nOpen-Domain Question Answering. arXiv preprint arXiv:2402.17497 (2024).\n[43] Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, and\nJian-Yun Nie. 2024. C-pack: Packed resources for general chinese embeddings.\nIn Proceedings of the 47th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval. 641\u2013649.\n[44] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett,\nJunaid Ahmed, and Arnold Overwijk. 2020. Approximate nearest neighbor nega-\ntive contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808\n(2020).\n[45] Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu. 2024.\nEvaluation of Retrieval-Augmented Generation: A Survey.\narXiv preprint\narXiv:2405.07437 (2024).\n[46] Cyril Zakka, Rohan Shad, Akash Chaurasia, Alex R Dalal, Jennifer L Kim, Michael\nMoor, Robyn Fong, Curran Phillips, Kevin Alexander, Euan Ashley, et al. 2024.\nAlmanac\u2014retrieval-augmented language models for clinical medicine. NEJM AI\n1, 2 (2024), AIoa2300068.\n[47] Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng,\nFangcheng Fu, Ling Yang, Wentao Zhang, and Bin Cui. 2024. Retrieval-augmented\ngeneration for ai-generated content: A survey. arXiv preprint arXiv:2402.19473\n(2024).\n[48] Wayne Xin Zhao, Jing Liu, Ruiyang Ren, and Ji-Rong Wen. 2024. Dense text\nretrieval based on pretrained language models: A survey. ACM Transactions on\nInformation Systems 42, 4 (2024), 1\u201360.\n[49] Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang\nZhu, Heng Ji, and Jiawei Han. 2022. Towards a unified multi-dimensional evalua-\ntor for text generation. arXiv preprint arXiv:2210.07197 (2022).\nReceived 20 February 2007; revised 12 March 2009; accepted 5 June 2009\n"}