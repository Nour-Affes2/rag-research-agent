{"metadata": {"pdf_filename": "2506.20051v1__Controlled Retrieval-augmented Context Evaluation for Long-form RAG.pdf", "source": "arXiv"}, "text": "arXiv:2506.20051v1  [cs.IR]  24 Jun 2025\nControlled Retrieval-augmented Context Evaluation for Long-form RAG\nJia-Huei Ju1\nSuzan Verberne2\nMaarten de Rijke1\nAndrew Yates3\n1University of Amsterdam\n2Leiden University\n3Johns Hopkins University\nAbstract\nRetrieval-augmented generation (RAG) en-\nhances large language models by incorporat-\ning context retrieved from external knowledge\nsources. While the effectiveness of the retrieval\nmodule is typically evaluated with relevance-\nbased ranking metrics, such metrics may be\ninsufficient to reflect the retrieval\u2019s impact on\nthe final RAG result, especially in long-form\ngeneration scenarios. We argue that provid-\ning a comprehensive retrieval-augmented con-\ntext is important for long-form RAG tasks like\nreport generation and propose metrics for as-\nsessing the context independent of generation.\nWe introduce CRUX, a Controlled Retrieval-\naUgmented conteXt evaluation framework de-\nsigned to directly assess retrieval-augmented\ncontexts. This framework uses human-written\nsummaries to control the information scope of\nknowledge, enabling us to measure how well\nthe context covers information essential for\nlong-form generation. CRUX uses question-\nbased evaluation to assess RAG\u2019s retrieval in\na fine-grained manner. Empirical results show\nthat CRUX offers more reflective and diagnos-\ntic evaluation. Our findings also reveal substan-\ntial room for improvement in current retrieval\nmethods, pointing to promising directions for\nadvancing RAG\u2019s retrieval. Our data and code\nare publicly available to support and advance\nfuture research on retrieval.1\n1\nIntroduction\nWith their emerging instruction-following capabili-\nties (Ouyang et al., 2022; Wei et al., 2021), large\nlanguage models (LLMs) have adopted retrieval-\naugmented generation (RAG) (Lewis et al., 2020;\nGuu et al., 2020) to tackle more challenging tasks,\nsuch as ambiguous question answering (QA) (Stel-\nmakh et al., 2022; Gao et al., 2023) or long-form\nresponse generation (Shao et al., 2024). The role of\n1https://anonymous.4open.science/r/\nrag-rerank-85CF\nOpen-ended Query  x\nLong-form Result y\nRetrieval Context \ng\nxYPGePWTvPk3DrAHBSvpFLVne6uIBFcG9f9dnJr6xubW/ntws7u3v5B8fCoqeNUMWywWMSqHVCNgktsG4EthOFNAoEtoLR7cxvPaHSPJb3ZpygH9GB5CFn1Fip/tArltyOwdZJV5GSpCh1it+dfsxSyOUhgmqdcdzE+NPqDKcCZwWuqnGhLIRHWDHUkj1P5kfuiUnFmlT8JY2ZKGzNXfExMaT2OAtsZUTPUy95M/M/\nrpCa89idcJqlByRaLwlQE5PZ16TPFTIjxpZQpri9lbAhVZQZm03BhuAtv7xKmhdl7JcqVdK1ZsjycwCmcgwdXUIU7qEDGCA8wyu8OY/Oi/PufCxac042cwx/4Hz+ALuvjOc=</latexit>Z\ng\nxYPGePWTvPk3DrAHBSvpFLVne6uIBFcG9f9dnJr6xubW/ntws7u3v5B8fCoqeNUMWywWMSqHVCNgktsG4EthOFNAoEtoLR7cxvPaHSPJb3ZpygH9GB5CFn1Fip/tArltyOwdZJV5GSpCh1it+dfsxSyOUhgmqdcdzE+NPqDKcCZwWuqnGhLIRHWDHUkj1P5kfuiUnFmlT8JY2ZKGzNXfExMaT2OAtsZUTPUy95M/M/\nrpCa89idcJqlByRaLwlQE5PZ16TPFTIjxpZQpri9lbAhVZQZm03BhuAtv7xKmhdl7JcqVdK1ZsjycwCmcgwdXUIU7qEDGCA8wyu8OY/Oi/PufCxac042cwx/4Hz+ALuvjOc=</latexit>Z\n however, \u2026 such as the \nrequirement for federal health \nplans to cover \u2026\nThe 1998 federal budget \nnegotiations were a \nchallenging and complex \nprocess that \u2026 One of the \nmain issues that delayed the \nnegotiations was the allocation \nof funds for education, \u2026  was \nincluded in a $500 billion \npackage that was hailed \u2026, \nbut was criticized by \nconservatives who expressed \nconcerns over the $20 billion \n\u2026.\n\u2026 over the $20 billion taken \nfrom the surplus to address \nemergencies \u2026\n\u2026 designate $20 billion as \nemergency' expenditures so \nthat did \u2026\n\u2026 are expected to vote in \nfavor of the $500 billion \npackage due \u2026\n\u2026 federal budget, with \ndisagreements over education \nbeing the \u2026\n\u2026 speci\ufb01c issues, such as the \nrequirement for federal health \nplans \nWrite a report that describes the 1998 federal budget negotiations, \nthe issues that delayed \u2026, and the eventual agreement that was \u2026\n\u2026.\nFigure 1: An example of long-form generation on\nCRUX with open-ended query x and desired response\ny. The underlined text marks relevant content in the re-\ntrieval (\n) that contributes to the final result. By directly\nassessing retrieval context Z, we can further explicitly\nidentify incomplete (\n) and redundant retrieval (\n).\nretrieval in RAG is to access information from ex-\nternal sources and prompt it as plug-in knowledge\nfor LLMs. To achieve this, typical RAG systems\nretrieve the k most relevant chunks as the retrieval-\naugmented context (abbreviated as retrieval con-\ntext, hereafter), and prompt the LLM to generate a\nresponse using this information.\nIt was found that a suboptimal retrieval context\nhinders the generation process (Asai et al., 2024;\nRau et al., 2024), triggering negative impacts and\nresulting in unsatisfying final RAG results. One of\nwidely-studied effects is the impact of noise from\nirrelevant retrieval (Yoran et al., 2023), which in-\ncreases the risk of hallucinations (Asai et al., 2022)\nand distractions (Shi et al., 2023). Such prior stud-\nies have mainly focused on short-answer tasks;\nhowever, recent RAG development has shifted to-\nwards generating comprehensive and structured re-\nports with open-ended queries (Zhao et al., 2024;\n1\n\nLawrie et al., 2024), as illustrated in Figure 1, in-\ntroducing new concerns of suboptimal retrieval.\nIn the scenario of open-ended queries where a\nshort answer is insufficient and a long-form result is\nrequired, incompleteness and redundancy emerge\nas the critical yet underexplored negative impacts\nfrom retrieval (Joren et al., 2024). Specifically, (i)\nincomplete retrieval fails to capture the full nuance\nof the query, leading to partial or misleading gen-\nerations. (ii) Redundant retrieval contexts restrict\nthe diversity of knowledge, undermining the use-\nfulness of augmented knowledge (Yu et al., 2024;\nChen and Choi, 2024). Figure 1 exemplifies such\nimpacts of suboptimal retrieval matters on the final\nlong-form RAG result.\nTo examine these effects, a suitable retrieval eval-\nuation framework is crucial for measuring com-\npleteness and redundancy in the retrieval context.\nCurrent retrieval evaluation approaches are insuffi-\ncient for measuring retrieval effectiveness in long-\nform RAG, as they are designed for web search (Ba-\njaj et al., 2016) or short-answer QA (Kwiatkowski\net al., 2019).\nThey only require a focus on\nrelevance-based ranking, which can be simply eval-\nuated with retrieval metrics such as MRR and\nRecall@k. In contrast, long-form RAG requires\nretrieving multiple aspects and subtopics to ensure\ncompleteness, which goes beyond surface-level rel-\nevance (Tan et al., 2024; Grusky et al., 2018).\nTo address the gap, we propose a Controllable\nRetrieval-aUgmented conteXt evaluation frame-\nwork (CRUX). The framework includes controlled\nevaluation datasets and coverage-based metrics,\nwhich directly assess the content of the retrieval\ncontext instead of relevance-based ranking. We\nuse human-written multi-document summaries to\ndefine the scope of retrieval context, enabling\na controlled oracle retrieval for more diagnostic\nevaluation results. Finally, we assess both (inter-\nmediate) retrieval context and (final) RAG result\nvia question-based evaluation (Sander and Dietz,\n2021), supporting fine-grained and more aligned\nevaluation between them.\nTo validate the usability of our evaluation frame-\nwork, we conduct empirical experiments with mul-\ntiple retrieval and re-ranking strategies, including\nrelevance and diversity re-ranking. Empirical re-\nsults explicitly reveal the limitations of suboptimal\nretrieval in terms of coverage and density. Our ad-\nditional metric analysis further demonstrates that\nrelevance ranking metrics lack coverage-awareness,\nhighlighting CRUX\u2019s strength in identifying re-\ntrieval impacts on long-form RAG. Notably, our\nframework balances scalability and reliability by\nintegrating LLM-based judgments with human-\ngrounded data. Our final human evaluation also\nconfirms CRUX\u2019s alignment with human percep-\ntion.\nOverall, our controlled retrieval context evalua-\ntion aims to identify suboptimal retrieval for long-\nform RAG scenario. Our contributions are as fol-\nlows:\n\u2022 We create a controlled dataset tailored for evalu-\nating retrieval context for long-form RAG;\n\u2022 We propose coverage-based metrics with upper\nbounds to help diagnosing retrieval context in\nterms of completeness and redundancy;\n\u2022 Our empirical results showcase the limitations of\nexisting retrieval for long-form RAG;\n\u2022 Our framework can serve as a reliable experi-\nmental testbed for developing more compatible\nretrieval for long-form RAG.\n2\nRelated Work\nThe importance of retrieval in RAG.\nLLMs are\nhighly effective at parameterizing world knowledge\nas memory; however, accessing long-tail knowl-\nedge (Mallen et al., 2023) or verifying facts (Mishra\net al., 2024; Min et al., 2023) often requires re-\ntrieving information from external sources. This\nhighlights the essential role of retrieval in augment-\ning reliable knowledge for downstream applica-\ntions (Zhang et al., 2024; Zhu et al., 2024; Rau\net al., 2024), which is especially important in long-\nform generation (Gao et al., 2023; Mayfield et al.,\n2024; Tan et al., 2024). Many studies point out\nthat the limitations of retrieval lead to unsatisfying\nRAG results (BehnamGhader et al., 2023; Su et al.,\n2024; Asai et al., 2024; Rau et al., 2024), raising\nthe critical question: how effectively can retrieval\naugment knowledge for LLMs?\nAutomatic evaluators for NLP tasks.\nLLMs\nhave shown promising instruction-following capa-\nbility, making them increasingly common as auto-\nmatic evaluators across various NLP tasks (Thakur\net al., 2025; Zheng et al., 2023; Chiang and Lee,\n2023). Due to their cost efficiency and scalabil-\nity, LLM-based evaluations have also been applied\nto information retrieval (IR) (Thomas et al., 2024;\nDietz, 2024) and short-form generation tasks (Saad-\nFalcon et al., 2024; Shahul et al., 2023). Instead of\nshort-form RAG, we target long-form generation\nwith open-ended query, which requires retrieval to\n2\n\nensure completeness in addition to surface-level\nrelevance. Reference-based metrics like ROUGE\nused in summarization also fall short in such sce-\nnarios (Krishna et al., 2021). Thus, a flexible frame-\nwork is needed to assess information completeness\nand redundancy in the retrieval context.\nEvaluating retrieval for long-form generation.\nEvaluation methodologies in IR and NLP have been\nstandardized and developed for decades (Voorhees,\n2002, 2004). In recent years, nugget-based (sub-\ntopics or sub-questions) evaluation (Pavlu et al.,\n2012; Clarke et al., 2008; Dang et al., 2008) has\nresurfaced as an important focus due to the feasi-\nbility of automatic judgments. Similarly, question-\nbased evaluation estimate the answerability (Eyal\net al., 2019; Sander and Dietz, 2021) of given text,\nis well-aligned with LLMs while preserving aspect-\nlevel granularity, making it particularly good for\nevaluating long-form generation. This helps to in-\nform the development of a unified evaluation setup\nfor both intermediate retrieval context and final\nlong-form results, thereby facilitating more infor-\nmative evaluation for RAG\u2019s retrieval methods.\n3\nControlled Retrieval-augmented\nContext Evaluation (CRUX)\nThis section introduces CRUX, a controlled evalu-\nation framework for assessing retrieval context in\nlong-form RAG. It comprises: (1) definitions of\nretrieval context and its sub-question answerabil-\nity (\u00a7 3.1); (2) curated evaluation datasets (\u00a7 3.2)\nand (3) answerability-driven performance metrics:\ncoverage and density (\u00a7 3.3).\n3.1\nRetrieval-augmented Context\nHere we focus on the retrieval context as the im-\nportant bottleneck in the long-form RAG pipeline.\nFormally, given an open-ended query x, a typical\nRAG pipeline is defined as:\ny \u2190G(x, Z, I),\nZ \u2190RA\u03b8(x, K).\n(1)\nRA\u03b8 denotes the retrieval modules that augment\nretrieval context Z from an external knowledge\nsource K (i.e., a corpus), and G is a LLM generator\nthat input with the query x, retrieval context Z and\na task-specific instruction prompt I, to generate\nthe final long-form RAG result y. Particularly, we\nargue that the quality of retrieval context is a key\nlimitation for achieving optimal RAG results and\npropose an evaluation framework to diagnose it.\nMulti-document Summary \n8\n9+4+Sg08cHA470ZuZ5EWdK2/a3lVlZXVvfyG7mtrZ3dvfy+wcNFcaS0DoJeShbHlaUM0HrmlOW5GkOPA4bXrDq4nfvKdSsVDc6iSiboD7gvmMYG2km+TutJsv2CV7CrRMnDkpVArFp8eH8bjWzX91eiGJAyo04ViptmNH2k2x1IxwOsp1YkUjTIa4T9uGChxQ5abTU0eoaJQe8kNpSmg0VX9PpDhQKgk80xlgPVCL3kT8z2vH2r90UyaiWFNBZov8mCMdo\nsnfqMckJZonhmAimbkVkQGWmGiTs6E4Cy+vEwaZyXnvFS+NmlUYsHMExnIADF1CBKtSgDgT68Ayv8GZx68V6tz5mrRlrPnMIf2B9/gB+95D6</latexit>y\u21e4\n8\n9+4+Sg08cHA470ZuZ5EWdK2/a3lVlZXVvfyG7mtrZ3dvfy+wcNFcaS0DoJeShbHlaUM0HrmlOW5GkOPA4bXrDq4nfvKdSsVDc6iSiboD7gvmMYG2km+TutJsv2CV7CrRMnDkpVArFp8eH8bjWzX91eiGJAyo04ViptmNH2k2x1IxwOsp1YkUjTIa4T9uGChxQ5abTU0eoaJQe8kNpSmg0VX9PpDhQKgk80xlgPVCL3kT8z2vH2r90UyaiWFNBZov8mCMdo\nsnfqMckJZonhmAimbkVkQGWmGiTs6E4Cy+vEwaZyXnvFS+NmlUYsHMExnIADF1CBKtSgDgT68Ayv8GZx68V6tz5mrRlrPnMIf2B9/gB+95D6</latexit>y\u21e4\nIt\u2019s a race for the governor's mansion in 11 states \ntoday, and the GOP could end the night at the helm of \nmore than two-thirds of the 50 states. The GOP \ncurrently controls 29 of the country's top state \u2026\nRelevant Retrieval \nContext \nE\nd/GadqFtv4w8PH/5zDnHD/mTGnH+bZyc/MLi0v5ZXtldW19o7C5VNRIgmtkohHsuFjRTkTtKqZ5rQRS4pDn9O6378c5fV7KhWLxK0exNQLcVewgBGsjXVTuTtoF4pOycmEZsGdQPH8wz6L37sSrvw2epEJAmp0IRjpZquE2svxVIzwunQbiWKxpj0cZc2DQocUuWl2ahDtGecDgoiaZ7QKHN/d6Q4VGoQ+qYyxLqnprOR+V/WTHRw6qVMxImgow/ChKOd\nIRGe6MOk5RoPjCAiWRmVkR6WGKizXVscwR3euVZqB2W3OPS0bVTLF/AWHnYgV3YBxdOoAxXUIEqEOjCAzBs8WtR+vFeh2X5qxJzb8kfX+AyUkLY=</latexit>P \u21e4\nE\nd/GadqFtv4w8PH/5zDnHD/mTGnH+bZyc/MLi0v5ZXtldW19o7C5VNRIgmtkohHsuFjRTkTtKqZ5rQRS4pDn9O6378c5fV7KhWLxK0exNQLcVewgBGsjXVTuTtoF4pOycmEZsGdQPH8wz6L37sSrvw2epEJAmp0IRjpZquE2svxVIzwunQbiWKxpj0cZc2DQocUuWl2ahDtGecDgoiaZ7QKHN/d6Q4VGoQ+qYyxLqnprOR+V/WTHRw6qVMxImgow/ChKOd\nIRGe6MOk5RoPjCAiWRmVkR6WGKizXVscwR3euVZqB2W3OPS0bVTLF/AWHnYgV3YBxdOoAxXUIEqEOjCAzBs8WtR+vFeh2X5qxJzb8kfX+AyUkLY=</latexit>P \u21e4\nSub-questions\nq 2 Q\nq 2 Q\n0\n1\n0\n0\n5\n4\n3\n0\n0\n3\n4\n5\n4\n0\n0\n1\n5\n2\n2\n3\nOpen-ended \nQuery x\nOracle Retrieval\nContext \nj\nfg2peFtv4Q+Pj/c8g5x485U9pxvq3M3PzC4lJ2V5ZXVvfyG1uVWUSEIrJOKRrPtYUc4ErWimOa3HkuLQ57Tm9y6Ge2eSsUicaP7MfVC3BEsYARrY13f3h20cnmn4IyEZsGdQP7swz6N37sciv32WxHJAmp0IRjpRquE2svxVIzwunAbiaKxpj0cIc2DAocUuWlo1EHaM84bRE0jyh0cj93ZHiUKl+6JvKEOums6G5n9ZI9HBiZcyESeaCjL+KEg40\nhEa7o3aTFKied8AJpKZWRHpYomJNtexzRHc6ZVnoXpYcI8KxSsnXzqHsbKwA7uwDy4cQwkuoQwVINCB3iCZ4tbj9aL9TouzViTnm34I+v9BzRQkMA=</latexit>Z\u21e4\nj\nfg2peFtv4Q+Pj/c8g5x485U9pxvq3M3PzC4lJ2V5ZXVvfyG1uVWUSEIrJOKRrPtYUc4ErWimOa3HkuLQ57Tm9y6Ge2eSsUicaP7MfVC3BEsYARrY13f3h20cnmn4IyEZsGdQP7swz6N37sciv32WxHJAmp0IRjpRquE2svxVIzwunAbiaKxpj0cIc2DAocUuWlo1EHaM84bRE0jyh0cj93ZHiUKl+6JvKEOums6G5n9ZI9HBiZcyESeaCjL+KEg40\nhEa7o3aTFKied8AJpKZWRHpYomJNtexzRHc6ZVnoXpYcI8KxSsnXzqHsbKwA7uwDy4cQwkuoQwVINCB3iCZ4tbj9aL9TouzViTnm34I+v9BzRQkMA=</latexit>Z\u21e4\nn\n2DvBzh5FBo9cOFwzr3ce0+YCG7A8z6czNz8wuJSdtldWV1b38htblWNSjVlFaqE0vWQGCa4ZBXgIFg90YzEoWC1sHcx8mt3TBu5A30ExbEpCN5xCkBK1WbDMj5YSuX94reGPgv8ackX9r9PLv/cgvlVu692VY0jZkEKogxDd9LIBgQDZwKNnSbqWEJoT3SYQ1LJYmZCQbja4d43yptHCltSwIeqz8nBiQ2ph+HtjMm0DWz3kj8z2ukEJ0GAy6TFJik0VRK\njAoPHodt7lmFETfEkI1t7di2iWaULABuTYEf/blv6R6UPSPi0fXNo1LNEW7aA9VEA+OkEldIXKqIoukUP6Ak9O8p5dF6c10lrxpnObKNfcN6+AVDnkfY=</latexit>\u2318= 3\nn\n2DvBzh5FBo9cOFwzr3ce0+YCG7A8z6czNz8wuJSdtldWV1b38htblWNSjVlFaqE0vWQGCa4ZBXgIFg90YzEoWC1sHcx8mt3TBu5A30ExbEpCN5xCkBK1WbDMj5YSuX94reGPgv8ackX9r9PLv/cgvlVu692VY0jZkEKogxDd9LIBgQDZwKNnSbqWEJoT3SYQ1LJYmZCQbja4d43yptHCltSwIeqz8nBiQ2ph+HtjMm0DWz3kj8z2ukEJ0GAy6TFJik0VRK\njAoPHodt7lmFETfEkI1t7di2iWaULABuTYEf/blv6R6UPSPi0fXNo1LNEW7aA9VEA+OkEldIXKqIoukUP6Ak9O8p5dF6c10lrxpnObKNfcN6+AVDnkfY=</latexit>\u2318= 3\nOracle Retrieval\nContext \nj\nfg2peFtv4Q+Pj/c8g5x485U9pxvq3M3PzC4lJ2V5ZXVvfyG1uVWUSEIrJOKRrPtYUc4ErWimOa3HkuLQ57Tm9y6Ge2eSsUicaP7MfVC3BEsYARrY13f3h20cnmn4IyEZsGdQP7swz6N37sciv32WxHJAmp0IRjpRquE2svxVIzwunAbiaKxpj0cIc2DAocUuWlo1EHaM84bRE0jyh0cj93ZHiUKl+6JvKEOums6G5n9ZI9HBiZcyESeaCjL+KEg40\nhEa7o3aTFKied8AJpKZWRHpYomJNtexzRHc6ZVnoXpYcI8KxSsnXzqHsbKwA7uwDy4cQwkuoQwVINCB3iCZ4tbj9aL9TouzViTnm34I+v9BzRQkMA=</latexit>Z\u21e4\nj\nfg2peFtv4Q+Pj/c8g5x485U9pxvq3M3PzC4lJ2V5ZXVvfyG1uVWUSEIrJOKRrPtYUc4ErWimOa3HkuLQ57Tm9y6Ge2eSsUicaP7MfVC3BEsYARrY13f3h20cnmn4IyEZsGdQP7swz6N37sciv32WxHJAmp0IRjpRquE2svxVIzwunAbiaKxpj0cIc2DAocUuWlo1EHaM84bRE0jyh0cj93ZHiUKl+6JvKEOums6G5n9ZI9HBiZcyESeaCjL+KEg40\nhEa7o3aTFKied8AJpKZWRHpYomJNtexzRHc6ZVnoXpYcI8KxSsnXzqHsbKwA7uwDy4cQwkuoQwVINCB3iCZ4tbj9aL9TouzViTnm34I+v9BzRQkMA=</latexit>Z\u21e4\nFigure 2: The controlled data generation derived from\nmulti-document summarization datasets.\nAnswerability measured by sub-questions.\nTo\nassess retrieval context quality beyond relevance-\nbased ranking, we adopt question-based evalua-\ntion (Eyal et al., 2019; Sander and Dietz, 2021).\nWe assess the content of an arbitrary text z with a\ndiverse set of knowledge-intensive sub-questions\nQ = {q1, q2, . . . , qn}. Such diversity enables these\nquestions to serve as a surrogate for evaluating\nmultiple aspects of a query, thereby facilitating\nexplicit diagnosis of underlying concerns such as\ncompleteness and redundancy. Specifically, we use\nan LLM to judge how well the text z answers each\nsub-question and estimate a binary sub-question\nanswerability value (answerability, hereafter):\nG(z, qi, Ig) \u2265\u03b7\n\u2200qi \u2208Q,\n(2)\nwhere Ig is a grading instruction prompt similar to\nthe rubrics proposed by Dietz (2024). The output\ngraded rating is on a scale of 0 to 5 (the prompt is\nincluded in Figure 8 in the Appendix A.1). \u03b7 is a\npredefined threshold determining whether the given\ntext-question pair is answerable. The threshold\nanalysis is reported in Section 4.4.\n3.2\nData Creation for Controlled Evaluation\nWe further construct datasets tailored for our eval-\nuation framework to support controlled analysis.\nAs illustrated in Figure 2, we treat human-written\nmulti-document summaries as the central anchor\nfor defining: (1) the explicit scope of relevant re-\ntrieval context Z\u2217; (2) an open-ended query x; (3)\na diverse set of sub-questions Q. Together, these\ncomponents support our assessment of complete-\nness and redundancy.\nExplicit scope of retrieval context.\nThe con-\ntrollability comes from the intrinsic relationships\nwithin the multi-document summarization datasets:\n3\n\nMulti-News (Fabbri et al., 2019) and DUC (Over\nand Yen, 2004), where each example consists of\na human-written summary and the corresponding\nmultiple documents. As illustrated in Figure 2, we\nconsider the human-written summary as the proxy\nof an oracle long-form RAG result;2 it is denoted as\ny\u2217. The corresponding documents D\u2217are naturally\nregarded as relevant, while the other documents\ncan be safely considered as irrelevant, forming an\nexplicit scope for each example. In addition, we de-\ncontextualize a document into passage-level chunks\nwith an LLM, obtaining the set of relevant passages\np \u2208P \u2217\u2286D\u2217. Decontextualization provides sev-\neral advantages (Choi et al., 2021), ensuring the\npassages fit the token length limitation of all retriev-\ners and are standalone while preserving main topics.\nSuch units also help us identifying redundancy and\nincompleteness; see Table 5 for an example.\nOpen-ended queries.\nWe use an LLM to synthe-\nsize a query with open-ended information needs\nfrom the human-written summary y\u2217via in-context\nprompting (Brown et al., 2020) (See an example\nin Figure 1 and 9 also). We denote these queries\nas x in Eq. (1), which is the initial input for both\nretrieval and generation. Such queries help expose\nlimitations in existing retrieval systems, which of-\nten return either irrelevant or redundant passages,\nresulting in incomplete retrieval contexts. Notably,\nthe query generation process is adaptable and can\nbe tailored to various kinds of queries (Yang et al.,\n2024) via similar in-context prompting.\nDiverse sub-questions and filtering.\nSimilarly,\nwe synthesize a diverse set of knowledge-intensive\nsub-questions Q from the human-written summary\nwhich cover the highlights in the oracle RAG re-\nsults (i.e., y\u2217). Thanks to the controlled settings,\nfor each query x, we enumerate all possible pairs of\nsub-questions q \u2208Q and relevant passages p \u2208P \u2217,\nthen judge them with an LLM. Hence, for each\nrelevant passage, we obtain a list of graded ratings\nfor all the sub-question as mentioned in Eq. (2). Fi-\nnally, we can obtain the matrix of graded ratings as\nshown in Figure 2. In addition, the judged ratings\ncan serve as consistency filtering to identify unan-\nswerable sub-questions for mitigating out-of-scope\nand hallucinated questions. These pre-judged rat-\nings can be further reused for evaluating retrieval\ncontext, which is also released with the data.\n2We assume the human-written summary satisfies complex\ninformation needs in the most precise and concise manner.\n1\n3\n4\n4\n2\nOpen-ended query \nL\nGoqbrq7gkRwbVz328mtrW9sbuW3Czu7e/sHxcOjpo5TxbDBYhGrdkA1Ci6xYbgR2E4U0igQ2ApGNzO/9YhK81jem3GCfkQHkoecUWOl+lOvWHL7hxklXgZKUGWq/41e3HLI1QGiao1h3PTYw/ocpwJnBa6KYaE8pGdIAdSyWNUPuT+aFTcmaVPgljZUsaMld/T0xopPU4CmxnRM1QL3sz8T+vk5rw2p9wmaQGJVsClNBTExmX5M+V8iMGFtCmeL2VsKGVFmbDYFG4K3/PIq\naV6UvctypV4pVW+zOPJwAqdwDh5cQRXuoAYNYIDwDK/w5jw4L86787FozTnZzDH8gfP5A+nBjQc=</latexit>x\nL\nGoqbrq7gkRwbVz328mtrW9sbuW3Czu7e/sHxcOjpo5TxbDBYhGrdkA1Ci6xYbgR2E4U0igQ2ApGNzO/9YhK81jem3GCfkQHkoecUWOl+lOvWHL7hxklXgZKUGWq/41e3HLI1QGiao1h3PTYw/ocpwJnBa6KYaE8pGdIAdSyWNUPuT+aFTcmaVPgljZUsaMld/T0xopPU4CmxnRM1QL3sz8T+vk5rw2p9wmaQGJVsClNBTExmX5M+V8iMGFtCmeL2VsKGVFmbDYFG4K3/PIq\naV6UvctypV4pVW+zOPJwAqdwDh5cQRXuoAYNYIDwDK/w5jw4L86787FozTnZzDH8gfP5A+nBjQc=</latexit>x\n\u2026\n4\n\u2026\n\u2026\n4\n3\n3\n2\n0\n1\n0\n\u2026\n\u2026\n2\n0\nRetrieval Context \np\npFLVne6uIBFcG9f9dnJr6xubW/ntws7u3v5B8fCoqeNUMWywWMSqHVCNgktsG4EthOFNAoEtoLR7cxvPaHSPJb3ZpygH9GB5CFn1Fip/tArltyOwdZJV5GSpCh1it+dfsxSyOUhgmqdcdzE+NPqDKcCZwWuqnGhLIRHWDHUkj1P5kfuiUnFmlT8JY2ZKGzNXfExMaT2OAtsZUTPUy95M/M/rpCa89idcJqlByRaLwlQE5PZ16TPFTIjxpZQpri9lbAhVZQZm03BhuAtv7xK\nmhdl7JcqVdK1ZsjycwCmcgwdXUIU7qEDGCA8wyu8OY/Oi/PufCxac042cwx/4Hz+ALuvjOc=</latexit>Z\np\npFLVne6uIBFcG9f9dnJr6xubW/ntws7u3v5B8fCoqeNUMWywWMSqHVCNgktsG4EthOFNAoEtoLR7cxvPaHSPJb3ZpygH9GB5CFn1Fip/tArltyOwdZJV5GSpCh1it+dfsxSyOUhgmqdcdzE+NPqDKcCZwWuqnGhLIRHWDHUkj1P5kfuiUnFmlT8JY2ZKGzNXfExMaT2OAtsZUTPUy95M/M/rpCa89idcJqlByRaLwlQE5PZ16TPFTIjxpZQpri9lbAhVZQZm03BhuAtv7xK\nmhdl7JcqVdK1ZsjycwCmcgwdXUIU7qEDGCA8wyu8OY/Oi/PufCxac042cwx/4Hz+ALuvjOc=</latexit>Z\nMax\n2\n4\n3\n1\n0\nFinal RAG Result y\ng\nwuGce7n3Hj/kTBvXfXeWldW19ZTG+nNre2d3czeflvLSBHaIpJLdeVjTkTtGWY4fQqVBQHPqcdf9xI/M6EKs2kuDTkPYDfCPYiBFsrHTdkJNcjwylyQ8yWbdwWqt4Jx5yC65b9UqVhHjVsldCRaskyNbzd5CgOci89YaSRAEVhnCsdbfohqYfY2UY4XSW7kWahpiM8Q3tWipwQHU/nl8Q8dWGaKRVLaEQXP1+0SMA62ngW87A2xu9W8vEf/yupEZ1foxE2FkqCLRaOIyNR\n8j4aMkWJ4VNLMFHM3orILVaYGBtS2obw9Sn6n7S9QrFSKF/YNM5hgRQcwhHkoAhVqMZNKEFBATcwyM8Odp5cJ6dl0XrkvM5cwA/4Lx+AHuxkZY=</latexit>Cov(\u00b7)\ng\nwuGce7n3Hj/kTBvXfXeWldW19ZTG+nNre2d3czeflvLSBHaIpJLdeVjTkTtGWY4fQqVBQHPqcdf9xI/M6EKs2kuDTkPYDfCPYiBFsrHTdkJNcjwylyQ8yWbdwWqt4Jx5yC65b9UqVhHjVsldCRaskyNbzd5CgOci89YaSRAEVhnCsdbfohqYfY2UY4XSW7kWahpiM8Q3tWipwQHU/nl8Q8dWGaKRVLaEQXP1+0SMA62ngW87A2xu9W8vEf/yupEZ1foxE2FkqCLRaOIyNR\n8j4aMkWJ4VNLMFHM3orILVaYGBtS2obw9Sn6n7S9QrFSKF/YNM5hgRQcwhHkoAhVqMZNKEFBATcwyM8Odp5cJ6dl0XrkvM5cwA/4Lx+AHuxkZY=</latexit>Cov(\u00b7)\n6\ncOFwzr3ce48Xca0b9bqaXldW19HpmY3Nreye7u9dWYSwJbZGQh/Law4pyJmhLM83pdSQpDjxO974NPE7t1QqForPYmoG+ChYD4jWBvp5oyKfI8MQl3oZ3N28aRedY4dZBdtu+aUqwlxahWnjEpGSZBrFO4gQbOfesNQhIHVGjCsVLdkh1pd4qlZoTWaYXKxphMsZD2jVU4IAqdzq/eIaOjDJAfihNCY3m6veJKQ6UmgSe6QywHqnfXiL+5XVj7dfdKRNRrKkgi0V+zJEO\nUfI+GjBJieYTQzCRzNyKyAhLTLQJKWNC+PoU/U/aTrFULVYuTRoXsEAaDuAQ8lCGjTgHJrQAgIC7uERnixlPVjP1suiNWV9zuzD1ivH2FukYU=</latexit>Den(\u00b7)\n6\ncOFwzr3ce48Xca0b9bqaXldW19HpmY3Nreye7u9dWYSwJbZGQh/Law4pyJmhLM83pdSQpDjxO974NPE7t1QqForPYmoG+ChYD4jWBvp5oyKfI8MQl3oZ3N28aRedY4dZBdtu+aUqwlxahWnjEpGSZBrFO4gQbOfesNQhIHVGjCsVLdkh1pd4qlZoTWaYXKxphMsZD2jVU4IAqdzq/eIaOjDJAfihNCY3m6veJKQ6UmgSe6QywHqnfXiL+5XVj7dfdKRNRrKkgi0V+zJEO\nUfI+GjBJieYTQzCRzNyKyAhLTLQJKWNC+PoU/U/aTrFULVYuTRoXsEAaDuAQ8lCGjTgHJrQAgIC7uERnixlPVjP1suiNWV9zuzD1ivH2FukYU=</latexit>Den(\u00b7)\nSub-Questions q 2 Q\nq 2 Q\nFigure 3: CRUX employs sub-question answerability\nto directly assess the textual content of both retrieval\ncontext Z and its corresponding RAG result y. The\nmetrics include coverage and density.\nRequired subset of relevant passages.\nOnce we\nhave pre-judgements of all relevant passages p \u2208\nP \u2217, we further identify which passages are truly\nnecessary and construct a smaller subset of relevant\npassages, denoted as P \u2217\u2217. Specifically, we define\nthis required subset if the passages in the subset can\ncollectively answer all sub-questions q \u2208Q. To do\nso, we first rank each relevant passage according\nto how many questions it can answer and greedily\nassigned it to the subset until no additional sub-\nquestions can be answered.3 The remaining are\ncategorized as either partially or fully redundant.\nData statistics.\nDue to the limited computational\nresources, we finally collected 100 open-ended\nqueries from Multi-News (Fabbri et al., 2019) and\n50 queries from DUC (Over and Yen, 2004). The\nknowledge source K has around 500K passages,\ncollected from training and test splits of Multi-\nNews and the DUC. We generate all data using\nopen-source Llama-3.1-70B-Instruct.4 (Meta,\n2024). Detailed data statistics and generation set-\ntings are reported in Appendix A.1.\n3.3\nEvaluation Metrics\nWe define three performance metrics to assess re-\ntrieval context for long-form RAG. We begin by\nmeasuring context\u2019s completeness using coverage,\nthen introduce derived metrics: ranked coverage\nand density to further take redundancy into account.\nCoverage (Cov).\nRather than evaluating the re-\ntrieval results based on only their relevance (e.g.,\nnDCG and MAP), we assess the content of the re-\ntrieval contexts based on answerability. Given a\n3The default answerability threshold \u03b7 is set to 3.\n4https://huggingface.co/meta-llama/Llama-3.\n1-70B-Instruct\n4\n\nretrieval context Z, we explicitly quantify the con-\ntext\u2019s coverage with how many questions it can\nanswer over the answerable sub-questions. To com-\npute this, we aggregate graded ratings by taking\nthe maximum across passages in retrieval context\nZ and obtain binary answerability as depicted in\nFigure 3. We finally normalize it by the total num-\nber of answerable sub-questions. Formally, the\ncoverage of the retrieval context is defined as:\nCov(Z) =\n#{q \u2208Q| max\n\u0010\nG(p \u2208Z, q, Ig)\n\u0011\n\u2265\u03b7}\n|Q|\n. (3)\nWe can also apply this formula to evaluate the cov-\nerage of the final RAG result y, allowing us to\ncompare the coverage of the retrieved passages to\nthe coverage of the generation.\nRanked coverage.\nWe bring coverage-awareness\nto the novelty ranking metric, \u03b1-nDCG Clarke\net al. (2008). \u03b1-nDCG evaluates novelty based\non subtopics, which is naturally compatible with\nour framework using sub-question answerabil-\nity. Specifically, we define the ranked coverage\nby treating the answerability of sub-questions as\nsubtopics, as follows:\n\u03b1-nDCG =\n|Z|\nX\nr=1\nng(r)\nlog(r + 1)/\n|Z\u2217|\nX\nr=1\nng\u2217(r)\nlog(r + 1),\nng(r) =\n|Q|\nX\ni=1\nIi,r(1 \u2212\u03b1)ci,r\u22121,\n(4)\nwhere r is the passage rank position in the retrieval\ncontext. The function ng is novelty gain, repre-\nsenting how much new information is covered with\nrespect to the position r and sub-question qi. Dis-\ncount factor \u03b1 is used for penalizing redundant\nsub-questions when accumulating gains.\nDensity (Den).\nWe evaluate the retrieval con-\ntext\u2019s density from a coverage perspective. The\noracle retrieval context Z\u2217is considered as the\nreference, enabling us to compute relative density\nbased on the total number of tokens. The density\nof the retrieval context Z is measured by:\nDen(Z) =\n\u0010 Cov(Z)/token(p \u2208Z)\nCov(Z\u2217)/token(p \u2208Z\u2217)\n\u0011w\n, (5)\nwhere token(\u00b7) means the total number of tokens,\nand w is a weighting factor. We set w as 0.5, assum-\ning that the information density grows monotoni-\ncally but has diminishing marginal returns when\nreaching the optimum.\n4\nExperiments\nTo validate CRUX\u2019s evaluation capability and us-\nability, we begin with controlled experiments with\nempirical retrieval contexts to enable more diagnos-\ntic retrieval evaluation. Next, we analyze metric\ncorrelations between the retrieval contexts Z and\nthe corresponding final results y. Finally, we assess\nCRUX\u2019s usability through human annotations and\nexamine other configuration impacts.\n4.1\nExperimental Setups\nInitial retrieval.\nOur experiments employ vary-\ning cascaded retrieval pipelines to augment context\nfrom the knowledge corpus. Given an open-ended\nquery x, we first retrieve top-100 relevant candi-\ndate passages. Three initial retrieval approaches\nare considered: lexical retrieval (LR) with BM25,5\ndense retrieval (DR) and learned sparse retrieval\n(LSR) using ContrieverFT (Izacard et al., 2021)\nand SPLADE-v3 (Lassance et al., 2024).\nCandidate re-ranking.\nWe further re-rank the\n100 candidate passages with more effective mod-\nels, constructing the final retrieval context Z. We\nexperiment with varying re-ranking strategies, in-\ncluding pointwise re-ranking models (#parameters):\nminiLM (220M) and monoT5 (3B). In addition,\nwe include state-of-the-art LLM-based listwise re-\nranking models: RankZephyr (7B) (Pradeep et al.,\n2023) and RankFirst (7B) (Reddy et al., 2024),\nas well as the Setwise re-ranking (3B) (Zhuang\net al., 2023). Lastly, we evaluate the maximal\nmarginal relevance (MMR) algorithm for diversity\nre-ranking to consider both relevance and diver-\nsity.6\nGeneration.\nLlama models (Meta, 2024) with\n8B parameters are used for generation. We use\nvLLM (Kwon et al., 2023) to accelerate the inference\nspeed and perform batch inference. For fair com-\nparisons, we adopt the same configurations for all\ngenerations. Details are provided in Appendix A.1.\nEvaluation protocol.\nAs our goal is to analyze\nhow incomplete and redundant retrieval context\naffects the final RAG result, we assess both the\nquality of retrieval context Z and further investigate\nthe relationships between them and final coverage\nand density: Cov(y) and Den(y). Notably, the\n5https://github.com/castorini/pyserini/\n6We follow Gao and Zhang (2024) and adopt the same\npre-trained encoder for MMR: https://huggingface.co/\nsentence-transformers/all-mpnet-base-v2\n5\n\nDUC\nMulti-News\nRetrieval Context\nCov(Z)\n\u03b1-nDCG\nCov(y)\nDen(Z)\nDen(y)\nCov(Z)\n\u03b1-nDCG\nCov(y)\nDen(Z)\nDen(y)\n(#1) Direct prompting\n-\n-\n26.7\n-\n-\n-\n-\n21.4\n-\n-\n(#2) Oracle result y\u2217\n-\n-\n95.3\n-\n108\n-\n-\n94.1\n-\n111\n(#3) Oracle retrieval Z\u2217\n100\n80.6\n64.6\n100\n93.8\n100\n80.6\n61.8\n100\n84.7\nBM25 (LR)\n44.4\n35.7\n35.8\n61.2\n53.4\n39.3\n35.4\n38.2\n50.6\n60.0\nContriever (DR)\n52.1\n45.2\n41.7\n70.3\n60.5\n43.1\n36.6\n36.6\n55.4\n58.3\nSPLADE-v3 (LSR)\n49.0\n45.0\n41.0\n67.7\n59.4\n45.4\n40.4\n41.3\n60.6\n64.3\nLR + MMR\n45.6\n36.7\n36.4\n65.8\n57.2\n41.4\n35.2\n37.9\n52.9\n58.9\nDR + MMR\n42.7\n35.1\n33.8\n62.6\n53.5\n39.0\n33.5\n36.1\n51.3\n57.6\nLSR + MMR\n44.2\n35.6\n36.5\n64.4\n56.5\n39.2\n33.8\n37.3\n51.6\n59.2\nLR + miniLM\n49.0\n42.5\n38.4\n67.9\n57.9\n45.3\n39.8\n41.2\n58.2\n63.0\nDR + miniLM\n49.3\n42.9\n39.9\n69.3\n59.7\n45.1\n40.3\n40.4\n57.8\n62.4\nLSR + miniLM\n49.4\n42.6\n39.2\n69.3\n59.2\n45.4\n40.3\n40.6\n58.0\n62.6\nLR + monoT5\n50.7\n42.4\n37.9\n66.5\n56.7\n47.9\n40.2\n41.6\n58.3\n64.0\nDR + monoT5\n53.2\n44.7\n40.7\n70.8\n60.0\n45.4\n40.0\n40.9\n56.6\n62.6\nLSR + monoT5\n52.8\n43.0\n41.1\n68.9\n59.2\n44.3\n37.7\n38.9\n55.4\n61.5\nLR + RankZephyr\n51.5\n45.9\n40.6\n69.9\n59.5\n52.9\n47.6\n43.9\n65.1\n67.7\nDR + RankZephyr\n51.1\n48.8\n40.6\n67.8\n59.2\n53.6\n47.2\n44.1\n66.0\n66.8\nLSR + RankZephyr\n50.4\n45.9\n41.2\n67.3\n60.0\n54.4\n49.1\n45.8\n67.0\n69.8\nLR + RankFirst\n52.0\n46.2\n43.9\n70.1\n63.4\n56.0\n49.1\n46.4\n68.0\n69.4\nDR + RankFirst\n53.8\n49.1\n44.6\n70.9\n64.0\n54.5\n47.6\n44.4\n66.2\n67.4\nLSR + RankFirst\n53.6\n48.2\n44.3\n70.9\n64.0\n54.5\n48.2\n46.0\n66.5\n69.2\nLR + SetwiseFlanT5\n49.6\n44.2\n42.5\n67.8\n61.9\n52.1\n44.9\n43.2\n63.9\n65.5\nDR + SetwiseFlanT5\n56.6\n48.4\n44.4\n74.9\n64.4\n49.9\n43.8\n41.0\n61.0\n62.5\nLSR + SetwiseFlanT5\n51.9\n46.0\n43.3\n70.1\n62.6\n52.0\n47.0\n45.1\n65.4\n67.8\nRank Corr. (Kendall \u03c4)\n0.676\n0.724\n-\n0.733\n-\n0.838\n0.800\n-\n0.810\n-\nTable 1: Evaluation results of empirical retrieval contexts Z and corresponding final results y (the columns in gray)\non CRUX-DUC and Multi-News. Scores with bold font and underlined are the highest and lowest. For each dataset,\ncolumns 1 and 2 show retrieval coverage and ranked coverage; column 3 shows the final result coverage. The last\ntwo columns are density of retrieval context and final result. The bottom row reports the ranking correlation between\nretrieval context and final results.\nexplicit scope of relevant passages allows us to\nreuse the pre-judgements for relevant passages as\nshown in Figure 3. Unless otherwise specified, we\nset the default answerability threshold \u03b7 to 3.\n4.2\nControlled Empirical Experiments\nCRUX suggests explicit oracle RAG settings of\nretrieval context Z\u2217, thereby facilitating more in-\ndicative evaluations by controlling: (i) the number\nof passages in the retrieval context (i.e., top-k),\nwhich is set to match the size of the oracle retrieval\ncontext, |Z\u2217|; (ii) the maximum generation token\nlength, which is constrained by the match token\nlength of the oracle retrieval, token(Z\u2217).7 The\nfollowing research questions guide our findings.\nWhat are the reference performance bounds of\nretrieval context and final RAG result?\nIn the\nfirst block of Table 1, we report the performance\nof three reference retrieval contexts and their fi-\nnal RAG results: (#1) zero-shot direct prompting;\n7We change the prompt accordingly and truncate the maxi-\nmum token length if the result exceeds.\n(#2) oracle results y\u2217(the human-written sum-\nmary); (#3) oracle retrieval context Z\u2217\u225cP \u2217\u2217,\nwhich is the required subset of relevant passages\ngiven in the test collection (See Section 3.2).\nUnsurprisingly, we observe the lowest coverage\nof RAG result without retrieval (#1), confirming\nthat parametric knowledge in the LLM alone is in-\nsufficient to achieve high performance. This condi-\ntion serves as the empirical lower bound of RAG. In\ncontrast, the oracle result using the human-written\nsummary (#2) achieves highest coverage with an-\nswering over 90% of sub-questions. It implies that\ngenerated sub-questions are answerable and vali-\ndate the framework\u2019s ability to capture complete-\nness. The RAG result with oracle retrieval context\n(#3) yields decent coverage of 64.6 and 61.8, out-\nperforming other empirical methods in subsequent\nblocks in the table. This demonstrates an empirical\nupper bound for RAG\u2019s retrieval, grounded in an\noracle retrieval context Z\u2217. Overall, CRUX pro-\nvides robust bounds for reference, enabling more\ndiagnostic evaluation of RAG\u2019s retrieval regardless\nof the generator\u2019s effects.\n6\n\nHow effective are empirical retrieval contexts\nregarding the performance of the final RAG re-\nsult?\nTo investigate this, we evaluate a range of\nempirical retrieval contexts from various cascaded\nretrieval pipelines. As reported in Table 1, each\npipeline is evaluated with both the quality of in-\ntermediate retrieval context Z and the final RAG\nresult y (the gray columns).\nThe second and third blocks in Table 1 show that\ninitial retrieval-only and MMR ranking struggle to\nretrieve useful information, resulting in poor perfor-\nmance of retrieval contexts. We also observe that\nsuch suboptimal retrieval contexts would directly\nreflect on the suboptimal final RAG result cover-\nage Cov(y) on both evaluation sets (underlined\nscores).\nNotably, on evaluation results of DUC, we ob-\nserve pointwise re-ranking models have robust\ngains on final RAG result coverage only when used\nwith weaker initial retrieval (e.g., LR + miniLM,\n35.8 \u219238.4). However, they degrade when adopt-\ning stronger initial retrieval (e.g., LSR + miniLM,\n41.0\u219239.2). Such patterns are also shown on\nintermediate retrieval context performance, demon-\nstrating CRUX\u2019s evaluation capability for retrieval\ncontext.\nIn contrast, more effective re-ranking consis-\ntently enhances overall performance, with visible\nperformance gains in both intermediate and final re-\nsults. For example, RankFirst (Reddy et al., 2024)\nand SetwiseFlanT5 (Zhuang et al., 2023), particu-\nlarly outperform all the other empirical pipelines\n(conditions marked in bold). Yet, they still have\na large gap compared to the oracle retrieval (#3),\nimplying that existing ranking models are not ex-\nplicitly optimized for coverage of long-form RAG\nresults.\nCan intermediate retrieval context performance\nextrapolate the final RAG result performance?\nFinally, to highlight the advantage of retrieval con-\ntext evaluation, we compute the ranking correlation\nin terms of Kendall \u03c4 between final result cover-\nage/density (i.e., Cov(y)/Den(y)) and the interme-\ndiate coverage, ranked coverage and density.\nWe find ranking correlation strengths of approx-\nimately 0.7 to 0.8 on both evaluation sets at the\nlast row in Table 1, demonstrating the strong align-\nment between retrieval context and RAG result.\nThis suggests that our framework can be a promis-\ning surrogate retrieval evaluation for extrapolating\nlong-form RAG results.\n0.00\n0.25\n0.50\n0.75\n1.00\nCov(y)\n oracle\n0.00\n0.25\n0.50\n0.75\n1.00\nCov(y)\n bm25\n3\n7\n8\n9\n13\n20\n28\n31\n33\n43\n0.00\n0.25\n0.50\n0.75\n1.00\nCov(y)\n dr_rankfirst\nLLM\nAnn-1 (p=0.90)\nAnn-2 (p=0.82)\nAnn-3 (p=0.79)\nFigure 4: Coverage of RAG results for 10 CRUX-DUC\nqueries (x-axis) under three retrieval contexts (y-axis).\nEach subplot shows LLM-judged coverage (line) and\nhuman judgments (markers); bars indicate the annota-\ntors\u2019 average. The Pearson correlations \u03c1 are computed\nbetween the LLM and each annotator\u2019s coverage.\n4.3\nMetric Alignment Analysis\nTo further validate our proposed evaluation met-\nrics, we analyze how these metrics align with hu-\nman judgments. Then, we compare these metrics\nagainst other relevance-based metrics, showing that\nthey are insufficient for evaluating retrieval mod-\nules in long-form RAG scenarios.\nHow does the evaluation method align with hu-\nman judgments?\nWe conduct human judgment\non 10 randomly selected open-ended queries from\nCRUX-DUC. We design two reading comprehen-\nsion tasks:8 T1: Long-form RAG result coverage\njudgement, and T2: Rubric-based passage judge-\nment. T1 investigates how well LLM-judged cov-\nerage align with human\u2019s. We collect binary an-\nswerability annotations for all enumerated result\nsub-question pairs {(y, q1), ..., (y, qn)} and com-\npute the corresponding coverage of final RAG re-\nsult Cov(y).\nWe evaluate RAG results across three retrieval\ncontexts Z: Oracle, BM25 and DR+RankFirst, as\nshown in the subplots in Figure 4. With the total of\n30 human-judged coverage, we compute the Spear-\nman correlation between them and LLM, obtaining\nhigh alignment (\u03c1 \u22650.8), and a moderate inter-\nannotator agreement (Fleiss\u2019 \u03ba = 0.52). We also\nfound that the controlled oracle retrieval Z\u2217has sig-\nnificantly better coverage from human judgements,\nconfirming the reliability of upper bound, while the\n8Appendix A.2 details the annotation tasks (e.g., process,\ninterface design and annotator, etc.).\n7\n\nCov(Z)\nRecall\nMAP\nnDCG\n-nDCG\nCov(y)\nCov(Z)\nRecall\nMAP\nnDCG\n-nDCG\nCov(y)\n1.00\n0.66\n0.64\n0.65\n0.78\n0.68\n0.66\n1.00\n0.84\n0.86\n0.71\n0.58\n0.64\n0.84\n1.00\n0.91\n0.71\n0.56\n0.65\n0.86\n0.91\n1.00\n0.76\n0.56\n0.78\n0.71\n0.71\n0.76\n1.00\n0.67\n0.68\n0.58\n0.56\n0.56\n0.67\n1.00\nFigure 5: Kendall \u03c4 rank correlations between evalua-\ntion metrics on CRUX-DUC, using 48 random sampled\nretrieval contexts Z. Metrics includes intermediate and\nfinal coverage, and other relevance-based metrics.\nCov(y)\nCov(Z)\nKendall \u03c4\n\u03b7 = 3\n50.1 (\u00b13.5)\n40.4 (\u00b13)\n0.676\n\u03b7 = 5\n42.6 (\u00b13.6)\n35.6 (\u00b12.5)\n0.562\nTable 2: Coverage metrics computed with different an-\nswerability thresholds \u03b7 on CRUX-DUC with empirical\nretrieval contexts Z. Mean and standard deviations are\nshown in the table and parentheses.\nother retrieval context are fluctuate among queries.\nHow do the other ranking metrics align with\nthe final RAG result?\nWe conduct a compara-\ntive analysis of various relevance-based ranking\nmetrics such as MAP, Recall and nDCG, to explore\nalternative metrics for evaluating retrieval effective-\nness in terms of corresponding RAG result com-\npleteness (i.e., Cov(y)). To this end, we sample\n16 retrieval contexts from three initial retrieval set-\ntings, yielding 48 retrieval contexts. Each retrieval\ncontext Z contains 10 passages randomly sampled\nfrom the top 50 retrieved passages. Figure 5 shows\nthe Kendall \u03c4 correlation between each ranking\nmetric and the coverage of RAG result (the last col-\numn). We observe that the retrieval context\u2019s cov-\nerage (Cov(Z)) and ranked coverage (\u03b1-nDCG)\nachieve higher correlations (0.68 and 0.67) than the\ncommon ranking metrics Recall, MAP, and nDCG.\nWhile the ranking metrics have \u03c4 < 0.6, they are\ncorrelated mutually with \u03c4 of 0.8 to 0.9, suggesting\nthey capture similar retrieval properties. In con-\ntrast, the coverage of the retrieval context is more\neffective for extrapolating final RAG result.\n4.4\nConfiguration Analysis\nWe finally analyze different configurations to ex-\namine CRUX\u2019s applicability and flexibility.\nAnswerability thresholds.\nWe first adjust the\nhigher answerability threshold (\u03b7 = 5 in Eq. (2)).\nOur analysis is conducted on CRUX-DUC evalua-\ntion set using the same empirical retrieval pipelines.\nIn Table 2, we observe the higher threshold leads\nk\nCov(Z)\n\u03b1-nDCG\nRecall\nMAP\nnDCG\nDen(Z)\n|Z\u2217|\n0.68\n0.70\n0.55\n0.57\n0.61\n0.73\nDUC\n10\n0.59\n0.68\n0.63\n0.62\n0.64\n0.54\n20\n0.60\n0.70\n0.66\n0.67\n0.70\n0.62\nMulti-\nNews\n|Z\u2217|\n0.84\n0.80\n0.75\n0.70\n0.76\n0.81\n10\n0.71\n0.74\n0.66\n0.72\n0.73\n0.59\n20\n0.56\n0.58\n0.40\n0.55\n0.58\n0.46\nTable 3: Kendall \u03c4 rank correlations between interme-\ndiate retrieval context and final result evaluation, under\ndifferent size of retrieval context and datasets. The\ncolumns 2 to 6 compare with final coverage Cov(y)\nand the last column compares final density Den(y).\nto lower coverage in both intermediate and final\nresults, Cov(Z) and Cov(y). While setting thresh-\nold as 3 demonstrates slightly larger variance (\u00b13)\nacross retrieval pipelines, which is more discrimina-\ntive and desirable. Similarly, we compute the rank-\ning correlations under two thresholds and justify\nthat \u03b7 = 3 achieves better alignment; we thereby\nset it as default throughout this study.\nSize of retrieval context.\nWe further examine the\nalignment with varying sizes of top-k chunks in the\nretrieval context: the size of oracle retrieval (|Z\u2217|)\nand the fixed 10 and 20. Table 3 shows the ranking\ncorrelation coefficients between coverage of RAG\nresult Cov(y), and the coverage of corresponding\nintermediate evaluation; we report the coverage\nand retrieval context and the other ranking metrics.\nWe observe our proposed metrics Cov(Z) and \u03b1-\nnDCG demonstrate higher correlation; however,\ncorrelations fluctuate as more retrieval context is\nconsidered (top-20). We hypothesize that it may\ndue to position biases and a lack of controllabil-\nity (Liu et al., 2024), making it harder to diagnose\nretrieval, which we leave it as our future targets.\n5\nConclusions\nWe introduced CRUX, an evaluation framework\nfor assessing retrieval in long-form RAG scenarios.\nCRUX provides controlled datasets and metrics, en-\nabling evaluation of the retrieval context\u2019s coverage\nof relevant information and of retrieval\u2019s impact on\nthe final result. The framework serves as a diag-\nnostic testbed for improving methods by tackling\nincomplete and redundant retrieval. Our experi-\nments demonstrate that existing retrieval methods\nhave substantial room for improvement. By doing\nso, we present new perspectives for advancing re-\ntrieval in long-form RAG scenarios and support\nexploration of retrieval context optimization as a\nkey future direction.\n8\n\nLimitations\nThe scope of knowledge.\nWe acknowledge that\nthe questions generated in CRUX may suffer from\nhallucinations or insufficiency. To mitigate hal-\nlucination, we filter out questions that cannot be\nanswered by the oracle retrieval context. However,\nthis approach risks underestimating the context, as\nthe required knowledge may not be comprehensive\nor even exist. We also recognize the limitations\nof our evaluation in assessing factual correctness,\nhighlighting the limitation of answerability. In ad-\ndition, the CRUX\u2019s passages are related to English\nNews, which constrains its contribution to low-\nresource languages and other professional domains\n(e.g., scientific and finance).\nStructural biases.\nIn this work, we decontextual-\nize documents into passage-level units to minimize\nthe concerns of granularity (Zhong et al., 2024) and\nensure that all retrieval contexts can be fairly com-\npared. However, this standardization might lead\nto discrepancies in evaluation results compared to\npractical applications, where contexts often exhibit\nnoisier structures. Another limitation is the im-\npacts from positional biases of relevant or irrele-\nvant passages (Liu et al., 2024; Cuconasu et al.,\n2024). To mitigate these concerns, we control the\nsettings with a maximum of 2500 tokens. However,\nthe evaluation is still subject to negative impacts\nfrom such biases, resulting in overestimated perfor-\nmance.\nHuman annotation variation.\nThe human judg-\nment evaluation only has moderate inter-annotator\nagreement. We speculate this may be attributed to\ntwo factors: (1) The samples are relatively small:\nour annotations only sampled from 10 reports and\nare evaluated by 3 annotators, due to the costly\nand time-consuming nature of assessing long-form\noutputs (see Figure 10). (2) The difficulty of long-\nform content assessment: The increasing content\nlength may lead to divergent assessments, as anno-\ntators may differ in their interpretation of specific\naspects. It is worth noting that such variance is\nnot uncommon in IR, particularly when assessing\ncomplex notions of relevance (Dietz et al., 2018).\nAcknowledgments\nWe acknowledge the Dutch Research Council\n(NWO) in The Netherlands for awarding this\nproject access to the LUMI supercomputer, owned\nby the EuroHPC Joint Undertaking, hosted by CSC\n(Finland) and the LUMI consortium through the\n\u2018Computing Time on National Computer Facilities\u2019\ncall.\nReferences\nAkari Asai,\nMatt Gardner,\nand Hannaneh Ha-\njishirzi. 2022. Evidentiality-guided generation for\nknowledge-intensive NLP tasks. In Proceedings of\nthe 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2226\u20132243,\nStroudsburg, PA, USA. Association for Computa-\ntional Linguistics.\nAkari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh,\nLuke Zettlemoyer, Hannaneh Hajishirzi, and Wen-\nTau Yih. 2024. Reliable, adaptable, and attributable\nlanguage models with retrieval. arXiv [cs.CL].\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder, An-\ndrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir\nRosenberg, Xia Song, Alina Stoica, Saurabh Tiwary,\nand Tong Wang. 2016. MS MARCO: A human gen-\nerated MAchine Reading COmprehension dataset.\narXiv [cs.CL].\nParishad BehnamGhader, Santiago Miret, and Siva\nReddy. 2023.\nCan retriever-augmented language\nmodels reason? The blame game between the re-\ntriever and the language model. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2023, pages 15492\u201315509, Stroudsburg, PA, USA.\nAssociation for Computational Linguistics.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, J Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, T Henighan, R Child, A Ramesh,\nDaniel M Ziegler, Jeff Wu, Clemens Winter, Christo-\npher Hesse, Mark Chen, Eric Sigler, Ma-Teusz\nLitwin, Scott Gray, Benjamin Chess, Jack Clark,\nChristopher Berner, Sam McCandlish, Alec Radford,\nI Sutskever, and Dario Amodei. 2020. Language\nModels are Few-Shot Learners. Neural Inf Process\nSyst, abs/2005.14165:1877\u20131901.\nHung-Ting Chen and Eunsol Choi. 2024. Open-world\nevaluation for retrieving diverse perspectives. arXiv\n[cs.CL].\nCheng-Han Chiang and Hung-Yi Lee. 2023. Can large\nlanguage models be an alternative to human evalua-\ntions? arXiv [cs.CL], pages 15607\u201315631.\nEunsol Choi, Jennimaria Palomaki, Matthew Lamm,\nTom Kwiatkowski, Dipanjan Das, and Michael\nCollins. 2021. Decontextualization: Making sen-\ntences stand-alone. Trans. Assoc. Comput. Linguist.,\n9:447\u2013461.\n9\n\nCharles L A Clarke, Maheedhar Kolla, Gordon V\nCormack, Olga Vechtomova, Azin Ashkan, Stefan\nB\u00fcttcher, and Ian MacKinnon. 2008. Novelty and\ndiversity in information retrieval evaluation. In Pro-\nceedings of the 31st annual international ACM SIGIR\nconference on Research and development in informa-\ntion retrieval, SIGIR \u201908, page 659\u2013666, New York,\nNY, USA. ACM.\nFlorin Cuconasu, Giovanni Trappolini, Federico Sicil-\niano, Simone Filice, Cesare Campagnano, Yoelle\nMaarek, Nicola Tonellotto, and Fabrizio Silvestri.\n2024.\nThe power of noise: Redefining retrieval\nfor RAG systems. In Proceedings of the 47th In-\nternational ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, volume 17,\npages 719\u2013729, New York, NY, USA. ACM.\nHoa Dang, Jimmy Lin, and Diane Kelly. 2008.\nOverview of the TREC 2006 Question Answering\nTrack.\nLaura Dietz. 2024.\nA workbench for autograding\nretrieve/generate systems.\nIn Proceedings of the\n47th International ACM SIGIR Conference on Re-\nsearch and Development in Information Retrieval,\nvolume 67 of SIGIR \u201924, pages 1963\u20131972, New\nYork, NY, USA. ACM.\nLaura Dietz, Manisha Verma, Filip Radlinski, and Nick\nCraswell. 2018. TREC complex answer retrieval\noverview. TREC.\nMatan Eyal, Tal Baumel, and Michael Elhadad. 2019.\nQuestion answering as an automatic evaluation met-\nric for news article summarization. In Proceedings of\nthe 2019 Conference of the North, pages 3938\u20133948,\nStroudsburg, PA, USA. Association for Computa-\ntional Linguistics.\nAlexander Fabbri, Irene Li, Tianwei She, Suyi Li, and\nDragomir Radev. 2019. Multi-News: A Large-Scale\nMulti-Document Summarization Dataset and Ab-\nstractive Hierarchical Model. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 1074\u20131084, Stroudsburg,\nPA, USA. Association for Computational Linguistics.\nHang Gao and Yongfeng Zhang. 2024. VRSD: Re-\nthinking similarity and diversity for retrieval in Large\nLanguage Models. arXiv [cs.IR].\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\n2023. Enabling large language models to generate\ntext with citations. arXiv [cs.CL].\nMax Grusky, Mor Naaman, and Yoav Artzi. 2018.\nNewsroom: A dataset of 1.3 million summaries with\ndiverse extractive strategies. In Proceedings of the\n2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long Pa-\npers), pages 708\u2013719, Stroudsburg, PA, USA. Asso-\nciation for Computational Linguistics.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Ming-Wei Chang. 2020. REALM: Retrieval-\nAugmented Language Model pre-training.\narXiv\n[cs.CL].\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\nbastian Riedel, Piotr Bojanowski, Armand Joulin,\nand Edouard Grave. 2021. Unsupervised dense in-\nformation retrieval with contrastive learning. arXiv\n[cs.IR].\nHailey Joren, Jianyi Zhang, Chun-Sung Ferng, Da-\nCheng Juan, Ankur Taly, and Cyrus Rashtchian. 2024.\nSufficient context: A new lens on Retrieval Aug-\nmented Generation systems. arXiv [cs.CL].\nKalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021.\nHurdles to progress in long-form question answering.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 4940\u20134957, Stroudsburg, PA, USA. Associa-\ntion for Computational Linguistics.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natural\nQuestions: A benchmark for question answering re-\nsearch. Trans. Assoc. Comput. Linguist., 7:453\u2013466.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gon-\nzalez, Hao Zhang, and Ion Stoica. 2023. Efficient\nmemory management for large language model serv-\ning with PagedAttention. arXiv [cs.LG].\nCarlos Lassance, Herv\u00e9 D\u00e9jean, Thibault Formal, and\nSt\u00e9phane Clinchant. 2024. SPLADE-v3: New base-\nlines for SPLADE. arXiv [cs.IR].\nDawn Lawrie, Sean MacAvaney, James Mayfield, Paul\nMcNamee, Douglas W Oard, Luca Soldaini, and\nEugene Yang. 2024. Overview of the TREC 2023\nNeuCLIR track. arXiv [cs.IR].\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K\u00fcttler, Mike Lewis, Wen-Tau Yih, Tim Rock-\nt\u00e4schel, Sebastian Riedel, and Douwe Kiela. 2020.\nRetrieval-augmented generation for knowledge-\nintensive NLP tasks. arXiv [cs.CL].\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-\njape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. 2024. Lost in the middle: How language\nmodels use long contexts. Trans. Assoc. Comput.\nLinguist., 12:157\u2013173.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\nDaniel Khashabi, and Hannaneh Hajishirzi. 2023.\nWhen not to trust language models: Investigating\neffectiveness of parametric and non-parametric mem-\nories. In Proceedings of the 61st Annual Meeting of\n10\n\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 9802\u20139822, Stroudsburg,\nPA, USA. Association for Computational Linguistics.\nJames Mayfield, Eugene Yang, Dawn Lawrie, Sean\nMacAvaney, Paul McNamee, Douglas W Oard, Luca\nSoldaini, Ian Soboroff, Orion Weller, Efsun Kayi,\nKate Sanders, Marc Mason, and Noah Hibbler. 2024.\nOn the evaluation of machine-generated reports. In\nProceedings of the 47th International ACM SIGIR\nConference on Research and Development in Infor-\nmation Retrieval, volume 7 of SIGIR \u201924, pages 1904\u2013\n1915, New York, NY, USA. ACM.\nLlama Team AI @ Meta. 2024. The Llama 3 herd of\nmodels. arXiv [cs.AI].\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis,\nWen-Tau Yih, Pang Koh, Mohit Iyyer, Luke Zettle-\nmoyer, and Hannaneh Hajishirzi. 2023. FActScore:\nFine-grained atomic evaluation of factual precision in\nlong form text generation. In Proceedings of the 2023\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 12076\u201312100, Stroudsburg,\nPA, USA. Association for Computational Linguistics.\nAbhika Mishra, Akari Asai, Vidhisha Balachandran,\nYizhong Wang, Graham Neubig, Yulia Tsvetkov, and\nHannaneh Hajishirzi. 2024. Fine-grained halluci-\nnation detection and editing for language models.\nArXiv, abs/2401.06855.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. arXiv [cs.CL].\nPaul Over and James Yen. 2004. An introduction to\nDUC-2004. National Institute of Standards and Tech-\nnology.\nVirgil Pavlu, Shahzad Rajput, Peter B Golbus, and\nJaved A Aslam. 2012. IR system evaluation using\nnugget-based test collections. In Proceedings of the\nfifth ACM international conference on Web search\nand data mining, WSDM \u201912, pages 393\u2013402, New\nYork, NY, USA. ACM.\nRonak Pradeep, Sahel Sharifymoghaddam, and Jimmy\nLin. 2023. RankZephyr: Effective and robust zero-\nshot listwise reranking is a breeze! arXiv [cs.IR].\nDavid Rau, Herv\u00e9 D\u00e9jean, Nadezhda Chirkova, Thibault\nFormal, Shuai Wang, Vassilina Nikoulina, and\nSt\u00e9phane Clinchant. 2024.\nBERGEN: A bench-\nmarking library for retrieval-Augmented Generation.\narXiv [cs.CL].\nRevanth Gangi Reddy, Jaehyeok Doo, Yifei Xu,\nMd Arafat Sultan, Deevya Swain, Avirup Sil, and\nHeng Ji. 2024.\nFIRST: Faster improved listwise\nreranking with single token decoding. arXiv [cs.IR].\nJon Saad-Falcon, Omar Khattab, Christopher Potts, and\nMatei Zaharia. 2024. ARES: An automated evalua-\ntion framework for retrieval-augmented generation\nsystems. In Proceedings of the 2024 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies (Volume 1: Long Papers), pages 338\u2013354,\nStroudsburg, PA, USA. Association for Computa-\ntional Linguistics.\nDavid P Sander and Laura Dietz. 2021. EXAM: How to\nevaluate retrieve-and-generate systems for users who\ndo not (yet) know what they want. DESIRES, pages\n136\u2013146.\nE S Shahul, Jithin James, Luis Espinosa Anke, and\nS Schockaert. 2023. RAGAs: Automated evalua-\ntion of Retrieval Augmented Generation. Conf Eur\nChapter Assoc Comput Linguistics, pages 150\u2013158.\nYijia Shao, Yucheng Jiang, Theodore A Kanell, Peter\nXu, Omar Khattab, and Monica S Lam. 2024. Assist-\ning in writing Wikipedia-like articles from scratch\nwith large language models. arXiv [cs.CL].\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan\nScales, David Dohan, Ed Chi, Nathanael Sch\u00e4rli,\nand Denny Zhou. 2023. Large language models can\nbe easily distracted by Irrelevant Context.\narXiv\n[cs.CL].\nIvan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-\nWei Chang. 2022. ASQA: Factoid questions meet\nlong-form answers.\nIn Proceedings of the 2022\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 8273\u20138288, Stroudsburg,\nPA, USA. Association for Computational Linguis-\ntics.\nHongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi,\nNiklas Muennighoff, Han-Yu Wang, Haisu Liu, Quan\nShi, Zachary S Siegel, Michael Tang, Ruoxi Sun, Jin-\nsung Yoon, Sercan O Arik, Danqi Chen, and Tao\nYu. 2024.\nBRIGHT: A realistic and challenging\nbenchmark for reasoning-intensive retrieval. arXiv\n[cs.CL].\nHaochen Tan, Zhijiang Guo, Zhan Shi, Lu Xu, Zhili Liu,\nYunlong Feng, Xiaoguang Li, Yasheng Wang, Lifeng\nShang, Qun Liu, and Linqi Song. 2024. ProxyQA:\nAn alternative framework for evaluating long-form\ntext generation with large language models. In Pro-\nceedings of the 62nd Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 6806\u20136827, Stroudsburg, PA, USA.\nAssociation for Computational Linguistics.\nNandan Thakur, Ronak Pradeep, Shivani Upadhyay,\nDaniel Campos, Nick Craswell, and Jimmy Lin.\n2025. Support evaluation for the TREC 2024 RAG\nTrack: Comparing human versus LLM judges. arXiv\n[cs.CL].\nPaul Thomas, Seth Spielman, Nick Craswell, and\nBhaskar Mitra. 2024. Large language models can\n11\n\naccurately predict searcher preferences. In Proceed-\nings of the 47th International ACM SIGIR Confer-\nence on Research and Development in Information\nRetrieval, volume 35 of SIGIR \u201924, pages 1930\u20131940,\nNew York, NY, USA. ACM.\nEllen Voorhees. 2004. Overview of the TREC 2003\nQuestion Answering Track.\nEllen M Voorhees. 2002. The philosophy of information\nretrieval evaluation. In Lecture Notes in Computer\nScience, Lecture notes in computer science, pages\n355\u2013370. Springer Berlin Heidelberg, Berlin, Heidel-\nberg.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\nguage models are zero-shot learners. arXiv [cs.CL].\nXiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla,\nXiangsen Chen, Sajal Choudhary, Rongze Daniel\nGui, Ziran Will Jiang, Ziyu Jiang, Lingkun Kong,\nBrian Moran, Jiaqi Wang, Yifan Ethan Xu, An Yan,\nChenyu Yang, Eting Yuan, Hanwen Zha, Nan Tang,\nLei Chen, Nicolas Scheffer, Yue Liu, Nirav Shah,\nRakesh Wanga, Anuj Kumar, Wen-Tau Yih, and\nXin Luna Dong. 2024. CRAG \u2013 Comprehensive\nRAG benchmark. arXiv [cs.CL].\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan\nBerant. 2023. Making retrieval-augmented language\nmodels robust to irrelevant context. Int Conf Learn\nRepresent, abs/2310.01558.\nYue Yu, Wei Ping, Zihan Liu, Boxin Wang, Jiaxuan You,\nChao Zhang, Mohammad Shoeybi, and Bryan Catan-\nzaro. 2024. RankRAG: Unifying context ranking\nwith retrieval-augmented generation in LLMs. arXiv\n[cs.CL].\nZihan Zhang, Meng Fang, and Ling Chen. 2024. Re-\ntrievalQA: Assessing adaptive retrieval-augmented\ngeneration for short-form open-domain question an-\nswering.\nAnnu Meet Assoc Comput Linguistics,\nabs/2402.16457:6963\u20136975.\nWeike Zhao, Chaoyi Wu, Xiaoman Zhang, Ya Zhang,\nYanfeng Wang, and Weidi Xie. 2024. RaTEScore: A\nmetric for radiology report generation. In Proceed-\nings of the 2024 Conference on Empirical Methods in\nNatural Language Processing, pages 15004\u201315019,\nStroudsburg, PA, USA. Association for Computa-\ntional Linguistics.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, E Xing, Haotong Zhang,\nJoseph E Gonzalez, and Ion Stoica. 2023. Judging\nLLM-as-a-judge with MT-bench and Chatbot Arena.\nNeural Inf Process Syst, abs/2306.05685.\nZijie Zhong, Hanwen Liu, Xiaoya Cui, Xiaofan Zhang,\nand Zengchang Qin. 2024.\nMix-of-granularity:\nOptimize the chunking granularity for retrieval-\nAugmented Generation. arXiv [cs.LG].\nAndrew Zhu, Alyssa Hwang, Liam Dugan, and Chris\nCallison-Burch. 2024.\nFanOutQA: A multi-hop,\nmulti-document question answering benchmark for\nlarge language models. Annu Meet Assoc Comput\nLinguistics, pages 18\u201337.\nShengyao Zhuang, Honglei Zhuang, Bevan Koopman,\nand Guido Zuccon. 2023. A Setwise approach for\neffective and highly efficient zero-shot ranking with\nLarge Language Models. arXiv [cs.IR].\n12\n\nDUC\nMulti-News\nTest\nTest\nTrain\n# Queries\n50\n4,986\n39,781\n# Passages\n565,015\nAverage token length\nQuery / question\n58 / 16\n51 / 17\n-\nPassage\n119\n109\n115\nOracle result\n530\n277\n-\nSubset size of relevant passages\nRequired (P \u2217\u2217)\n274\n14,659\n\u2013\nRedundant (P \u2217\\ P \u2217\u2217)\n1,657\n47,467\n\u2013\nTable 4: The dataset statistics of CRUX. Token length\nis calculated by Llama-3.1-70B tokenizer. The last\nblock indicates the required subset and the other relevant\npassages (see Section 3.2).\nA\nAppendix\nA.1\nEmpirical Evaluation\nEvaluation datasets.\nTable 4 details the statistics\nof CRUX. The corpus is constructed from 500K\nNews passages with relatively shorter lengths. For\nDUC, we select all 50 examples in our experiments.\nFor Multi-News, we only select 100 random ex-\namples due to the computational cost of conduct-\ning online judgments for final RAG results using\nLlama-3.1-70B-Instruct. However, the graded\nrelevance ratings for all relevant passages (P \u2217) for\nall 4,986 examples are offline computed and in-\ncluded with the released data and code.\nInference settings.\nWe adopt larger Llama mod-\nels (Meta, 2024), Llama-3.1-70B-Instruct, to\ngenerate the CRUX evaluation datasets: CRUX-\nDUC and CRUX-Multi-News (test split). For train-\ning data generation using the Multi-News train split,\nwe employ Llama-3.1-8B-Instruct due to the\nhigh computational cost of large-scale generation.\nGeneration is performed under two different set-\ntings. For text generation (e.g., queries, passages,\nand questions), we use a temperature of 0.7 and top-\np of 0.95. For judgement generation (i.e., graded\nratings for answerability), we follow Thomas et al.\n(2024) and use a temperature of 0.0 and top-p of 1.0.\nTo accelerate inference, we leverage vLLM (Kwon\net al., 2023). The entire data generation process\nis conducted on 4 AMD MI200X GPUs and takes\napproximately 14 days.\nPrompts for data generation.\nFigures 6, 7, 8,\nand 9 display the prompts we used for curating\nthe evaluation data. Table 5 is an example of all\ngenerated data (e.g., queries, sub-questions, etc.).\nEmpirical Experiments.\nThe indexes are built\nusing Pyserini.9 The IR ranking metrics used in\nthis study are implemented in ir-measure. 10\nA.2\nHuman Evaluation\nOverview\nWe conducted human annotation us-\ning the Prolific crowdsourcing platform.11 We re-\ncruited three annotators with university-level edu-\ncation and demonstrated fluency in English reading.\nAnnotation could be completed flexibly across mul-\ntiple sessions, each annotator spent approximately\n6\u20139 hours in total. Annotators were rewarded at a\nrate of 9.50 pounds per hour with fair-pay guide-\nlines and were informed that the annotations would\nbe used for academic research purposes. Each an-\nnotators is assigned two-stage reading comprehen-\nsion task on our CRUX-DUC dataset.\nAnnotation task 1\u2013report coverage judgment.\nWe include 30 machine-generated RAG results (re-\nports), with each result containing 15 sub-questions\nto be labeled as either answerable or unanswerable.\nThe guideline is reported in Figure 10. The 30\nreports are from three types of retrieval contexts:\nOracle, BM25, and DR+RankFirst (10 each), to en-\nsure a balanced distribution across retrieval settings.\nThe human coverage reported in Figure 4 is calcu-\nlated in line with LLM judgement using the same\nset of answerable sub-questions (see Sec. 3.3).\nAnnotation task 2\u2013passage-level judgement with\nrubric-based graded rating.\nIn T2, we randomly\nselect oracle relevant passages and ask annotators\nto label graded ratings from 0 to 5 for two random\nsub-questions, simulating the LLM-based judge-\nment using the prompt shown in Figure 8. We\ncollected 226 human ratings (ground truth) and\ncompared them to LLM predictions. We observe\nprecision above 0.6 for both answerable (\u03b7 \u22653)\nand unanswerable (\u03b7 < 3) cases. While recall is\nhigh for unanswerable questions, it drops to 0.4\nfor answerable ones. This indicates the LLM tends\nto make conservative predictions, underestimating\nanswerable content. A key challenge for improving\nCRUX is generating sub-questions that are both\nmore discriminative and better aligned with human\nperception.\nAnnotation platform.\nWe develop an annota-\ntion platform tailored for CRUX, and use it to col-\n9https://github.com/castorini/pyserini\n10https://ir-measur.es/\n11https://www.prolific.com/\n13\n\nSub-questions Generation\nInstruction: Write {n} diverse questions that can reveal the information contained in the given\ndocument. Each question should be self-contained and have the necessary context. Write the\nquestion within \u2018<q>\u2019 and \u2018</q>\u2019 tags.\nDocument: {c\u2217}\nQuestions:\n<q>\nFigure 6: The prompts used for generating a sequence of questions. We set n = 15 for CRUX-DUC and n = 10 for\nMulti-News, as the average length of Multi-News summaries are shorter.\nPassage Generation\nInstruction: Break down the given document into 2-3 standalone passages of approximately 200\nwords each, providing essential context and information. Use similar wording and phrasing as the\noriginal document. Write each passages within \u2018<p>\u2019 and \u2018</p>\u2019 tags.\nDocument: {d\u2217}\nPassages:\n<p>\nFigure 7: The prompt for generating decontextualized passages from a document. We segment the document into\nmultiple documents when the length is longer than 1024.\nGraded Rating Generation\nInstruction: Determine whether the question can be answered based on the provided context? Rate\nthe context with on a scale from 0 to 5 according to the guideline below. Do not write anything\nexcept the rating.\nGuideline:\n5: The context is highly relevant, complete, and accurate.\n4: The context is mostly relevant and complete but may have minor gaps or inaccuracies.\n3: The context is partially relevant and complete, with noticeable gaps or inaccuracies.\n2: The context has limited relevance and completeness, with significant gaps or inaccuracies.\n1: The context is minimally relevant or complete, with substantial shortcomings.\n0: The context is not relevant or complete at all.\nQuestion: {q}\nContext: {c}\nRating:\nFigure 8: The prompts used for judging passage. We independently pair the question q with context c and obtain the\nanswerability scores. The output with incorrect format will be regarded as 0.\n14\n\nOpen-ended Query Generation\nInstruction: Create a statement of report request that corresponds to given report. Write the report\nrequest of approximately 50 words within <r> and </r> tags.\nReport: Whether you dismiss UFOs as a fantasy or believe that extraterrestrials are vis-\niting the Earth and flying rings around our most sophisticated aircraft, the U.S. government has\nbeen taking them seriously for quite some time. \u201cProject Blue Book\u201d, commissioned by the U.S.\nAir Force, studied reports of \u201cflying saucers\u201d but closed down in 1969 with a conclusion that\nthey did not present a threat to the country. As the years went by UFO reports continued to be\nmade and from 2007 to 2012 the Aerospace Threat Identification Program, set up under the\nsponsorship of Senator Harry Reid, spent $22 million looking into the issue once again. Later,\nthe Pentagon formed a \u201cworking group for the study of unidentified aerial phenomena\u201d. This\nstudy, staffed with personnel from Naval Intelligence, was not aimed at finding extraterrestrials,\nbut rather at determining whether craft were being flown by potential U.S. opponents with\nnew technologies. In June, 2022, in a report issued by the Office of the Director for National\nIntelligence and based on the observations made by members of the U.S. military and intelligence\nfrom 2004 to 2021 it was stated that at that time there was, with one exception, not enough informa-\ntion to explain the 144 cases of what were renamed as \u201cUnidentified Aerial Phenomena\u201d examined.\nReport request:\n<r> Please produce a report on investigations within the United States\nin either the public or private sector into Unidentified Flying Objects (UFOs). The report should\ncover only investigative activities into still unidentified phenomena, and not the phenomena\nthemselves. It should include information on the histories, costs, goals, and results of such\ninvestigations.</r>\nReport: {c\u2217}\nReport request: <r>\nFigure 9: We use an example from report generation tasks (Lawrie et al., 2024) and adopt in-context prompting to\ncurate multi-faceted topics.\n15\n\nCRUX-test: Multi-News-4583\nOpen-ended Query. Research the graduation ceremony of Portsmouth High School in New Hampshire and write a\nreport on the activities that took place during the event. Include details on the valedictorian\u2019s speech and the surprise\ndance routine performed by the graduating class.\nSub-questions. (2 questions are filtered by Oracle Passages)\n(#1) What was the initial reaction of the audience when Colin Yost started dancing during his commencement\nspeech?\n(#2) How did Colin Yost prepare his classmates for the surprise dance routine?\n(#3) What song did Colin Yost choose for the flash mob dance routine?\n(#4) What was the main theme of Colin Yost\u2019s commencement speech?\n(#5) What did Colin Yost plan to study in college?\n(#6) What was the audience\u2019s reaction to the flash mob dance routine?\n(#7) How did Colin Yost convince the school administration to allow the flash mob dance routine during the\ngraduation ceremony?\n(#8) What college will Colin Yost be attending in the fall?\n2. How many students from Portsmouth High School\u2019s senior class participated in the choreographed dance celebra-\ntion? 8. Did Colin Yost have any prior dance training before the graduation ceremony?\nOracle Passage. #1. Colin Yost, the valedictorian at Portsmouth High School in Portsmouth, New Hampshire,\ndelivered an unforgettable commencement speech that ended with a surprise dance routine to Taylor Swift\u2019s \u00a8Shake It\nOff. \u00a8He had been planning this moment for some time, inspired by his desire to do a flash mob and showcase his\nclass\u2019s cohesion. Yost worked with a few friends to choreograph the dance and shared an instructional video with\nthe class on YouTube. The administration was on board with the plan, allowing the seniors to use five graduation\nrehearsals to perfect the routine.\nAnswerability (3/10) : [0, 0, 5, 5, 0, 0, 0, 0, 5, 0] \u2013> {#2, #3, #7}\nOracle Passage #2. As Yost began his speech, he emphasized the importance of embracing one\u2019s inner nerd and\nstriving for perfection in anything one is passionate about. He then ended his speech with the iconic line \u00e4ll you\nhave to do is shake it off,\u00a8before breaking into dance. The initial reaction was mixed, with some parents laughing and\nothers looking confused. However, as the front row joined in, followed by another row, the energy shifted, and the\naudience was soon filled with laughter and tears.\nAnswerability (3/10) : [5, 0, 0, 0, 5, 0, 5, 0, 0, 0] \u2013> {#1, #4, #6}\nOracle Passage #3. Yost\u2019s creative and entertaining approach to his commencement speech has gained attention,\nespecially during a season when many notable figures, including President Obama and Stephen Colbert, have been\ndelivering inspiring speeches. Yost\u2019s message of embracing individuality and having fun was well-received by his\nclassmates and their families. As he prepares to attend Princeton in the fall, where he plans to major in chemical\nand biological engineering, Yost\u2019s unique approach to his commencement speech will undoubtedly be remembered.\nAnswerability (2/10) : [0, 0, 0, 0, 5, 5, 0, 0, 0, 5] \u2013> {(#4), #5, #8}\nOracle Result (human-written summary). Parents who thought they were going to have to sit through a boring\ngraduation in a stuffy gym got anything but at Portsmouth High School on Friday. Colin Yost, the valedictorian for the\nNew Hampshire school\u2019s senior class, decided he wanted to shake things up\u2014and off\u2014during his commencement\nspeech, so after his words of inspiration, he stepped out from behind the podium and (#3:) started dancing, by\nhimself, to Taylor Swift\u2019s \"Shake It Off,\" eliciting laughter and some \u00f6h gosh, what is he doing?\u00a8reactions, MTV\nreports. Soon, however, his intentions were made clear as the rest of his graduating class (more than 230 in all) stood\nup and joined Colin in a choreographed celebration of the end of their high school career. While Colin\u2019s flash mob\nsurprised the audience, it was far from spontaneous. (#2:) The senior posted a video tutorial on YouTube for his\nclassmates to study and (#7:) cajoled the school\u2019s administration beforehand into letting him use five graduation\nrehearsals to get the moves down just right, MTV notes. \u00c4s we practiced, the energy was just building and everyone\nwas feeling how great it was to work together and send this positive message,\u00a8he tells the station. He adds that the\nsong-and-dance show played perfectly into what he had talked about in his speech on embracing your inner nerd,\nthe Portsmouth Herald notes. But despite the Taylor-made two-stepping\u2019s success, we probably won\u2019t be seeing\nColin\u2014who admits he\u2019s never taken a dance lesson\u2014on So You Think You Can Dance: He\u2019s headed to Princeton to\nstudy chemical and biological engineering, per MTV. (Hopefully no one got arrested for cheering.)\nAnswerability (4/8): [5 0 0 0 5 5 0 5] \u2013> {#1, #5, #6, #8}\nTable 5: An evaluation example of CRUX-Multi-News.\n16\n\nlect annotations for both tasks. The platform is\nlightweight and built on Django. It is also released\nalong with the data and code repository.\nA.3\nCase Study\nTable 5 presents an example of data from CRUX-\ntest. In this example, the subset of required pas-\nsages (p \u2208P \u2217\n3 ) comprises three passages: oracle\npassages #1, #2, and #3. These passages are greed-\nily selected from all relevant passages (p \u2208P \u2217), as\ndescribed in Section 3.2. The answerability scores\nare also provided as references. The subset can\nanswer 8 out of the 10 generated questions. Conse-\nquently, the 2 unanswered questions are discarded,\nthereby controlling the upper bound of coverage\nand density. This filtering can also mitigate the\nhallucination problem. Interestingly, we observe\nthat the human-written summary does not always\nanswer all the questions generated from it. For\ninstance, questions #2, #3, and #7 have zero an-\nswerability scores. However, upon closer inspec-\ntion, these questions are indeed answerable based\non the summary (i.e., the highlighted texts). This\ncase highlights potential position biases (Liu et al.,\n2024) that may occur when the information in the\nsummary is too dense. It also suggests that decon-\ntextualization could mitigate such biases as each\npassage can answer fewer questions than the con-\ndensed summary.\n17\n\nAnnotation Task 1a: Answerability Judgement\nYour first step is to evaluate whether each of the 15 questions (Q-0 to Q-14) is answerable based\nsolely on the machine-generated report.\n\u2022 Carefully read the entire report before starting the questions (the open-ended query is just for\nyou reference).\n\u2022 Click the corresponding button (e.g. Q-0, Q-1, etc.) to view the question.\n\u2022 Decide if the report contains enough information to answer the question.\n\u2013 If the report provides enough information to answer the question, select \"1 (Answer-\nable)\".\n\u2013 If the report does not provide any information, select \"0 (Unanswerable)\".\n(Note) Your judgment should be based on whether the information is present. You do not need to\nverify external truth.\nAnnotation Task 1b: Nugget Highlighting Support\nFor every question you marked as \u201cAnswerable (1)\u201d, you must also highlight the supporting span(s)\nof text in the report.\n\u2022 Use the provided Nugget Highlighter tool to highlight the exact sentence(s) or phrase(s) that\nsupport the answer.\n\u2022 You may include multiple spans if needed.\n(Note) Do not leave the highlight area blank if you select \"1 (Answerable)\". Each \"1\" must be\njustified with at least one highlighted span.\nFigure 10: The annotation guidelines for task 1a and 1b. They are shown with the annotation interface in Figure 11.\n18\n\nFigure 11: Annotation interface for T1. The sub-questions are fixed and offline-generated. Task 1 requires the\nannotator to first read the report and decide the sub-question answerability. The text area is used for confirming the\nannotator\u2019s rationale by selecting supporting text in the report.\n19\n\nFigure 12: Annotation interface for T2. The two sub-questions are randomly selected from the answerable and\nunanswerable sub-questions labeled previously by annotators. Task 2 requires the annotator to label based on the\nrubric and decide on the scale of 0 to 5.\n20\n"}