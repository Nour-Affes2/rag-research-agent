{"metadata": {"pdf_filename": "2302.03754v1__Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories.pdf", "source": "arXiv"}, "text": "Augmenting Zero-Shot Dense Retrievers with Plug-in\nMixture-of-Memories\nSuyu Ge1\u2217, Chenyan Xiong2, Corby Rosset2, Arnold Overwijk2, Jiawei Han1, Paul Bennett2\n1 University of Illinois Urbana-Champaign\n2 Microsoft Research\n{suyuge2,hanj}@illinois.edu\n{chenyan.xiong,corbyrosset,arnold.overwijk,paul.n.bennett}@microsoft.com\nAbstract\nIn this paper we improve the zero-shot general-\nization ability of language models via Mixture-\nOf-Memory Augmentation (MoMA), a mech-\nanism that retrieves augmentation documents\nfrom multiple information corpora (\u201cexternal\nmemories\u201d), with the option to \u201cplug in\u201d new\nmemory at inference time. We develop a joint\nlearning mechanism that trains the augmenta-\ntion component with latent labels derived from\nthe end retrieval task, paired with hard nega-\ntives from the memory mixture. We instan-\ntiate the model in a zero-shot dense retrieval\nsetting by augmenting a strong T5-based re-\ntriever with MoMA. Our model, MoMA, ob-\ntains strong zero-shot retrieval accuracy on the\neighteen tasks included in the standard BEIR\nbenchmark. It outperforms systems that seek\ngeneralization from increased model parame-\nters and computation steps. Our analysis fur-\nther illustrates the necessity of augmenting with\nmixture-of-memory for robust generalization,\nthe bene\ufb01ts of augmentation learning, and how\nMoMA utilizes the plug-in memory at infer-\nence time without changing its parameters. We\nplan to open source our code.\n1\nIntroduction\nScaling up language models\u2014with more parameters,\ncompute, and annotation data\u2014improves model gen-\neralization ability on downstream applications (Raffel\net al., 2019; Brown et al., 2020; Smith et al., 2022), but\nwith diminishing return: linear improvements on down-\nstream metrics often require exponentially more parame-\nters and computing cost (Kaplan et al., 2020; Hoffmann\net al., 2022). Hence, scaling pretrained language mod-\nels in this way is economically unsustainable (Strubell\net al., 2020; Bender et al., 2021; Zhang et al., 2022).\nRetrieval augmented language models provide a\npromising alternative. They allow language models\nto ef\ufb01ciently access vast resources from an external cor-\npus (Guu et al., 2020; Borgeaud et al., 2022) that serves\nas a kind of \u201cmemory\u201d they can refer to when making\npredictions, alleviating the need to memorize as much\n\u2217Work partly done during Suyu\u2019s internship at Microsoft.\ninformation in their own network parameters (Roberts\net al., 2020). This open-book approach helps language\nmodels to better generalize on token prediction tasks and\nmachine translation (Khandelwal et al., 2019; Borgeaud\net al., 2022), and tasks which already involve a \ufb01rst-\nstage retrieval component, e.g., OpenQA (Borgeaud\net al., 2022; Izacard et al., 2022). Existing retrieval\naugmentation methods usually stick to one single re-\ntrieval corpus throughout training and inference so that\nthe retrieval component can be indirectly guided by the\nsupervision from end tasks.\nIn this paper we improve the zero-shot generalization\nability of language models using \u201cmixture-of-memory\u201d\n(MoMA), a new retrieval augmentation mechanism. In-\nstead of a single corpus, MoMA retrieves documents\nfrom a \u201cmixture\u201d of multiple external corpora and en-\njoys the merits of a larger and more comprehensive\nsource of knowledge. This mechanism also allows re-\nmoving and/or \u201cplugging-in\u201d new corpora during in-\nference time, when more information from the target\ntask is revealed, or as an additional way for users to\ncontrol the model. Speci\ufb01cally, we apply MoMA on the\nzero-shot dense retrieval task, which is the foundation of\nmany important real-world applications (Thakur et al.,\n2021a; Kim, 2022) and also the retrieval component of\nrecent retrieval augmented language models (Guu et al.,\n2020; Izacard et al., 2022). However, it is not trivial\nto guide a retrieval model to leverage multiple corpora.\nWe need to jointly train the augmentation component\nand dense retriever using supervised relevance signals\nand self-mined hard negatives.\nWe instantiate MoMA with a T5 encoder-decoder\nmodel (Ni et al., 2022) and apply it to the dense retrieval\ntask (Karpukhin et al., 2020). Our end task retriever uses\na set of augmenting documents from the mixture-of-\nmemories to enhance its representation of the query with\nimportant context; the retriever then uses the enhanced\nquery representation to retrieve a \ufb01nal candidate set.\nAt inference time, we plug in the target task\u2019s corpus\nto the memory mixture to introduce in-domain context\ninformation, without updating any parameter.\nWe experimented on eighteen zero-shot dense re-\ntrieval tasks included in BEIR (Thakur et al., 2021a), the\nstandard ZeroDR benchmark. The results demonstrate\nthe improved zero-shot ability of MoMA. When paired\nwith the ANCE (Xiong et al., 2020) training framework\narXiv:2302.03754v1  [cs.CL]  7 Feb 2023\n\non a T5 model, it outperforms counterparts without the\nMoMA augmentation component, as well as recent state-\nof-the-art dense retrieval systems of the same scale, by\nlarge margins. To validate its effectiveness when paired\nwith advanced models, we further instantiate MoMA\nwith a contrastively pretrained T5 model. MoMA then\nachieves comparable or even stronger performance to\nZeroDR systems with larger model scales and heavier\ncomputation costs.\nOur analysis reveals that large and diverse corpora in\nthe memory leads to the best performance; while only\nusing a single corpus during training does not improve\nperformance on unseen target tasks. The learning of\naugmentation component is also important for MoMA\nto utilize the diverse information from the mixture. Our\nanalysis and case studies illustrate how MoMA lever-\nages the plug-in memory at testing time to enrich its\nquery representations with in-domain information that\nwas not available in training.\n2\nRelated Work\n2.1\nRetrieval Augmentation\nRecent research has explored two common ways to\nconstruct the external memory in retrieval-augmented\nlanguage models. The \ufb01rst is to retrieve similar tokens\nfor language models to copy from when predicting the\nnext token (Khandelwal et al., 2019; Zhong et al., 2022).\nThe second is to retrieve the related documents (text\nsequences) from an in-domain corpus as additional in-\nput (Guu et al., 2020; Borgeaud et al., 2022). Our work\nfalls into this category as document-based models bet-\nter align with knowledge-intensive tasks (Petroni et al.,\n2020), such as retrieval and OpenQA (Chen et al., 2017).\nLearning to retrieve useful documents to augment the\nlanguage model is a challenging task, since human anno-\ntations on the usefulness of augmentation documents are\ncostly and seldom available. The most straightforward\nway is to use representations from raw pretrained lan-\nguage models to \ufb01nd documents similar to the task input,\ni.e., as unsupervised dense retrieval (Guu et al., 2020;\nBorgeaud et al., 2022). Adapting dense retrieval mod-\nels trained for relevance matching is another common\nchoice (Izacard and Grave, 2020b; Lewis et al., 2020;\nYu et al., 2021). A more formal solution is to jointly\nlearn the augmentation components end-to-end using\nsupervision from the \ufb01nal task, for example, treating the\naugmentation as latent variables and applying EM (Zhao\net al., 2021), or distilling the augmentation component\nfrom feedback of the \ufb01nal model (Izacard and Grave,\n2020a). In a parallel work, Izacard et al. (2022) found\nthe most effective one is attention distillation method\n(ADist), which trains the augmentation component us-\ning soft labels derived from the end model\u2019s attention\non augmentation documents.\nThe motivation for query augmentation coincides\nwith the query expansion methods in the traditional\nIR community, whereby the user\u2019s original query\nis augmented by new features with similar mean-\nings (Carpineto and Romano, 2012). As feature selec-\ntion usually requires additional semantic analysis, the\nef\ufb01ciency and usability of traditional query expansion\nmethods remain limited when faced with a new domain.\nTo overcome this, recent work relies on dense retrieval\nresults to expand the query (Yu et al., 2021). The re-\ntrieved relevant documents serve as pseudo relevance\nfeedback signals for the model, which are concatenated\nwith the original query as the augmented model input.\nOur work augments queries with feedback from multi-\nple corpora and learns to select important augmentation\ndocuments automatically.\n2.2\nZero-shot Dense Retrieval\nDense retrieval models trained on a resource rich source\ntasks, e.g., web search, usually do not perform as well\nwhen zero-shot transferred to other domains (Thakur\net al., 2021b). This is concerning since many impor-\ntant real-world scenarios do not have the luxury of web\ncorpus training signals and must rely on near zero-shot\ntransfer, e.g., the medical domains (Kim, 2022). Xin\net al. (2021) analyzed the challenge of shifting between\ntraining and testing domains, and leveraged domain-\ninvariant learning to mitigate the gap. Another common\napproach is to \ufb01rst generate domain-speci\ufb01c pseudo\nlabels for each task, and then use them to train dense\nretriever (Thakur et al., 2021b; Wang et al., 2022). Ad-\nditionally, continuous pretraining the language model\nalso improves its generalization ability in ZeroDR (Izac-\nard et al., 2021; Gao and Callan, 2022; Yu et al., 2022).\nFollowing works (Izacard et al., 2021; Yu et al., 2022)\nfurther contrastively pretrained the retriever on source\nor target corpus with a sentence matching loss. Other\nmethods seek better generalization ability in ZeroDR\nfrom various resources, for example, combining with\nsparse retrieval to introduce exact match signals (For-\nmal et al., 2021), using multiple vectors per documents\nfor term-level matching (Khattab and Zaharia, 2020a),\nor scaling up the retrieval model using larger language\nmodels (Ni et al., 2021; Neelakantan et al., 2022).\n3\nMethod\nIn this section we \ufb01rst describe our Mixture-of-Memory\nAugmentation. Then we discuss how it is jointly learned\nwith the end system and enables plug-in memory at\ninference time.\n3.1\nMixture-of-Memory Augmentation\nBefore going to the details of MoMA, we \ufb01rst recap\nsome preliminaries in ZeroDR.\nPreliminaries. The dense retrieval (DR) task aims to\n\ufb01nd relevant documents d from a corpus C for the given\nquery q by representing them in a shared embedding\nspace. Speci\ufb01cally, the retrieval score in DR is often\ncalculated as:\nf(q, d) = q \u00b7 d; q = g(q); d = g(d).\n(1)\n\nIt uses dot product as the scoring function to match the\nembeddings q and d, which is known to support ef\ufb01cient\nnearest neighbor search (ANN) (Johnson et al., 2019). A\npretrained language model is often the encoder of choice\ng(). We use the ST5-EncDec variant of Sentence-T5 (Ni\net al., 2022):\ng(x) = Dec(Enc(x)),\n(2)\nwhich feeds in the text sequence (prepended by a special\n[CLS] tokens) to the encoder of T5, Enc(), and uses\nthe output representation of the [CLS] token from the\ndecoder, Dec(), as the text representation. This naturally\nleverages the attention from decoder to encoder at all\nTransformer layers (Raffel et al., 2019), as a \ufb01ne-grained\ninformation gathering mechanism.\nThe training of dense retrieval systems often applies\nstandard ranking loss and pairs the relevant documents\nd+ \u2208D+ for each query q with hard negatives d\u2212\u2208\nD\u2212:\nL =\nX\nq\nX\nd+\u2208D+\nX\nd\u2212\u2208D\u2212\nl(f(q, d+), f(q, d\u2212));\nD\u2212\u223cANNC\nf(q,\u25e6) \\ D+.\n(3)\nEqn. 3 uses ANCE hard negatives, which are the top-\nretrieved documents from C using the retriever it-\nself (Xiong et al., 2020). The loss function l() can\nbe any standard ranking loss such as cross entropy. A\nZeroDR model is trained on qs and documents ds \u2208Cs\nfrom a source task, often web search, and tested on tar-\nget tasks qt and Ct; supervision signals are only present\nfrom the source.\nMixture-of-Memory Augmentation. The key idea\nof (document-based) retrieval augmented language mod-\nels is to enrich the representation g(q) with additional\ncontextual input for the model, i.e., augmentation doc-\numents da retrieved from an external memory M. In-\nstead of using a single document corpus, MoMA uses\nmultiple corpora to provide richer and more diverse ex-\nternal resources for augmentation. For example, M\ncan be composed by the source corpus Cs, a general\nencyclopedia, a domain speci\ufb01c knowledge graph, etc.\nThen we can retrieve the augmentation documents Da :\nDa = ANNM\nf a(x,\u25e6); M = {C1, ..., CM}.\n(4)\nThis augmentation component uses another dense re-\ntriever f a() (also a Sentence T5 model), with param-\neters distinct from those in g(). Note that instead of\nretrieving Da separately from M different ANN mem-\nory sources and merging results, Eqn. 4 combines them\ninto one ANN index. This requires the augmentation\ncomponent f a() to be \ufb02exible enough handle various\ncorpora in the mixture.\nUsing the encoder-decoder architecture for g() in\nEqn. 2 enables a simple extension to incorporate the\naugmentation documents using the fusion-in-decoder\n(FiD) mechanism (Izacard and Grave, 2020b):\ngMoMA(q) = Dec(Enc(q), Enc(da\n1), ..., Enc(da\nK));\nDa = {da\n1, ..., da\nK}.\n(5)\n\ud835\udc51%\n!\n\ud835\udc51)\n!\nMedical KG\nPlug-in\nCorpus\n\ud835\udc5e\n\ud835\udc53!(\ud835\udc5e,\u2218)\n\ud835\udc92\ud835\udc82\n[CLS]\nMixture of Memory\n\ud835\udc53+,+-(\ud835\udc5e!,\u2218)\nAug\nAug\nEnc\nEnc\nEnc\nAttention\nFusing\nDec\nFigure 1: Illustraion of the Mixture-of-Memory Aug-\nmentation.\nIt feeds in the K augmentation documents separately\nto the T5 encoder of g(). Then it fuses the encoded\ndocuments together with Enc(q) using one decoder that\nattends to all encoded vectors, as illustrated in Figure 1.\nThe FiD approach in Eqn 5 is a nice balance of ef-\n\ufb01ciency and capacity when modeling multiple text se-\nquences (Izacard and Grave, 2020b). It is more ef\ufb01cient\nthan concatenating all text pieces together, while also\nremaining expressive enough to model the nuances from\nmany sequences. (Izacard and Grave, 2020a; Izacard\net al., 2022).\nWhen instantiating MoMA in the dense retrieval set-\nting, we focus on augmenting the query representation\nq, as queries are often short, ambiguous, and bene\ufb01t\nmore from additional contextual information (Lavrenko\nand Croft, 2017; Yu et al., 2021). This leads to the\nfollowing de\ufb01nition of MoMA:\nf MoMA(q, d) =qa \u00b7 d;\nqa = gMoMA(q),d = g(d),\n(6)\nusing the construction of gMoMA() in Eqn. 5 upon the\naugmentation documents de\ufb01ned in Eqn. 4.\n3.2\nJoint Learning in MoMA and Inference with\nPlug In Memory\nMoMA has two sets of parameters to learn, in the main\nmodel f MoMA() and the augmentation component f a().\nBoth have their own T5 encoder-decoder parameters.\nThe two components are bridged by the augmentation\ndocuments, which are retrieved by f a() from M and\nused by f MoMA() to produce query representation qa.\nMain Model Learning. Given the relevance labels\nfrom the source task and an augmentation model, train-\ning f MoMA() is straightforward. We can use the standard\ndense retrieval training to \ufb01netune the enriched query\nencoder gMoMA() and the document encoder g():\nLMoMA =\nX\nqs\nX\nd+\nX\nd\u2212\nl(f MoMA(qs, d+), f MoMA(qs, d\u2212));\nd+ \u2208Ds+, d\u2212\u2208Ds\u2212\n(7)\nDs\u2212\u223cANNCs\nf MoMA(qs,\u25e6) \\ Ds+.\n(8)\nThe training signals come from the source task, includ-\ning qs, its relevant documents Ds+, and ANCE hard\nnegatives Ds\u2212retrieved from the source corpus Cs.\n\nAugmentation Learning. Training f a() is challeng-\ning as it is hard to label whether an augmentation docu-\nment is useful. Propagating gradients from the \ufb01nal loss\nto f a() is also prohibitive as the retrieval operation in\nEqn. 4 is discrete. Fortunately, recent research found the\nattention scores from the FiD decoder to each encoded\ninputs (Eqn. 5) are good approximations to the useful-\nness of augmentation documents (Izacard and Grave,\n2020a):\nFidAtt(da\ni ) =\nX\nlayers\nX\npositions\nX\nheads\nAttDec\u2192Enc(gMoMA(da\ni )).\n(9)\nIt sums the attentions from gMoMA()\u2019s special token at\nthe decoder\u2019s [CLS] position over all layers, input po-\nsitions, and attention heads. Ideally, higher FidAtt() is\nassigned to da\ni that provides useful contextual informa-\ntion.\nPreviously, FidAtt scores are often used as soft labels\nfor the augmentation model (Izacard and Grave, 2020a;\nIzacard et al., 2022). Doing so with memory mixtures\nis risky as it is too sparse and over\ufb01ts memory resource\nthat appears earlier in the training, which are the only\nones available for the decoder to attend on. To improve\nthe learning robustness, we introduce ANCE-style hard\nnegative mining to train the augmentation component\nas well.\nFirst, we formulate the positive set of augmentation\ndocuments as:\nDa+ = Ds+ \u222aTop-NFidAtt(da\ni ),Da.\n(10)\nwhich combines relevant documents Ds+ and the aug-\nmenting ones that received N-highest attention scores\nfrom gMoMA(). Then we pair them with hard negatives\nto formulate the training of f a() as:\nLa =\nX\nqs\nX\nd+\u2208Da+\nX\nd\u2212\u2208Da\u2212\nl(f a(qs, d+), f a(qs, d\u2212));\n(11)\nDa\u2212\u223cANNM\nf a(qs,\u25e6) \\ Da+.\n(12)\nNotice the negatives for f a() have comprehensive cov-\nerage from multiple corpora.\nIterative Training. The learning of f MoMA() and\nf a() is an iterative process that \ufb01ts naturally into the\ntraining procedure of dense retrieval training with hard\nnegatives. We follow the standard iterations in ANCE\nand construct the t-th training episode of MoMA:\n1. Construct hard negatives Ds\u2212via Eqn. 8 using\nweights f MoMA\nt\u22121\n() from the last episode;\n2. Retrieve augmentation Da via Eqn. 4 using\nweights f a\nt\u22121() from the last episode;\n3. Train f MoMA\nt\n() as Eqn. 7;\n4. Formulate new positive augmentation docu-\nments Da+, using updated attention scores from\nf MoMA\nt\n(), and mine negative augmentation docu-\nments Da\u2212using f a\nt\u22121();\n5. Train f a\nt () following Eqn. 11.\nBoth f MoMA\n0\n() and f a\n0 () can be initialized with a BM25\nwarmed-up T5 retriever. Steps 1 and 3 above are in-\nherited from standard dense retrieval training. The rest\nare introduced by MoMA. The additional computation\nin the training side mainly resides updating the index\nfor the memory mixture, a standard cost in retrieval-\naugmented language models (Guu et al., 2020; Izacard\net al., 2022).\nZero-Shot Retrieval with Plug in Memories. To\nperform zero-shot retrieval on unseen tasks, MoMA\n\ufb01rst retrieves augmented documents using f a() from M\nfor the target query qt, and retrieves target documents\ndt \u2208Ct with the augmented model f MoMA() without\nchanging any model parameters. MoMA allows f a()\nto attend over the target corpus as well if it is plugged\nin: M = M \u222aCt \\ Cs, which conveys in-domain\ninformation. The augmenting corpus can also be engi-\nneered by users manually to inject their preference or\ndomain knowledge, e.g., as \u201cmemory engineering\u201d. In\nthis work we focus on swapping out the source corpus\nfor the target corpus; we leave other explorations for\nfuture work.\n4\nExperimental Methodologies\nDatasets.\nWe choose the MS MARCO passage\ndataset (Bajaj et al., 2016) as the source domain dataset,\nwhereas the target domains are from the 18 datasets\nin BEIR (Thakur et al., 2021b) benchmark, which in-\nclude including biomedical, scienti\ufb01c and \ufb01nancial texts.\nMore details can be found in Appendix A.1. The evalu-\nation metric NDCG@10 is the same with BEIR bench-\nmark, which measures Normalized Discounted Cumula-\ntive Gain (Wang et al., 2013) of top 10 prediction. The\nhigher NDCG@10 value indicates better performance.\nAugmenting Corpora. During training, the mixture-\nof-memory is composed of source training corpus\n(MARCO), Wikipedia and a medical knowledge\ngraph.\nWe use the Wikipedia chunk prepossessed\nby (Karpukhin et al., 2020) without further process-\ning1. The medical knowledge graph is extracted from\nthe Medical Subject Headings (MeSH)2, an open-source\ndatabase for indexing and cataloging of biomedical and\nhealth-related information. Since it is hierarchical in\nstructure, we linearize it by concatenating spans with\ntext information. During testing, we directly replace\nMARCO with the corresponding document sets from\nBEIR. Each task from BEIR is augmented indepen-\ndently. More dataset and preprocessing details can be\nfound in Appendix A.1.\nBaselines and Model Choices. We compare our\nMoMA with standard sparse and dense retrieval mod-\nels on BEIR. We also compare MoMA with advanced\n1https://huggingface.co/datasets/wiki_dpr\n2https://www.ncbi.nlm.nih.gov/mesh/\n\nTable 1: NDCG@10 on the BEIR benchmark. We also include an averaged score on datasets used by Contriever\nfor a fair comparison. The best result each task is marked bold. An \u2217denotes unfair comparison, as NQ is used in\ntraining for GTR. \u2020: GenQ generated pseudo labels to train an independent model for each task. \u2021: Larger models\nBM25\nDPR\nANCE\nT5-ANCE\ncoCondenser\nGenQ\u2020\nColBERT\nContriever\nGTRbase\u2217\nGTRlarge\u2217\u2021\nMoMA\n(T5-ANCE)\nMoMA\n(COCO)\nParameters#\n\u2014\n110M\n110M\n110M*2\n110M\n66M*18\n110M\n110M\n110M\n335M\n110M*2\n110M*2\nTREC-COVID\n0.656\n0.575\n0.654\n0.653\n0.715\n0.619\n0.677\n0.596\n0.539\n0.557\n0.762\n0.761\nBioASQ\n0.465\n0.232\n0.306\n0.322\n0.318\n0.398\n0.474\n\u2014\n0.271\n0.320\n0.372\n0.371\nNFCorpus\n0.325\n0.210\n0.237\n0.275\n0.307\n0.319\n0.305\n0.328\n0.308\n0.329\n0.307\n0.333\nNQ\n0.329\n0.398\n0.446\n0.452\n0.494\n0.358\n0.524\n0.498\n0.495\n0.547\n0.490\n0.544\nHotpotQA\n0.603\n0.371\n0.456\n0.487\n0.566\n0.534\n0.593\n0.638\n0.535\n0.579\n0.539\n0.589\nFiQA-2018\n0.236\n0.274\n0.295\n0.294\n0.285\n0.308\n0.317\n0.329\n0.349\n0.424\n0.320\n0.329\nSignal-1M\n0.330\n0.238\n0.249\n0.246\n0.274\n0.281\n0.274\n\u2014\n0.261\n0.265\n0.258\n0.264\nTREC-NEWS\n0.398\n0.366\n0.382\n0.379\n0.389\n0.396\n0.393\n\u2014\n0.337\n0.343\n0.413\n0.453\nRobust04\n0.408\n0.344\n0.392\n0.412\n0.399\n0.362\n0.391\n\u2014\n0.437\n0.470\n0.469\n0.475\nArguAna\n0.414\n0.414\n0.415\n0.415\n0.411\n0.493\n0.233\n0.446\n0.511\n0.525\n0.438\n0.463\nTouch\u00e9-2020\n0.367\n0.208\n0.240\n0.312\n0.190\n0.182\n0.202\n0.230\n0.205\n0.219\n0.271\n0.299\nQuora\n0.789\n0.842\n0.852\n0.836\n0.863\n0.830\n0.854\n0.865\n0.881\n0.890\n0.847\n0.843\nDBPedia-entity\n0.313\n0.236\n0.281\n0.290\n0.356\n0.328\n0.392\n0.413\n0.347\n0.391\n0.347\n0.383\nSCIDOCS\n0.158\n0.107\n0.122\n0.115\n0.140\n0.143\n0.145\n0.165\n0.149\n0.158\n0.143\n0.145\nFever\n0.753\n0.589\n0.669\n0.655\n0.678\n0.669\n0.771\n0.758\n0.660\n0.712\n0.723\n0.745\nClimate-Fever\n0.213\n0.176\n0.198\n0.194\n0.184\n0.175\n0.184\n0.237\n0.241\n0.262\n0.235\n0.233\nSciFact\n0.665\n0.475\n0.507\n0.566\n0.600\n0.644\n0.671\n0.677\n0.600\n0.639\n0.632\n0.630\nCQADupStack\n0.299\n0.281\n0.296\n0.283\n0.330\n0.347\n0.350\n0.345\n0.357\n0.384\n0.283\n0.294\nContriever Sub Avg\n0.437\n0.368\n0.408\n0.416\n0.438\n0.425\n0.445\n0.466\n0.442\n0.471\n0.453\n0.471\nAvg\n0.428\n0.352\n0.391\n0.399\n0.417\n0.410\n0.431\n\u2014\n0.416\n0.444\n0.436\n0.453\nTable 2: Computational analysis in the pretraining stage\nof different models.\nModel\nPretraining Corpus\nBatch Size\nTraining Steps\nMoMA (T5-ANCE)\n0\n0\n0\nMoMA (COCO)\nMARCO\n128\n50k\nGTR\nNQ, CQA\n2048\n800k\nContriever\nCCNet\n2048\n500k\nWikipedia\n2048\n200k\napproaches that are speci\ufb01cally designed for zero-shot\ngeneralization. They involve techniques that are not di-\nrectly comparable with this paper, including pretraining\non extra data, in-domain continuous pretraining, and\ngenerating target pairs using another pretrained gener-\native model. Besides, some baselines use larger scale\nlanguage model as their backbone. We list the details of\nbaselines in Appendix A.2.\nAs a plug-in-and-play method, MoMA can be com-\nbined with other techniques. We initiate MoMA on\ntwo versions of T5 model checkpoints.\nThe primi-\ntive MoMA (T5-ANCE) is built on the original T5\nmodel checkpoint. By comparing it with T5-ANCE,\nwe can clearly observe the performance gain brought\nby MoMA. To demonstrate it can integrate techniques\nfrom other models to achieve higher performances, we\napply MoMA with a better pretrained T5-based model.\nFollowing previous work (Gao and Callan, 2022; Yu\net al., 2022), we continuously trained the T5 model on\nthe MARCO corpus using a sentence-level contrastive\nloss, combined with the original masked language mod-\neling loss. We then performed the same MoMA training\non top of the continuously pretrained T5 checkpoint\nand denoted it as MoMA (COCO). Both MoMA (T5-\nANCE) and MoMA (COCO) are trained iteratively\nwith ANCE-style (Xiong et al., 2020) hard negatives,\nthe only difference is the initialized model start point.\nWe compare their pretraining details with other models\nin Table 2. Unlike previous work (Yu et al., 2022), we\ndid not include target datasets and augmenting corpora\nin the COCO pretraining stage. Since MARCO contains\nonly 0.5M documents, it adds fewer computational over-\nhead compared to other methods listed in the table, e.g.,\nContriever.\nImplementation Details. For MoMA, we use the T5-\nbase (Raffel et al., 2019) architecture (12-layer Trans-\nformer, 768 hidden size) by directly loading the check-\npoint from HuggingFace3. To warm up the language\nmodel for dense retrieval, we followed (Xiong et al.,\n2020) to further train it using BM25 negatives for 10\nepochs. After warming up, we jointly trained the two\ncomponents for three episodes, each episode including\nthree training epochs. After three joint episodes, the end\nretriever reaches the best performance on MSMARCO,\nso we select this checkpoint for evaluation. The ratio\nbetween positive and hard negative pairs is 1:7 for both\nmodels. The main hyperparameters in MoMA include\nthe total number of grounding documents K and the at-\ntention threshold number N in Equation 10. We directly\nset K=10 and N=5 without any parameter tuning. More\ndetails on hyperparameters and experimental settings\ncan be found in Appendix A.3.\n5\nEvaluation Results\nOur experiments evaluate the zero-shot ability of\nMoMA, its performance with different memory sources,\nthe in\ufb02uence of memory mixture learning, and the ben-\ne\ufb01ts of plug-in memory.\n5.1\nZero-Shot Retrieval Accuracy and Ef\ufb01ciency\nThe retrieval accuracy of MoMA and baselines are listed\nin Table 1. Besides baselines of similar parameter count,\nwe also include larger models (GTRlarge) or those us-\ning multiple vectors per document (ColBERT). MoMA\n(COCO) shows the strongest zero-shot accuracy against\nprevious state-of-the-art methods that do continuous\ncontrastive pretraining (coCondenser), generate pseudo\nlabels (GenQ), or consume additional training signals\n3https://huggingface.co/t5-base\n\nTable 3: Ef\ufb01ciency of MoMA search and training.\nOperation\nOf\ufb02ine\nOnline\nBM25 Index Build\n1.8h\n\u2014\nBM25 Retrieval Per Query\n\u2014\n43ms\nMoMA Inference\nEncoding of Corpus/Per Doc\n1.5h/4.5ms\n\u2014\nQuery Encoding\n\u2014\n55ms\nANN Retrieval (batched q)\n\u2014\n9ms\nDense Retrieval Total\n\u2014\n64ms\nMoMA Training\nEncoding of Corpus/Per Doc\n1.5h/4.5ms\n\u2014\nANN Index Build\n10s\n\u2014\nNeg Construction Per Batch (32 queries)\n45ms\n\u2014\nBack Propagation Per Batch (32 queries)\n330ms\n\u2014\nin both continuous pretraining and \ufb01netuning phrases\n(GTRbase). MoMA (T5-ANCE) also achieved nearly\ncomparable zero-shot accuracy against larger models\nlike GTRlarge, and ColBERT, which scales up the num-\nber of vectors per documents (one per token). This\ncon\ufb01rms that retrieval-augmentation provides another\npath to improve language models\u2019 generalization ability\nbesides scaling up. MoMA (T5-ANCE) also outper-\nforms T5-ANCE, which MoMA (T5-ANCE) uses as\na subroutine for retrieval augmentation, on all but one\nretrieval task, showing the robustly improved general-\nization ability from plug-in mixture of memory.\nWe evaluate the ef\ufb01ciency of MoMA in two stages:\nof\ufb02ine model training and online inference. In of\ufb02ine\ntraining from Table 2, MoMA (T5-ANCE) is signi\ufb01-\ncantly cheaper than other methods as we do not re-\nquire pretraining on large external corpora, which saves\nhundreds of hours training time. MoMA (COCO) addi-\ntionally pretrain on MARCO for 50k steps, which is far\nfewer than the other compared methods. In online in-\nference, similar with other retrieval enhanced language\nmodels, MoMA imposes a necessary cost of retrieval\naugmented model upon the baseline T5-ANCE. We fur-\nther provide detailed ef\ufb01ciency analysis on MoMA in\nTable 3. The online latency is measured on one query\nand 100 retrieved documents. Due to the query augmen-\ntation, query encoding is more costly and takes about\n55ms per query. Even with the augmentation cost, the\nfull dense retrieval total online inference cost is 64ms,\nonly slightly above the BM25 retrieval latency. The\nANN retrieval is very ef\ufb01cient, only takes 9ms. In ad-\ndition, the complexity of ANN retrieval is sub-linear\nto the corpus size, in most ANN framework such as\nFAISS. Thus the extra round of ANN retrieval operation\nin MoMA is not the bottleneck even when the size of\nmemory mixture scales up.\n5.2\nPerformance with Different Memories\nTable 4 evaluates how MoMA behaves under different\ncombinations of external memories. Compared with the\nMoMA (T5-ANCE), MoMA (COCO) may lean towards\nthe MARCO corpus since it is continuously pretrained\non it. To avoid unfair comparison between MARCO\nand other corpora, we choose MoMA (T5-ANCE) as\nthe Full model version for ablation studies. Unsurpris-\ningly, using a single out-of-domain memory for retrieval\naugmentation does not help, for example, even though\nMARCO is the source domain corpus, solely grounding\non it reduces zero-shot accuracy. MeSH as the sole aug-\nmenting corpus also lowers performance, even on some\nmedical retrieval tasks such as BioASQ. Interestingly,\nwhen we expand the memory to include MARCO, Wiki,\nand MeSH, but keep the target corpus excluded (w/o\nTarget), MoMA exhibits better accuracy compared to\nthe no-memory T5-ANCE. Our conclusion is that more\nmemory sources achieves better generalization, espe-\ncially when no target domain information is available.\nIn the Full setting, the 3-memory mixture of MARCO,\nWiki, and MeSH is jointly learned with \ufb01nal task at\ntraining time. At test time, MARCO is swapped out for\nthe target corpus. The Full improves zero-shot accuracy\nover both the w/o Target setting (where the target corpus\nis excluded at test time), and the w/o Learning setting\n(wherein the augmentation component is not learned).\nAs expected, plugging in the target corpus at test time\nis the most valuable source of generalization power. It\nis also the most realistic, as access to the target corpus\nmay only be available at testing time.\n5.3\nEffect of Memory Mixture Learning\nTo study the effect of our joint learning mechanism on\nthe memory mixture, we compare it with recent state-\nof-the-art Attention Distillation (ADist), which is \ufb01rst\nused in Izacard and Grave (2020a) and recently updated\nin a parallel work Izacard et al. (2022). It jointly trains\nthe augmentation model using attention scores from the\nend language model as pseudo-labels. We also enrich\nADist with relevance labels from MARCO for more\ndirect supervision, which was shown to be effective in\ndistilling a dense retriever from stronger cross-encoder\nranking model (Hofst\u00e4tter et al., 2021). Similar to previ-\nous section, to exclude the performance gain brought by\ncontrastive pretraining, we choose MoMA (T5-ANCE)\nas our own method for comparison. The performances\nof these joint learning methods are listed in Table 5. We\npick six BEIR tasks whose domains are closely related\nto the augmentation corpora: TREC-COVID, BIOASQ,\nand NFCorpus are medical search and closely related to\nMeSH. NQ, HotpotQA, and FEVER are all Wikipedia\nbased. The results show that ADist, either standalone\nor enriched with MARCO labels, does not improve the\n\ufb01nal accuracy compared to using a supervised dense\nretriever as the augmentation component without joint\nlearning. The main difference is that the supervised\nretriever has been trained effectively using hard neg-\native sampling (Xiong et al., 2020). Jointly learning\nusing soft labels without hard negatives downgraded\nthe augmentation accuracy. Hence, MoMA is a simple\ntechnique to learn the end task signals via the attention\nscores together with hard negatives, which improves\nquality over a supervised retriever alone.\nTo further illustrate the joint training process, we\ntrack the attention scores of documents from different\n\nTable 4: NDCG@10 of MoMA (T5-ANCE) under different memory compositions: no memory, single memory,\nand a mixture of memories. w/o Learning uses the end retriever to select augmenting documents without use of an\naugmentation component. w/o Target excludes the target from memory.\nNo Memory\nSingle Memory\nMemory Mixture\nT5-ANCE\nMARCO\nWiki\nMeSH\nTarget\nw/o Learning\nw/o Target\nFull\nTREC-COVID\n0.653\n0.576\n0.592\n0.669\n0.731\n0.759\n0.664\n0.762\nBioASQ\n0.322\n0.247\n0.262\n0.219\n0.361\n0.359\n0.271\n0.372\nNFCorpus\n0.275\n0.295\n0.302\n0.282\n0.319\n0.317\n0.301\n0.307\nNQ\n0.452\n0.472\n0.486\n0.393\n0.483\n0.510\n0.484\n0.490\nHotpotQA\n0.487\n0.481\n0.519\n0.462\n0.538\n0.539\n0.520\n0.539\nFiQA-2018\n0.294\n0.296\n0.286\n0.280\n0.320\n0.304\n0.285\n0.320\nSignal-1M\n0.246\n0.239\n0.225\n0.238\n0.250\n0.248\n0.240\n0.258\nTREC-NEWS\n0.379\n0.381\n0.391\n0.372\n0.416\n0.410\n0.398\n0.413\nRobust04\n0.412\n0.435\n0.443\n0.428\n0.483\n0.446\n0.452\n0.469\nArguAna\n0.415\n0.439\n0.438\n0.442\n0.439\n0.427\n0.438\n0.438\nTouch\u00e9-2020\n0.312\n0.281\n0.281\n0.252\n0.331\n0.275\n0.272\n0.271\nQuora\n0.836\n0.809\n0.798\n0.835\n0.781\n0.813\n0.812\n0.847\nDBPedia-entity\n0.290\n0.340\n0.341\n0.287\n0.335\n0.331\n0.342\n0.347\nSCIDOCS\n0.115\n0.128\n0.121\n0.130\n0.146\n0.134\n0.127\n0.143\nFever\n0.655\n0.663\n0.735\n0.610\n0.694\n0.718\n0.737\n0.723\nClimate-Fever\n0.194\n0.231\n0.238\n0.231\n0.228\n0.222\n0.240\n0.235\nSciFact\n0.566\n0.583\n0.587\n0.585\n0.624\n0.618\n0.598\n0.632\nCQADupStack\n0.283\n0.207\n0.218\n0.203\n0.283\n0.235\n0.215\n0.283\nAvg\n0.399\n0.395\n0.403\n0.384\n0.431\n0.426\n0.411\n0.436\nTable 5: Zero-shot Performances of different distillation methods. We observe consistent trend on all BEIR datasets.\nWe present results on 6 representative datasets from Wikipedia or medical domains.\nDistillation Method\nTREC-COVID\nBIOASQ\nNFCorpus\nNQ\nHotpotQA\nFEVER\nAvg\nSoft Attention Distill\nADist (Izacard et al., 2022)\n0.609\n0.185\n0.227\n0.351\n0.387\n0.615\n0.396\nADist + MSMARCO rel\n0.664\n0.220\n0.255\n0.397\n0.394\n0.624\n0.426\nw/o Distilling (Fixed)\n0.741\n0.361\n0.301\n0.472\n0.513\n0.684\n0.512\nMoMA (T5-ANCE)\n0.762\n0.372\n0.307\n0.490\n0.539\n0.723\n0.532\nmemory sources as well as their ratio in the augmenta-\ntion set in Figure 2. We also split MARCO documents\nby whether they are labeled as Relevant (Rel) for the\ncorresponding query.\nFirstly, MoMA learns to increasingly attend to, and\nretrieve, relevant documents from the memory mixture\nthroughout training. In Figure 2a, more attention is\npaid to MARCO Relevant documents than to any other\ntype in the memory. Although the number of MARCO\nRelevant documents is not signi\ufb01cant as a percentage of\nthe augmenting set in Figure 2c, a query level analysis\ncon\ufb01rms that percentage of queries having at least one\nrelevant document in the augmenting set increases from\n46% in Epi-0 to 62% in Epi-2.\nThis apparent discrepancy can be explained by the\nfact that MARCO has only one relevant label per query\non average, leaving plenty of room for other types of\ndocuments to be included in the augmenting set.\nSecondly, the amount of attention paid to certain\ntypes of documents by MoMA is positively correlated\nwith their representation in the augmenting set. This\ncon\ufb01rms that the joint learning effectively conveys the\nfeedback signals from the end model to the augmenta-\ntion component. For instance, in Figure 2a, MoMA pays\na high level of attention to MARCO Other documents, a\nsignal re\ufb02ected in the composition of its augmentation\nset in Figure 2c. Even though MARCO Other doc-\numents were not labeled relevant for the query, they\ncan still prove to be valuable as an augmenting docu-\nment because they may contain partial information that\nhelps query understanding (Lavrenko and Croft, 2017)\nor it was simply not annotated in MARCO\u2019s sparse\nlabels (Bajaj et al., 2016). In comparison, the correla-\ntion of the two in ADist is weak as the model seems to\ninclude 60% augmenting documents from MeSH, far\ngreater than the fraction of medical queries in MARCO.\n5.4\nGeneralization of Plug-In Memory\nIn the previous section, we observed how MoMA learns\nto attend to, and retrieve, informative documents from\nmemories on which it was trained. In this section, we\nexamine the zero-shot behavior of MoMA (T5-ANCE)\non new corpora plugged-in at test time (keeping Wiki\nand MeSH as before).\nFigure 3 compares documents from the plugged-in\ntarget versus the remaining memory mixture in terms of\nmembership in the augmenting set (Doc Ratio) and at-\ntention. Again, on all tasks, MoMA (T5-ANCE) heavily\nattends to \u2013 and successfully retrieves \u2013 in-domain doc-\numents, even if those in-domain documents were only\njust plugged in. This con\ufb01rms that the augmentation\nmodel achieves the zero-shot ability to capture relevant\ninformation from unseen corpora.\nIn the medical domain, the model pays more attention\n\nEpi-0\nEpi-1\nEpi-2\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nMeSH\nWiki\nMarco Rel\nMarco Others\n(a) MoMA Att. Score.\nEpi-0\nEpi-1\nEpi-2\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n(b) ADist Att. Score.\nEpi-0\nEpi-1\nEpi-2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(c) MoMA Doc Ratio.\nEpi-0\nEpi-1\nEpi-2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(d) ADist Doc Ratio.\nFigure 2: Grounding component breakdown for different distillation methods in each learning iteration. We display\nthe regularized doc and att. score ratio of documents from different augmentation sources.\nNQ\nHotpotQA\nFEVER\n0\n20\n40\n60\n80\n100\nTarget\nWiki\nMeSH\n(a) Doc Ratio. (Wiki)\nNFCorpus\nTREC-Covid\nBIOASQ\n0\n20\n40\n60\n80\n100\n(b) Doc Ratio. (Med)\nNQ\nHotpotQA\nFEVER\n0\n20\n40\n60\n80\n100\n(c) Att. Score Ratio. (Wiki)\nNFCorpus\nTREC-Covid\nBIOASQ\n0\n20\n40\n60\n80\n100\n(d) Att. Score Ratio. (Med)\nFigure 3: The inclusion of Plug-In memory during testing (grouped by the Wiki and Medical domains).\nto MeSH documents, especially on TREC-Covid task\nsince MeSH includes high quality updated information\nrelated to COVID-19. Wikipedia documents received\nmore attention on the Wiki-centric tasks like FEVER, as\nexpected. Some tasks may need a small amount of pre-\ncise information from Wikipedia to answer the detailed\nquestion, e.g. in HotpotQA. Similar with the training\nprocess, there is a non-trivial correspondence between\nattention score of a memory and its membership in the\naugmentation set.\n5.5\nCase Studies\nTable 6 shows examples of how augmenting documents\nchosen by MoMA can provide valuable contextual in-\nformation for the query. The \ufb01rst example is a training\nquery from MARCO, where the augmenting documents\nhelp disambiguate the query word \"rating\". In the sec-\nond one, documents from the of\ufb01cial Wiki and Hot-\npotQA\u2019s Wiki corpus are descriptions of the two entities\nin HotpotQA\u2019s comparison question. It illustrates how\nMoMA provides more comprehensive augmentation by\nincorporating information from different sources.\n6\nConclusion\nIn this paper we propose a new plug-in mixture-of-\nmemory mechanism for the retrieval augmented lan-\nguage models to improve their zero-shot ability on the\ndense retrieval task. To learn the memory mixture we\ndevelop a new joint learning approach that trains the\naugmentation component using the positive signals from\nthe end task, the language model\u2019s attention scores, and\nTable 6: MoMA retrieves augmenting documents during\ntraining (Marco) and testing (BEIR).\nQueries\nAugmentation Docs\nTraining\n[Marco]\nWhat\nis\nhotel\ntran-\nsylvania\nrated\n[Marco] Why is Hotel Transylvania 2 rated\nPG? It is rated PG for some scary images,\naction and rude humor. [Wiki] Another re-\nview aggregate calculated an average score\nof 47 out of 100, indicating \u201cmixed or av-\nerage reviews\u201d.\nZero-Shot Testing\n[HotpotQA]\nWere Scott\nDerrickson\nand\nEd\nWood\nof\nthe\nsame\nnationality?\n[Wiki] Scott Derrickson (born July 16,\n1966) is an American director, screenwriter\nand producer. [HotpotQA] Edward Davis\nWood Jr. (October 10, December 10, 1978)\nwas an American \ufb01lmmaker, actor, writer,\nproducer, and director.\nhard negatives retrieved from the mixture of augmen-\ntation corpora. This leads to our \ufb01nal model MoMA\n(T5-ANCE) and MoMA (COCO) that achieve strong\nzero-shot accuracy on 18 retrieval tasks included in\nBEIR. Our analysis shows the importance of augment-\ning with diverse memory sources and in-domain infor-\nmation for robust generalization. We also share our\nobservations and insights on how the model learns to\nleverage the augmentation information from multiple\ncorpora during training and testing. We hope our \ufb01nd-\nings and illustrations can inspire more future research in\nbetter augmenting language models, to provide other al-\nternatives to achieve generalization ability beyond solely\nrelying on model scale.\n\nLimitations\nAlthough MoMA (T5-ANCE) and MoMA (COCO)\nachieve strong zero-shot performances, we mainly ver-\nify their ef\ufb01cacy from the empirical performances\non BEIR tasks, where the target corpora, Wiki and\nMARCO serve as readily available retrieval sources.\nIn a real-world scenario, the grounding corpora usually\nneed to be customized according to query domains and\nuser needs. Thus, how to choose effective grounding\ncorpora and ef\ufb01ciently evaluate their relative contribu-\ntion remain an open problem. These analyses will go\nbeyond our empirical settings and reveal a wider appli-\ncation scenario of MoMA.\nEthics Statement\nAll data in this study are publicly available and used\nunder ethical considerations. Text and \ufb01gures in the\npaper are used for illustration only, they do not represent\nthe ethical attitude of the authors.\nReferences\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder,\nAndrew McNamara, Bhaskar Mitra, Tri Nguyen,\net al. 2016. MS MARCO: A human generated ma-\nchine reading comprehension dataset. arXiv preprint\narXiv:1611.09268.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\npages 610\u2013623.\nAlexander Bondarenko, Maik Fr\u00f6be, Meriem Be-\nloucif, Lukas Gienapp, Yamen Ajjour, Alexander\nPanchenko, Chris Biemann, Benno Stein, Henning\nWachsmuth, Martin Potthast, and Matthias Hagen.\n2020.\nOverview of Touch\u00e9 2020: Argument Re-\ntrieval. In Working Notes Papers of the CLEF 2020\nEvaluation Labs, volume 2696 of CEUR Workshop\nProceedings.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\nImproving language models by retrieving from tril-\nlions of tokens. In International Conference on Ma-\nchine Learning, pages 2206\u20132240. PMLR.\nVera Boteva, Demian Gholipour, Artem Sokolov, and\nStefan Riezler. 2016. A full-text learning to rank\ndataset for medical information retrieval. In Euro-\npean Conference on Information Retrieval, pages\n716\u2013722. Springer.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nClaudio Carpineto and Giovanni Romano. 2012. A\nsurvey of automatic query expansion in information\nretrieval. Acm Computing Surveys (CSUR), 44(1):1\u2013\n50.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to Answer Open-\nDomain Questions. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 1870\u20131879.\nArman Cohan, Sergey Feldman, Iz Beltagy, Doug\nDowney, and Daniel Weld. 2020.\nSPECTER:\nDocument-level\nrepresentation\nlearning\nusing\ncitation-informed transformers.\nIn Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 2270\u20132282, Online.\nAssociation for Computational Linguistics.\nThomas Diggelmann, Jordan Boyd-Graber, Jannis Bu-\nlian, Massimiliano Ciaramita, and Markus Leippold.\n2020. CLIMATE-FEVER: A dataset for veri\ufb01ca-\ntion of real-world climate claims. arXiv preprint\narXiv:2012.00614.\nThibault Formal, Benjamin Piwowarski, and St\u00e9phane\nClinchant. 2021. Splade: Sparse lexical and expan-\nsion model for \ufb01rst stage ranking. In Proceedings\nof the 44th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval,\npages 2288\u20132292.\nLuyu Gao and Jamie Callan. 2022. Unsupervised cor-\npus aware language model pre-training for dense pas-\nsage retrieval. In ACL 2022.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Ming-Wei Chang. 2020. REALM: Retrieval-\naugmented language model pre-training. In ICML.\nFaegheh Hasibi, Fedor Nikolaev, Chenyan Xiong, Krisz-\ntian Balog, Svein Erik Bratsberg, Alexander Kotov,\nand Jamie Callan. 2017. DBpedia-Entity v2: A test\ncollection for entity search. In Proceedings of the\n40th International ACM SIGIR Conference on Re-\nsearch and Development in Information Retrieval,\nSIGIR \u201917, page 1265\u20131268, New York, NY, USA.\nAssociation for Computing Machinery.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nSebastian Hofst\u00e4tter, Sheng-Chieh Lin, Jheng-Hong\nYang, Jimmy Lin, and Allan Hanbury. 2021.\nEf-\n\ufb01ciently teaching an effective dense retriever with\nbalanced topic aware sampling. In Proceedings of\nSIGIR 2021, pages 113\u2013122.\n\nSebastian Hofst\u00e4tter, Sheng-Chieh Lin, Jheng-Hong\nYang, Jimmy Lin, and Allan Hanbury. 2021.\nEf-\n\ufb01ciently teaching an effective dense retriever with\nbalanced topic aware sampling. In Proceedings of\nthe 44th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval,\npage 113\u2013122. Association for Computing Machin-\nery.\nDoris Hoogeveen, Karin M. Verspoor, and Timothy\nBaldwin. 2015. CQADupStack: A benchmark data\nset for community question-answering research. In\nProceedings of the 20th Australasian Document Com-\nputing Symposium, ADCS \u201915, New York, NY, USA.\nAssociation for Computing Machinery.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\nbastian Riedel, Piotr Bojanowski, Armand Joulin, and\nEdouard Grave. 2021. Towards unsupervised dense\ninformation retrieval with contrastive learning. arXiv\npreprint arXiv:2112.09118.\nGautier Izacard and Edouard Grave. 2020a. Distilling\nknowledge from reader to retriever for question an-\nswering. arXiv preprint arXiv:2012.04584.\nGautier Izacard and Edouard Grave. 2020b. Leveraging\npassage retrieval with generative models for open\ndomain question answering.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nJeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. 2019.\nBillion-scale similarity search with gpus.\nIEEE\nTransactions on Big Data, 7(3):535\u2013547.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models.\narXiv\npreprint arXiv:2001.08361.\nVladimir Karpukhin, Barlas O\u02d8guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020.\nDense passage retrieval for\nopen-domain question answering.\narXiv preprint\narXiv:2004.04906.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2019. Generalization\nthrough memorization: Nearest neighbor language\nmodels. arXiv preprint arXiv:1911.00172.\nOmar Khattab and Matei Zaharia. 2020a. Colbert: Ef\ufb01-\ncient and effective passage search via contextualized\nlate interaction over bert. In Proceedings of the 43rd\nInternational ACM SIGIR conference on research and\ndevelopment in Information Retrieval, pages 39\u201348.\nOmar Khattab and Matei Zaharia. 2020b. Colbert: Ef-\n\ufb01cient and effective passage search via contextual-\nized late interaction over bert. In Proceedings of\nthe 43rd International ACM SIGIR Conference on\nResearch and Development in Information Retrieval,\npage 39\u201348, New York, NY, USA. Association for\nComputing Machinery.\nYubin Kim. 2022. Applications and future of dense\nretrieval in industry. In Proceedings of the 45th In-\nternational ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, pages 3373\u2013\n3374.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\n\ufb01eld, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics, 7:452\u2013466.\nVictor Lavrenko and W Bruce Croft. 2017. Relevance-\nbased language models. In ACM SIGIR Forum, vol-\nume 51, pages 260\u2013267. ACM New York, NY, USA.\nPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rock-\nt\u00e4schel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. arXiv preprint\narXiv:2005.11401.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. RoBERTa:\nA Robustly Optimized BERT Pretraining Approach.\narXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nJing Lu, Gustavo Hernandez Abrego, Ji Ma, Jianmo Ni,\nand Yinfei Yang. 2021. Multi-stage training with im-\nproved negative contrast for neural passage retrieval.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n6091\u20136103, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nMacedo Maia, Siegfried Handschuh, Andr\u00e9 Freitas,\nBrian Davis, Ross McDermott, Manel Zarrouk, and\nAlexandra Balahur. 2018. WWW\u201918 open challenge:\nFinancial opinion mining and question answering. In\nCompanion Proceedings of the The Web Conference\n2018, WWW \u201918, page 1941\u20131942, Republic and\nCanton of Geneva, CHE. International World Wide\nWeb Conferences Steering Committee.\nArvind Neelakantan, Tao Xu, Raul Puri, Alec Rad-\nford, Jesse Michael Han, Jerry Tworek, Qiming Yuan,\nNikolas Tezak, Jong Wook Kim, Chris Hallacy, et al.\n2022. Text and code embeddings by contrastive pre-\ntraining. arXiv preprint arXiv:2201.10005.\n\nJianmo Ni, Gustavo Hernandez Abrego, Noah Constant,\nJi Ma, Keith Hall, Daniel Cer, and Yinfei Yang. 2022.\nSentence-t5: Scalable sentence encoders from pre-\ntrained text-to-text models. In Findings of the As-\nsociation for Computational Linguistics: ACL 2022,\npages 1864\u20131874.\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-\ntavo Hern\u00e1ndez \u00c1brego, Ji Ma, Vincent Y Zhao,\nYi Luan, Keith B Hall, Ming-Wei Chang, et al.\n2021. Large dual encoders are generalizable retriev-\ners. arXiv preprint arXiv:2112.07899.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. Advances in\nneural information processing systems, 32.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Mail-\nlard, et al. 2020.\nKilt: a benchmark for knowl-\nedge intensive language tasks.\narXiv preprint\narXiv:2009.02252.\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang\nRen, Wayne Xin Zhao, Daxiang Dong, Hua Wu,\nand Haifeng Wang. 2021. RocketQA: An optimized\ntraining approach to dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 5835\u20135847, On-\nline. Association for Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uni\ufb01ed text-to-text trans-\nformer. Journal of Machine Learning Research.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the parame-\nters of a language model? In EMNLP.\nStephen Robertson, Hugo Zaragoza, et al. 2009. The\nprobabilistic relevance framework: Bm25 and be-\nyond. Foundations and Trends in Information Re-\ntrieval, 3(4):333\u2013389.\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, et al. 2022. Using deep-\nspeed and megatron to train megatron-turing nlg\n530b, a large-scale generative language model. arXiv\npreprint arXiv:2201.11990.\nIan Soboroff, Shudong Huang, and Donna Harman.\n2018. Trec 2018 news track overview.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2020.\nEnergy and policy considerations for\nmodern deep learning research. In Proceedings of\nthe AAAI Conference on Arti\ufb01cial Intelligence, vol-\nume 34, pages 13693\u201313696.\nAxel Suarez, Dyaa Albakour, David Corney, Miguel\nMartinez, and Jos\u00e9 Esquivel. 2018. A data collection\nfor evaluating the retrieval of related tweets to news\narticles. In European Conference on Information\nRetrieval, pages 780\u2013786. Springer.\nNandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Ab-\nhishek Srivastava, and Iryna Gurevych. 2021a. Beir:\nA heterogenous benchmark for zero-shot evalua-\ntion of information retrieval models. arXiv preprint\narXiv:2104.08663.\nNandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Ab-\nhishek Srivastava, and Iryna Gurevych. 2021b. BEIR:\nA heterogenous benchmark for zero-shot evalua-\ntion of information retrieval models. arXiv preprint\narXiv:2104.08663.\nJames\nThorne,\nAndreas\nVlachos,\nChristos\nChristodoulopoulos,\nand\nArpit\nMittal.\n2018.\nFEVER: a large-scale dataset for fact extraction\nand VERi\ufb01cation.\nIn Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers),\npages 809\u2013819, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nGeorge Tsatsaronis, Georgios Balikas, Prodromos\nMalakasiotis, Ioannis Partalas, Matthias Zschunke,\nMichael R Alvers, Dirk Weissenborn, Anastasia\nKrithara, Sergios Petridis, Dimitris Polychronopou-\nlos, et al. 2015. An overview of the BIOASQ large-\nscale biomedical semantic indexing and question an-\nswering competition. BMC bioinformatics, 16(1):1\u2013\n28.\nEllen Voorhees, Tasmeer Alam, Steven Bedrick, Dina\nDemner-Fushman, William R. Hersh, Kyle Lo, Kirk\nRoberts, Ian Soboroff, and Lucy Lu Wang. 2021.\nTREC-COVID: Constructing a pandemic informa-\ntion retrieval test collection. SIGIR Forum, 54(1).\nEllen M Voorhees et al. 2004. Overview of the trec\n2004 robust retrieval track. In Trec, pages 69\u201377.\nHenning Wachsmuth, Shahbaz Syed, and Benno Stein.\n2018. Retrieval of the best counterargument without\nprior topic knowledge. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 241\u2013251,\nMelbourne, Australia. Association for Computational\nLinguistics.\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu\nWang, Madeleine van Zuylen, Arman Cohan, and\nHannaneh Hajishirzi. 2020. Fact or \ufb01ction: Verifying\nscienti\ufb01c claims. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 7534\u20137550, Online. As-\nsociation for Computational Linguistics.\nKexin Wang, Nandan Thakur, Nils Reimers, and Iryna\nGurevych. 2022. GPL: Generative pseudo labeling\nfor unsupervised domain adaptation of dense retrieval.\nIn Proceedings of the 2022 Conference of the North\n\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nSeattle, United States. Association for Computational\nLinguistics.\nYining Wang, Liwei Wang, Yuanzhi Li, Di He, Wei\nChen, and Tie-Yan Liu. 2013. A theoretical analysis\nof ndcg ranking measures. In Proceedings of the 26th\nannual conference on learning theory (COLT 2013),\nvolume 8, page 6. Citeseer.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzm\u00e1n, Ar-\nmand Joulin, and Edouard Grave. 2020.\nCCNet:\nExtracting high quality monolingual datasets from\nweb crawl data. In Proceedings of the 12th Lan-\nguage Resources and Evaluation Conference, pages\n4003\u20134012, Marseille, France. European Language\nResources Association.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38\u201345, Online. Association\nfor Computational Linguistics.\nJi Xin, Chenyan Xiong, Ashwin Srinivasan, Ankita\nSharma, Damien Jose, and Paul Bennett. 2022. Zero-\nshot dense retrieval with momentum adversarial do-\nmain invariant representations. In Findings of the As-\nsociation for Computational Linguistics: ACL 2022,\npages 4008\u20134020, Dublin, Ireland. Association for\nComputational Linguistics.\nJi Xin, Chenyan Xiong, Ashwin Srinivasan, Ankita\nSharma, Damien Jose, and Paul N Bennett. 2021.\nZero-shot dense retrieval with momentum adversar-\nial domain invariant representations. arXiv preprint\narXiv:2110.07581.\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul Bennett, Junaid Ahmed, and Arnold\nOverwijk. 2020. Approximate nearest neighbor nega-\ntive contrastive learning for dense text retrieval. arXiv\npreprint arXiv:2007.00808.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W. Cohen, Ruslan Salakhutdinov, and\nChristopher D. Manning. 2018.\nHotpotQA: A\nDataset for Diverse, Explainable Multi-hop Ques-\ntion Answering. In Proceedings of the Conference on\nEmpirical Methods in Natural Language Processing,\npages 2369\u20132380.\nHongChien Yu, Chenyan Xiong, and Jamie Callan. 2021.\nImproving query representations for dense retrieval\nwith pseudo relevance feedback.\narXiv preprint\narXiv:2108.13454.\nYue Yu, Chenyan Xiong, Si Sun, Chao Zhang, and\nArnold Overwijk. 2022. Coco-dr: Combating dis-\ntribution shifts in zero-shot dense retrieval with con-\ntrastive and distributionally robust learning. arXiv\npreprint arXiv:2210.15212.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nChen Zhao, Chenyan Xiong, Jordan Boyd-Graber, and\nHal Daum\u00e9 III. 2021. Distantly-supervised evidence\nretrieval enables question answering without evidence\nannotation. arXiv preprint arXiv:2110.04889.\nZexuan Zhong, Tao Lei, and Danqi Chen. 2022. Train-\ning language models with memory augmentation.\narXiv preprint arXiv:2205.12674.\n\nA\nAppendix\nA.1\nDatasets Details\nEvaluation Datasets\nTarget domain datasets used\nin our experiments are collected in the BEIR bench-\nmark (Thakur et al., 2021b)4 and include the following\ndomains:\n\u2022 Open-domain Question Answering (QA): Hot-\npotQA (Yang et al., 2018), NQ (Kwiatkowski et al.,\n2019), and FiQA (Maia et al., 2018).\n\u2022 Bio-Medical\nInformation\nRetrieval:\nTREC-\nCOVID (Voorhees et al., 2021), NFCorpus (Boteva\net al., 2016), and BioASQ (Tsatsaronis et al., 2015).\n\u2022 Argument Retrieval:\nWebis-Touch\u00e92020 (Bon-\ndarenko et al., 2020) and ArguAna (Wachsmuth et al.,\n2018).\n\u2022 News Retrieval: TREC-NEWS (Soboroff et al., 2018)\nand Robust04 (Voorhees et al., 2004).\n\u2022 Tweet Retrieval: Signal-1m (Suarez et al., 2018).\n\u2022 Duplicate Question Retrieval: Quora (Thakur et al.,\n2021b) and CQADupStack (Hoogeveen et al., 2015).\n\u2022 Entity Retrieval: DBPedia (Hasibi et al., 2017)\n\u2022 Citation Prediction: SCIDOCS (Cohan et al., 2020)\n\u2022 Fact Checking:\nSciFact (Wadden et al., 2020),\nFEVER\n(Thorne\net\nal.,\n2018),\nand\nClimate-\nFEVER (Diggelmann et al., 2020)\nWe list the statistics of the BEIR benchmark in Table 7.\nAugmenting Corpora\nCorpus size We \ufb01rst introduce\nmore details on how we preprocessed the Medical Sub-\nject Headings (MeSH) Database. We select text in-\nformation from the Quali\ufb01er Record Set and Descrip-\ntor Record Set. Each set contains multiple <Concept>\nelements, which is composed of three sub-elecments,\ni.e., <ConceptName>, <ScopeNote> and <TermList>.\nAmong the sub-elecments, <ScopeNote> is the major\ntextual information source, which is usually a short de-\nscription to a medical term or phenomenon. We directly\nconsider each <ScopeNote> as a document entry and\nconcatenate it with corresponding <ConceptName>.\nWe list the statistics of the augmenting corpora in\nTable 8.\nA.2\nBaselines\nWe use the baselines from the current BEIR leader-\nboard (Thakur et al., 2021b) and recent papers. These\nbaselines can be divided into four groups: dense re-\ntrieval, dense retrieval with generated queries5, lexical\nretrieval and late interaction.\n4https://github.com/beir-cellar/beir\n5We separate them from dense retrieval since they usually\nrely on Seq2seq models to generate pseudo query-document\npairs, and they train a model for each dataset independently\ninstead of using a single model for all datasets.\nDense Retrieval\nFor dense retrieval, the baselines\nare the same dual-tower model as ours. We consider\nDPR (Karpukhin et al., 2020), ANCE (Xiong et al.,\n2020), T5-ANCE, coCondenser (Gao and Callan,\n2022) and one recently-proposed model GTR (Ni et al.,\n2021) with different size con\ufb01guration in this paper.\n\u2022 DPR uses a single BM25 retrieval example and in-\nbatch examples as hard negative examples to train\nthe model. Different from the original paper (Thakur\net al., 2021b) that train the DPR on QA datasets, we\ntrain DPR on MS MARCO (Bajaj et al., 2016) Dataset\nfor fair comparison. Notice that this also lead to better\nresults according to Xin et al. (2022).\n\u2022 ANCE constructs hard negative examples from an\nANN index of the corpus. The hard negative training\ninstances are updated in parallel during \ufb01ne-tuning of\nthe model. The model is a RoBERTa (Liu et al., 2019)\nmodel trained on MS MARCO for 600k steps.\n\u2022 T5-ANCE Different with default ANCE setting, we\nreplace the backbone language model RoBERTa with\nT5-base. All the other model settings are the same\nwith the original ANCE. We include this baseline\nbecause as a subroutine for MoMA, it could be viewed\nas an ablation without memory augmentation. We\ncan directly observe the impact of plug-in mixture of\nmemory by comparing T5-ANCE with MoMA.\n\u2022 coCondenser is a continuous pre-trained model\nbased on BERT, with the equivalent amount of param-\neters to BERT-base. It enhances the representation\nability of [CLS] token by changing the connections\nbetween different layers of Transformer blocks. Fine-\ntuning of coCondenser uses BM25 and self-mined\nnegatives.\n\u2022 Contriever conducts unsupervised contrastive pre-\ntraining with data augmentations and momentum\nqueues on Wikipedia and the larger CC-Net (Wen-\nzek et al., 2020) corpora for 500k steps.\n\u2022 GTR initializes the dual encoders from the T5 mod-\nels (Raffel et al., 2019). It is \ufb01rst pre-trained on Com-\nmunity QA6 with 2 billion question-answer pairs then\n\ufb01ne-tuned on NQ and MS Marco dataset. In addition,\nthey use the hard negatives released by RocketQA (Qu\net al., 2021) when \ufb01netuning with MS Marco data and\nthe hard negatives release by (Lu et al., 2021) for Nat-\nural Questions. GTRbase leverages the same T5-base\nmodel as MoMA, while GTRlarge is based on T5-large,\nwhich is not directly comparable to our method as it\ntriples the parameters.\nDense Retrieval with Generated Queries\nGenQ\n\ufb01rst \ufb01ne-tunes a T5-base (Raffel et al., 2019) model on\nMS MARCO for 2 epochs and then generate 5 queries\n6Unfortunately, this corpus has not been released by the\nauthors.\n\nTable 7: Statistics of datasets in the BEIR benchmark. The table is taken from the original BEIR benchmark\npaper (Thakur et al., 2021b).\nSplit (\u2192)\nTrain\nDev\nTest\nAvg. Word Lengths\nTask (\u2193)\nDomain (\u2193)\nDataset (\u2193)\nTitle\nRelevancy\n#Pairs\n#Query\n#Query\n#Corpus\nAvg. D / Q\nQuery\nDocument\nPassage-Retrieval\nMisc.\nMS MARCO\n\u0017\nBinary\n532,761\n\u2014-\n6,980\n8,841,823\n1.1\n5.96\n55.98\nBio-Medical\nBio-Medical\nTREC-COVID\n\u0013\n3-level\n\u2014-\n\u2014-\n50\n171,332\n493.5\n10.60\n160.77\nInformation\nBio-Medical\nNFCorpus\n\u0013\n3-level\n110,575\n324\n323\n3,633\n38.2\n3.30\n232.26\nRetrieval (IR)\nBio-Medical\nBioASQ\n\u0013\nBinary\n32,916\n\u2014-\n500\n14,914,602\n4.7\n8.05\n202.61\nQuestion\nWikipedia\nNQ\n\u0013\nBinary\n132,803\n\u2014-\n3,452\n2,681,468\n1.2\n9.16\n78.88\nAnswering\nWikipedia\nHotpotQA\n\u0013\nBinary\n170,000\n5,447\n7,405\n5,233,329\n2.0\n17.61\n46.30\n(QA)\nFinance\nFiQA-2018\n\u0017\nBinary\n14,166\n500\n648\n57,638\n2.6\n10.77\n132.32\nTweet-Retrieval\nTwitter\nSignal-1M (RT)\n\u0017\n3-level\n\u2014-\n\u2014-\n97\n2,866,316\n19.6\n9.30\n13.93\nNews\nNews\nTREC-NEWS\n\u0013\n5-level\n\u2014-\n\u2014-\n57\n594,977\n19.6\n11.14\n634.79\nRetrieval\nNews\nRobust04\n\u0017\n3-level\n\u2014-\n\u2014-\n249\n528,155\n69.9\n15.27\n466.40\nArgument\nMisc.\nArguAna\n\u0013\nBinary\n\u2014-\n\u2014-\n1,406\n8,674\n1.0\n192.98\n166.80\nRetrieval\nMisc.\nTouch\u00e9-2020\n\u0013\n3-level\n\u2014-\n\u2014-\n49\n382,545\n19.0\n6.55\n292.37\nDuplicate-Question\nStackEx.\nCQADupStack\n\u0013\nBinary\n\u2014-\n\u2014-\n13,145\n457,199\n1.4\n8.59\n129.09\nRetrieval\nQuora\nQuora\n\u0017\nBinary\n\u2014-\n5,000\n10,000\n522,931\n1.6\n9.53\n11.44\nEntity-Retrieval\nWikipedia\nDBPedia\n\u0013\n3-level\n\u2014-\n67\n400\n4,635,922\n38.2\n5.39\n49.68\nCitation-Prediction\nScienti\ufb01c\nSCIDOCS\n\u0013\nBinary\n\u2014-\n\u2014-\n1,000\n25,657\n4.9\n9.38\n176.19\nWikipedia\nFEVER\n\u0013\nBinary\n140,085\n6,666\n6,666\n5,416,568\n1.2\n8.13\n84.76\nFact Checking\nWikipedia\nClimate-FEVER\n\u0013\nBinary\n\u2014-\n\u2014-\n1,535\n5,416,593\n3.0\n20.13\n84.76\nScienti\ufb01c\nSciFact\n\u0013\nBinary\n920\n\u2014-\n300\n5,183\n1.1\n12.37\n213.63\nTable 8: Statistics of the augmenting corpora.\nDatasets\nCorpus Size\nAvg. Doc Length\nMS MARCO\n502,939\n56.0\nMeSH\n32,326\n16.8\nWiki\n21,015,324\n100.0\nfor each passage as additional training data for the target\ndomain to continue to \ufb01ne-tune the TAS-B (Hofst\u00e4tter\net al., 2021) model.\nLexical Retrieval\nLexical retrieval is a score func-\ntion for token matching calculated between two\nhigh-dimensional sparse vectors with token weights.\nBM25 (Robertson et al., 2009) is the most commonly\nused lexical retrieval function. We use the BM25 results\nreported in Thakur et al. (2021b) for comparison.\nLate Interaction\nWe also consider a late interac-\ntion baseline, namely ColBERT (Khattab and Zaharia,\n2020b). The model computes multiple contextualized\nembeddings for each token of queries and documents,\nand then uses a maximum similarity function to retrieve\nrelevant documents. This type of matching requires sig-\nni\ufb01cantly more disk space for indexes and has a higher\nlatency.\nA.3\nDetailed Experimental Settings and\nhyperparameters\nOur implementation uses PyTorch (Paszke et al., 2019)\nwith Hugging Face Transformers (Wolf et al., 2020).\nWe optimize the model using AdamW (Loshchilov and\nHutter, 2019) with a peak learning rate at 5e-6, weight\ndecay of 0.01, and linear learning rate decay. The global\nbatch size is set to 256. The maximum length of query\nand passage are set to 32 and 128 respectively. We\nsummarize all hyperparameter settings in Table 9. The\nmodel is trained with 8 Nvidia A100 80GB GPUs and\nTable 9: The hyperparameters of MoMA.\nHyperparameters\nSettings\nGrounding document number\n10\nAttention threshold number\n5\nNegative mining depth\n200\nGlobal batch size (query size per batch)\n256\nPositive number per query\n1\nNegative number per query\n7\nPeak learnig rate\n5e-6\nLearnig rate decay\n0.01\nOptimizer\nAdamW\nScheduler\nLinear\nMARCO Maximum query length\n32\nMARCO Maximum document length\n128\nFP16 mixed-precision training. The total running time\nis 6.6 hrs for three episodes of augmentation component\ntraining and 6.3 hrs for end retriever training. We detail\nthe training time of each episode in Table 10.\nWhen evaluating on the BEIR benchmark, we fol-\nlow the setting in GTR (Ni et al., 2021), which use\nsequences of 64 tokens for the questions and 512 for the\ndocuments in all datasets except Trec-News, Robust-04\nand ArguAna. In particular, we set the document length\nto 768 for Trec-News and Robust-04. For ArguAna,\nwe set both question and document length to 128. The\nabove length setting is in accordance to the average\nquery and document lengths in these datasets.\n\nTable 10: Training time for MoMA with three training\nepisodes. We use 8 Nvidia A100 80GB GPUs with\nFP16 mixed-precision training.\nStage\nAugmentation Component\nEnd Retriever\nEpi-1\n0.8h\n1.5h\nEpi-2\n0.8h\n1.5h\nEpi-3\n0.8h\n1.5h\nIndex refresh\n1.4h\n0.6h\nRefresh number\n3\n3\nOverall\n6.6h\n6.3h\n"}