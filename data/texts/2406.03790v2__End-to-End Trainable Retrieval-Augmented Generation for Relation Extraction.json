{"metadata": {"pdf_filename": "2406.03790v2__End-to-End Trainable Retrieval-Augmented Generation for Relation Extraction.pdf", "source": "arXiv"}, "text": "Date of publication xxxx 00, 2024, date of current version Oct. 15, 2024.\nDigital Object Identifier 10.1109/ACCESS.2024.0429000\nEnd-to-End Trainable Retrieval-Augmented\nGeneration for Relation Extraction\nKOHEI MAKINO, MAKOTO MIWA, and YUTAKA SASAKI\nToyota Technological Institute, 2-12-1 Hisakata, Tempaku-ku, Nagoya, 468-8511 Japan\nCorresponding author: Yutaka Sasaki (e-mail: yutaka.sasaki@toyota-ti.ac.jp).\nABSTRACT This paper addresses a crucial challenge in retrieval-augmented generation-based relation\nextractors; the end-to-end training is not applicable to conventional retrieval-augmented generation due to\nthe non-differentiable nature of instance retrieval. This problem prevents the instance retrievers from being\noptimized for the relation extraction task, and conventionally it must be trained with an objective different\nfrom that for relation extraction. To address this issue, we propose a novel End-to-end Trainable Retrieval-\nAugmented Generation (ETRAG), which allows end-to-end optimization of the entire model, including\nthe retriever, for the relation extraction objective by utilizing a differentiable selection of the k nearest\ninstances. We evaluate the relation extraction performance of ETRAG on the TACRED dataset, which is\na standard benchmark for relation extraction. ETRAG demonstrates consistent improvements against the\nbaseline model as retrieved instances are added. Furthermore, the analysis of instances retrieved by the\nend-to-end trained retriever confirms that the retrieved instances contain common relation labels or entities\nwith the query and are specialized for the target task. Our findings provide a promising foundation for\nfuture research on retrieval-augmented generation and the broader applications of text generation in Natural\nLanguage Processing.\nINDEX TERMS Differentiable retrieval, End-to-end training, Relation extraction, Retrieval-augmented\ngeneration\nI. INTRODUCTION\nRelation extraction is a fundamental task in Natural Language\nProcessing (NLP), which involves identifying and classify-\ning semantic relationships between entity mentions, such as\npeople, organization, and location names, in text [1]. Relation\nextraction plays a crucial role in understanding and interpret-\ning the underlying meaning of sentences by analyzing how\nentities are interrelated [2], [3]. 1\nRelation extraction is used in practical applications in sev-\neral domains. For instance, in knowledge graph construction,\nrelation extraction aids in transforming unstructured text into\nstructured data, which can then be used to populate and enrich\nknowledge graphs [4]. In domain-specific scenarios, such\nas biomedical text mining [5], it can extract relationships\nbetween genes, diseases, and drugs, which are helpful for\nadvanced research and discovery such as search systems [6]\nand prediction of novel things [7]. Thus, relation extraction is\nused for a wide variety of purposes.\n1This work has been submitted to the IEEE for possible publication.\nCopyright may be transferred without notice, after which this version may\nno longer be accessible.\nDeveloping relation extractors is an ongoing endeavor in\nNLP. Relation extractors are designed to accurately identify\nand classify relations in text, which requires understanding\nthe nuanced and sometimes complex language structures.\nAdvances in machine learning and NLP methods have shaped\nthe evolution of the extractors. Recent extractors are based on\ndeep learning models to obtain high-performance [8]\u2013[10],\nwhile traditional extractors are rule-based models [11] and\nfeature-based models [12].\nThe Pretrained Language Models (PLMs) [13]\u2013[15] have\nbecome a de facto standard in recent NLP, fundamentally\nchanging the field landscape. PLMs are neural network mod-\nels pretrained on a common NLP task, such as language\nmodeling [16] and masked language modeling [14], and are\nused by fine-tuning it to fit a target task. Many studies on\nrelation extraction with PLMs have been conducted because\nof the high performance [17], [18].\nWith the advent of PLMs, well-trained text generation\nmodels, including Large Language Models (LLMs) [19]\u2013\n[21] trained on a larger corpus with larger-scale parame-\nters, are attracting attention [16], [22], [23]. Text generation\nmodel-based relation extractors [24]\u2013[26] are used because\nVOLUME 11, 2023\n1\narXiv:2406.03790v2  [cs.CL]  10 Oct 2024\n\nMakino et al.: End-to-End Trainable Retrieval-Augmented Generation for Relation Extraction\nthe pretraining and the fine-tuning are similar and it helps the\nextractor training. These models have been adapted to treat\nrelation extraction as a question-answering task [24], a text\nsummarization task [27], or a language modeling [25], lever-\naging their knowledge and understanding of natural language.\nSome existing works utilize relation instances, which con-\nsist of text and relation between entities, to improve the\nperformance. Instance-based methods perform better in low-\nresource settings with insufficient training data because they\ncan use known examples as anchors. A typical instance-based\nextractor using PLM predicts the label of the nearest known\ninstance on the encoded representation [28]. However, the\ninference with simple voting with nearest neighbor methods\nis weak in terms of the overall performance compared to other\nmodels.\nCombining text generation models with instance utiliza-\ntion has been particularly effective with In-Context Learning\n(ICL) [19]. In relation extraction via ICL, instructions and\nfew-shot instances verbalized as text are used as prompts to\nsupervise text generation models from context, and the model\ngenerates the text convertible to relation labels conditioned by\nthe prompt [25]. ICL, which predicts based on context, can\nproduce output using instances as hints rather than methods\nsuch as the nearest neighbor method. Since standard ICL\nuses pre-defined instances in the prompt, ICL methods cannot\naccess instances adaptable to input.\nTo solve these problems, Retrieval-Augmented Generation\n(RAG) [29] introduces the instances relevant to the input into\nthe text prompt instead of pre-defined instances. In RAG-\nbased relation extractors [26], retrievers prepared separately\nfrom the base model select instances. The retrievers use the\ntarget text as a query and select instances from the relation\ninstances stored as a database. The selected instances are then\nintroduced into the text prompt. Generally, the retrieval is\ndone by embedding the inputs and instances into a common\ndense feature space by a separately trained encoder model\nand selecting the nearest neighbors. Thus, RAG introduces\nadaptive instances to the prompt of the text generation model.\nThe retriever used in RAG requires training to retrieve the\nappropriate instance when used in the target task. However,\nthe instance retriever needs to be trained separately from\nthe target relation extraction task because the text generation\nmodel with the retriever cannot be trained end-to-end. In other\nwords, designing a retriever for each target problem is neces-\nsary, which increases the development cost [26]. Therefore,\nend-to-end training allows direct optimization of the retriever\nto the target task, reducing development costs by eliminating\nthe need to devise case-specific training methods.\nOur goal is to create an environment where the entire model\nwith RAG can be optimized for the relation extraction task.\nTherefore, this study aims to make the RAG model end-\nto-end trainable. Specifically, we eliminate indifferentiable\noperations in the retriever that prevent the training of deep\nlearning models by replacing them with differentiable oper-\nations. By integrating the operations, we propose ETRAG\n(End-to-end Trainable Retrieval-Augmented Retrieval) as a\nRAG that can train the entire model end-to-end. Our proposed\nmethod is evaluated by a relation extraction task to confirm\neffectiveness and characteristics.\nContributions in this paper are threefold.\n\u2022 We propose end-to-end trainable RAG, ETRAG, which\nreplaces the k-nearest neighbor method with differen-\ntiable operations and uses obtained instances as soft\nprompts. ETRAG enables an entire model, including the\nretriever, to fine-tune for the relation extraction task.\n\u2022 We confirm that ETRAG consistently improves extrac-\ntion performance for the TACRED, a benchmark for\nrelation extraction, and is particularly effective in situ-\nations where training data is limited.\n\u2022 Our analysis reveals that ETRAG can select instances\nstrongly related to the target task. For the relation extrac-\ntion, instances with the same relationship labels as the\nextraction target and instances containing the same sur-\nface entities account for more than 70% of the instances.\nThe remainder of this paper is organized as follows: Sec-\ntion II presents related work, further elaborating on the evolu-\ntion and current state of PLM (Section II-A), instance-based\nmethods (Section II-B), and relation extraction (Section II-C).\nSection II-C contains notations and explanations used in\nsubsequent sections. Section III explains our methodology\nby describing the proposed method ETRAG with differen-\ntiable k-nearest neighbor (kNN) (Section III-A) and neu-\nral prompting (Section III-B), and the training methods of\nETRAG (Section III-C). The evaluations are performed in\nSection IV for the extraction performance and Section V for\nthe retriever analysis. Finally, Section VI concludes this study\nand describes future directions.\nII. RELATED WORK\nA. PRETRAINED LANGUAGE MODELS\nPLMs are large-scale neural network models trained on a\nlarge corpus with NLP tasks and are usually fine-tuned when\napplied to various tasks. Pretraining tasks are extensive such\nas language modeling (LM) [16], bidirectional language mod-\neling (biLM) [13], sequence-to-sequence language model-\ning (Seq2Seq LM) [22], and masked language modeling\n(MLM) [14]. PLMs employ appropriate neural network struc-\ntures such as ELMo [13] for the bidirectional language mod-\neling task with LSTMs [30], and Transformer [31] is mainly\nused recent systems including BERT [14] for masked lan-\nguage modeling, T5 [22] for sequence-to-sequence language\nmodeling, and GPT-2 [16] for language modeling. PLMs are\nextended to a larger scale and use a larger corpus such as Flan-\nT5 [20] in Seq2Seq LM and GPT-3 in LM [19].\nPLMs are stochastic models to estimate the probability of\nword sequences based on the pretraining tasks; collectively,\nthese are called text generation models G. Since LM is a\ntask to predict the subsequent text from a given input, the\nmodel trained on LM can estimate the probability of input text\nsequence, formulated as G(x) with input x. On the other hand,\nthe model trained on Seq2Seq LM can assign a probability to\n2\nVOLUME 11, 2023\n\nMakino et al.: End-to-End Trainable Retrieval-Augmented Generation for Relation Extraction\ninput-output pairs used in pretraining, formulated as G(y|x)\nwith input x and output y.\nPLMs are computationally expensive to fine-tune the entire\nparameters when adapted to the target task. Thus, they are\ntuned by controlling the generated text with text prompts\nor by parameter-efficient tuning. ICL performs the target\ntask learned from the context of tuned text prompts [19].\nHowever, the performance on the target task is often lower\nthan that of a model tuned specifically to the target task,\nand the prompt tuning process depends on the expert. Instead\nof the text prompt, prompt tuning [32] or prefix-tuning [33]\ntrains soft prompts, which are trainable vectors inserted to\nembedded text sequence. Neural prompting [34] has extended\nsoft prompts to create them by encoding external information\nrather than pre-prepared embeddings. Since such prompt-\nbased tuning can tune the context but not the model param-\neters, lightweight model tuning can also be used [35], [36].\nLow-Rank Adaptation (LoRA) [36] inserts low-rank param-\neters into the base model to learn the difference between the\ntarget task. We utilize the LoRA in our experiments to update\nPLM and the neural prompting in our method to introduce\ninstances.\nB. RETRIEVAL-BASED METHODS\nRetrieval-based methods, as typified by a nearest neighbor\nmethod [37], have been used for various NLP tasks, such\nas Part-Of-Speech (POS) tagging [38], named entity recog-\nnition [39], dependency parsing [40], and relation extrac-\ntion [41]. The methods are used to mitigate a training data\nscarcity situation. This paper replaces the indifferentiable\noperations in the kNN algorithm with differentiable ones in\norder to relax to a soft operation.\nRecently, PLMs are often used following the pretraining\ntask to use instances by ICL, which predicts the following\ncontext from the given prompt. Since ICL is characterized\nby the prompt design, the instance selection plays a vital\nrole. Since fixed prompts are usually used, the instances are\nalso typically fixed. However, the demand to select the most\nappropriate instances for input has led to use RAG [29], which\ntrains a retriever in advance and uses the instances obtained\nby the retriever. Since the instance selection operation is not\ndifferentiable because of sampling, the general retriever is\ntrained separately to the target task [26]. This study tackles\nthis separate training of the target task model and the retriever\nso that it can be trained end-to-end with the target task.\nC. RELATION EXTRACTION\nThe fundamental relation extraction task is sentence-level\nrelation extraction [42], [43], where only the entity pairs in\na single sentence are targets for extraction. Since sentence-\nlevel relation extraction ignores relations across sentences, it\nis extended to document-level relation extraction [44], [45],\nwhich also extracts these relations. Since this paper focuses\non a retriever that retrieves instances tied to a single relation\nfor simplicity, we develop an extractor primarily on sentence-\nlevel relation extraction.\nHistorically, relation extractors have evolved from tradi-\ntional approaches to modern deep learning techniques. Ini-\ntially, rule-based systems relied on hand-crafted rules to iden-\ntify relations [11]. Kernel-based models and feature-based\napproaches later emerged, offering more flexibility and bet-\nter handling linguistic variations [12], [46]. Deep learning\nrevolutionized the field, introducing models that could learn\ncomplex patterns and relationships directly from data [47].\nGiven input sentence x with head entity h and tail entity\nt of a target entity pair and relation r \u2208R between them, a\nstandard relation extractor in Equation (1) formulates a classi-\nfication task using a stochastic model P which is constructed\nwith deep learning.\n\u02c6r = argmax\nr\u2208R\nP(r|x, h, t)\n(1)\n1) Relation Extraction with Pretrained Language Models\nMost recent relation extractors employ PLMs for modeling\nP as feature extractors to prepare the feature vector for di-\nrect classification [48] or prepare the features for the fol-\nlowing relation extraction-specific model [49]. On the other\nhand, PLMs are also used as the text generation model with\nprompt engineering [26] or fine-tuning [25]. For example,\nSuRE (Summarization as Relation Extraction) [27] extracts\nthe relation with summarization via PLM and mapping the\nsummarized text output to relations. SuRE measures the prob-\nability of a pair of input text prompts and verbalizes relations\nin a summary form to predict the relation with the highest\nprobability.\nTypical text generation-based extractors prepare in-\nput and output templates with placeholders to fill with\ninformation to ask about relations. Let an input tem-\nplate\nbe\nTemplatein(x, h, t),\nan\noutput\ntemplate\nbe\nTemplateout(x, h, t, r). Equation (2) and Equation (3) show\nexamples of them.\nTemplatein(x, h, t) = \u2018\u2018The head entity is h .\nThe tail entity is t . x\u2019\u2019\n(2)\nTemplateout(x, h, t, r = no_relation) =\n\u2018\u2018h has no known relations to t .\u2019\u2019\n(3)\nLet a text generation model that computes the probability\nof the output template conditioned by the input sequence be\nG(Templateout | Templatein), Equation (4) represents the\ntext generation-based relation extractor.\n\u02c6r = argmax\nr\u2208R\nG(Templateout(x, h, t, r) | Templatein(x, h, t))\n(4)\nSuRE is a typical method for relation extraction in this\nform, which improves the efficiency of text-generation-based\nrelation extraction according to Equation (4). Calculating\nprobabilities for all relation labels in a straightforward manner\nis expensive due to the large number of calculations required\nby PLM. Therefore, SuRE prepares a trie tree of templates\nfor all relation labels and searches for the template with\nthe highest probability by beam search on the trie, thereby\nVOLUME 11, 2023\n3\n\nMakino et al.: End-to-End Trainable Retrieval-Augmented Generation for Relation Extraction\nrealizing the prediction of relations by text generation with a\nsmall number of calculations. Note that the model training is\nthe same as that of normal text generation models.\n2) Relation Extractor Using Instances\nDespite recent advancements in deep learning models, re-\nlation extraction remains challenging, primarily due to the\nscarcity of annotated data. Deep learning models, in partic-\nular, require large amounts of labeled data for training [8].\nHowever, manually annotating data is expensive and time-\nconsuming, making it a significant bottleneck in developing\neffective relation extraction systems [50]. This environment\nmakes it difficult to classify with high performance for com-\nplex relations types. In this context, the relation extractor via\nthe nearest neighbor approach [28], where relation extrac-\ntors leverage similar instances during inference, has shown\npromise in efficiently utilizing limited data. The goal of this\nstudy is to reveal that using instances, or specific instances\nof relations within texts, is crucial in relation extraction. By\nleveraging these instances, models can better generalize from\nlimited instances and improve their ability to extract and\nclassify relations in varied contexts accurately.\nThe relation extractors with the text generation model also\nuse instances by introducing selected instances as text into\nthe prompt [25], [26]. Such methods extend the templates to\naccept selected instances so that the text generation model can\nhandle instances by preparing placeholders for them. When\nusing a retriever [26], instances are prepared according to\nthe input, and when not using a retriever [25], instances are\nprepared in advance manually.\nThe instances enhanced text generation model involves\ntwo processes: the retrieved instances into prompts compat-\nible with the text generation model. In the traditional RAG\nframework [29], the retriever identifies neighboring instances\nand directly utilizes the text of these instances as prompts.\nConversely, in neural prompting [34], instances are selected\nbased on string matching and processed through a neural\nnetwork to use the instances as soft prompts, offering a more\nnuanced and adaptable approach to instance utilization. Let an\nexternal database for a reference, which includes the elements\nof a text and relation information (x, h, t, r), be D, a set of\nselected instances be D\u2032 \u2286D, the template that accepts\ninstances as input be Templatein(x, h, t, D\u2032), and the retrieval\nprocess to obtain D\u2032 from D using input information be\nD\u2032 = Retrieve(x, h, t, D), the text generation-based relation\nextractor with instances are shown in Equation (5).\n\u02c6r = argmax\nr\u2208R\nG(Templateout(x, h, t, r) |\nTemplatein(x, h, t, D\u2032))\n(5)\nIn the retriever that searches nearest instances in the em-\nbedding space, each instance d in the database D is converted\ninto L embeddings Ed:\nEd = [E1,d, E2,d, . . . , EL,d]\u22a4\u2208RL\u00d7N.\n(6)\nThe retriever aims to select instances that are near the input\nembedding Ein \u2208RL\u00d7N, which is embedded similarly to Ed\nusing x, h, and t. The method to embed instances and the\ninput is usually engineered to suit the purpose; for example,\nmodern applications [51] often use PLMs such as BERT [14].\nOur method also uses PLM to embed instances. Nearest\ninstance search requires the distance between input and each\ninstance calculated with a function to compute distance as\nDist(Ein, Ed). Since the kNN retriever\u2019s objective is to emit\ninstances within a distance of up to k-th, a set of target\ninstances D\u2032 becomes Equation (7), where argTopK returns\na set of top-k indices:\nD\u2032 =\n\u001a\nDi | i \u2208argTopK\nj\n\u2212Dist(Ein, Ej)\n\u001b\n(7)\nAfter the retrieval, the instances are converted into a format\nthat can be input into the model in the embedding process,\ne.g., the traditional method writes down into text prompts for\nPLM [29].\nIII. METHODOLOGY\nWe propose ETRAG, which enables end-to-end training of\ntext generation models with RAG. In order to fit a model\npretrained for a general task to the target task, the parameters\nneed fine-tuning to the objective of the target task. However,\nthe retriever part of general RAG cannot be trained end-to-\nend, and the instances selection cannot be optimized when\ntraining the model on the target task. The entire model must\nconsist of differentiable operations to compute gradient when\ntraining a deep learning model end-to-end. However, two in-\ndifferentiable operations prevent end-to-end training: select-\ning instances by retriever and making the selected instances\ninto a text prompt.\nTo overcome the indifferentiable processes, ETRAG re-\nplaces the retriever\u2019s process of selecting instances with a\nsoft kNN and introduces instances as soft prompts as shown\nin Figure 1. For the retriever selection, we employ a soft\nkNN, which selects instances softly inspired by neural nearest\nneighbor networks [52]. When introducing selected instances,\nETRAG injects the instances to input as soft prompts, which\nconcatenates instance embeddings into embedded input to-\nkens rather than text prompts, like neural prompting [34].\nTo extract relations by a text-generating model, we employ\nSuRE [27] as the base relation extractor. SuRE is a method\nof inference by a text-generating model, and since the model\nis a regular text-generation model, we propose a method for\nintroducing instances for the text-generation model.\nWe will explain the method in the following sections:\nETRAG consisting of differentiable k-nearest instance selec-\ntion (Section III-A) and integration of the instances drawn\nby it (Section III-B). The end-to-end training and training\ntechniques for ETRAG are shown in Section III-C.\nA. DIFFERENTIABLE K-NEAREST INSTANCE SELECTION\nThe differentiable k-nearest instance selection is achieved\nby weighted selection over multiple instances. In contrast,\n4\nVOLUME 11, 2023\n\nMakino et al.: End-to-End Trainable Retrieval-Augmented Generation for Relation Extraction\nThe head entity is Bolivarian Alternative for the Americas. The tail entity is Fidel Castro. The \ntype of Bolivarian Alternative for the Americas is ORGANIZATION . The type of Fidel Castro \nis PERSON . ALBA -- the Bolivarian Alternative for the Americas -- was founded by \nVenezuelan President Hugo Chavez and Cuban leader Fidel Castro in 2004 and also includes \nBolivia , Nicaragua and the Caribbean island of Dominica .\nThe head entity is Bolivarian Alternative for the Americas. The tail entity is ALBA. \u2026 The \nmember countries of the Bolivarian Alternative for the Americas ( ALBA ) ratified \u2026 \nBolivarian Alternative for the Americas has the alternate name ALBA.\nPrompting\nRelation\nInstance\nDatabase\nEmbedding\nInput applied to\nToken embeddings\n\u2026\nK soft prompts\nInsert\nText\nGeneration\nModel___\nRetrieval\norg:founded_by\n__ : Head entity  __: Tail entity\nBolivarian Alternative for the Americas \nwas founded by Fidel Castro.\nPrediction in\nFIGURE 1. An overview of ETRAG\nthe standard k-nearest instance selection is indifferentiable\nbecause instances are selected with sampling operation as\nshown in Equation (7). Therefore, in this section, we first\ndescribe the embedding of instances created by weighted\nsums of the instances and then explain the creation of weights.\nWe formulate the differentiable instance selection by creat-\ning a weighted sum of the instance embeddings, paying atten-\ntion to the important instances. Let weights be W \u2208RK\u00d7|D|,\nwhere Wk,d means the weight of d-th data for k-th selection,\nk-th selected instance embedding Sk \u2208RL\u00d7N is defined as\nSk = P|D|\nd=1 Wk,dEd.\nWe then explain how to compute the weights in a differ-\nentiable manner. The simplest solution to sample the nearest\ninstance is applying softmax to the distance between in-\nstances such as Gumbel softmax [53]. However, this approach\nis possible to select only the nearest neighbor instance, not\nthe second, third, and subsequent instances. In order to re-\nalize subsequent selection, instances with heavy weights in\nprevious steps are penalized, and the weights in subsequent\nsteps are computed based on them so that the instances once\nheavily weighted process almost exclusively, as described in\nEquation (8).\nWk,d =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\nexp (1\u2212Dist(Ein,Ed))\nP|D|\nd\u2032=1 exp (1\u2212Dist(Ein,Ed\u2032))\nif k = 1\nexp (1 \u2212Dist(Ein, Ed) + Pk\u22121\nl=1 log(1 \u2212Wl,d))\nP|D|\nd\u2032=1 exp (1 \u2212Dist(Ein, Ed\u2032) + Pk\u22121\nl=1 log(1 \u2212Wl,d\u2032))\notherwise\n(8)\nThe weights Wk become a nearly one-hot vector since the\nsoftmax function computes the selection weight. We assume\nthe distance Dist(Ein, Ed) is an average of the cosine distance\nbetween Ein and Ed as defined in Equation (9), where Dist is\nbounded from 0 to 1 (0 \u2264Dist \u22641).\nDist(Ein, Ed) = 1\n2L\nL\nX\nl=1\n1 \u2212\nE\u22a4\nin,lEd,l\n|Ein,l||Ed,l|\n(9)\nThe retrieval for relation extraction requires the preparation\nof embeddings E. We use representations of head and tail\nentities and relation labels, which are considered helpful for\nrelation extraction [54]. These representations are obtained\nby writing down the relation instances in text using templates,\nembedding them using another PLM, and extracting the rep-\nresentations of the text corresponding to the head entity, tail\nentity, and relation labels. Specifically, we use an average of\nembeddings in the span of entities or relations, where E\u00b7,1 and\nE\u00b7,2 are the embeddings of head and tail entity and E\u00b7,3 is the\nembedding of relation.\nThese processes provide K weights for each instance and K\nselected instances. When using this k-nearest instance selec-\ntion, the neural model constructed using the weights to select\ninstances can select instances without losing trainability and\nobtain their embeddings.\nB. NEURAL PROMPTING WITH TRAINABLE INSTANCE\nSELECTION\nWe propose a neural prompting that creates soft prompts\nfor the text generation model from the selection weights via\ndifferentiable k-nearest instance selection while the general\nretriever creates prompts as text. We compose soft prompts\nfrom instances softly selected by weighted summing of the\nembeddings over the instances using the selection weights in\nSection III-A instead of text prompts. The retriever becomes\ndifferentiable by composing with this process.\nFor the detailed procedure, the k-th selected instance em-\nbedding Sk is computed in the retrieval process. Now, we\nneed to prepare soft prompts from selected instances by\nreshaping selected instance embeddings S to the shape for\na soft prompt P\n\u2208\nRKL\u00d7N by stacking them as P\n=\n[S1,1, S1,2, . . . S1,L, S2,1, . . . SK,L]. Connecting the prompt and\na text generation model involves joining the prompt to the\ninput sequence embeddings. In this case, the length of the\nprompt KL is added to the length of the input series |x|,\nresulting in a new series with the length of |x| + KL.\nVOLUME 11, 2023\n5\n\nMakino et al.: End-to-End Trainable Retrieval-Augmented Generation for Relation Extraction\nTABLE 1. Statistics of the TACRED Dataset with Different Proportion.\nProportion\nTrain\nDev.\nTest\n100%\n68,125\n22,631\n15,509\n10%\n6,815\n2,265\n5%\n3,407\n1,133\n1%\n682\n227\nC. TRAINING\nThe training of the retriever proposed in this paper is simply\na matter of optimizing the ETRAG model with the objective\nfunction of the target task. Since the retriever consists entirely\nof differentiable operations in ETRAG, the model with the\nretriever is end-to-end trainable. Our method innovatively\ntransforms the retriever into an end-to-end trainable model,\nenhancing its applicability to relation extractors.\nThe primary challenge in training the retriever is its com-\nputational intensity, which requires calculating distances for\nall instances in D. To mitigate this during training, we employ\na strategy of random sampling a subset of instances. This ap-\nproach significantly reduces the computational burden while\nmaintaining the retriever\u2019s efficacy.\nSince the base model, SuRE, is subsequently connected to\nthe retriever, a stable training method for the base model is\nneeded. The training process of the text generation model eas-\nily affects the retriever\u2019s performance. Therefore, we employ\na warm-up step in which the retriever is trained in advance.\nThe base model is frozen, and the retriever is updated during\nthe warm-up steps. All the parameters are updated after the\nwarm-up step.\nIV. EVALUATION OF RELATION EXTRACTION\nPERFORMANCE\nThis section evaluates the relation extraction performance and\ncompares the ETRAG integrated model to existing relation\nextraction methods. Section IV-A presents the settings of sub-\nsequent experiments about datasets, baseline methods, model\nsettings, and training parameters. Based on the settings, Sec-\ntion IV-B shows the performance evaluations of our proposal\nby comparing it with other methods. Section IV-C is the\nablation study to show the elements that affect performance.\nA. EXPERIMENTAL SETTINGS\nTo assess the effectiveness of our relation extraction method,\nwe experiment on the TACRED dataset [55], a standard\nbenchmark in the sentence-level relation extraction, where\neach instance has an entity pair within a sentence and a gold\nrelation between the pair. TACRED is the only dataset for\nwhich templates are available, as it is the dataset of the target\nevaluated by the base model of SuRE. To understand the\nimpact of training data size, we experimented with reduced\ntraining and development data scenarios for TACRED: 100%,\n10%, 5%, and 1% of the entire dataset, following existing\nstudy [56]. The statistics for each scenario are shown in\nTables 1 and 2. We evaluated the mean value of the micro-\naveraged F1 score for three runs, treating the no_relation\nThe head entity is ${head entity}. The tail entity is ${tail entity}.\nThe type of ${head entity} is ${label of head entity} . The type\nof ${tail entity} is ${label of tail entity} . ${input sentence} .\nFIGURE 2. The input template of SuRE. ${\u00b7} are placeholders to replace\nwith input.\nclass as a negative example for TACRED following the offi-\ncial evaluation settings. We calculate the loss to development\ndata for every 100 update steps and report the score obtained\nwhen training in early stopping with the early stopping pa-\ntience of 3 for the TACRED dataset.\nWe compared our method against state-of-the-art models:\nSuRE (based on Pegasus-large) [27], DeepStruct [17], kNN-\nRE [28], and NLI_DeBERTa [56]. SuRE is, as described in\nSection II-C, the model extracting relation with the text gen-\neration model by formulating the relation extraction task to\nthe text summarization task using Seq2Seq LM. DeepStruct is\na PLM trained for structured prediction tasks, which include\nrelation extraction. kNN-RE performs kNN algorithm on the\nPLM embedding space of relation instances. NLI_DeBERTa\nextracts relations with a natural language inference task,\nwhich recognizes fact inclusion in a hypothesis, by identify-\ning the implication of verbalized relations in a target text.\nWe employed SuRE [27] as a generation-based relation\nextraction model. We introduced the ETRAG to SuRE by\nadding soft prompts into the input sequence between the BOS\ntoken and the following prompt. The hyperparameters of the\nSuRE were the same as those of the original research. The\ntemplates were the same as in the original paper: the templates\nfor the SuRE input were in the form of Figure 2, from which\nthe summary templates defined for each relation label are\npredicted as in Table 3. The beam search width, a parameter\nused in SuRE classification, was set to 4, the same value as in\nthe original paper of SuRE.\nWe use the Flan-T5 large model [20] in the SuRE frame-\nwork with and without the addition of ETRAG because the\ntuning before experiments showed the training of Pegasus-\nlarge [57] based SuRE with ETRAG was unstable. When\nwe introduced ETRAG into the Pegasus-based model, the\nmodel predicted only no_relation for the same setting.\nSince our method used two PLMs, the base relation extraction\nmodel and the embedding model for the retriever, the larger\nmodels were unacceptable for our computational resources.\nWe conducted trials introducing 10-neighbor instances as\nprompts, i.e., K = 10, pretraining the retriever for 300 steps\nbefore end-to-end training as the warm-up step described in\nSection III-C. Due to computational constraints, the database\nD was constructed from randomly sampled 5,000 instances\nin the training dataset if training data has more than 5,000\ninstances. At the training time, 32 instances are randomly\nsampled as the subset of D before retrieval. For the embed-\ndings E, we used the entity and relation representations of\nPLM by averaging their spans, where the PLM input was\ncreated by concatenating the template in Figure 2 with the\nrelation template. The entity spans were underlined parts of\n6\nVOLUME 11, 2023\n\nMakino et al.: End-to-End Trainable Retrieval-Augmented Generation for Relation Extraction\nTABLE 2. Statistics of Relation Labels for Each Split and Proportion of TACRED dataset.\n100%\n10%\n5%\n1%\nRelation Label\nTrain\nDev.\nTrain\nDev.\nTrain\nDev.\nTrain\nDev.\nTest\nno_relation\n55,112\n17,195\n5,513\n1,721\n2,756\n861\n552\n173\n12,184\nper:title\n2,443\n919\n244\n92\n122\n46\n24\n9\n500\norg:top_members/employees\n1,890\n534\n190\n53\n95\n26\n19\n5\n346\nper:employee_of\n1,524\n375\n152\n38\n76\n19\n15\n4\n264\norg:alternate_names\n808\n338\n80\n34\n40\n17\n8\n3\n213\norg:country_of_headquarters\n468\n177\n47\n18\n24\n9\n5\n2\n108\nper:countries_of_residence\n445\n226\n44\n22\n22\n11\n4\n2\n148\nper:age\n390\n243\n40\n24\n20\n12\n4\n2\n200\norg:city_of_headquarters\n382\n109\n38\n10\n19\n5\n4\n1\n82\nper:cities_of_residence\n374\n179\n38\n18\n19\n9\n4\n2\n189\nper:stateorprovinces_of_residence\n331\n72\n33\n8\n16\n4\n3\n1\n81\nper:origin\n325\n210\n32\n21\n16\n11\n3\n2\n132\norg:subsidiaries\n296\n113\n30\n12\n15\n6\n3\n1\n44\norg:parents\n286\n96\n28\n10\n14\n5\n3\n1\n62\nper:spouse\n258\n159\n26\n16\n13\n8\n3\n2\n66\norg:stateorprovince_of_headquarters\n229\n70\n23\n7\n11\n4\n2\n1\n51\nper:children\n211\n99\n21\n10\n10\n5\n2\n1\n37\nper:other_family\n179\n80\n18\n8\n9\n4\n2\n1\n60\norg:members\n170\n85\n17\n8\n9\n4\n2\n1\n31\nper:siblings\n165\n30\n17\n3\n9\n1\n2\n0\n55\nper:parents\n152\n56\n16\n6\n8\n3\n2\n1\n88\nper:schools_attended\n149\n50\n14\n5\n7\n3\n1\n1\n30\nper:date_of_death\n134\n206\n13\n20\n6\n10\n1\n2\n54\norg:founded_by\n124\n76\n12\n8\n6\n4\n1\n1\n68\norg:member_of\n122\n31\n12\n3\n6\n1\n1\n0\n18\nper:cause_of_death\n117\n168\n12\n17\n6\n9\n1\n2\n52\norg:website\n111\n86\n11\n9\n5\n5\n1\n1\n26\norg:political/religious_affiliation\n105\n10\n10\n1\n5\n0\n1\n0\n10\nper:alternate_names\n104\n38\n10\n4\n5\n2\n1\n0\n11\norg:founded\n91\n38\n10\n4\n5\n2\n1\n0\n37\nper:city_of_death\n81\n118\n8\n12\n4\n6\n1\n1\n28\norg:shareholders\n76\n55\n8\n6\n4\n3\n1\n1\n13\norg:number_of_employees/members\n75\n27\n8\n2\n4\n1\n1\n0\n19\nper:charges\n72\n105\n8\n10\n4\n5\n1\n1\n103\nper:city_of_birth\n65\n33\n7\n3\n4\n1\n1\n0\n5\nper:date_of_birth\n63\n31\n7\n3\n4\n1\n1\n0\n9\nper:religion\n53\n53\n6\n6\n3\n3\n1\n1\n47\nper:stateorprovince_of_death\n49\n41\n4\n4\n2\n2\n0\n0\n14\nper:stateorprovince_of_birth\n38\n26\n4\n2\n2\n1\n0\n0\n8\nper:country_of_birth\n28\n20\n2\n2\n1\n1\n0\n0\n5\norg:dissolved\n23\n8\n2\n0\n1\n0\n0\n0\n2\nper:country_of_death\n6\n46\n0\n5\n0\n3\n0\n1\n9\nFigure 2. The relation span was the relation template part\nwhen the database was embedded and the EOS token when\nthe prediction target was embedded. We applied LoRA [36]\nto Flan-T5 with rank r = 32 and dropout rate 0.1 to all kinds\nof layers of Transformer. The LoRA is a method for efficient\nfine-tuning, and we employed it because our pilot experi-\nments showed full tuning and LoRA tuning performances are\nnot significantly different. The dropout rate was set to 0.1.\nAdamW was used to optimize the models, with a learning\nrate of 5 \u00d7 10\u22124 for Flan-T5 and 1 \u00d7 10\u22123 for the other\nparameters, and weights of 5 \u00d7 10\u22126 for the bias and layer\nnormalization parameters. The batch size was set to 64. The\nparameters used for evaluation were those used when early\nstopping completed training with patience set to 5. A single\nNVIDIA A100 was used for each experiment.\nB. EXTRACTION PERFORMANCE COMPARISON\nThe results in Table 4 indicate that ETRAG consistently\nimproved performance from the model without ETRAG for\nthe TACRED dataset. The results confirm that ETRAG can\nenhance relations extraction by text generation. Addition-\nally, ETRAG outperforms the existing models, SuRE and\nNLI_DeBERTa, in scenarios with limited training data (10%).\nThis is the new state-of-the-art result for the setting of the\nTACRED dataset under the 10% setting.\nComparing Pegasus-based and T5-based SuRE, the\nPegasus-based SuRE performed better. This is because Pe-\ngasus is a model created specifically for the summarization\ntask, which matches SuRE\u2019s objective. Even in this situation,\nthe ETRAG boosted the performance of the Flan-T5-based\nmodel and achieved the best performance on the 10% setting.\nCompared to another instance-based method, kNN-RE,\nadding generation-based prediction of relations to neighbor-\nhood method-based inference confirms the improved extrac-\ntion performance. This proves that simple instance utilization\nis insufficient and that inference capability is essential.\nVOLUME 11, 2023\n7\n\nMakino et al.: End-to-End Trainable Retrieval-Augmented Generation for Relation Extraction\nTABLE 3. The output template of SuRE. ${\u00b7} are placeholder replaced with input.\nRelation Label\nTemplate\nno_relation\n${subj} has no known relations to ${obj}\nper:title\n${subj} is a ${obj}\norg:top_members/employees\n${subj} has the high level member ${obj}\nper:employee_of\n${subj} is the employee of ${obj}\norg:alternate_names\n${subj} is also known as ${obj}\norg:country_of_headquarters\n${subj} has a headquarter in the country ${obj}\nper:countries_of_residence\n${subj} lives in the country ${obj}\nper:age\n${subj} has the age ${obj}\norg:city_of_headquarters\n${subj} has a headquarter in the city ${obj}\nper:cities_of_residence\n${subj} lives in the city ${obj}\nper:stateorprovinces_of_residence\n${subj} lives in the state or province ${obj}\nper:origin\n${subj} has the nationality ${obj}\norg:subsidiaries\n${subj} owns ${obj}\norg:parents\n${subj} has the parent company ${obj}\nper:spouse\n${subj} is the spouse of ${obj}\norg:stateorprovince_of_headquarters\n${subj} has a headquarter in the state or province ${obj}\nper:children\n${subj} is the parent of ${obj}\nper:other_family\n${subj} is the other family member of ${obj}\norg:members\n${subj} has the member ${obj}\nper:siblings\n${subj} is the siblings of ${obj}\nper:parents\n${subj} has the parent ${obj}\nper:schools_attended\n${subj} studied in ${obj}\nper:date_of_death\n${subj} died in the date ${obj}\norg:founded_by\n${subj} was founded by ${obj}\norg:member_of\n${subj} is the member of ${obj}\nper:cause_of_death\n${subj} died because of ${obj}\norg:website\n${subj} has the website ${obj}\norg:political/religious_affiliation\n${subj} has political affiliation with ${obj}\nper:alternate_names\n${subj} has the alternate name ${obj}\norg:founded\n${subj} was founded in ${obj}\nper:city_of_death\n${subj} died in the city ${obj}\norg:shareholders\n${subj} has shares hold in ${obj}\norg:number_of_employees/members\n${subj} has the number of employees ${obj}\nper:charges\n${subj} is convicted of ${obj}\nper:city_of_birth\n${subj} was born in the city ${obj}\nper:date_of_birth\n${subj} has birthday on ${obj}\nper:religion\n${subj} has the religion ${obj}\nper:stateorprovince_of_death\n${subj} died in the state or province ${obj}\nper:stateorprovince_of_birth\n${subj} was born in the state or province ${obj}\nper:country_of_birth\n${subj} was born in the country ${obj}\norg:dissolved\n${subj} dissolved in ${obj}\nper:country_of_death\n${subj} died in the country ${obj}\nTABLE 4. Comparison of Relation Extraction Performance [%]\n100%\n10%\n5%\n1%\nDeepStruct [17]\n76.8\n\u2013\n\u2013\n\u2013\nSuRE (Pegasus) [27]\n75.1\n70.7\n64.9\n52.0\nNLI_DeBERTa [56]\n73.9\n67.9\n69.0\n63.0\nkNN-RE [28]\n70.6\n\u2013\n\u2013\n\u2013\nSuRE (Flan-T5)\n71.4 \u00b11.6\n68.5 \u00b11.5\n65.0 \u00b13.1\n53.5 \u00b11.4\n+ ETRAG\n73.3 \u00b10.5\n71.5 \u00b10.5\n68.3 \u00b12.0\n54.6 \u00b11.1\nC. ABLATION STUDIES\nThis section delves into the factors influencing the extraction\nperformance observed in Section IV-B and investigates the\nmodel\u2019s behavior. We conducted ablation studies in the 10%\ntraining instance setting for the TACRED dataset to confirm\nwhen our method showed notable improvements.\nOur ablation study aimed to pinpoint the elements critical\nto our method\u2019s enhanced performance. We examined various\nscenarios: employing k-nearest neighbor instances without\nretriever training (No Retriv. Training), omitting the warm-\nup phase in Retriever training (No Warm-up), using randomly\nchosen instances (Random), and utilizing CLS token repre-\nsentations (CLS).No Retriv. Training aims to investigate the\nusefulness of the end-to-end trainable retriever by using the\ninitial parameter for the retriever and operations of ETRAG.\nNo Warm-up omits the warm-up step but trains the retriever,\nwhich checks the stability effect of the warm-up. Random\npicks up instances randomly and uses soft prompts in the\nsame embedding process as ETRAG, where the experiment\nchecks retrieval process training effectiveness. CLS uses only\nCLS token representations instead of the relation extraction-\nspecific representation. The other settings of experiments are\nthe same as settings in Section IV-A except for the number of\nruns for evaluation that changed from 3 runs to 1 run.\nThe results in Table 5 reveal that omitting any of these\ncomponents results in lower F1 scores, underscoring their\ncollective importance. The No Retriv. Training caused a\nperformance loss of 2.2 percentage points, which was not\nmuch different from the performance of SuRE without any\nretrievers. This result indicates that in the relation extraction\n8\nVOLUME 11, 2023\n\nMakino et al.: End-to-End Trainable Retrieval-Augmented Generation for Relation Extraction\nTABLE 5. Ablation Study Results [%]\nSuRE (Flan-T5)\n68.5\n+ ETRAG\n71.7\nNo Retriv. Training\n69.2\nNo Warm-up\n70.7\nRandom\n71.0\nCLS\n68.8\nwith text generation model, the retriever needs to be trained\nfor the relation extraction objective to improve performance\nwhen relation instances are introduced with the retriever.\nThe No Warm-up reduced extraction performance by 1.0\npercentage points; the decrease was relatively smaller than in\nthe other cases, No Retriv. Training and CLS. This may be due\nto the lack of treatment for convergence stability, although\nsimilar processing and training of the model is carried out.\nIn the case of Random, where random instances are used\nwithout training the retrieval process, the performance drop is\nrelatively small, around 0.7 percentage points. This compari-\nson allows us to evaluate the improvement separately due to\nthe introduction and selection of instances. Compared to the\ncase where no retrieval process is used (i.e., SuRE), introduc-\ning randomly selected instances shows a 2.4 percentage point\nimprovement. The results suggest that introducing examples\nimproves performance, and further progress is made when the\nmodel selects instances.\nFor comparison of representations used in ETRAG, the per-\nformance with the CLS representation degraded 2.9 percent-\nage points from the one engineered for relation extraction.\nAdditionally, the performance was almost the same as one of\nSuRE without a retriever. Thus, engineering a representation\nspecializing in the target task is essential to using ETRAG.\nOverall, these analyses highlight the contributions of our\nmethod, ETRAG, to performance, and we confirmed that all\nits components are effective. Instance selection and training\nhelped improve relation extraction outcomes.\nV. RETRIEVER ANALYSIS\nSection IV showed the performance improvement by our\nproposed ETRAG that was integrated into the text generation\nmodel (Section IV-B). The ablation studies in Section IV-C\nshowed the factors contributing to performance. However,\nit is not yet clear what happened inside ETRAG that led to\nthe improvements. Therefore, we also analyzed the retriever\nfrom the perspective of instances. Section V-A confirms the\nimpact of the instances by changing the number of instances\ncreated by the retriever and checking their behavior at that\ntime; Section V-B shows what instances were retrieved and\nused after training by statistically analyzing the instances to\nanalyze the actual retrieved instances directly.\nA. PERFORMANCE VARIATION WITH NUMBER OF\nINSTANCES\nWe investigated the sensitivity of the number of instances k by\nvarying k from 0 to 20, where 0 instance means ETRAG is not\nused. The F1, precision, and recall scores versus k are shown\n0\n1\n5\n10\n15\n20\nk\n64\n66\n68\n70\n72\n74\n76\nMetric [%]\nF1 Score\nPrecision\nRecall\nFIGURE 3. Change in Extraction Performance with the Number of\nInstances k Used in Prompts\nTABLE 6. Comparison of contents between retrieved instances and\nextraction target. Label means the relation label is the same. Entity means\neither head entity or tail entity is contained. Related means either head\nentity, tail entity, or relation label is contained.\nLabel\nHead entity\nTail entity\nEntity\nRelated\ntop-1\n72.2\n7.1\n3.4\n10.0\n73.6\ntop-3\n67.9\n8.3\n4.4\n12.0\n77.4\ntop-5\n61.8\n10.9\n6.3\n15.7\n78.9\ntop-10\n60.4\n18.6\n10.9\n25.8\n82.1\nin Figure 3. The F1 score reached its maximum at k = 10\nand no significant change in the 1 \u2264k \u226415 interval. On the\nother hand, the balance between precision and recall changed\nsignificantly. In the 0 \u2264k \u22645 interval, precision increased,\nand recall decreased as overall performance improved. Con-\nversely, recall gradually increased, and precision decreased\nin the interval k > 5, except for k = 20. Since precision and\nrecall were balanced at k = 10, the F1 score was the largest,\ndefined as the harmonic mean of the precision and recall.\nThese characteristics are useful in real-world applications\nand can be used to make performance trade-offs of precision\nand recall to suit the situation. For example, applications\nthat require users to retrieve necessary relational information,\nsuch as a search system, could use a larger k for coverage.\nB. RETRIEVED INSTANCES\nSince the characteristics of retrievers are most evident in\nretrieved instances, we investigate the retrieved instances in\nETRAG. However, because selected instances in ETRAG are\na weighted sum of the instance embeddings, obtaining what\nwas chosen explicitly is impossible. Therefore, we analyze k\nnearest instances specified by the actual kNN algorithm on\nthe feature space after training of ETRAG, which turns out\nthat they are almost the same instances used in ETRAG.\nWe took statistics on the retrieved instances linked to the\nextraction target. The targets for statistics are the relation\nlabels, head entities, and tail entities, which are closely related\nto the relation extraction. We calculate the percentage of\nmatches between the retrieved instances and the target of\nextraction in relation labels and the surface of entities.\nThe statistics in Table 6 show the percentage of the re-\ntrieved instances that contain objects related to relation ex-\ntraction. First, for the Label column, the percentage gradually\nVOLUME 11, 2023\n9\n\nMakino et al.: End-to-End Trainable Retrieval-Augmented Generation for Relation Extraction\ndecreases from top-1 to top-10. This indicates that the feature\nspace of the retrieval is structured based on the type of relation\nlabels and that instances with the same relation labels are\nplaced close to each other. For the Entity column, the percent-\nage gradually increases from top-1 to top-10, indicating that\nincluding entities is a second retrieval perspective, although\nthe percentages are less than those in the Label column.\nThe results for the Head and Tail entity columns show that\ninstances containing the head entity are more intensive. From\nthe results of the top-10 row in the Related column, more\nthan 80% have been selected that contain labels or entities\nrelevant for relation extraction. This may be due to the use of\nrelation and entity features for retrieval. As a result of end-to-\nend training from these features, the distance becomes smaller\nwhen the relation labels match or entities are included. These\nproperties were not given intentionally but were acquired only\nby training through end-to-end relation extraction, indicating\nthat the retriever does not require any special training.\nVI. CONCLUSIONS\nThis study introduced a novel approach to the text generation\nmodel by implementing a retriever with the differentiable k-\nnearest neighbor selection for end-to-end trainable modeling.\nExisting models with retrievers cannot train end-to-end due\nto the non-differentiable environments of the instance selec-\ntion part and the integration part of instances. Therefore, we\nproposed a fully differentiable and end-to-end trainable RAG\nETRAG by differentiable k-nearest neighbor selection and\nintegration as a soft prompt. Our method, centered around\nneural prompting, significantly enhances the retriever\u2019s ca-\npability to select instances for use in prompts.\nExperimental findings underscore this approach\u2019s effec-\ntiveness, particularly in scenarios with limited training data\nin evaluating relation extraction performance. We evaluated\nthe model with ETRAG and compared the model without\nETRAG with existing methods with the TACRED dataset.\nOur experiments showed that our proposal ETRAG consis-\ntently improved from the baseline model without ETRAG.\nMoreover, the model reported outstanding performance in\nlow-resource settings, especially the new state-of-the-art for\nthe TACRED dataset in the 10% training data setting.\nOur analysis confirmed that the number of retrieved in-\nstances introduced by ETRAG can balance the precision-\nrecall trade-off. We also confirmed that the end-to-end trained\nretriever referred to the instances involved in relation extrac-\ntion. However, our study also identified limitations in our\nmethod\u2019s performance when training instances are sufficient.\nFuture work could focus on refining the retriever\u2019s training\nprocess to adapt more effectively to varying sizes of training\ndatasets and exploring ways to optimize instance selection for\na broader range of data scenarios. Moreover, since we evalu-\nated on only relation extraction while ETRAG can be applied\nto other text generation models, further evaluation of other\ntasks, such as question answering [58], will be conducted to\nconfirm the applicability of ETRAG.\nACKNOWLEDGEMENTS\nThe authors would like to express gratitude to the members\nof the Computational Intelligence Laboratory for their help-\nful comments on this study. This work was supported by\nGrant-in-Aid for JSPS Fellows Grant Number JP22J14025.\nComputational resources of AI Bridging Cloud Infrastructure\n(ABCI) provided by the National Institute of Advanced In-\ndustrial Science and Technology (AIST) were used.\nREFERENCES\n[1] N.\nA.\nChinchor,\n\u2018\u2018Overview\nof\nMUC-7,\u2019\u2019\nin\nSeventh\nMessage\nUnderstanding Conference (MUC-7): Proceedings of a Conference\nHeld in Fairfax, Virginia, April 29 - May 1, 1998, 1998. [Online].\nAvailable: https://aclanthology.org/M98-1001\n[2] E. F. Tjong Kim Sang and F. De Meulder, \u2018\u2018Introduction to the CoNLL-\n2003 shared task: Language-independent named entity recognition,\u2019\u2019 in\nProceedings of the Seventh Conference on Natural Language Learning\nat HLT-NAACL 2003, 2003, pp. 142\u2013147. [Online]. Available: https:\n//www.aclweb.org/anthology/W03-0419\n[3] L. Wang, Z. Cao, G. de Melo, and Z. Liu, \u2018\u2018Relation classification via\nmulti-level attention CNNs,\u2019\u2019 in Proceedings of the 54th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers).\nBerlin, Germany: Association for Computational Linguistics, Aug. 2016,\npp. 1298\u20131307. [Online]. Available: https://aclanthology.org/P16-1123\n[4] K. Du, B. Yang, S. Wang, Y. Chang, S. Li, and G. Yi, \u2018\u2018Relation\nextraction for manufacturing knowledge graphs based on feature fusion\nof attention mechanism and graph convolution network,\u2019\u2019 Knowledge-\nBased Systems, vol. 255, p. 109703, 2022. [Online]. Available: https:\n//www.sciencedirect.com/science/article/pii/S0950705122008620\n[5] A. P. Davis, C. G. Murphy, C. A. Saraceni-Richards, M. C. Rosenstein,\nT. C. Wiegers, and C. J. Mattingly, \u2018\u2018Comparative Toxicogenomics\nDatabase: a knowledgebase and discovery tool for chemical\u2013gene\u2013disease\nnetworks,\u2019\u2019 Nucleic Acids Research, vol. 37, no. suppl_1, pp. D786\u2013D792,\n09 2008. [Online]. Available: https://doi.org/10.1093/nar/gkn580\n[6] K. Zhu, J. Huang, and K. C.-C. Chang, \u2018\u2018Descriptive knowledge\ngraph in biomedical domain,\u2019\u2019 in Proceedings of the 2023 Conference\non\nEmpirical\nMethods\nin\nNatural\nLanguage\nProcessing:\nSystem\nDemonstrations, Y. Feng and E. Lefever, Eds.\nSingapore: Association for\nComputational Linguistics, Dec. 2023, pp. 462\u2013470. [Online]. Available:\nhttps://aclanthology.org/2023.emnlp-demo.42\n[7] Y. Long, M. Wu, Y. Liu, Y. Fang, C. K. Kwoh, J. Chen, J. Luo, and X. Li,\n\u2018\u2018Pre-training graph neural networks for link prediction in biomedical\nnetworks,\u2019\u2019 Bioinformatics, vol. 38, no. 8, pp. 2254\u20132262, 02 2022.\n[Online]. Available: https://doi.org/10.1093/bioinformatics/btac100\n[8] M. Miwa and M. Bansal, \u2018\u2018End-to-end relation extraction using LSTMs\non sequences and tree structures,\u2019\u2019 in Proceedings of the 54th Annual\nMeeting of the Association for Computational Linguistics (Volume\n1: Long Papers).\nBerlin, Germany: Association for Computational\nLinguistics, Aug. 2016, pp. 1105\u20131116. [Online]. Available: https:\n//www.aclweb.org/anthology/P16-1105\n[9] G. Nan, Z. Guo, I. Sekulic, and W. Lu, \u2018\u2018Reasoning with latent structure\nrefinement for document-level relation extraction,\u2019\u2019 in Proceedings of the\n58th Annual Meeting of the Association for Computational Linguistics.\nOnline: Association for Computational Linguistics, Jul. 2020, pp.\n1546\u20131557. [Online]. Available: https://www.aclweb.org/anthology/2020.\nacl-main.141\n[10] Y. Ma, A. Wang, and N. Okazaki, \u2018\u2018DREEAM: Guiding attention\nwith evidence for improving document-level relation extraction,\u2019\u2019 in\nProceedings of the 17th Conference of the European Chapter of the\nAssociation for Computational Linguistics, A. Vlachos and I. Augenstein,\nEds.\nDubrovnik, Croatia: Association for Computational Linguistics,\nMay 2023, pp. 1971\u20131983. [Online]. Available: https://aclanthology.org/\n2023.eacl-main.145\n[11] E. Riloff, \u2018\u2018Automatically constructing a dictionary for information extrac-\ntion tasks,\u2019\u2019 in AAAI, vol. 1, no. 1.\nCiteseer, 1993, pp. 2\u20131.\n[12] D. Zelenko, C. Aone, and A. Richardella, \u2018\u2018Kernel methods for relation\nextraction,\u2019\u2019 Journal of machine learning research, vol. 3, no. Feb, pp.\n1083\u20131106, 2003.\n[13] M. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\nand L. Zettlemoyer, \u2018\u2018Deep contextualized word representations,\u2019\u2019 in\nProceedings of the 2018 Conference of the North American Chapter\n10\nVOLUME 11, 2023\n\nMakino et al.: End-to-End Trainable Retrieval-Augmented Generation for Relation Extraction\nof the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers).\nNew Orleans, Louisiana:\nAssociation for Computational Linguistics, Jun. 2018, pp. 2227\u20132237.\n[Online]. Available: https://www.aclweb.org/anthology/N18-1202\n[14] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u2018\u2018BERT: Pre-\ntraining of deep bidirectional transformers for language understanding,\u2019\u2019\nin Proceedings of the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers).\nMinneapolis, Minnesota:\nAssociation for Computational Linguistics, Jun. 2019, pp. 4171\u20134186.\n[Online]. Available: https://www.aclweb.org/anthology/N19-1423\n[15] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V. Stoyanov, \u2018\u2018Roberta: A robustly optimized bert\npretraining approach,\u2019\u2019 2019.\n[16] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\n\u2018\u2018Language models are unsupervised multitask learners,\u2019\u2019 2019.\n[17] C. Wang, X. Liu, Z. Chen, H. Hong, J. Tang, and D. Song, \u2018\u2018DeepStruct:\nPretraining of language models for structure prediction,\u2019\u2019 in Findings of\nthe Association for Computational Linguistics: ACL 2022, S. Muresan,\nP. Nakov, and A. Villavicencio, Eds.\nDublin, Ireland: Association for\nComputational Linguistics, May 2022, pp. 803\u2013823. [Online]. Available:\nhttps://aclanthology.org/2022.findings-acl.67\n[18] J. Y. Huang, B. Li, J. Xu, and M. Chen, \u2018\u2018Unified semantic typing with\nmeaningful label inference,\u2019\u2019 in Proceedings of the 2022 Conference of the\nNorth American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, M. Carpuat, M.-C. de Marneffe, and I. V.\nMeza Ruiz, Eds.\nSeattle, United States: Association for Computational\nLinguistics, Jul. 2022, pp. 2642\u20132654. [Online]. Available: https://\naclanthology.org/2022.naacl-main.190\n[19] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-\nVoss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu,\nC. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess,\nJ. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and\nD. Amodei, \u2018\u2018Language models are few-shot learners,\u2019\u2019 in Advances in Neu-\nral Information Processing Systems, H. Larochelle, M. Ranzato, R. Had-\nsell, M. Balcan, and H. Lin, Eds., vol. 33.\nCurran Associates, Inc., 2020,\npp. 1877\u20131901.\n[20] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li,\nX. Wang, M. Dehghani, S. Brahma, A. Webson, S. S. Gu, Z. Dai, M. Suz-\ngun, X. Chen, A. Chowdhery, A. Castro-Ros, M. Pellat, K. Robinson,\nD. Valter, S. Narang, G. Mishra, A. Yu, V. Zhao, Y. Huang, A. Dai, H. Yu,\nS. Petrov, E. H. Chi, J. Dean, J. Devlin, A. Roberts, D. Zhou, Q. V. Le, and\nJ. Wei, \u2018\u2018Scaling instruction-finetuned language models,\u2019\u2019 2022.\n[21] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,\nB. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin,\nE. Grave, and G. Lample, \u2018\u2018Llama: Open and efficient foundation language\nmodels,\u2019\u2019 2023.\n[22] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou,\nW. Li, and P. J. Liu, \u2018\u2018Exploring the limits of transfer learning with a unified\ntext-to-text transformer,\u2019\u2019 J. Mach. Learn. Res., vol. 21, no. 1, jan 2020.\n[23] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-\nVoss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler,\nJ. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,\nB. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever,\nand D. Amodei, \u2018\u2018Language models are few-shot learners,\u2019\u2019 2020.\n[24] A. D. Cohen, S. Rosenman, and Y. Goldberg, \u2018\u2018Supervised relation clas-\nsification as twoway span-prediction,\u2019\u2019 in 4th Conference on Automated\nKnowledge Base Construction, 2022.\n[25] S. Wadhwa, S. Amir, and B. Wallace, \u2018\u2018Revisiting relation extraction in the\nera of large language models,\u2019\u2019 in Proceedings of the 61st Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers),\nA. Rogers, J. Boyd-Graber, and N. Okazaki, Eds.\nToronto, Canada:\nAssociation for Computational Linguistics, Jul. 2023, pp. 15 566\u201315 589.\n[Online]. Available: https://aclanthology.org/2023.acl-long.868\n[26] Z. Wan, F. Cheng, Z. Mao, Q. Liu, H. Song, J. Li, and S. Kurohashi,\n\u2018\u2018GPT-RE: In-context learning for relation extraction using large language\nmodels,\u2019\u2019 in Proceedings of the 2023 Conference on Empirical Methods\nin Natural Language Processing, H. Bouamor, J. Pino, and K. Bali,\nEds.\nSingapore: Association for Computational Linguistics, Dec.\n2023, pp. 3534\u20133547. [Online]. Available: https://aclanthology.org/2023.\nemnlp-main.214\n[27] K. Lu, I.-H. Hsu, W. Zhou, M. D. Ma, and M. Chen, \u2018\u2018Summarization as\nindirect supervision for relation extraction,\u2019\u2019 in Findings of the Association\nfor Computational Linguistics: EMNLP 2022, Y. Goldberg, Z. Kozareva,\nand Y. Zhang, Eds.\nAbu Dhabi, United Arab Emirates: Association\nfor Computational Linguistics, Dec. 2022, pp. 6575\u20136594. [Online].\nAvailable: https://aclanthology.org/2022.findings-emnlp.490\n[28] Z. Wan, Q. Liu, Z. Mao, F. Cheng, S. Kurohashi, and J. Li, \u2018\u2018Rescue\nimplicit and long-tail cases: Nearest neighbor relation extraction,\u2019\u2019 in\nProceedings of the 2022 Conference on Empirical Methods in Natural\nLanguage Processing, Y. Goldberg, Z. Kozareva, and Y. Zhang, Eds.\nAbu Dhabi, United Arab Emirates: Association for Computational\nLinguistics, Dec. 2022, pp. 1731\u20131738. [Online]. Available: https:\n//aclanthology.org/2022.emnlp-main.113\n[29] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. K\u00fcttler,\nM. Lewis, W.-t. Yih, T. Rockt\u00e4schel, S. Riedel, and D. Kiela, \u2018\u2018Retrieval-\naugmented generation for knowledge-intensive nlp tasks,\u2019\u2019 in Advances\nin Neural Information Processing Systems, H. Larochelle, M. Ranzato,\nR. Hadsell, M. Balcan, and H. Lin, Eds., vol. 33.\nCurran Associates,\nInc., 2020, pp. 9459\u20139474.\n[30] S. Hochreiter and J. Schmidhuber, \u2018\u2018Long short-term memory,\u2019\u2019 Neural\nComput., vol. 9, no. 8, p. 1735\u20131780, Nov. 1997. [Online]. Available:\nhttps://doi.org/10.1162/neco.1997.9.8.1735\n[31] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. u. Kaiser, and I. Polosukhin, \u2018\u2018Attention is all you need,\u2019\u2019 in Advances\nin Neural Information Processing Systems 30, I. Guyon, U. V. Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett,\nEds.\nCurran Associates, Inc., 2017, pp. 5998\u20136008. [Online]. Available:\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf\n[32] B. Lester, R. Al-Rfou, and N. Constant, \u2018\u2018The power of scale for\nparameter-efficient prompt tuning,\u2019\u2019 in Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Processing, M.-F. Moens,\nX. Huang, L. Specia, and S. W.-t. Yih, Eds.\nOnline and Punta Cana,\nDominican Republic: Association for Computational Linguistics, Nov.\n2021, pp. 3045\u20133059. [Online]. Available: https://aclanthology.org/2021.\nemnlp-main.243\n[33] X. L. Li and P. Liang, \u2018\u2018Prefix-tuning: Optimizing continuous prompts\nfor generation,\u2019\u2019 in Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th International\nJoint Conference on Natural Language Processing (Volume 1: Long\nPapers), C. Zong, F. Xia, W. Li, and R. Navigli, Eds.\nOnline: Association\nfor Computational Linguistics, Aug. 2021, pp. 4582\u20134597. [Online].\nAvailable: https://aclanthology.org/2021.acl-long.353\n[34] Y. Tian, H. Song, Z. Wang, H. Wang, Z. Hu, F. Wang, N. V. Chawla, and\nP. Xu, \u2018\u2018Graph neural prompting with large language models,\u2019\u2019 2023.\n[35] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe,\nA. Gesmundo, M. Attariyan, and S. Gelly, \u2018\u2018Parameter-efficient transfer\nlearning for NLP,\u2019\u2019 in Proceedings of the 36th International Conference\non Machine Learning, ser. Proceedings of Machine Learning Research,\nK. Chaudhuri and R. Salakhutdinov, Eds., vol. 97.\nPMLR, 09\u201315 Jun\n2019, pp. 2790\u20132799. [Online]. Available: https://proceedings.mlr.press/\nv97/houlsby19a.html\n[36] E. J. Hu, yelong shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,\nand W. Chen, \u2018\u2018LoRA: Low-rank adaptation of large language models,\u2019\u2019\nin International Conference on Learning Representations, 2022. [Online].\nAvailable: https://openreview.net/forum?id=nZeVKeeFYf9\n[37] E. L. Kaplan and P. Meier, \u2018\u2018Nonparametric estimation from incomplete\nobservations,\u2019\u2019 Journal of the American Statistical Association, vol. 53,\nno. 282, pp. 457\u2013481, 1958. [Online]. Available: https://www.tandfonline.\ncom/doi/abs/10.1080/01621459.1958.10501452\n[38] A. S\u00f8gaard, \u2018\u2018Semi-supervised condensed nearest neighbor for part-\nof-speech tagging,\u2019\u2019 in Proceedings of the 49th Annual Meeting\nof the Association for Computational Linguistics: Human Language\nTechnologies, D. Lin, Y. Matsumoto, and R. Mihalcea, Eds.\nPortland,\nOregon, USA: Association for Computational Linguistics, Jun. 2011, pp.\n48\u201352. [Online]. Available: https://aclanthology.org/P11-2009\n[39] Y. Yang and A. Katiyar, \u2018\u2018Simple and effective few-shot named entity\nrecognition with structured nearest neighbor learning,\u2019\u2019 in Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), B. Webber, T. Cohn, Y. He, and Y. Liu, Eds.\nOnline: Association for Computational Linguistics, Nov. 2020, pp. 6365\u2013\n6375. [Online]. Available: https://aclanthology.org/2020.emnlp-main.516\n[40] H.\nOuchi,\nJ.\nSuzuki,\nS.\nKobayashi,\nS.\nYokoi,\nT.\nKuribayashi,\nM.\nYoshikawa,\nand\nK.\nInui,\n\u2018\u2018Instance-based\nneural\ndependency\nparsing,\u2019\u2019\nTransactions\nof\nthe\nAssociation\nfor\nComputational\nVOLUME 11, 2023\n11\n\nMakino et al.: End-to-End Trainable Retrieval-Augmented Generation for Relation Extraction\nLinguistics,\nvol.\n9,\npp.\n1493\u20131507,\n2021.\n[Online].\nAvailable:\nhttps://aclanthology.org/2021.tacl-1.89\n[41] I. Mani and I. Zhang, \u2018\u2018knn approach to unbalanced data distributions: a\ncase study involving information extraction,\u2019\u2019 in Proceedings of workshop\non learning from imbalanced datasets, vol. 126, no. 1.\nICML, 2003, pp.\n1\u20137.\n[42] C. Walker and L. D. Consortium, ACE 2005 Multilingual Training\nCorpus, ser. LDC corpora.\nLinguistic Data Consortium, 2005. [Online].\nAvailable: https://books.google.co.jp/books?id=SbjjuQEACAAJ\n[43] I. Hendrickx, S. N. Kim, Z. Kozareva, P. Nakov, D. \u00d3 S\u00e9aghdha, S. Pad\u00f3,\nM. Pennacchiotti, L. Romano, and S. Szpakowicz, \u2018\u2018SemEval-2010 task 8:\nMulti-way classification of semantic relations between pairs of nominals,\u2019\u2019\nin Proceedings of the 5th International Workshop on Semantic Evaluation.\nUppsala, Sweden: Association for Computational Linguistics, Jul. 2010.\n[Online]. Available: https://www.aclweb.org/anthology/S10-1006\n[44] S. K. Sahu, F. Christopoulou, M. Miwa, and S. Ananiadou, \u2018\u2018Inter-\nsentence relation extraction with document-level graph convolutional\nneural network,\u2019\u2019 in Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics.\nFlorence, Italy: Association\nfor Computational Linguistics, Jul. 2019, pp. 4309\u20134316. [Online].\nAvailable: https://www.aclweb.org/anthology/P19-1423\n[45] F. Christopoulou, M. Miwa, and S. Ananiadou, \u2018\u2018Connecting the dots:\nDocument-level neural relation extraction with edge-oriented graphs,\u2019\u2019 in\nProceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP).\nHong Kong, China:\nAssociation for Computational Linguistics, Nov. 2019, pp. 4925\u20134936.\n[Online]. Available: https://www.aclweb.org/anthology/D19-1498\n[46] M. Miwa and Y. Sasaki, \u2018\u2018Modeling joint entity and relation extraction with\ntable representation,\u2019\u2019 in Proceedings of the 2014 Conference on Empirical\nMethods in Natural Language Processing (EMNLP).\nDoha, Qatar:\nAssociation for Computational Linguistics, Oct. 2014, pp. 1858\u20131869.\n[Online]. Available: https://www.aclweb.org/anthology/D14-1200\n[47] D. Zeng, K. Liu, S. Lai, G. Zhou, and J. Zhao, \u2018\u2018Relation classification\nvia convolutional deep neural network,\u2019\u2019 in Proceedings of COLING\n2014, the 25th International Conference on Computational Linguistics:\nTechnical Papers, J. Tsujii and J. Hajic, Eds. Dublin, Ireland: Dublin City\nUniversity and Association for Computational Linguistics, Aug. 2014, pp.\n2335\u20132344. [Online]. Available: https://aclanthology.org/C14-1220\n[48] W. Zhou, K. Huang, T. Ma, and J. Huang, \u2018\u2018Document-level relation\nextraction with adaptive thresholding and localized context pooling,\u2019\u2019 in\nProceedings of the AAAI Conference on Artificial Intelligence, 2021.\n[49] T.-J. Fu, P.-H. Li, and W.-Y. Ma, \u2018\u2018GraphRel: Modeling text as relational\ngraphs for joint entity and relation extraction,\u2019\u2019 in Proceedings of the\n57th Annual Meeting of the Association for Computational Linguistics.\nFlorence, Italy: Association for Computational Linguistics, Jul. 2019,\npp. 1409\u20131418. [Online]. Available: https://www.aclweb.org/anthology/\nP19-1136\n[50] X. Han, H. Zhu, P. Yu, Z. Wang, Y. Yao, Z. Liu, and M. Sun, \u2018\u2018FewRel:\nA large-scale supervised few-shot relation classification dataset with\nstate-of-the-art evaluation,\u2019\u2019 in Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing, E. Riloff, D. Chiang,\nJ. Hockenmaier, and J. Tsujii, Eds.\nBrussels, Belgium: Association\nfor Computational Linguistics, Oct.-Nov. 2018, pp. 4803\u20134809. [Online].\nAvailable: https://aclanthology.org/D18-1514\n[51] Z. Wu, Y. Wang, J. Ye, Z. Wu, J. Feng, J. Xu, and Y. Qiao, \u2018\u2018OpenICL: An\nopen-source framework for in-context learning,\u2019\u2019 in Proceedings of the 61st\nAnnual Meeting of the Association for Computational Linguistics (Volume\n3: System Demonstrations), D. Bollegala, R. Huang, and A. Ritter, Eds.\nToronto, Canada: Association for Computational Linguistics, Jul. 2023, pp.\n489\u2013498. [Online]. Available: https://aclanthology.org/2023.acl-demo.47\n[52] T. Pl\u00f6tz and S. Roth, \u2018\u2018Neural nearest neighbors networks,\u2019\u2019 in Ad-\nvances in Neural Information Processing Systems, S. Bengio, H. Wallach,\nH. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, Eds., vol. 31.\nCurran Associates, Inc., 2018.\n[53] E.\nJang,\nS.\nGu,\nand\nB.\nPoole,\n\u2018\u2018Categorical\nreparameterization\nwith\ngumbel-softmax,\u2019\u2019\nin\nInternational\nConference\non\nLearning\nRepresentations, 2017. [Online]. Available: https://openreview.net/forum?\nid=rkE3y85ee\n[54] P. Verga, E. Strubell, and A. McCallum, \u2018\u2018Simultaneously self-attending\nto all mentions for full-abstract biological relation extraction,\u2019\u2019 in\nProceedings of the 2018 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers).\nNew Orleans, Louisiana:\nAssociation for Computational Linguistics, Jun. 2018, pp. 872\u2013884.\n[Online]. Available: https://www.aclweb.org/anthology/N18-1080\n[55] Y. Zhang, V. Zhong, D. Chen, G. Angeli, and C. D. Manning,\n\u2018\u2018Position-aware attention and supervised data improve slot filling,\u2019\u2019\nin Proceedings of the 2017 Conference on Empirical Methods in\nNatural Language Processing.\nCopenhagen, Denmark: Association for\nComputational Linguistics, Sep. 2017, pp. 35\u201345. [Online]. Available:\nhttps://aclanthology.org/D17-1004\n[56] O. Sainz, O. Lopez de Lacalle, G. Labaka, A. Barrena, and E. Agirre,\n\u2018\u2018Label verbalization and entailment for effective zero and few-shot relation\nextraction,\u2019\u2019 in Proceedings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, M.-F. Moens, X. Huang, L. Specia,\nand S. W.-t. Yih, Eds.\nOnline and Punta Cana, Dominican Republic:\nAssociation for Computational Linguistics, Nov. 2021, pp. 1199\u20131212.\n[Online]. Available: https://aclanthology.org/2021.emnlp-main.92\n[57] J. Zhang, Y. Zhao, M. Saleh, and P. Liu, \u2018\u2018PEGASUS: Pre-training with\nextracted gap-sentences for abstractive summarization,\u2019\u2019 in Proceedings of\nthe 37th International Conference on Machine Learning, ser. Proceedings\nof Machine Learning Research, H. D. III and A. Singh, Eds., vol.\n119.\nPMLR, 13\u201318 Jul 2020, pp. 11 328\u201311 339. [Online]. Available:\nhttps://proceedings.mlr.press/v119/zhang20ae.html\n[58] G. Kim, S. Kim, B. Jeon, J. Park, and J. Kang, \u2018\u2018Tree of clarifications:\nAnswering ambiguous questions with retrieval-augmented large language\nmodels,\u2019\u2019 in Proceedings of the 2023 Conference on Empirical Methods\nin Natural Language Processing, H. Bouamor, J. Pino, and K. Bali, Eds.\nSingapore: Association for Computational Linguistics, Dec. 2023, pp. 996\u2013\n1009. [Online]. Available: https://aclanthology.org/2023.emnlp-main.63\nKOHEI MAKINO received B.E. and M.E. de-\ngrees from Toyota Technological Institute, Aichi,\nNagoya, Japan, where he is currently pursuing a\ndoctor\u2019s degree. His research interests include deep\nlearning, natural language processing, and infor-\nmation extraction.\nMAKOTO MIWA received a Ph.D. degree from\nthe University of Tokyo in 2008. He is currently a\nprofessor at the Toyota Technological Institute and\nan invited researcher at the National Institute of\nAdvanced Industrial Science and Technology. His\nresearch interests include natural language pro-\ncessing, deep learning, and information extraction.\n12\nVOLUME 11, 2023\n\nMakino et al.: End-to-End Trainable Retrieval-Augmented Generation for Relation Extraction\nYUTAKA SASAKI received his B.E., M.Eng.\nand Ph.D. in Engineering from the University of\nTsukuba in 1986, 1988, 2000, respectively. From\n1988 to 2006, he was with the NTT laboratories,\nJapan. During 2004-2006, he was a department\nhead at the Advanced Telecommunication Re-\nsearch Institute International (ATR), Kyoto, Japan.\nIn 2006, he joined the School of Computer Sci-\nence, the University of Manchester and the Na-\ntional Center for Text Mining (NaCTeM) in UK. In\n2009, he moved to the Toyota Technological Institute, Nagoya, Japan since\nthen he is a Professor at the Department of Advanced Science and Tech-\nnology. He is also an adjoint professor at the Toyota Technological Institute\nat Chicago. His recent research interests include machine learning, natural\nlanguage processing, deep state-space models, and biomedical/materials\ninformatics.\nVOLUME 11, 2023\n13\n"}