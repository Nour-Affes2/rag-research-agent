{"metadata": {"pdf_filename": "2505.07917v1__Efficient and Reproducible Biomedical Question Answering using Retrieval Augment.pdf", "source": "arXiv"}, "text": "Efficient and Reproducible Biomedical Question\nAnswering using Retrieval Augmented Generation\nLinus Stuhlmann\u2217, Michael Alexander Saxer\u2217, Jonathan F\u00a8urst\nSchool of Engineering, Zurich University of Applied Sciences, Winterthur, Switzerland\nE-mails: {stuhllin, saxermi1}@students.zhaw.ch, jonathan.fuerst@zhaw.ch\n\u2217Equal contribution.\nAbstract\u2014Biomedical question-answering (QA) systems re-\nquire effective retrieval and generation components to ensure\naccuracy, efficiency, and scalability. This study systematically\nexamines a Retrieval-Augmented Generation (RAG) system for\nbiomedical QA, evaluating retrieval strategies and response time\ntrade-offs. We first assess state-of-the-art retrieval methods,\nincluding BM25, BioBERT, MedCPT, and a hybrid approach,\nalongside common data stores such as Elasticsearch, MongoDB,\nand FAISS, on a \u224810% subset of PubMed (2.4M documents)\nto measure indexing efficiency, retrieval latency, and retriever\nperformance in the end-to-end RAG system. Based on these\ninsights, we deploy the final RAG system on the full 24M\nPubMed corpus, comparing different retrievers\u2019 impact on\noverall performance. Evaluations of the retrieval depth show\nthat retrieving 50 documents with BM25 before reranking with\nMedCPT optimally balances accuracy (0.90), recall (0.90), and\nresponse time (1.91s). BM25 retrieval time remains stable (82ms \u00b1\n37ms), while MedCPT incurs the main computational cost. These\nresults highlight previously not well-known trade-offs in retrieval\ndepth, efficiency, and scalability for biomedical QA. With open-\nsource code, the system is fully reproducible and extensible.\nIndex Terms\u2014Biomedical Information Retrieval, Retrieval-\nAugmented Generation, Hybrid Retrieval, Large Language Mod-\nels, PubMed, Information Retrieval Systems.\nI. INTRODUCTION\nLarge Language Models (LLMs) have demonstrated strong\nbiomedical question-answering (QA) capabilities [1]. How-\never, LLMs can produce factual inaccuracies, lack specific\ndomain knowledge, and lack verifiability [2]. A major con-\ncern is hallucination, where LLMs generate factually in-\ncorrect responses due to their probabilistic nature. These\nhallucinations, together with a lack of verifiability, are par-\nticularly problematic in healthcare, where misinformation\ncan lead to serious consequences. To mitigate these risks,\nRetrieval-Augmented Generation (RAG) systems leverage ex-\nternal knowledge sources at inference time by selecting rel-\nevant documents from a data store to enhance accuracy,\ntransparency, and traceability [3].\nDespite the potential of biomedical RAG systems, exist-\ning solutions often suffer from limited scalability, poor re-\nproducibility, and suboptimal retrieval performance on large\ndatasets such as PubMed1. Existing benchmarks for medical\nquestion answering, such as MedExpQA [4] and MIRAGE [5],\nlack reproducibility and scalable retrieval solutions. Most\n1https://pubmed.ncbi.nlm.nih.gov/\nretrieval methods rely on either sparse bag-of-words vec-\ntors such as BM25 [6] or dense vectors created through\ntransformer-based models such as BioBERT [7] and Med-\nCPT [8]. However, hybrid approaches that integrate both\ntechniques remain under-investigated, especially from a\nsystem perspective: the inherent trade-offs between retrieval\nstrategies, their indexing and response times, and the resulting\ngenerator accuracy are crucial for practical RAG applications\nand have been largely unexplored. Hybrid retrieval methods\ncombine the strengths of sparse and dense retrieval: a proba-\nbilistic retriever (e.g., BM25) efficiently reduces the search\nspace by filtering a large corpus, while a neural reranker\n(e.g., MedCPT\u2019s cross-encoder) refines document rankings\nbased on semantic relevance. This two-step approach balances\ncomputational efficiency and retrieval precision, mitigating the\nlimitations of stand-alone methods. Although hybrid retrieval\nhas been explored in general NLP tasks [9], its application\nin large-scale biomedical QA remains limited, particularly in\nreal-world implementations. Developing an effective biomed-\nical QA system requires addressing several challenges: (i)\nEfficient Retrieval at Scale: Processing millions of biomedi-\ncal documents under reasonable indexing times while main-\ntaining low-latency retrieval; (ii) Relevance Optimization:\nImproving document ranking by integrating lexical retrieval\nwith neural reranking; (iii) Context Integration: Structuring\nretrieved documents effectively to generate factually accurate\nand verifiable responses. This work presents a scalable and\nreproducible RAG system for biomedical QA, systematically\nevaluating hybrid retrieval strategies. The key contributions\ninclude:\n\u2022 Hybrid\nRetrieval\nApproach: A two-stage retrieval\npipeline that integrates BM25 (lexical retrieval) with\nMedCPT\u2019s cross-encoder (semantic reranking), improv-\ning recall and precision.\n\u2022 Scalability and Performance Analysis: Comparative\nevaluation of three common methods and systems,\nMongoDB, Elasticsearch and FAISS, for large-scale doc-\nument retrieval efficiency.\n\u2022 Reproducibility and Transparency: Explicit citation\nof retrieved documents using PubMed IDs to ensure\ntraceability in biomedical QA.\narXiv:2505.07917v1  [cs.IR]  12 May 2025\n\nThis work advances scalable and reproducible biomedical\nQA systems, enhancing their real-world applicability in clini-\ncal and research environments. All our code is open-sourced.2\nII. BIOMEDICAL QUESTION-ANSWERING WITH RAG\nRetriever\nLLM\nUser\nQuestion\nIndexer\nVector Store / \nInverted \nIndex\nBiomedical \nDocuments\nOffline Phase\nOnline Phase\n\u201cYes. It is generally known that \nthe presence of a CpG island \naround the TSS is related to the \nexpression pattern of the gene\u2026\nDo CpG\nislands colocalise \nwith transcription \nstart sites\nGenerated Answer\nrelevant\ndocs\n2\n1\n3\n1 Indexing and Query Time\n2 Document Relevancy\n3 Answer Correctness\nFig. 1: Biomedical Question Answering with Retrieval-\nAugmented Generation (RAG). Offline phase: biomedical doc-\numents are processed, indexed, and stored in a vector store.\nOnline phase: users ask questions, for which a retriever re-\ntrieves relevant documents that are appended with the question\nand fed to an LLM. Based on the question and context, the\nLLM generates an answer the PubMed IDs it used.\nRetrieval-Augmented Generation enhances the capabilities\nof LLMs by incorporating external data and grounding\nresponses in verifiable and up-to-date information. This en-\nsures that outputs incorporate relevant biomedical knowledge\nfrom sources such as PubMed [10], improving accuracy and\ntransparency. Figure 1 illustrates our biomedical QA system\nbased on RAG, which consists of two main phases: an offline\nphase for indexing biomedical literature and an online phase\nfor retrieving relevant documents and generating responses.\nIn the offline phase, biomedical documents are preprocessed\nand indexed into a vector store (dense vectors) and/or an\ninverted index (sparse vectors) for efficient retrieval. The\nchoices directly affect retrieval speed and system scalability.\nDuring the online phase, users submit biomedical queries,\nwhich the retriever processes to fetch relevant documents.\nThese retrieved documents, along with the query, are provided\nas context to an LLM, ensuring responses remain grounded in\nauthoritative biomedical sources. The used sources are cited\n(PMIDs) and provided as references to users. In this setting,\nsystem performance should be evaluated based on three key\naspects, as highlighted in Figure 1: (1) indexing and query\ntime, which measures retrieval efficiency; (2) relevance of\nretrieved documents, ensuring the most informative sources are\nselected; and (3) answer correctness, verifying that the LLM-\ngenerated response aligns with biomedical evidence. Evaluat-\ning and optimizing these factors improves the reliability and\ntransparency of biomedical QA systems.\nIII. EXPERIMENTAL EVALUATION\nWe evaluate the efficiency and effectiveness of different\nretrieval and text-generation methods for biomedical question-\nanswering (QA). Our experiments focus on selecting optimal\n2https://github.com/slinusc/medical RAG system\ncomponents for document retrieval, text generation, and over-\nall system performance following the three key aspects from\nSection II and using a common biomedical QA benchmark.\nA. Experimental Setup\nDatasets. We evaluate our biomedical RAG system on the\nBIOASQ [11] QA benchmark, which builds on the PubMed\ndatabase [5]. Specifically, we use a 10% randomly sampled\nsubset of 2.4M biomedical papers for the component analysis,\nwhile for the final system, we use the entire dataset of\n24M. Each entry includes a PubMed ID (PMID), title, and\nabstract, with an average abstract length of 296 tokens. We\nevaluate our system using the Task-B dataset, which contains\nexpert-annotated questions paired with supporting PubMed\nIDs (PMIDs). To ensure answerability within our PubMed\nsubset, we first exclude factoid and list questions, which\noften require full-text access, making evaluation less precise.\nSecond, we retain only questions with at least one PMID in\nour dataset to ensure they can be answered using our subset.\nIndexing and Retrieval Systems. We compare three stor-\nage and query systems: Elasticsearch, FAISS, and MongoDB.\nMongoDB3, a NoSQL document database, supports full-\ntext search with TF-IDF-like scoring in its self-hosted version.\nWhile BM25 ranking is available in MongoDB Atlas Search,\nit is a cloud-only service and was not used.\nElasticsearch4, built on Apache Lucene, uses BM25 ranking\nand inverted indexing for efficient text-based retrieval.\nFAISS (Facebook AI Similarity Search) optimizes dense\nvector similarity search, commonly used in NLP and recom-\nmendation systems [12]. We deployed FAISS using a Flask-\nbased server with a FlatL2 index for exhaustive search.\nMetrics: We evaluate indexing speed and response time on\n2.4M PubMed papers to determine the best trade-off between\nefficiency and retrieval performance.\nRetrieval Methods. Based on recall and precision, we\nevaluate four retrieval methods\u2014BM25, BioBERT, MedCPT,\nand a hybrid approach (BM25 + MedCPT).\nBM25 [13] is a ranking algorithm that improves upon TF-\nIDF [14] by incorporating term frequency, document length\nnormalization, and inverse document frequency. It ranks doc-\numents based on query relevance using a probabilistic scor-\ning function. We implemented BM25 in Elasticsearch, with\nstopword removal for improved efficiency. BioBERT [7] is\na domain-specific adaptation of BERT [15], pre-trained on\nPubMed abstracts and PMC articles to enhance biomedical text\nunderstanding. We use BioBERT to encode PubMed abstracts\ninto semantic vectors via FAISS, computing document-query\nsimilarity with squared Euclidean distance. MedCPT [8] is a\ncontrastive learning-based retriever trained on 255M PubMed\nquery-article interactions. It consists of a query encoder,\ndocument encoder, and a cross-encoder reranker. The cross-\nencoder refines retrieval results by reranking top candidates\nbased on query-document contextual interactions. We use\n3https://www.mongodb.com/\n4https://www.elastic.co/elasticsearch\n\nPrompt Template for Medical QA\nSystem Prompt: You are a scientific medical assistant designed to synthe-\nsize responses from specific medical documents. Only use the information\nprovided in the documents to answer questions. The first documents\nshould be the most relevant. Do not use any other information except\nfor the documents provided. When answering questions, always format\nyour response as a JSON object with fields for \u2019response\u2019, \u2019used PMIDs\u2019.\nCite all PMIDs your response is based on in the \u2019used PMIDs\u2019 field.\nPlease think step-by-step before answering questions and provide the most\naccurate response possible. Provide your answer to the question in the\n\u2019response\u2019 field.\nUser Prompt: Answer the following question: ...\nContext Prompt: Here are the documents:\n\"doc1\": {\n\"PMID\": {...},\n\"title\": {...},\n\"content\": {...}\n\"relevance_score\": {...}\n}, ...\nFig. 2: Prompting approach for biomedical QA.\nMedCPT to encode 2.4M abstracts. We filter results based\non positive relevance scores. The Hybrid Retriever integrates\nBM25 and MedCPT for enhanced retrieval performance.\nBM25 first ranks a broad set of documents in Elasticsearch,\nafter which MedCPT\u2019s cross-encoder reranks the top-k results.\nThis combination leverages BM25\u2019s efficiency and MedCPT\u2019s\nsemantic understanding to improve recall and precision.\nMetrics: We assess how well each method retrieves rele-\nvant documents. Since recall is critical for ensuring compre-\nhensive retrieval, we prioritize methods that maximize relevant\ndocument retrieval while maintaining high precision.\nText Generation. For text generation, we experiment with\ndifferent prompting strategies for OpenAI\u2019s GPT-3.5-turbo\n(API version May 2024, temperature=0), ensuring that gen-\nerated responses are accurate and contextually relevant. Given\nthe biomedical domain\u2019s strict accuracy requirements, we fo-\ncus on structured prompts that enhance factual consistency. We\nexperimented with multiple prompting approaches, following\nbest practices in medical NLP [5], [16]. Due to resource con-\nstraints, we evaluated GPT-3.5, with limited testing of GPT-\n4. Observations showed no significant differences in output\nquality. As illustrated in Figure 2, our final prompt consists\nof three components: (1) a system prompt with task-specific\ninstructions, (2) a user query, and (3) retrieved documents with\nPubMed IDs (PMIDs), titles and content.\nMetrics: For text generation, we evaluate answer correct-\nness in terms of accuracy, recall, precision, and F1 score.\nB. Indexing and Query Time\nTable I summarizes the performance of Elasticsearch,\nFAISS, and MongoDB. Elasticsearch excels in full-text re-\ntrieval but is less efficient for semantic vector search, which\nFAISS optimizes for. However, due to their complex data\nmanagement and indexing mechanisms, MongoDB and Elas-\nticsearch exhibit the slowest indexing speeds.\nMongoDB, while providing a flexible NoSQL document\nstorage solution, uses TF-IDF-based text ranking in its self-\nhosted version, which leads to significantly slower query\nresponse times compared to Elasticsearch and FAISS. The self-\nhosted MongoDB lacks efficient semantic retrieval, limiting its\neffectiveness in large-scale biomedical QA.\nBased on these results, we selected Elasticsearch for full-\ntext retrieval and FAISS for semantic vector search. Despite\nits slower indexing speed, Elasticsearch provides a robust text-\nbased search framework, while FAISS offers superior response\ntimes for vector-based queries.\nTABLE I: Performance metrics for different search methods.\nMethod\nType\nIndex\nResponse Time\nIndexing Speed\nMongoDB\nSparse\nTF-IDF\n26.4 s \u00b1 1.72 s\n10.41 min\nElasticsearch\nSparse\nBM25\n82 ms \u00b1 37 ms\n156 min\nElasticsearch\nDense\nKNN\n24.6 s \u00b1 1.23 s\n171 min\nFAISS\nDense\nL2 Distance\n657 ms \u00b1 127 ms\n41 min\nC. Document Relevancy\nTable II summarizes the retrievers\u2019 performance. The Hy-\nbrid Retriever achieved the highest recall (0.567), balancing\nefficiency and accuracy. BM25 exhibited strong precision\nbut lower recall. MedCPT improved semantic retrieval but\nunderperformed in recall, while BioBERT had the weakest\nresults due to a lack of fine-tuning for question-answering\ntasks. Note that a low recall score does not necessarily indicate\nincorrect retrieval; rather, it means that the retrieved documents\nmay not be included in the BioASQ-curated set.\nTABLE II: Performance comparison of different retrievers.\nRetriever\nVector Type\nRecall\nPrecision\nHybrid Retriever\nHybrid\n0.567\n0.319\nBM25\nSparse\n0.537\n0.322\nMedCPT\nDense\n0.273\n0.205\nBioBERT\nDense\n0.07\n0.07\nD. Answer Correctness of the RAG System (End-to-End)\nFor BM25, the query is processed using term-based re-\ntrieval, ranking documents based on query term occurrence.\nThe top k ranked documents are embedded into the LLM\ncontext for response generation. For MedCPT, the query\nis encoded into a vector and compared against document\nembeddings for similarity search. The retrieved documents\nare reranked by a cross-encoder, and only those with positive\nrelevance scores are used for response generation. For Hybrid\nRetrieval, BM25 first retrieves k candidate documents, which\nare then reranked by MedCPT\u2019s cross-encoder. Only relevant\ndocuments are passed to the LLM. Our results show that the\nhybrid retriever achieves the best answer correctness on all\nmetrics (Table III).\nTABLE III: Performance metrics of the end-to-end RAG\nsystem using different retrievers.\nRAG with Retriever\nAccuracy\nRecall\nPrecision\nF1 Score\nGPT-3.5 / Hybrid Retriever\n0.86\n0.86\n0.89\n0.86\nGPT-3.5 / MedCPT\n0.83\n0.83\n0.86\n0.84\nGPT-3.5 / BM25\n0.72\n0.72\n0.83\n0.74\nGPT-3.5 / BioBERT\n0.63\n0.63\n0.85\n0.67\nIV. EVALUATION OF THE FINAL SYSTEM\nAfter selecting the most efficient and effective compo-\nnents for our RAG system, we evaluate the final Retrieval-\nAugmented Generation (RAG) system with the hybrid retrieval\n\napproach on the full 24M document PubMed corpus. for\nretrieval effectiveness, response time, and answer correctness.\nWe use a Linux server with an Intel Broadwell processor (16\ncores, 30GB RAM) and an NVIDIA A30 GPU (24GB VRAM)\nfor cross-encoder reranking.\nA. Effect of Retrieval Depth on Performance\nTo evaluate the impact of retrieval depth on performance, we\nexperimented with different configurations of BM25 retrieval,\nvarying the number of initially retrieved documents while\nkeeping the reranking step fixed at the top 10 (Table IV).\nTABLE IV: Comparison for different retrieval depths (BM25),\nwith reranking applied to the top 10 documents.\nDocs\nAccuracy\nRecall\nPrecision\nF1 Score\nRetrieval Time (s)\nTotal Time (s)\n20\n0.89\n0.88\n0.89\n0.88\n0.39 \u00b1 0.07\n1.52 \u00b1 0.42\n50\n0.90\n0.90\n0.89\n0.90\n0.82 \u00b1 0.13\n1.91 \u00b1 0.36\n100\n0.87\n0.87\n0.88\n0.87\n1.54 \u00b1 0.16\n2.62 \u00b1 0.44\nB. Analysis of Retrieval Depth Trade-offs\nElasticsearch BM25 retrieval has an average response time\nof 82 \u00b1 37ms, which remains constant across all retrieval\ndepths since it ranks all documents regardless of how many\nare later passed to reranking. The primary factor affecting\nresponse time is the cross-encoder reranking step using Med-\nCPT, which processes a subset of the retrieved documents and\nincurs additional computational overhead.\nIncreasing the number of retrieved documents leads to\nmarginal accuracy improvements but significantly increases\nthe rerank time. Retrieving 50 documents before reranking\nyields the best accuracy (0.90) and F1 score (0.90) while\nkeeping response time manageable at 1.91 seconds. However,\nretrieving 100 documents leads to a drop in accuracy (0.87)\nand an increase in total response time to 2.62 seconds,\nsuggesting diminishing returns beyond 50 documents.\nThe text generation phase relies on the OpenAI API, which\nintroduces additional latency. The mean response time for\ngeneration is 1.07 seconds, with a standard deviation of 0.41\nseconds. Since the generation time remains stable across con-\nfigurations, the overall system latency is primarily determined\nby the retrieval depth and reranking time.\nThese results demonstrate that increasing the number of\nretrieved documents beyond a certain threshold does not\nnecessarily improve system performance. Instead, balancing\nretrieval depth with reranking efficiency is critical for real-\nworld biomedical question-answering applications.\nV. CONCLUSION AND FUTURE DIRECTIONS\nBiomedical question-answering (QA) systems require both\nefficient retrieval and generation components for accuracy and\nscalability. This study examines a Retrieval-Augmented Gen-\neration (RAG) system for biomedical QA, evaluating retrieval\nstrategies and response time trade-offs.\nWe assess retrieval methods, including BM25, BioBERT,\nMedCPT, and a hybrid approach, alongside data stores such\nas Elasticsearch, MongoDB, and FAISS. Despite strong per-\nformance, some limitations remain. The reliance on OpenAI\u2019s\nGPT-3.5 for text generation poses reproducibility challenges\ndue to model updates and API latency. Additionally, retriever\nand database system evaluations remain limited, requiring\nbroader comparisons.\nFuture work should explore additional retrievers and evalu-\nate alternative databases for indexing efficiency. Efforts should\nalso focus on retrieval optimization, integrating open-source\nLLMs, and enabling real-time biomedical applications. Our\nwork highlights trade-offs in retrieval depth, efficiency, and\nscalability. The system is fully reproducible and extensible,\nsupporting future advancements in retrieval and model inte-\ngration for research and clinical applications.\nREFERENCES\n[1] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou,\nK. Clark, S. Pfohl, H. Cole-Lewis, D. Neal, M. Schaekermann,\nA. Wang, M. Amin, S. Lachgar, P. Mansfield, S. Prakash, B. Green,\nE. Dominowska, B. A. y Arcas, N. Tomasev, Y. Liu, R. Wong,\nC. Semturs, S. S. Mahdavi, J. Barral, D. Webster, G. S. Corrado,\nY. Matias, S. Azizi, A. Karthikesalingam, and V. Natarajan, \u201cTowards\nexpert-level medical question answering with large language models,\u201d\n2023. [Online]. Available: https://arxiv.org/abs/2305.09617\n[2] A. Asai, Z. Zhong, D. Chen, P. W. Koh, L. Zettlemoyer, H. Hajishirzi,\nand W.-t. Yih, \u201cReliable, adaptable, and attributable language models\nwith retrieval,\u201d arXiv preprint arXiv:2403.03187, 2024.\n[3] S. M. T. I. Tonmoy, S. M. M. Zaman, V. Jain, A. Rani, V. Rawte,\nA. Chadha, and A. Das, \u201cA comprehensive survey of hallucination\nmitigation techniques in large language models,\u201d 2024.\n[4] I. Alonso, M. Oronoz, and R. Agerri, \u201cMedexpqa: Multilingual bench-\nmarking of large language models for medical question answering,\u201d\n2024.\n[5] G. Xiong, Q. Jin, Z. Lu, and A. Zhang, \u201cBenchmarking retrieval-\naugmented generation for medicine,\u201d arXiv preprint arXiv:2402.13178,\n2024.\n[6] S. Robertson, H. Zaragoza et al., \u201cThe probabilistic relevance frame-\nwork: Bm25 and beyond,\u201d Foundations and Trends\u00ae in Information\nRetrieval, vol. 3, no. 4, pp. 333\u2013389, 2009.\n[7] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang,\n\u201cBiobert: a pre-trained biomedical language representation model for\nbiomedical text mining,\u201d Bioinformatics, vol. 36, no. 4, p. 1234\u20131240,\nSep. 2019. [Online]. Available: http://dx.doi.org/10.1093/bioinformatics/\nbtz682\n[8] Q. Jin, W. Kim, Q. Chen, D. C. Comeau, L. Yeganova, W. J. Wilbur, and\nZ. Lu, \u201cMedcpt: Contrastive pre-trained transformers with large-scale\npubmed search logs for zero-shot biomedical information retrieval,\u201d\nBioinformatics, vol. 39, no. 11, p. btad651, Nov 2023.\n[9] X. Ma, Z. Yang, and H. Zhang, \u201cA hybrid first-stage retrieval model\nfor biomedical literature,\u201d in Proceedings of the CEUR Workshop\non\nBiomedical\nInformation\nRetrieval,\n2020.\n[Online].\nAvailable:\nhttps://ceur-ws.org/Vol-2696/paper 92.pdf\n[10] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal,\nH. K\u00a8uttler, M. Lewis, W.-t. Yih, T. Rockt\u00a8aschel et al., \u201cRetrieval-\naugmented generation for knowledge-intensive nlp tasks,\u201d Advances in\nNeural Information Processing Systems, vol. 33, pp. 9459\u20139474, 2020.\n[11] A. Krithara, A. Nentidis, K. Bougiatiotis, and G. Paliouras, \u201cBioasq-\nqa: A manually curated corpus for biomedical question answering,\u201d\nScientific Data, vol. 10, no. 1, p. 170, 2023.\n[12] M. Douze, A. Guzhva, C. Deng, J. Johnson, G. Szilvasy, P.-E. Mazar\u00b4e,\nM. Lomeli, L. Hosseini, and H. J\u00b4egou, \u201cThe faiss library,\u201d 2024.\n[13] S. E. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and\nM. Gatford, \u201cOkapi at trec-3,\u201d NIST Special Publication, vol. 500-225,\npp. 109\u2013126, 1994.\n[14] K. Sp\u00a8arck Jones, \u201cA statistical interpretation of term specificity and its\napplication in retrieval,\u201d Journal of Documentation, vol. 28, no. 1, pp.\n11\u201321, 1972.\n[15] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training\nof deep bidirectional transformers for language understanding,\u201d 2019.\n[16] B. Mesk\u00b4o, \u201cPrompt engineering as an important emerging skill for\nmedical professionals: Tutorial,\u201d J Med Internet Res, vol. 25, p. e50638,\n2023.\n"}