{"metadata": {"pdf_filename": "2506.00054v1__Retrieval-Augmented Generation A Comprehensive Survey of Architectures Enhanceme.pdf", "source": "arXiv"}, "text": "arXiv:2506.00054v1  [cs.IR]  28 May 2025\nThis is a preprint under review at ACM TOIS. Do not redistribute the final version without permission.\nRetrieval-Augmented Generation: A Comprehensive Survey of Architectures,\nEnhancements, and Robustness Frontiers\nCHAITANYA SHARMA, Independent Researcher, United States\nRetrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to enhance large language models (LLMs) by conditioning\ngeneration on external evidence retrieved at inference time. While RAG addresses critical limitations of parametric knowledge\nstorage\u2014such as factual inconsistency and domain inflexibility\u2014it introduces new challenges in retrieval quality, grounding fidelity,\npipeline efficiency, and robustness against noisy or adversarial inputs. This survey provides a comprehensive synthesis of recent\nadvances in RAG systems, offering a taxonomy that categorizes architectures into retriever-centric, generator-centric, hybrid, and\nrobustness-oriented designs. We systematically analyze enhancements across retrieval optimization, context filtering, decoding control,\nand efficiency improvements, supported by comparative performance analyses on short-form and multi-hop question answering tasks.\nFurthermore, we review state-of-the-art evaluation frameworks and benchmarks, highlighting trends in retrieval-aware evaluation,\nrobustness testing, and federated retrieval settings. Our analysis reveals recurring trade-offs between retrieval precision and generation\nflexibility, efficiency and faithfulness, and modularity and coordination. We conclude by identifying open challenges and future\nresearch directions, including adaptive retrieval architectures, real-time retrieval integration, structured reasoning over multi-hop\nevidence, and privacy-preserving retrieval mechanisms. This survey aims to consolidate current knowledge in RAG research and serve\nas a foundation for the next generation of retrieval-augmented language modeling systems.\nCCS Concepts: \u2022 Information systems \u2192Retrieval models and ranking; Evaluation of retrieval results; Information retrieval\nquery processing; Retrieval tasks and goals; Document representation.\nAdditional Key Words and Phrases: Retrieval-Augmented Generation, Query Reformulation, Context Filtering, Reranking, Multi-hop\nReasoning, Hallucination Mitigation, Robustness, Dynamic Retrieval, Evaluation Benchmarks, Federated Retrieval, Faithfulness,\nEfficiency Optimization, Document Ranking, LLM Alignment, Open-Domain QA\n1\nIntroduction\nLarge Language Models (LLMs) have demonstrated impressive generalization across natural language tasks, but\ntheir reliance on static, parametric knowledge remains a fundamental limitation. This restricts their ability to handle\nqueries requiring up-to-date, verifiable, or domain-specific information, often resulting in hallucinations or factual\ninconsistencies [19, 40].\nRetrieval-Augmented Generation (RAG) addresses this issue by coupling pretrained language models with non-\nparametric retrieval modules that fetch external evidence during inference. By conditioning generation on retrieved\ndocuments, RAG systems offer greater transparency, factual grounding, and adaptability to evolving knowledge bases.\nThese properties have made RAG central to tasks such as open-domain QA, biomedical reasoning, knowledge-grounded\ndialogue, and long-context summarization.\nHowever, integrating retrieval with generation introduces unique challenges: retrieval noise and redundancy can\ndegrade output quality; misalignment between retrieved evidence and generated text can lead to hallucinations;\nand pipeline inefficiencies and latency make deployment costly at scale. Moreover, balancing modularity with tight\nretrieval\u2013generation interaction remains an open architectural trade-off.\nIn this survey, we first present a high-level taxonomy of RAG architectures based on where core innovations\noccur\u2014within the retriever, the generator, or through their joint coordination (Section 3). We begin with a background\nAuthor\u2019s Contact Information: Chaitanya Sharma, Independent Researcher, United States.\nManuscript submitted to ACM\n1\n\nRetrieval-Augmented Generation: A Survey\n2\nFig. 1. Retrieval-Augmented Generation (RAG) workflow. A user query is processed by the retriever, which may perform\nquery expansion before retrieving documents from external knowledge sources (e.g., databases, APIs, or document stores). Retrieved\ndocuments are re-ranked by relevance, and the Top-K are passed to the generator as factual context. The generator synthesizes\na response conditioned on both the query and retrieved content. An optional post-processing step (e.g., ranking, rewriting, or\nfact-checking) may further refine the output, enhancing factual consistency, real-time adaptability, and overall response quality in\nlarge language models (LLMs).\non RAG\u2019s mathematical formulation and components (Section 2.2), and then explore advances in retrieval strategies,\nfiltering, and control mechanisms (Section 4). We further analyze how RAG systems are benchmarked (Section 6),\ncompare prominent frameworks (Section 5), and conclude with open research challenges and future directions (Section 7).\n2\nBackground and foundations of retrieval-augmented generation\nRetrieval-Augmented Generation (RAG) is a framework that augments large language models (LLMs) with external\nknowledge access via document retrieval. It builds on the intuition that generating grounded and verifiable responses\nrequires not only parametric knowledge stored in model weights, but also non-parametric access to a dynamic evidence\ncorpus. This section outlines the core components of RAG systems and presents the mathematical formulation that\nunderpins their design.\n2.1\nComponents of a RAG System\nAt a high level, a RAG system consists of three modules:\nManuscript submitted to ACM\n\nRetrieval-Augmented Generation: A Survey\n3\nQuery Encoder: Encodes the input \ud835\udc65into a query representation \ud835\udc5e, which is used to retrieve relevant documents.\nThis can be either a neural encoder or a rule-based template.\nRetriever: Given the query \ud835\udc5e, the retriever fetches a ranked list of documents \ud835\udc511,\ud835\udc512, . . . ,\ud835\udc51\ud835\udc58from a corpus C.\nRetrievers may be sparse (e.g., BM25 [54]), dense (e.g., DPR [36]), hybrid, or generative.\nGenerator: The generator conditions on the input \ud835\udc65and the retrieved documents \ud835\udc51\ud835\udc56to produce the final output \ud835\udc66.\nThis is typically a pretrained transformer model (e.g., T5 [51], BART [39], GPT [5]).\n2.2\nMathematical Formulation\nFormally, the generation process in Retrieval-Augmented Generation (RAG) can be expressed as modeling the conditional\ndistribution:\n\ud835\udc43(\ud835\udc66| \ud835\udc65) =\n\u2211\ufe01\n\ud835\udc51\u2208C\n\ud835\udc43(\ud835\udc66| \ud835\udc65,\ud835\udc51) \u00b7 \ud835\udc43(\ud835\udc51| \ud835\udc65)\n(1)\nwhere:\n\u2022 \ud835\udc65is the input (e.g., a question or prompt),\n\u2022 \ud835\udc51is a retrieved document from corpus C,\n\u2022 \ud835\udc66is the generated response.\nIn practice, the summation is approximated by retrieving the top-\ud835\udc58documents \ud835\udc511, . . . ,\ud835\udc51\ud835\udc58, yielding:\n\ud835\udc43(\ud835\udc66| \ud835\udc65) \u2248\n\ud835\udc58\n\u2211\ufe01\n\ud835\udc56=1\n\ud835\udc43(\ud835\udc66| \ud835\udc65,\ud835\udc51\ud835\udc56) \u00b7 \ud835\udc43(\ud835\udc51\ud835\udc56| \ud835\udc65)\n(2)\nThis decomposition reflects two key probabilities:\n\u2022 \ud835\udc43(\ud835\udc51\ud835\udc56| \ud835\udc65): the relevance score of document \ud835\udc51\ud835\udc56given the input \ud835\udc65, often derived from a retriever or reranker.\n\u2022 \ud835\udc43(\ud835\udc66| \ud835\udc65,\ud835\udc51\ud835\udc56): the probability of generating output \ud835\udc66conditioned on \ud835\udc65and document \ud835\udc51\ud835\udc56, modeled by the language\nmodel.\nVariants of RAG differ in how they estimate and combine these components. Some use a fixed retriever and let the\ngenerator handle noisy inputs, while others jointly optimize retrieval and generation to maximize downstream utility.\n3\nTaxonomy of RAG Architectures\nTo contextualize recent advances in Retrieval-Augmented Generation (RAG), we propose a taxonomy that categorizes\nexisting systems based on their architectural focus\u2014retriever-centric, generator-centric, hybrid, and robustness-oriented\ndesigns. This classification highlights key design patterns and illustrates how different frameworks tackle the core\nchallenges of retrieval, grounding, and reliability.\n3.1\nRetriever-Based RAG Systems\nRetriever-based Retrieval-Augmented Generation (RAG) systems delegate architectural responsibility primarily to\nthe retriever, treating the generator as a passive decoder. These systems operate under the premise that the fidelity\nand relevance of the retrieved context are the most critical factors for generating accurate and grounded outputs.\nInnovations in this space typically fall into one of three design patterns: input-side query enhancement, retriever-side\nadaptation, and retrieval granularity optimization.\nManuscript submitted to ACM\n\nRetrieval-Augmented Generation: A Survey\n4\nFig. 2. Figure 2: Taxonomy of Retrieval-Augmented Generation (RAG) Systems. This taxonomy categorizes RAG frameworks\nbased on their primary architectural orientation\u2014retriever-based, generator-based, hybrid, and robustness-focused designs. Retriever-\nbased models are further grouped into query-centric, retriever-centric, and granularity-aware approaches, while generator-based\nmodels include faithfulness-aware decoding, context compression, and retrieval-guided generation. Hybrid systems are organized by\nretrieval dynamics (e.g., multi-round, utility-driven), and robustness-oriented models address challenges such as noise, hallucination,\nand adversarial vulnerabilities. This structure highlights the diverse innovations shaping the RAG landscape.\nQuery-Driven Retrieval: A prominent strategy focuses on refining and structuring user intent before retrieval to\nmaximize alignment with relevant corpus segments. This includes decomposition, rewriting, generative reformulation,\nand the incorporation of structured priors to guide retrieval. Notable examples include RQ-RAG (Refine Query for\nRAG) [6], which decomposes multi-hop queries into latent sub-questions, and GMR (Generative Multi-hop Retrieval) [38],\nwhich uses a generative LLM to autoregressively formulate complex multi-hop queries. RAG-Fusion [50] further\nimproves recall by combining results from multiple reformulated queries through reciprocal rank fusion [13]. Structured\napproaches have also emerged: KRAGEN (Knowledge Retrieval Augmented Generation ENgine) [43] introduces graph-of-\nthoughts prompting to decompose complex queries into subproblems, retrieving relevant subgraphs to guide multi-hop\nreasoning. Additionally, LQR (Layered Query Retrieval) [25] implements hierarchical planning over multi-hop questions,\nwhile Sparse Context Selection [90] emphasizes efficient sparse reformulations for both recall and speed.\nManuscript submitted to ACM\n\nRetrieval-Augmented Generation: A Survey\n5\nRetriever-Centric Adaptation: Another line of work modifies the retriever itself through architectural enhance-\nments or task-specific learning. Re2G (Retrieve, Rerank, Generate) [20] blends symbolic and neural retrieval via reranking\nlayers, while SimRAG (Self-Improving RAG) [73] employs self-training over synthetic QA pairs to improve domain\ngeneralization. Frameworks like RankRAG [83] and uRAG (unified RAG) [57] emphasize retriever versatility\u2014either by\nunifying reranking and generation within a single backbone or by training general-purpose retrievers across varied\ndownstream tasks. Additionally, systems such as ToolRerank [88] dynamically adapt retrieval strategies based on query\nsemantics, optimizing tool selection in specialized retrieval settings. Relatedly, SEER (Self-Aligned Evidence Extraction\nfor RAG) [87] focuses on post-retrieval adaptation, advancing corpus-based evidence extraction by aligning evidence\nselection with faithfulness, helpfulness, and conciseness criteria, thereby improving evidence quality for open-domain\nand temporally sensitive queries.\nGranularity-Aware Retrieval: This pattern addresses retrieval precision by optimizing the unit of retrieval\u2014from\nfull documents to fine-grained, semantically aligned segments. LongRAG [31] exemplifies this by retrieving compressed\nlong-context chunks, constructed through document grouping, to better exploit long-context language models. Similarly,\nFILCO (Filter Context) [69] enhances retrieval granularity by filtering irrelevant or low-utility spans from retrieved\npassages before generation, improving the faithfulness and efficiency of RAG outputs. In parallel, the Sufficient Context\nanalysis framework [34] offers a complementary lens, evaluating whether retrieved contexts contain enough information\nto support accurate generation, thereby highlighting the importance of granular retrieval quality for system robustness.\nEach of these patterns anchors its innovation in the retriever, preserving modularity and interpretability. However,\nthey also introduce trade-offs in latency, redundancy, and sensitivity to ambiguous or underspecified queries. Sec-\ntion 4 elaborates on how downstream enhancements\u2014such as reranking, adaptive filtering, and utility-based context\nselection\u2014further address these limitations.\n3.2\nGenerator-Based RAG Systems\nGenerator-based RAG systems concentrate architectural innovation on the decoding process, assuming the retrieved\ncontent is sufficiently relevant and shifting the burden of factual grounding and integration to the language model.\nThese systems enhance output quality through mechanisms for self-verification, compression, and controlled generation.\nWe identify three recurring design patterns within this category: faithfulness-aware decoding, context compression and\nutility filtering, and retrieval-conditioned generation control.\nFaithfulness-Aware Decoding: To reduce hallucinations and improve factual grounding, several systems embed\nmechanisms for self-reflection, verification, or correction during generation. SELF-RAG (Self-Reflective RAG) [1]\nintroduces a critique\u2013generate loop, allowing the model to assess and revise its outputs before finalization. SelfMem [9]\nbuilds on this by incorporating a self-memory module that enables the model to revisit and refine prior generations.\nINFO-RAG [76] treats the LLM as a denoising module trained with contrastive objectives. Collectively, these systems\ndecouple output faithfulness from retrieval fidelity, enabling recovery even when retrieval is suboptimal.\nContext Compression and Utility Filtering: To address context window limitations, several systems optimize\nretrieval inputs into denser or more structured forms. FiD-Light [24], a streamlined variant of the Fusion-in-Decoder\n(FiD) architecture [27], improves decoding efficiency by compressing encoder outputs across retrieved passages and\npruning cross-passage attention without modifying retrieval mechanisms. xRAG [10] projects document embeddings\ndirectly into the model\u2019s representation space, minimizing token overhead through modality fusion. Rich Answer\nEncoding (RAE) [26] enhances retrieval relevance by embedding answer-aligned semantics into retriever outputs rather\nthan relying on token overlap. GenRT [75] further refines retrieval utility by reranking and dynamically truncating\nManuscript submitted to ACM\n\nRetrieval-Augmented Generation: A Survey\n6\nretrieved lists, retaining only the most contextually valuable candidates for generation. Complementing these designs,\nan information bottleneck-based filtering approach [89] selectively preserves evidence most informative for generation.\nTogether, these strategies advance decoding efficiency and context quality, particularly for long-form and multi-hop\nRAG tasks.\nRetrieval-Guided Generation: A third strategy modulates generation based on retrieval metadata, task-specific\ncues, or agentic decision-making. AU-RAG (Agent-based Universal RAG) [28] exemplifies this by using an agent to\ndecide dynamically between retrieved and parametric knowledge across diverse data environments. RAG-Ex [62]\nperturbs retrieval context to analyze how variability influences model behavior and reliance on external evidence. R2AG\n(Retrieval information into RAG) [82] extends this by recursively reranking candidates during generation, dynamically\nprioritizing evidence based on the evolving answer state. In high-stakes domains, Confidence-Calibrated RAG [48]\nshows that document ordering and prompt structure affect output certainty, highlighting the need for calibration\nalongside factual accuracy.\nThese architectures are particularly suited to domains where factual correctness, reasoning transparency, or structured\noutput formats are essential\u2014such as biomedical QA, finance, and enterprise workflows. While they leave the retriever\nfixed, many of their techniques are complementary to retrieval-side enhancements and can be layered atop other RAG\nvariants. Section 4.2 further explores compression, reranking, and decoding control strategies in these systems.\n3.3\nHybrid RAG Systems\nHybrid RAG systems tightly couple the retriever and generator, moving beyond modular architectures to treat retrieval\nand generation as co-adaptive reasoning agents. These systems emphasize iterative feedback, utility-aware coordination,\nand dynamic control over retrieval actions, particularly in open-domain, multi-hop, and evolving knowledge contexts.\nWe identify three dominant architectural patterns: iterative or multi-round retrieval, utility-driven joint optimization,\nand retrieval-aware generation control.\nIterative or Multi-Round Retrieval: These systems interleave retrieval and generation across multiple reasoning\nsteps, allowing for evidence refinement and progressive answer construction. IM-RAG (Inner Monologue RAG) [80]\nsimulates an \u201cinner monologue\u201d by alternating between generation and retrieval phases, supporting multi-step reasoning.\nGenerate-Then-Ground (GenGround) [59] follows a similar philosophy, generating a provisional answer first and then\nretrieving supporting evidence to substantiate or revise it\u2014improving factuality and interpretability in multi-hop settings.\nG-Retriever [22] retrieves graph-structured subcomponents as generation unfolds, enhancing complex reasoning over\ntextual graphs.\nUtility-Driven Joint Optimization: Several frameworks seek to align retriever outputs with their downstream\nutility for generation through joint objectives or reinforcement learning. Stochastic RAG [84] treats retrieval as an\nexpected utility maximization problem, updating both retriever and generator end-to-end using REINFORCE-based\ngradients. M-RAG [70] applies multi-agent reinforcement learning, coordinating distributed retrievers and generators\nvia shared memory and task-specific roles. MedGraphRAG [72] integrates knowledge graphs into the joint learning loop,\nfacilitating domain-specific reasoning with structured priors. These systems improve factuality and answer consistency,\nparticularly in biomedical and enterprise domains.\nDynamic Retrieval Triggering: A growing class of systems dynamically controls when and how to retrieve, condi-\ntioned on generation uncertainty, task complexity, or intermediate outputs. DRAGIN (Dynamic Retrieval Augmented\nGeneration based on the Information Needs of LLMs) [61] triggers retrieval at the token level using entropy-based\nconfidence signals, while FLARE (Forward-Looking Active REtrieval augmented generation () [32] selectively retrieves\nManuscript submitted to ACM\n\nRetrieval-Augmented Generation: A Survey\n7\nbased on low-confidence predictions during sentence generation. SELF-ROUTE [41] dynamically routes tasks between\nretrieval and generation modules based on model self-assessed difficulty, and AU-RAG [28] leverages agentic decision-\nmaking to mediate between diverse retrieval sources and procedural knowledge. TA-ARE (Time-Aware Adaptive\nREtrieval) [86] introduces a retrieval trigger classifier that adaptively determines when retrieval is necessary and adjusts\nthe granularity of evidence based on query needs. A related approach, CRAG (Corrective RAG) [79], evaluates retrieved\nevidence quality before generation and dynamically decides whether to proceed with generation, re-trigger retrieval, or\ndecompose the input into simpler sub-queries. This corrective mechanism positions CRAG within the hybrid class, as it\ntightly coordinates retrieval assessment with adaptive generation pathways under uncertainty.\nThese architectures reflect a broader trend toward treating retrieval as a controllable, contextualized act rather than\na fixed preprocessing step. Their strength lies in adaptivity, coordination, and the capacity to handle under-specified\nor evolving queries. However, they introduce new challenges in training stability, latency, and system transparency\u2014\nespecially when retrieval is performed mid-decoding. These trade-offs, as well as efficiency-oriented enhancements like\npipelining and reranking, are further explored in Section 4.\n3.4\nRobustness and Security-Oriented RAG Systems\nRobustness- and security-oriented RAG systems are designed to preserve output quality in the face of noisy, irrelevant,\nor adversarially manipulated retrieval contexts. Unlike models that optimize retrieval or generation under ideal\nassumptions, these systems explicitly address worst-case scenarios\u2014such as hallucination under misleading evidence,\nretrieval failures, or corpus poisoning. We identify three major design strategies in this category: noise-adaptive training,\nhallucination-aware decoding constraints, and adversarial robustness.\nNoise-Adaptive Training Objectives: These systems aim to make RAG outputs resilient to degraded or spurious\ninput evidence by training under perturbed, irrelevant, or misleading contexts. RAAT [18] classifies retrieved passages\ninto relevant, irrelevant, or counterfactual categories and introduces an adversarial training objective to maximize worst-\ncase performance. Bottleneck Noise Filtering [89] applies information bottleneck theory to identify the intersection\nof useful and noisy information, compressing retrieved context into minimal, high-utility representations. These\napproaches are particularly effective in retrieval-heavy pipelines where context precision cannot be guaranteed.\nHallucination-Aware Decoding Constraints: To mitigate factual inaccuracies in generation, several systems in-\ntroduce decoding-time constraints or architectures designed to enforce grounding. RAGTruth [46] provides benchmarks\nand evaluation protocols for hallucination detection, guiding system-level design. Structured retrieval-based approaches\nhave also been explored: one method [24] retrieves executable templates (e.g., JSON workflows) to constrain output\ngeneration, minimizing reliance on generative interpolation and reducing domain-specific hallucination. RAG-Ex [62]\nsimulates retrieval variability by injecting perturbed documents during training, improving robustness to inconsistent\nor adversarial context. In high-stakes domains such as healthcare, Confidence-Calibrated RAG [48] explores how\ndocument ordering and prompt design affect both answer accuracy and model certainty.\nAdversarial Robustness and Security: Emerging work also highlights new vulnerabilities. BadRAG [78] and\nTrojanRAG [8] demonstrate that adversarially poisoned passages can serve as semantic backdoors, triggering spe-\ncific behaviors in LLM outputs even when base models remain unmodified. These attacks rely on stealthy corpus\nmanipulations that are hard to detect and pose significant threats in open-domain or API-exposed RAG systems.\nCollectively, these systems complement retrieval- and generation-oriented architectures by offering essential safety\nguarantees in real-world deployments. Their robustness strategies\u2014ranging from retrieval verification and context\ncompression to constrained generation\u2014are modular and often integrable into existing RAG pipelines.\nManuscript submitted to ACM\n\nRetrieval-Augmented Generation: A Survey\n8\n4\nEnhancements in RAG\nRecent advancements in Retrieval-Augmented Generation (RAG) increasingly focus on targeted enhancements across\nthe retrieval\u2013generation pipeline. Beyond architectural baselines, these enhancements address key limitations in\nretrieval quality, context integration, computational efficiency, robustness to perturbations, and ranking precision.\nThis section delineates five core areas of optimization\u2014retrieval, filtering, efficiency, robustness, and reranking\u2014each\ncontributing to the development of more reliable and performant RAG systems, and collectively summarized in Table 1,\nwhich compares representative methods based on their mechanisms, strengths, limitations, and ideal use cases.\n4.1\nRetrieval Enhancement\nRAG systems have increasingly adopted smarter retrieval strategies to mitigate inefficiencies such as redundant lookups,\nirrelevant context, and computational overhead. These improvements can be categorized into four major families:\nadaptive retrieval, multi-source retrieval, query refinement, and hybrid or structured retrieval. Each addresses a distinct\nbottleneck in the retrieval pipeline, offering trade-offs in latency, scalability, and faithfulness.\nAdaptive retrieval dynamically adjusts when to retrieve based on model uncertainty or predictive confidence.\nTA-ARE replaces static thresholds with a learned estimator, reducing redundant retrievals by 14.9% in short-form tasks.\nDRAGIN takes this further by applying retrieval at the token level, using entropy signals to detect knowledge gaps and\ntriggering retrieval through a self-attentive query formulation process. Though it improves multi-hop QA precision,\nDRAGIN introduces notable inference costs, mitigated through adaptive frequency thresholds. FLARE proactively\nanticipates knowledge needs before uncertainty arises, improving faithfulness but requiring careful thresholding to\navoid excessive retrieval.\nMulti-source retrieval targets adaptability across evolving corpora or specialized domains. AU-RAG introduces\nagent-based retrieval, dynamically selecting sources based on metadata heuristics. This improves domain coverage\nbut necessitates hierarchical pipelines to manage source prioritization. SimRAG enhances retrieval precision using\nself-supervised learning on synthetic QA pairs, filtered via round-trip consistency. While it achieves 1.2\u20138.6% accuracy\ngains across datasets, it risks overfitting, mitigated by human-in-the-loop validation.\nQuery refinement techniques enhance retrieval relevance by modifying ambiguous or underspecified queries.\nRQ-RAG uses perplexity-driven decomposition and rewriting to improve relevance, especially in multi-fact scenarios.\nHowever, this incurs inference overhead, mitigated through selective refinement based on query ambiguity. R2AG\nimproves post-retrieval alignment by injecting retrieval metadata into prompts, bridging the retriever\u2013generator\nsemantic gap. Though effective, it adds computational cost, addressed by only enabling metadata prompting when\nretrieval scores fall below a relevance threshold.\nHybrid and structured retrieval approaches improve coherence by integrating unstructured and structured\nsources. M-RAG clusters knowledge into semantic partitions, with dual agents selecting and refining content. It reduces\nnoise but introduces latency, mitigated by dynamic partition expansion. KRAGEN retrieves subgraphs from knowledge\ngraphs, using Graph-of-Thoughts prompting for relational reasoning. This reduces hallucinations by 20\u201330%, though it\nincreases memory overhead, controlled via selective node expansion.\nExtending hybrid retrieval designs, the Dual-Pathway KG-RAG framework [74] combines structured retrieval\nfrom knowledge graphs with unstructured corpus retrieval in parallel, enhancing factual consistency and reducing\nhallucinations by 18% in biomedical QA tasks. Similarly, Graph RAG [16] constructs entity-centric graphs from retrieved\npassages and uses community summarization to scale RAG to large corpora, improving multi-hop QA recall by 6.4\nManuscript submitted to ACM\n\nRetrieval-Augmented Generation: A Survey\n9\npoints compared to baseline retrieval. Likewise, Customer Service QA [77] integrates RAG with knowledge graphs\nconstructed from issue-tracking tickets, achieving a 77.6% improvement in retrieval MRR and a 28.6% reduction in\nresolution time when deployed at LinkedIn\u2019s customer service team.\nIn a complementary direction, Doan et al. [15] propose a lightweight hybrid retrieval strategy that combines\nunstructured text embeddings with structured knowledge graph embeddings without requiring complex retriever\nre-training, achieving up to 13.1% improvements in retrieval correctness and ranking precision in domain-specific RAG\ndeployments.\n4.2\nEnhancing Context Relevance through Filtering\nDespite advances in retrieval models, RAG systems often integrate irrelevant, redundant, or semantically noisy docu-\nments that degrade generation quality. Filtering techniques aim to reduce hallucinations and improve answer relevance\nby selecting only contextually appropriate content. These methods vary in supervision, granularity, and efficiency, and\ncan be categorized into three groups: lexical/statistical filters, information-theoretic optimizers, and self-supervised\npassage scoring.\nLexical filters such as FILCO apply word overlap and statistical relevance scoring. Using STRINC and CXMI metrics,\nFILCO removes low-relevance passages and reduces hallucinations by up to 64%, improving EM by +8.6. However, its\nreliance on lexical similarity limits its adaptability across domains and query styles.\nInformation-theoretic methods like IB Filtering [89] use principles from the information bottleneck framework\nto retain only high-utility input features while discarding noise. Though computation-heavy, IB Filtering improves\nEM by +3.2 with a 2.5% compression ratio, offering a balance between precision and conciseness. Similarly, Stochastic\nFiltering models retrieval as an expected utility maximization problem and re-ranks passages based on marginal value,\nachieving consistent retrieval effectiveness gains with minimal retriever changes.\nSelf-supervised methods like SEER and RAG-Ex use internal feedback signals to filter noisy retrievals. SEER\napplies label-free training and generates pseudo-relevance judgments, improving F1 by 13.5% and achieving a 9.25\u00d7\nreduction in context length. RAG-Ex perturbs retrieved passages and compares generation outcomes, selecting those\nthat maximize semantic consistency. It aligns with human-assessed faithfulness 76.9% of the time and is model-agnostic,\nthough computationally expensive due to multiple inference passes.\nCollectively, these methods balance retrieval compression, answer faithfulness, and domain adaptability. While\nlexical filters are efficient, self-supervised models provide deeper semantic filtering and support long-form reasoning.\n4.3\nEfficiency Enhancements\nWhile Retrieval-Augmented Generation (RAG) significantly enhances factual consistency in large language models\n(LLMs) by integrating external document retrieval, it introduces new inefficiencies. These include increased memory\noverhead, latency from retrieval-processing pipelines, and redundancy in passage selection. This section synthesizes\nkey research efforts aimed at improving retrieval efficiency across four areas: sparse retrieval and context selection,\ninference acceleration, caching and redundancy reduction, and retrieval faithfulness.\nSparse context selection and retrieval-aware generation techniques aim to reduce the input length and improve\nsemantic alignment without sacrificing output quality. Sparse RAG addresses this by filtering low-relevance content\nbefore self-attention, retaining only high-signal tokens via parallel encoding. While it builds on Fusion-in-Decoder (FiD),\nit improves efficiency by avoiding dense input concatenation. However, it may discard useful context under suboptimal\nretrieval, requiring fine-tuning to maintain robustness. R2AG takes a complementary approach by embedding retrieval\nManuscript submitted to ACM\n\nRetrieval-Augmented Generation: A Survey\n10\nrepresentations directly into the LLM\u2019s context space, enhancing semantic alignment. Unlike prompt-based methods\n(e.g., REPLUG [58]), R2AG bypasses explicit concatenation, reducing redundant processing. Both approaches enhance\nefficiency at different stages but require retriever fine-tuning and increase model complexity.\nInference acceleration strategies focus on reducing decoding latency in autoregressive models by minimizing\nredundant token processing. FiD-Light achieves this through token-level passage compression, which lowers decoding\ntime while preserving key information. Though effective, aggressive filtering can marginally reduce retrieval precision.\nSpeculative Pipelining [71] further reduces latency by overlapping retrieval and generation. It incrementally processes\ntop-\ud835\udc58candidates before retrieval completes, lowering time-to-first-token (TTFT) by 20\u201330%. However, it risks speculative\nhallucinations unless controlled by fallback mechanisms and selective decoding checkpoints. This line of work opens the\ndoor for future speculative decoding architectures\u2014discussed in Section 8\u2014that balance responsiveness and reliability\nin low-latency applications.\nCaching and redundancy reduction techniques aim to eliminate recomputation overhead in repetitive or high-\nthroughput workloads. RAGCache [33] introduces a hierarchical caching system that stores key-value tensors from\nprior retrievals. PGDSF extends this with prefix-aware eviction that prioritizes frequent and important documents.\nWhile these methods significantly improve efficiency in common-query settings, their impact diminishes on long-tail\ndistributions and introduces cache complexity.\nRetrieval faithfulness and answer relevance methods go beyond lexical similarity to ensure that retrieved\ndocuments are factually aligned with the generated output. Rich Answer Encoding (RAE) addresses this using a\nRetriever-as-Answer Classifier (RAC) and Dense Knowledge Similarity (DKS), which rescore documents based on their\nplausibility. RAE reduces hallucinations and improves grounding but requires retriever retraining, increasing cost.\nTaken together, these optimization strategies enhance efficiency across the RAG pipeline: Sparse RAG and R2AG\nimprove alignment between retrieved documents and generation; FiD-Light and Speculative Pipelining reduce latency\nduring inference; RAGCache and PGDSF minimize recomputation in high-throughput environments; and RAE advances\nretrieval faithfulness. Collectively, they represent a move toward more scalable, accurate, and computationally efficient\nRAG systems.\n4.4\nEnhancing Robustness\nRAG systems improve factual accuracy in language models by retrieving external information. However, they remain\nvulnerable to retrieval noise, hallucinations, and adversarial attacks. While past research has addressed these challenges\nseparately\u2014such as noise resilience, hallucination control, and retrieval security\u2014a unified perspective is essential. This\nsection groups robustness techniques into three areas: noise mitigation, hallucination reduction, and security defenses.\nEmpirical studies further support this need for a unified view; a recent study identifies seven recurrent failure\npoints in operational RAG systems, spanning retrieval errors, context consolidation failures, hallucinated outputs, and\nincomplete answers [4].\nNoise mitigation strategies target irrelevant, misleading, or adversarial content that can degrade RAG accuracy.\nHowever, recent work challenges the assumption that all retrieval noise is detrimental; Cuconasu et al. [14] demonstrate\nthat carefully positioned random documents can paradoxically improve LLM reasoning and answer quality by promoting\nevidence selection behaviors. Two contrasting approaches address this: RAAT and CRAG. RAAT uses adversarial\npretraining to expose models to subtle and counterfactual retrieval noise, improving F1/EM scores by 20\u201330%. Its\nhigh training cost limits it to static, high-stakes domains. CRAG filters low-confidence retrievals at inference time and\nManuscript submitted to ACM\n\nRetrieval-Augmented Generation: A Survey\n11\nworks well in real-time systems, reducing retrieval errors by 12\u201318%. However, it struggles with \u201csoft noise,\u201d where\nsuperficially relevant content misleads the model.\nHallucination reduction techniques such as Structured RAG [2] and IM-RAG aim to improve the faithfulness of\ngenerated content. Structured RAG constrains retrieval to verified corpora, lowering hallucination rates by 30\u201340% with\nminimal compute cost. Its drawback is poor adaptability, requiring manual updates. IM-RAG uses iterative retrieval\nrefinement, achieving +5.3 F1 / +7.2 EM on HotPotQA. Though more accurate in evolving domains, it is computationally\nintensive and slower at inference.\nSecurity defenses focus on adversarial threats such as data poisoning and backdoor attacks. Research into BadRAG\nshows that poisoning just 0.04% of a corpus can lead to a 98.2% attack success rate and 74.6% system failure. Defenses like\ncryptographic document signing or adversarial filtering are only partially effective. TrojanRAG embeds backdoors in\nretrieval embeddings, bypassing traditional sanitization. Stronger mitigations\u2014secure training and integrity validation\u2014\nare needed but require proactive design. Beyond adversarial attacks, privacy vulnerabilities in RAG systems have also\nbeen identified; Zeng et al. [85] show that both retrieval databases and pretraining corpora can be exploited through\nstructured prompting, although retrieval can paradoxically help reduce memorization leakage by acting as a grounding\nmechanism.\n4.5\nEnhancements and Optimizations in Reranking\nReranking plays a vital role in improving the relevance and faithfulness of Retrieval-Augmented Generation (RAG)\noutputs. While initial retrieval stages often return noisy results, reranking refines document ordering before passage\nselection and generation, reducing hallucinations and improving response accuracy. Recent work advances reranking\nacross three key areas: adaptive reranking, unified pipelines, and fusion-based reranking.\nAdaptive reranking methods dynamically adjust the number of documents reranked based on query complexity.\nRLT [44] uses ranked list truncation to improve MRR/nDCG while reducing retrieval noise by 15%. ToolRerank further\nadapts reranking depth based on familiarity with seen vs. unseen tools, boosting recall by 12% in hierarchical retrieval\ntasks. These methods optimize computation by avoiding unnecessary reranking in low-complexity scenarios.\nUnified reranking pipelines combine retrieval, document ranking, and generation within a single architecture.\nRankRAG fine-tunes a language model to jointly score documents and generate answers, improving MRR@10 by 7.8%\nwhile reducing latency. uRAG extends this to multiple tasks\u2014like QA and fact verification\u2014using shared reranking\nlogic and user-feedback signals, improving cross-task generalization by 8% MRR@10. These approaches eliminate the\noverhead of separate ranking modules and increase retrieval consistency.\nFusion-based reranking strategies aggregate evidence from multiple query variants to improve answer robustness.\nRAG-Fusion generates multiple subqueries and applies reciprocal rank fusion, improving answer accuracy by 9%. R2AG\nrefines these rankings iteratively, reducing irrelevant retrievals by 15% through recursive feedback. These models are\nespecially effective for multi-hop and ambiguous tasks.\nReranking methods significantly boost the efficiency and faithfulness of RAG systems. Future work may explore\nhybrid approaches combining adaptive truncation with fusion-based aggregation, as well as domain-adaptive reranking\nfor enterprise scalability. As RAG expands to more tasks and domains, reranking will remain essential to enabling\ncontext-aware, trustworthy generation.\nManuscript submitted to ACM\n\nRetrieval-Augmented Generation: A Survey\n12\nTable 1. Summary of RAG System Enhancements. This table categorizes enhancements across five dimensions\u2014retrieval, filtering,\nefficiency, robustness, and reranking. Each entry specifies the enhancement type, method, mechanism, key strengths, known\nlimitations, and ideal use cases.\nEnhancement\nType\nCategory\nMethod\nMechanism\nStrengths\nLimitations\nBest Use Case\nRetrieval\nAdaptive\nTA-ARE\nDynamic confidence estima-\ntion\nReduces redundant retrieval\nEstimator latency\nShort-form QA\nAdaptive\nDRAGIN\nToken-level entropy-based trig-\ngers\nImproves multi-hop QA preci-\nsion\nHigh inference cost\nMulti-hop QA\nAdaptive\nFLARE\nPreemptive uncertainty detec-\ntion\nEnhances faithfulness\nRisk of over-retrieval\nLong-form generation\nMulti-source\nAU-RAG\nAgent-based source selection\nHigh domain adaptability\nSource management overhead\nEvolving corpora\nMulti-source\nSimRAG\nSynthetic QA + round-trip fil-\ntering\nCross-domain accuracy gains\nOverfitting risk\nSpecialized domains\nQuery\nRQ-RAG\nPerplexity-based query rewrit-\ning\nImproves query clarity and rel-\nevance\nAdditional inference steps\nMulti-fact queries\nQuery\nR2AG\nRetrieval-aware prompt injec-\ntion\nEnhances factual grounding\nPrompt expansion overhead\nLow-confidence queries\nHybrid\nM-RAG\nSemantic partitioning + dual\nagents\nReduces retrieval noise\nPartition latency\nContext-heavy reasoning\nHybrid\nKRAGEN\nKnowledge graph subgraph re-\ntrieval\nImproves structured reasoning\nMemory and compute inten-\nsive\nBiomedical, graph-based tasks\nFiltering\nLexical\nFILCO\nSTRINC + CXMI scoring\n+8.6 EM, 64% hallucination re-\nduction\nQuery-style bias\nStructured QA\nInfo-Theoretic\nIB Filtering\nBottleneck-based compression\n+3.2 EM, 2.5% compression\nComputation overhead\nHigh-precision QA\nInfo-Theoretic\nStochastic\nFiltering\nUtility-maximizing re-ranking\nImproves effectiveness\nNeeds custom scoring\nLightweight retrieval tasks\nSelf-Supervised\nSEER\nPseudo-relevance\nvia\nself-\ntraining\n+13.5% F1, 9.25x context reduc-\ntion\nHigh training cost\nOpen-domain QA\nSelf-Supervised\nRAG-Ex\nGeneration perturbation com-\nparison\n76.9% human-aligned faithful-\nness\nMultiple inference passes\nFaithful generation\nEfficiency\nSparse Selection\nSparse RAG\nRetains high-signal tokens\nReduces memory, improves rel-\nevance\nMay discard useful docs\nLong-context tasks\nSparse Selection\nR2AG\nContext-aware retrieval injec-\ntion\nEnhances coherence, lowers re-\ndundancy\nRetriever fine-tuning needed\nKnowledge-intensive QA\nInference Acceler-\nation\nFiD-Light\nCompresses passages\nFaster decoding\nSlight loss in recall\nLow-latency applications\nCaching\nSpeculative\nPipelining\nOverlaps retrieval and genera-\ntion\n20\u201350% TTFT reduction\nRisk of hallucination\nReal-time applications\nCaching\nRAGCache\nHierarchical cache w/ PGDSF\nEliminates recomputation\nCache complexity in long-tail\nHigh-throughput workloads\nRetrieval Quality\nRAE\nRetriever-as-answer scorer\nBoosts grounding and preci-\nsion\nRequires scoring/retraining\nFactual QA\nRobustness\nNoise Mitigation\nRAAT\nAdversarial training\n+20\u201330% F1/EM\nHigh training cost\nOffline pretraining\nNoise Mitigation\nCRAG\nInference-time filtering\n+12\u201318% precision gain\nIneffective on \u201csoft\u201d noise\nReal-time support\nHallucination\nControl\nStructured\nRAG\nCurated corpus retrieval\n30\u201340% hallucination reduction\nLow adaptability\nStatic domains\nHallucination\nControl\nIM-RAG\nIterative retrieval refinement\n+5.3 F1 / +7.2 EM\nInference latency\nMulti-hop QA\nSecurity\nBadRAG\nAdversarial retrieval poisoning\nDemonstrates\ncorpus-level\nthreat\nNeeds stronger filtering\nSecurity evaluation\nSecurity\nTrojanRAG\nEmbedding-level backdoor\nPersistent attack vector\nRequires secure training\nSecurity-sensitive pipelines\nReranking\nAdaptive\nRLT\nDynamic list truncation\n+15% noise reduction\nHeuristic tuning needed\nReal-time QA\nAdaptive\nToolRerank\nFamiliarity-aware reranking\n+12% recall for unseen tools\nComplexity\nfor\nun-\nseen/frequent tools\nTool-aware retrieval\nUnified Pipeline\nRankRAG\nJoint rerank + generate\n+7.8% MRR@10\nDomain-specific tuning\nEnd-to-end QA systems\nUnified Pipeline\nuRAG\nShared reranking engine\n+8% MRR@10, task generaliza-\ntion\nHigher setup cost\nMulti-task enterprise RAG\nFusion-based\nRAG-Fusion\nReciprocal rank fusion\n+9% accuracy\nQuery explosion risk\nComplex multi-hop QA\nFusion-based\nR2AG\nRecursive\nreranking\nrefine-\nment\n15% irrelevant retrieval reduc-\ntion\nHigher latency\nIterative reasoning\nManuscript submitted to ACM\n\nRetrieval-Augmented Generation: A Survey\n13\nTable 2. Comparative Performance of Retrieval-Augmented Generation Frameworks Across Multi-Hop and Short-Form\nQA Benchmarks. This table reports relative performance improvements achieved by each RAG framework over two baselines: (i) the\nraw backbone language model (B) and (ii) the same model augmented with a standard retrieval module (B+R). Results are shown\nacross multi-hop benchmarks (HotpotQA [81], 2Wiki [23], MuSiQue [67]) and short-form QA datasets (PopQA [42], TriviaQA [35],\nARC-Challenge [12], NQ [37]), with metrics including F1, Exact Match (EM), and Accuracy (Acc). Frameworks are grouped by\narchitectural category: retriever-based, generator-based, and hybrid. A \u201c\u2013\u201d indicates that the corresponding score was not reported in\nthe original publication. Backbone LLMs referenced in this table include LLaMA 2 [66], LLaMA 3 [21], GPT-3.5/4 [47], Vicuna [11],\nMistral [29], Mixtral [30], Gemini [53], and Gemma [64].\nFramework\nBackbone\nHotpotQA\n2Wiki\nMusiQue\nPopQA\nTriviaQA\nARC-Challenge\nNQ\nB/B+R\nB/B+R\nB/B+R\nB/B+R\nB/B+R\nB/B+R\nB/B+R\nRetriever-Based RAG\nRQ-RAG\nLLaMA2-7B\n8.485/2.749 (F1)\n1.8/1.396 (F1)\n12.9/4.635 (F1)\n2.884/0.434 (Acc)\n-/-\n2.133/1.379 (Acc)\n-/-\nSimRAG\nLLaMA3-8B\n-/-\n-/-\n-/-\n-/-\n-/-\n0.145/- (Acc)\n-/-\nSimRAG\nGemma2-27B\n-/-\n-/-\n-/-\n-/-\n-/-\n0.034/- (Acc)\n-/-\nSEER\nLLaMA2-7B-Chat\n0.104/0.037 (F1)\n-/-\n-/-\n-/-\n-/-\n-/-\n-/-\nRankRAG\nLLaMA3-8B\n-/0.079 (F1)\n-/0.323 (F1)\n-/-\n-/-\n-/-\n-/-\n-/-\nRankRAG\nLLaMA3-70B\n-/0.242 (F1)\n-/0.376 (F1)\n-/-\n-/-\n-/-\n-/-\n-/-\nLQR\nLLaMA3-8B\n2.081/0.516 (F1)\n0.706/0.141 (F1)\n2.922/0.841 (F1)\n-/-\n-/-\n-/-\n-/-\nLongRAG\nGPT-4o\n0.517/- (EM)\n-/-\n-/-\n-/-\n-/-\n-/-\n-/-\nLongRAG\nGemini-1.5-Pro\n0.696/- (EM)\n-/-\n-/-\n-/-\n-/-\n-/-\n-/-\nFILCO\nLLaMA2-7B\n-/0.057 (EM)\n-/-\n-/-\n-/-\n-/0.056 (EM)\n-/-\n-/0.298 (EM)\nRe2G\nBART Large\n-/-\n-/-\n-/-\n-/-\n0.251/- (Acc)\n-/-\n0.144/-\nGenerator-Based RAG\nxRAG\nMistral-7B\n0.26/-0.122 (EM)\n-/-\n-/-\n-/-\n0.152/-0.002 (EM)\n-/-\n0.293/-0.085 (EM)\nxRAG\nMixtral-8x7B\n0.207/-0.087 (EM)\n-/-\n-/-\n-/-\n0.043/0.054 (EM)\n-/-\n0.126/0.047 (EM)\nINFO-RAG\nLLaMA2-7B\n0.182/- (EM)\n-/-\n0.163/- (EM)\n-/-\n-/-\n-/-\n-/-\nINFO-RAG\nLLaMA2-13B\n0.222/- (EM)\n-/-\n0.358/- (EM)\n-/-\n-/-\n-/-\n-/-\nINFO-RAG\nLLaMA2-13B-chat\n0.011/- (EM)\n-/-\n0.018/- (EM)\n-/-\n-/-\n-/-\n-/-\nSELF-RAG\nLLaMA2-7B\n-/-\n-/-\n-/-\n2.735/0.437 (Acc)\n1.177/0.562 (Acc)\n2.092/0.404 (Acc)\n-0.505/- (Acc)\nSELF-RAG\nLLaMA2-13B\n-/-\n-/-\n-/-\n2.796/0.221 (Acc)\n0.8/0.474 (Acc)\n-/-\n-/-\nFiD-Light\nFiD+DPR\n-/-\n-/-\n-/-\n-/-\n0.185/- (EM)\n-/-\n0.27/- (EM)\nR2AG\nLLaMA2-7B\n3.231/- (F1)\n34.52/4.445 (F1)\n-/-\n-/-\n-/-\n-/-\n0.824/- (Acc)\nHybrid RAG\nDRAGIN\nLLaMA2-7B-chat\n0.218/0.338 (F1)\n0.311/0.148 (F1)\n-/-\n-/-\n-/-\n-/-\n-/-\nDRAGIN\nLLaMA2-13B-chat\n0.368/0.144 (F1)\n0.445/0.169 (F1)\n-/-\n-/-\n-/-\n-/-\n-/-\nDRAGIN\nVicuna-13B-v1.5\n0.279/0.179 (F1)\n0.575/0.371 (F1)\n-/-\n-/-\n-/-\n-/-\n-/-\nFLAREdirect\nGPT-3.5\n-/-\n0.622/0.223 (F1)\n-/-\n-/-\n-/-\n-/-\n-/-\nFLAREinstruct\nGPT-3.5\n-/-\n0.353/0.02 (F1)\n-/-\n-/-\n-/-\n-/-\n-/-\nGenGround\nGPT-3.5\n0.236/0.093 (F1)\n0.219/0.122 (F1)\n0.359/0.361 (F1)\n-/-\n-/-\n-/-\n-/-\nStochastic RAG\nFiD-Light (T5-Base)\n0.066/- (F1)\n-/-\n-/-\n-/-\n-/0.036 (EM)\n-/-\n-/0.013 (EM)\nStochastic RAG\nFiD-Light (T5-XL)\n0.065/- (F1)\n-/-\n-/-\n-/-\n-/0.016 (EM)\n-/-\n-/0.037 (EM)\nCRAG\nLLaMA2-7B\n-/-\n-/-\n-/-\n3.034/0.471 (Acc)\n-/-\n1.514/0.173 (Acc)\n0.045/- (Acc)\nSelf-CRAG\nLLaMA2-7B\n-/-\n-/-\n-/-\n3.204/0.533 (Acc)\n-/-\n2.083/0.439 (Acc)\n-/-\nTA-ARE\nGPT-3.5\n-/-\n-/-\n-/-\n-/-\n-/-\n-/-\n-/-\nTA-ARE\nGPT-4\n-/-\n-/-\n-/-\n-/-\n-/-\n-/-\n-/-\nTA-ARE\nLLaMA2-7B\n-/-\n-/-\n-/-\n-/-\n-/-\n-/-\n-/-\n5\nComparative Analysis\nTo assess the empirical effectiveness of design innovations in Retrieval-Augmented Generation (RAG), this section\npresents a comparative analysis of representative frameworks across three key evaluation settings: short-form question\nanswering, multi-hop reasoning, and robustness under retrieval perturbations. Results are reported as relative improve-\nments over both raw and retrieval-augmented baselines, normalized for model and dataset variability. Additionally, we\nreview ablation studies from the literature to disentangle the contributions of specific components such as retrieval\ntriggers, filtering layers, reranking mechanisms, and robustness modules. These insights offer a clearer understand-\ning of which enhancements most significantly impact performance, faithfulness, and efficiency across diverse RAG\nconfigurations.\nManuscript submitted to ACM\n\nRetrieval-Augmented Generation: A Survey\n14\n5.1\nComparative Analysis of Framework Performance on Short-Form QA\nThis section presents a comparative analysis of Retrieval-Augmented Generation (RAG) frameworks in short-form\nquestion answering, emphasizing their relative improvements over raw large language model (LLM) baselines and\nretrieval-augmented baselines. As shown in Table 2, these comparisons focus on relative gains (e.g., a value of 2.7\nindicates a 270% improvement) rather than absolute performance metrics, which normalize for variations in backbone\narchitectures, prompting strategies, and evaluation protocols. This approach enables a meaningful comparison across\ndiverse experimental setups.\nAmong generator-based RAG systems primarily optimized for accuracy, SELF-RAG consistently demonstrates\nsubstantial gains across multiple datasets. It achieves over a 270% improvement from the raw LLM baseline on PopQA [42]\nand over 200% on ARC-Challenge [12], illustrating the effectiveness of deep context integration for enhancing short-\nform factual recall. FiD-Light, although also a generator-side enhancement, adopts a different optimization philosophy\ncentered on lightweight, efficient fusion of retrieved documents during decoding, yielding more moderate improvements\nof 18\u201327% across TriviaQA [35] and NQ [37]. R2AG, another generator-based approach, shows promising gains, with\nover 80% improvement from the baseline on NQ, further validating the benefits of integrating retrieval signals within\ngeneration. We note that generator-based frameworks primarily designed for efficiency, such as xRAG, are discussed\nseparately due to their distinct optimization focus.\nRetriever-based frameworks such as RQ-RAG and SimRAG also demonstrate notable gains. RQ-RAG achieves a\n288% improvement on PopQA and over 210% on ARC-Challenge, reaffirming the importance of retrieval quality in\nevidence-centric QA. SimRAG also shows strong improvements on ARC, although gains are more modest (approximately\n14%). Additionally, retriever-side re-ranking approaches like FILCO deliver moderate but meaningful gains, with 5\u201330%\nimprovements across NQ and TriviaQA, further highlighting the incremental value of retrieval refinement strategies.\nHybrid frameworks exhibit a more heterogeneous pattern. CRAG and Self-CRAG achieve impressive gains, with\nSelf-CRAG delivering a 320% improvement on PopQA and a 208% improvement on ARC-Challenge, suggesting that\ncombining retrieval refinement with generation adaptation can be highly effective when well aligned. However, TA-ARE,\ndespite achieving a significant 28\u00d7 improvement over raw baselines on RetrievalQA, occasionally underperforms relative\nto the standard retrieval baseline, indicating that retrieval frequency reduction strategies, while efficient, may introduce\ntrade-offs. Stochastic RAG frameworks, meanwhile, display relatively modest gains (typically under 4%), reflecting that\nintroducing retrieval randomness increases diversity without consistently boosting short-form QA accuracy.\nEfficiency-focused generator-based systems such as xRAG exhibit mixed results. While xRAG achieves 10\u201329%\nimprovements over raw LLM baselines on datasets such as NQ and TriviaQA, its gains over retrieval baselines are\nmarginal or occasionally negative. This suggests that while resource-efficient designs are promising for scaling RAG\nsystems, further optimization is needed to maintain competitive factual accuracy in short-form tasks.\nFinally, robustness-oriented frameworks such as RAAT demonstrate strong performance, with a 116% improvement\nfrom the raw baseline and over 27% gain compared to retrieval on RAG-Bench\u2014a robustness-focused variant of NQ,\nWebQ, and TriviaQA. Although evaluated under challenging retrieval noise conditions, RAAT\u2019s results suggest that\nrobustness-driven retrieval strategies can effectively complement factual QA objectives.\nOverall, retrieval- and generation-enhanced frameworks deliver substantial relative gains in short-form QA, while\nhybrid and efficiency-focused approaches offer promising but variable results depending on dataset and retrieval\ncomplexity. These findings underscore the critical role of retrieval optimization and generation-adaptive strategies in\nadvancing retrieval-augmented short-form question answering.\nManuscript submitted to ACM\n\nRetrieval-Augmented Generation: A Survey\n15\n5.2\nComparative Analysis of Framework Performance on Multi-Hop QA\nA comparative evaluation of various Retrieval-Augmented Generation (RAG) frameworks reveals distinct patterns in\ntheir ability to enhance multi-hop question answering, assessed through improvements over both raw large language\nmodels (LLMs) and standard retrieval-augmented baselines. Similar to the previous section, this analysis focuses on\nrelative gains rather than absolute scores to normalize for architectural and experimental variations. The results,\nsummarized in Table 2, enable a consistent comparison of framework contributions across diverse multi-hop QA\nsettings.\nAmong retrieval-based RAG systems, models such as RQ-RAG, RankRAG, LQR, and LongRAG demonstrate substantial\nrelative gains. Notably, RQ-RAG achieves over an 800% improvement from its raw LLM baseline on HotpotQA [81],\nand a 275% improvement over standard retrieval, highlighting the effectiveness of sophisticated query decomposition\ntechniques in multi-hop settings. Similarly, LQR achieves a 292% improvement from the raw baseline and an 84%\nimprovement over retrieval in the MuSiQue dataset [67], suggesting that intelligent retrieval-ranking substantially\nboosts multi-hop reasoning. LongRAG also exhibits strong performance, improving by over 50% from the raw LLM\nbaseline on HotpotQA, further emphasizing the value of extended retrieval for complex question answering. These\npatterns collectively affirm that optimizing retrieval quality remains a dominant driver of performance gains in multi-hop\nRAG applications.\nGenerator-based RAG frameworks, including R2AG, INFO-RAG, and xRAG, display more varied relative improve-\nments. R2AG shows consistent strong gains, improving by over 300% relative to the baseline on HotpotQA, demonstrating\nthe benefits of tightly integrating retrieval signals into the generation process. In contrast, INFO-RAG exhibits more\nmodest improvements, with relative gains around 16\u201335% across different backbones and datasets, suggesting that while\ngenerator-side augmentations enhance output faithfulness, their standalone effect may be limited without concurrent\nretrieval refinement. xRAG, while improving from raw baselines by approximately 20\u201326%, shows negative or marginal\ngains compared to the retrieval baseline in some settings, indicating that extreme context compression, although\nefficient, may compromise the model\u2019s ability to utilize retrieved evidence effectively for complex multi-hop reasoning.\nHybrid RAG frameworks, such as DRAGIN, FLARE, GenGround, and Stochastic RAG, present a diverse range of\noutcomes. DRAGIN frameworks achieve moderate improvements, typically ranging between 22\u201344% over raw LLMs\nand 14\u201334% over retrieval baselines, reflecting the incremental gains from dynamically adapting retrieval to evolving\ninformation needs. FLAREdirect stands out, achieving a 62% improvement from the raw LLM and a 22% improvement\nover standard retrieval on 2Wiki [23], suggesting that model-guided active retrieval significantly strengthens multi-hop\nevidence gathering. GenGround reports relatively smaller improvements (13\u201336% from the baseline) but is evaluated\nagainst already-strong baselines, which partially accounts for the more conservative gains. Stochastic RAG frameworks\noffer consistent yet modest gains (6%), indicating that introducing randomness into retrieval can modestly diversify and\nenhance evidence coverage without destabilizing performance.\nOverall, retrieval-based RAG frameworks demonstrate the most consistent and substantial improvements across\nmulti-hop QA tasks, particularly when retrieval quality, ranking, and query decomposition are optimized. Generator-\nbased adaptations, while beneficial in specific cases, often require complementary retrieval-side enhancements to\nrealize their full potential. Hybrid frameworks offer promising but more variable results, underscoring the challenge\nof harmonizing retrieval and generation strategies dynamically. These findings highlight retrieval optimization as a\ncritical lever for advancing complex reasoning capabilities in RAG systems.\nManuscript submitted to ACM\n\nRetrieval-Augmented Generation: A Survey\n16\n5.3\nComparative Robustness Analysis: Framework Gains Over Retrieval-Only Baselines\nTo assess robustness in Retrieval-Augmented Generation (RAG) systems, we report incremental improvements each\nframework achieves over its retrieval-augmented LLM baseline. This isolates the added value of mechanisms such as\ncritique, reranking, and filtering, independent of the baseline retrieval gain. Evaluations span multiple datasets and\nfocus on gains in precision, recall, and FactScore. By standardizing on relative improvements, the analysis enables fair\ncomparisons across models with differing backbone architectures. A summary of these results is provided in Table 3.\nAmong hybrid systems, the most substantial gains in factual consistency are observed. Self-CRAG yields the highest\nFactScore improvement\u2014+0.456 on the Biography dataset [45]\u2014significantly surpassing other frameworks, most of\nwhich report \u22640.05 gains. The multi-sentence compositional nature of the Biography task likely benefits from Self-\nCRAG\u2019s feedback-based reranking and correction loop, which aligns generation with retrieved evidence. Comparable\nimprovements are evident with Self-RAG and CRAG, reporting +0.372 and +0.252 gains on the same dataset, underscoring\nthe importance of evidence-aware generation refinement. On 2Wiki, Flare-Direct improves both precision and recall\nby +21.6%, while Flare-Instruct\u2014despite using the same retrieval backbone\u2014offers negligible gains, illustrating how\nprompt design alone can meaningfully impact robustness in multi-hop settings. In contrast, Stochastic RAG shows only\nmarginal FactScore gains (\u2264+0.008) on Fever, suggesting that entropy-driven retrieval without subsequent verification\nmay be insufficient to ensure factual reliability.\nGenerator-based systems present more variable, task-dependent performance. SELF-RAG, evaluated on ASQA [60],\nachieves sizable improvements in precision (+22\u201330%) and recall (+16\u201319%), though its FactScore gains remain modest\n(+0.03\u20130.04), implying improved evidence usage without equivalent advances in factual accuracy. DRAGIN similarly\nimproves precision and recall by +9\u201322% on HotPotQA, leveraging entropy-based token-level triggers suited for multi-\nhop reasoning. However, lacking reported FactScore, its contribution to factual consistency remains indeterminate.\nOther generator-oriented systems, including GenRT and Rich Answer Encoding, achieve smaller recall gains (\u2264+0.1) on\ndatasets such as TriviaQA, KILT-WoW [49], and MSMARCO [3]. These modest improvements suggest better document\nselection but limited post-retrieval validation, constraining their robustness impact.\nRetriever-based systems exhibit consistent yet comparatively modest gains. Re2G reports +17.8% precision and\n+15.9% recall on TriviaQA, reflecting the benefits of retrieval-aware prompt optimization. FILCO, by contrast, improves\nprecision by +3.25% on Fever but fails to enhance recall or FactScore, indicating that filtering irrelevant context improves\nselectivity, but without downstream verification, its robustness contribution is limited. Not all frameworks report all\nthree metrics across datasets; while relative improvement facilitates normalization, incomplete coverage\u2014particularly\nof FactScore\u2014may obscure the full extent of a system\u2019s capabilities.\nIn sum, Self-CRAG on Biography delivers the strongest FactScore gain (+0.456), while SELF-RAG on ASQA achieves\nthe best precision (+29.56%) and recall (+18.81%) improvements. Flare-Direct, outperforming Flare-Instruct by over 20%\non 2Wiki, highlights the sensitivity of robustness to prompt design. At the lower end, Stochastic RAG on FEVER [65]\nrecords the smallest impact (\u2264+0.008 FactScore), reinforcing the necessity of combining retrieval strategies with\ndownstream verification to enhance factual fidelity.\nCollectively, these findings affirm that retrieval alone is insufficient for robust generation. The most effective\nframeworks tightly couple retrieval, generation, and verification in iterative loops, ensuring that generation is guided\nby critique and alignment rather than treated as a terminal step.\nManuscript submitted to ACM\n\nRetrieval-Augmented Generation: A Survey\n17\nTable 3. Comparative Robustness Analysis of RAG Frameworks Across Architectures. Relative improvements in precision,\nrecall, and FactScore over retrieval-augmented baselines across multiple datasets. A dash (\u2013) denotes missing values in the original\npaper.\nTaxonomy\nFramework\nBackbone\nDataset\nPrecision\nRecall\nFactScore\nRetriever-based RAG\nRe2G\nKGI0\nNQ\n0.096984\n0.074569\n\u2013\nRe2G\nKGI1\nTriviaQA\n0.177981\n0.159062\n\u2013\nRe2G\nKGI2\nFever\n0.120986\n0.073732\n\u2013\nFILCO\nRAG\nFever\n3.25\n\u2013\n\u2013\nGeneration-based RAG\nSELF-RAG\nLLaMA2-7B\nASQA\n22.06897\n15.95\n0.041026\nSELF-RAG\nLLaMA2-7B\nASQA\n29.56522\n18.80556\n0.034839\nRich Answer Encoding\nRAG\nMSMARCO\n\u2013\n0.086957\n\u2013\nRich Answer Encoding\nRAG\nKILT-WoW\n\u2013\n0.107293\n\u2013\nDRAGIN\nLLaMA2-13B\nHotPotQA\n0.185934\n0.09893\n\u2013\nDRAGIN\nVICUNA-13B\nHotPotQA\n0.222447\n0.105114\n\u2013\nGenRT\nRAG\nNQ\n\u2013\n0.023232\n\u2013\nGenRT\nRAG\nTriviaQA\n\u2013\n0.026239\n\u2013\nHybrid RAG\nCRAG\nLLaMA2-7B\nBiography\n\u2013\n\u2013\n0.251689\nSelf-CRAG\nLLaMA2-7B\nBiography\n\u2013\n\u2013\n0.456081\nFlare-Instruct\nGPT-3.5\n2Wiki\n0.010288\n0.019417\n\u2013\nFlare-Direct\nGPT-3.5\n2Wiki\n0.216049\n0.215534\n\u2013\nStochastic RAG\nFiD-Light (T5-Base)\nFever\n\u2013\n\u2013\n0.008685\nStochastic RAG\nFiD-Light (T5-XL)\nFever\n\u2013\n\u2013\n0.00355\n5.4\nAblation Studies\nAblation studies serve as a crucial methodological lens for disentangling the contributions of individual components\nin Retrieval-Augmented Generation (RAG) frameworks. Across the surveyed literature, these studies primarily target\nretrieval triggers, filtering layers, reranking strategies, compression modules, and corrective mechanisms, offering\nempirical insights into performance, efficiency, and robustness.\nAdaptive Retrieval and Query Reformulation. Frameworks such as TA-ARE, FLARE, and IM-RAG demonstrate\nthat retrieval adaptivity is central to long-form and multi-hop reasoning. Ablating dynamic query triggers (e.g., forward-\nlooking or self-reflective prompts) consistently results in degraded factual accuracy and increased hallucinations,\nconfirming the value of retrieval-awareness throughout the generation process.\nFiltering, Reranking, and Evidence Quality. Systems like SEER, CRAG, and Re2G show that context filtering,\nreranking, and correction layers significantly influence downstream performance. Ablations reveal that removing\ncontext evaluators or decomposing mechanisms leads to verbosity and reduced grounding fidelity. Notably, reranking-\ntruncation co-designs (e.g., in GenRT and ToolRerank) outperform static top-\ud835\udc58approaches by improving answer\nfaithfulness and retrieval precision.\nCompression and Efficiency Trade-offs. FiD-Light and RAGCache demonstrate that passage compression and\ncaching can substantially reduce latency without compromising accuracy. Ablating vector sparsity or caching mecha-\nnisms (e.g., speculative pipelining or prefix-aware replacement) increases inference time up to 4\u00d7, underscoring the\noperational significance of architectural optimization in production RAG systems.\nRobustness and Security. Studies like BadRAG and TrojanRAG emphasize that security-focused ablations reveal\nnovel vulnerabilities. Even lightweight retrieval poisoning or trigger crafting can steer model outputs, while mitigation\nstrategies (e.g., summarization, distance thresholds) offer partial resilience but require further study.\nSynthesis. Ablation studies consistently reinforce that high-performing RAG frameworks are modular, with com-\nplementary retrieval, filtering, and generation components. Performance degradation in ablation settings not only\nvalidates novel modules but also guides design toward more interpretable, efficient, and secure RAG pipelines.\nManuscript submitted to ACM\n\nRetrieval-Augmented Generation: A Survey\n18\n6\nEvaluation and Benchmarking of RAG Systems\nRetrieval-Augmented Generation (RAG) systems introduce unique challenges for evaluation due to their hybrid\narchitecture combining a retriever and a generator. Accurate evaluation demands assessing multiple interdependent\ncomponents, including retrieval relevance, faithfulness of generated responses, and overall answer utility. In this section,\nwe synthesize recent advancements in automated evaluation frameworks, retrieval quality assessment techniques, and\nbenchmark construction to provide a comprehensive overview of evaluation practices in RAG systems.\n6.1\nEvaluation Dimensions\nThe core dimensions [55] used to evaluate RAG systems include:\n(1) Context Relevance: Measures how pertinent the retrieved documents are to the input query.\n(2) Answer Faithfulness: Assesses whether the generated output remains grounded in the retrieved evidence.\n(3) Answer Relevance: Evaluates whether the output adequately addresses the user query.\nThese dimensions are interdependent: poor context relevance often cascades into reduced faithfulness and answer\nrelevance, underscoring the need for joint evaluation. Frameworks such as ARES and RAGAS have formalized these\ndimensions, incorporating both automated judgment and reference-free evaluation.\n6.2\nAutomated Evaluation Frameworks\nARES [55] introduces an LLM-based judge system that uses few-shot prompted language models to generate synthetic\ndatasets. These judges are trained on three classification tasks corresponding to the core dimensions and use prediction-\npowered inference (PPI) to align model-based scoring with human judgment. ARES shows significant improvements in\naccuracy and annotation efficiency, outperforming RAGAS [17] by up to 59.3 percentage points in context relevance.\nRAGAS employs a modular framework that decomposes generated answers into atomic factual statements, then\nevaluates each against the retrieved context using LLMs. This structure provides high-resolution feedback, revealing\nwhich parts of an answer are hallucinated.\nThese frameworks automate the evaluation of faithfulness, grounding, and contextual relevance\u2014enabling scalable,\nreference-free analysis of RAG performance.\n6.3\nEvaluating Retrieval Quality\neRAG [56] challenges traditional relevance label techniques by applying the RAG generator to each retrieved document\nindividually. The performance of each document, assessed via downstream task metrics, serves as a relevance label.\nThis method provides a retrieval-aware, document-level granularity and has shown significantly improved correlation\nwith actual RAG performance.\nINFO-RAG introduces an unsupervised training paradigm that improves the LLM\u2019s ability to refine retrieved\ninformation under three scenarios: redundant, noisy, or insufficient context. By viewing the LLM as an \u201cinformation\nrefiner,\u201d it enables the model to extract relevant content, reject misinformation, and infer missing details\u2014enhancing\nretrieval robustness without supervised relevance labels.\nuRAG proposes a unified retrieval system that serves multiple RAG models across diverse downstream tasks. It\nintroduces a shared reranker trained on feedback signals (e.g., EM, accuracy) from various black-box LLMs, treating\neach LLM as a user of the search engine. uRAG\u2019s training protocol enables evaluation and optimization of retrieval\nManuscript submitted to ACM\n\nRetrieval-Augmented Generation: A Survey\n19\nbased on downstream task performance, offering retrieval diagnostics grounded in actual utility rather than surface\nsimilarity.\n6.4\nBenchmarking RAG Capabilities\nAs RAG systems mature, a growing suite of benchmarks has emerged to evaluate them across dimensions like robustness,\nfactuality, adaptivity, and domain sensitivity. These benchmarks not only reflect the evolving needs of real-world RAG\ndeployments but also shape future directions by surfacing recurrent failure modes and task-specific limitations.\nRobustness to retrieval noise is a core requirement in operational RAG systems. RGB [7] evaluates four fundamental\ncapacities\u2014noise robustness, negative rejection, information integration, and counterfactual resistance\u2014revealing\nconsistent weaknesses in LLMs when handling distracting or misleading context. Complementing this, RAG-Bench [18]\nintroduces a noise-centric benchmark simulating three retrieval corruption types\u2014relevant-but-incomplete, irrelevant,\nand counterfactual\u2014and applies adaptive adversarial training to improve model tolerance. These benchmarks enable\nfine-grained analysis of how retrieval perturbations degrade end-task performance and inform robust retrieval-policy\ndesign.\nFaithfulness and hallucination detection benchmarks have taken center stage in evaluating generation quality.\nRAGTruth [46] provides nearly 18,000 annotated examples from QA, summarization, and data-to-text generation,\noffering both response- and span-level hallucination labels across four types: subtle vs. evident, and conflict vs. baseless\ninformation. Uniquely, it supports training hallucination detectors and benchmarking span-level detection precision\nand recall\u2014tasks not addressed by coarse-grained metrics. This makes it foundational for measuring factual integrity in\nRAG outputs.\nReasoning and retrieval chaining are central to multi-hop question answering, where evidence spans multiple\ndocuments. MultiHop-RAG [63] targets this challenge through linked question-answer pairs, bridge entities, and explicit\nmulti-hop query types, enabling systematic assessment of retrieval chaining, evidence linking, and document-level\nreasoning\u2014all key bottlenecks in complex RAG workflows.\nAdaptive retrieval and necessity estimation are benchmarked in RetrievalQA [86], which mixes queries requiring\nexternal retrieval with those answerable via the base LLM alone. This design tests whether models can intelligently\ntoggle retrieval based on query uncertainty, supporting the development of resource-efficient, retrieval-aware systems\nthat avoid introducing unnecessary context.\nDomain-specific evaluation is exemplified by MIRAGE [48], a benchmark tailored to medical RAG. It contains 7,663\nquestions sourced from five clinical and biomedical QA datasets and incorporates real-world evaluation constraints:\nzero-shot generalization, multiple-choice formats, retrieval necessity assessment, and question-only retrieval. This\nmulti-faceted setup tests reliability under high-stakes conditions where factual errors can be consequential.\nCross-corpus and federated retrieval are explored in FeB4RAG [68], a benchmark constructed from 16 BEIR sub-\ncollections. It evaluates federated retrieval through 790 conversational queries with LLM-graded relevance judgments\nand quantifies the impact of resource selection and result merging strategies. This benchmark surfaces key risks in\nmulti-source RAG pipelines, especially retrieval inconsistency and hallucination amplification due to poor corpus\ncoordination.\nEvaluation infrastructure and reproducibility are addressed by BERGEN [52], a benchmarking library designed\nto unify assessment across RAG components. It offers modular templates for measuring retrieval precision, generation\nfaithfulness, and their interplay across datasets and model configurations. BERGEN facilitates consistent and extensible\nRAG benchmarking in both academic and applied settings.\nManuscript submitted to ACM\n\nRetrieval-Augmented Generation: A Survey\n20\nTable 4. Emerging Benchmarks for Evaluating Retrieval-Augmented Generation (RAG) Systems. This table summarizes recent\nbenchmarks developed to assess key aspects of RAG systems, including robustness, multi-hop reasoning, medical-domain adaptation,\nand federated retrieval. These benchmarks differ in evaluation granularity\u2014ranging from query-level to document-level\u2014and employ\nvaried annotation methods such as manual labeling, programmatic perturbation, and LLM-based scoring. Distinctive features, such as\nnoise stress-testing (RGB), zero-shot medical QA (MIRAGE), and federated source merging (FeB4RAG), support targeted evaluations\nof both retriever components and full RAG pipelines.\nBenchmark\nEvaluation Focus\nGranularity\nAnnotation\nType\nUnique Features\nEvaluation Tar-\nget\nRGB\nRobustness (noise, inte-\ngration, hallucination)\nQuery-context\npair\nNone\nStress\ntests\nfor\nnoise,\ncontradiction, and multi-\nsource fusion\nFull pipeline\nMultiHop-RAG\nMulti-hop\nreasoning\nand retrieval chaining\nDocument-level\nManual + derived\nLinked multi-hop queries\nand bridge-entity chaining\nFull pipeline\nRAGTruth\nHallucination\ndetec-\ntion\nand\nfactuality\nevaluation\nResponse-level\n(yes/no),\nspan-\nlevel (exact)\nHuman-labeled\n18,000+ examples, 4 hallu-\ncination types, span-level\nF1\nGenerator\nMIRAGE\nMedical domain QA\nunder real-world con-\nstraints\nQuery-level\nDataset-native\nZero-shot,\nmulti-choice,\nquestion-only\nretrieval\n(MEDRAG)\nFull pipeline\nFeB4RAG\nFederated\nretrieval\nevaluation\nDocument + re-\nsource\nLLM-labeled\nMeasures retrieval + merg-\ning across 16 BEIR sources\nRetriever\nRetrievalQA\nAdaptive retrieval ne-\ncessity detection\nQuery-level\nDerived\nQueries with and without\nneed for retrieval\nRetriever\nRAG-Bench\nRetrieval robustness to\nnoise\nQuery-level\nProgrammatic\nIrrelevant, incomplete, and\ncounterfactual\nretrieval\nnoise\nFull pipeline\nBERGEN\nRetrieval, generation,\nand joint evaluation\nQuery-context\nand\ndocument-\nlevel\nConfigurable\n(task-dependent)\nUnified benchmarking li-\nbrary across datasets and\nmodels\nFull pipeline\nThis section outlines the rapidly evolving landscape of RAG evaluation and benchmarking. Future RAG development\nhinges not only on improving generation quality but also on designing principled, scalable, and interpretable evaluation\nstrategies that reflect real-world usage and complexities. To advance the field meaningfully, the community must\nprioritize the creation of standardized, efficient, and adaptive evaluation protocols that can serve both research and\nproduction contexts.\n7\nFuture Directions\nAs Retrieval-Augmented Generation (RAG) systems continue to evolve, a number of unresolved challenges remain that\nlimit their deployment in dynamic, open-ended, and high-stakes applications. These challenges span retrieval efficiency,\nsemantic misalignment, hallucination control, generalization, and trust. Based on the synthesis of contemporary research\ngaps, we outline five interrelated future directions that represent promising trajectories for advancing the field.\n7.1\nRetrieval Adaptivity and Semantic Alignment\nCurrent RAG architectures often rely on static retrieval policies and fixed embedding transformations, limiting their\nadaptability to complex or evolving user queries. Future systems must support dynamically calibrated retrieval strategies\nManuscript submitted to ACM\n\nRetrieval-Augmented Generation: A Survey\n21\nthat adjust depth, modality, and source selection in response to task difficulty and contextual cues. This calls for co-\noptimized retriever\u2013generator pipelines that leverage reinforcement signals, uncertainty estimates, or semantic control\nlayers to align evidence retrieval with generative intent in real time.\n7.2\nRobustness under Noise and Adversarial Conditions\nDespite recent advances in noise filtering and adversarial training, RAG systems remain vulnerable to retrieval per-\nturbations, misleading content, and corpus-level poisoning attacks. Future work should move toward retrieval-aware\nadversarial defenses that incorporate noise-aware loss functions, retrieval-type-specific regularization, and semantic\nprovenance filtering. This includes evaluation protocols that stress-test systems against contextually plausible yet\nmisleading passages and group-triggered semantic attacks, as exemplified by recent backdoor threat models.\n7.3\nMulti-Hop Reasoning and Structured Compositionality\nMany knowledge-intensive tasks require aggregating evidence across multiple retrieval steps and reasoning over entity\nor schema-level structures. Current models exhibit limited capacity for compositional inference or procedural synthesis.\nFuture RAG systems should support multi-turn retrieval\u2013generation loops, structured subgoal decomposition, and\ngraph-augmented reasoning pipelines that maintain discourse coherence and entity consistency across long-range\ndependencies.\n7.4\nCross-Domain Generalization and Temporal Adaptivity\nRAG performance often degrades in the face of domain shifts, novel schema, or temporal drift. Addressing this\nwill require pretraining retrieval modules on diverse proxy tasks, developing meta-retrievers capable of adapting to\nunseen query distributions, and incorporating recency-aware document scoring. Additionally, the design of temporally\nevolving benchmarks and evaluation suites will be necessary to assess the robustness of RAG systems under realistic,\ntime-sensitive knowledge conditions.\n7.5\nExplainability, Personalization, and Trust Calibration\nAs RAG systems are increasingly integrated into user-facing applications, demands for interpretability, personalization,\nand secure behavior intensify. Future architectures should expose transparent interfaces for explaining retrieval\ndecisions and generation provenance, while supporting privacy-preserving personalization through user-clustered\nretrieval, memory-efficient modeling, or differential privacy mechanisms. Furthermore, integrating retrieval calibration\nsignals\u2014such as factual salience, source trustworthiness, or hallucination risk\u2014can enhance user trust and system\naccountability.\nReferences\n[1] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024. Self-RAG: Learning to Retrieve, Generate, and Critique through\nSelf-Reflection. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=hSyW5go0v8\n[2] Orlando Ayala and Patrice Bechard. 2024. Reducing hallucination in structured outputs via Retrieval-Augmented Generation. In Proceedings\nof the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6:\nIndustry Track), Yi Yang, Aida Davani, Avi Sil, and Anoop Kumar (Eds.). Association for Computational Linguistics, Mexico City, Mexico, 228\u2013238.\ndoi:10.18653/v1/2024.naacl-industry.19\n[3] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri\nNguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. 2018. MS MARCO: A Human Generated MAchine Reading\nCOmprehension Dataset. arXiv:1611.09268 [cs.CL] https://arxiv.org/abs/1611.09268\nManuscript submitted to ACM\n\nRetrieval-Augmented Generation: A Survey\n22\n[4] Scott Barnett, Stefanus Kurniawan, Srikanth Thudumu, Zach Brannelly, and Mohamed Abdelrazek. 2024. Seven Failure Points When Engineering a\nRetrieval Augmented Generation System. In Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for\nAI (Lisbon, Portugal) (CAIN \u201924). Association for Computing Machinery, New York, NY, USA, 194\u2013199. doi:10.1145/3644815.3644945\n[5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey\nWu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam\nMcCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Proceedings of the 34th International\nConference on Neural Information Processing Systems (Vancouver, BC, Canada) (NIPS \u201920). Curran Associates Inc., Red Hook, NY, USA, Article 159,\n25 pages.\n[6] Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo, and Jie Fu. 2024. RQ-RAG: Learning to Refine Queries for Retrieval\nAugmented Generation. In First Conference on Language Modeling. https://openreview.net/forum?id=tzE7VqsaJ4\n[7] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. Benchmarking large language models in retrieval-augmented generation. In Proceedings\nof the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and\nFourteenth Symposium on Educational Advances in Artificial Intelligence (AAAI\u201924/IAAI\u201924/EAAI\u201924). AAAI Press, Article 1980, 9 pages. doi:10.1609/\naaai.v38i16.29728\n[8] Pengzhou Cheng, Yidong Ding, Tianjie Ju, Zongru Wu, Wei Du, Haodong Zhao, Ping Yi, Zhuosheng Zhang, and Gongshen Liu. 2024. TrojanRAG:\nRetrieval-Augmented Generation Can Be Backdoor Driver in Large Language Models. https://openreview.net/forum?id=RfYD6v829Y\n[9] Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan. 2023. Lift yourself up: retrieval-augmented text generation with\nself-memory. In Proceedings of the 37th International Conference on Neural Information Processing Systems (New Orleans, LA, USA) (NIPS \u201923). Curran\nAssociates Inc., Red Hook, NY, USA, Article 1899, 20 pages.\n[10] Xin Cheng, Xun Wang, Xingxing Zhang, Tao Ge, Si-Qing Chen, Furu Wei, Huishuai Zhang, and Dongyan Zhao. 2024. xRAG: Extreme Context\nCompression for Retrieval-augmented Generation with One Token. In The Thirty-eighth Annual Conference on Neural Information Processing Systems.\nhttps://openreview.net/forum?id=6pTlXqrO0p\n[11] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,\nIon Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. https://lmsys.org/blog/2023-03-\n30-vicuna/\n[12] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have Solved\nQuestion Answering? Try ARC, the AI2 Reasoning Challenge. arXiv:1803.05457 [cs.AI] https://arxiv.org/abs/1803.05457\n[13] Gordon V. Cormack, Charles L A Clarke, and Stefan Buettcher. 2009. Reciprocal rank fusion outperforms condorcet and individual rank learning\nmethods. In Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval (Boston, MA, USA)\n(SIGIR \u201909). Association for Computing Machinery, New York, NY, USA, 758\u2013759. doi:10.1145/1571941.1572114\n[14] Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio Silvestri.\n2024. The Power of Noise: Redefining Retrieval for RAG Systems. In Proceedings of the 47th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval (Washington DC, USA) (SIGIR \u201924). Association for Computing Machinery, New York, NY, USA, 719\u2013729.\ndoi:10.1145/3626772.3657834\n[15] Nguyen Nam Doan, Aki H\u00e4rm\u00e4, Remzi Celebi, and Valeria Gottardo. 2024. A Hybrid Retrieval Approach for Advancing Retrieval-Augmented\nGeneration Systems. In Proceedings of the 7th International Conference on Natural Language and Speech Processing (ICNLSP 2024), Mourad Abbas and\nAbed Alhakim Freihat (Eds.). Association for Computational Linguistics, Trento, 397\u2013409. https://aclanthology.org/2024.icnlsp-1.41/\n[16] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa\nNess, and Jonathan Larson. 2025. From Local to Global: A Graph RAG Approach to Query-Focused Summarization. arXiv:2404.16130 [cs.CL]\nhttps://arxiv.org/abs/2404.16130\n[17] Shahul Es, Jithin James, Luis Espinosa Anke, and Steven Schockaert. 2024. RAGAs: Automated Evaluation of Retrieval Augmented Generation. In\nProceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, Nikolaos Aletras\nand Orphee De Clercq (Eds.). Association for Computational Linguistics, St. Julians, Malta, 150\u2013158. https://aclanthology.org/2024.eacl-demo.16/\n[18] Feiteng Fang, Yuelin Bai, Shiwen Ni, Min Yang, Xiaojun Chen, and Ruifeng Xu. 2024. Enhancing Noise Robustness of Retrieval-Augmented Language\nModels with Adaptive Adversarial Training. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 10028\u201310039.\ndoi:10.18653/v1/2024.acl-long.540\n[19] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun, Haofen Wang, and Haofen Wang. 2023. Retrieval-\naugmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997 2 (2023), 1.\n[20] Michael Glass, Gaetano Rossiello, Md Faisal Mahbub Chowdhury, Ankita Naik, Pengshan Cai, and Alfio Gliozzo. 2022. Re2G: Retrieve, Rerank,\nGenerate. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz (Eds.). Association for Computational Linguistics, Seattle,\nUnited States, 2701\u20132715. doi:10.18653/v1/2022.naacl-main.194\n[21] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan\nSchelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev,\nManuscript submitted to ACM\n\nRetrieval-Augmented Generation: A Survey\n23\nArthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie\nChern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne\nWong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu,\nDhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily\nDinan, Eric Michael Smith, Filip Radenovic, Francisco Guzm\u00e1n, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind\nThattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,\nImanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay\nMahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie\nWang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik\nPrasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal\nBhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan,\nLubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas,\nMaria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan,\nNaman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur \u00c7elebi, Patrick Alrassy,\nPengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He,\nQingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari,\nRohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini,\nSahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng\nShen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin\nGururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor\nMihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish\nVogeti, V\u00edtor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang,\nXiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song,\nYuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava,\nAbha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei\nBaevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu,\nAndrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury,\nAshley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd,\nBeto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido,\nBritt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu,\nChris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu,\nDavide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil,\nElaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei\nSun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella\nSchwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid\nShojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan\nZhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice\nLam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian\nJin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U,\nKaran Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun\nHuang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron\nMoshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso,\nMaxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik\nVyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal,\nNandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich\nLaptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan\nBalaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad\nAlao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan,\nRobin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun\nDhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay,\nSheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal,\nSoji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer\nDeng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler,\nThomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez,\nVijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang,\nWenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen,\nManuscript submitted to ACM\n\nRetrieval-Augmented Generation: A Survey\n24\nYe Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi\nHe, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. 2024. The Llama 3 Herd of Models.\narXiv:2407.21783 [cs.AI] https://arxiv.org/abs/2407.21783\n[22] Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, and Bryan Hooi. 2024. G-Retriever: Retrieval-\nAugmented Generation for Textual Graph Understanding and Question Answering. In The Thirty-eighth Annual Conference on Neural Information\nProcessing Systems. https://openreview.net/forum?id=MPJ3oXtTZl\n[23] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing A Multi-hop QA Dataset for Comprehensive Evaluation\nof Reasoning Steps. In Proceedings of the 28th International Conference on Computational Linguistics, Donia Scott, Nuria Bel, and Chengqing Zong\n(Eds.). International Committee on Computational Linguistics, Barcelona, Spain (Online), 6609\u20136625. doi:10.18653/v1/2020.coling-main.580\n[24] Sebastian Hofst\u00e4tter, Jiecao Chen, Karthik Raman, and Hamed Zamani. 2023. FiD-Light: Efficient and Effective Retrieval-Augmented Text Generation.\nIn Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (Taipei, Taiwan) (SIGIR \u201923).\nAssociation for Computing Machinery, New York, NY, USA, 1437\u20131447. doi:10.1145/3539618.3591687\n[25] Jie Huang, Mo Wang, Yunpeng Cui, Juan Liu, Li Chen, Ting Wang, Huan Li, and Jinming Wu. 2024. Layered Query Retrieval: An Adaptive\nFramework for Retrieval-Augmented Generation in Complex Question Answering for Large Language Models. Applied Sciences 14, 23 (2024).\ndoi:10.3390/app142311014\n[26] Wenyu Huang, Mirella Lapata, Pavlos Vougiouklis, Nikos Papasarantopoulos, and Jeff Pan. 2023. Retrieval Augmented Generation with Rich Answer\nEncoding. In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of\nthe Association for Computational Linguistics (Volume 1: Long Papers), Jong C. Park, Yuki Arase, Baotian Hu, Wei Lu, Derry Wijaya, Ayu Purwarianti,\nand Adila Alfa Krisnadhi (Eds.). Association for Computational Linguistics, Nusa Dua, Bali, 1012\u20131025. doi:10.18653/v1/2023.ijcnlp-main.65\n[27] Gautier Izacard and Edouard Grave. 2021. Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering. In\nProceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Paola Merlo, Jorg Tiedemann,\nand Reut Tsarfaty (Eds.). Association for Computational Linguistics, Online, 874\u2013880. doi:10.18653/v1/2021.eacl-main.74\n[28] Jisoo Jang and Wen-Syan Li. 2024. AU-RAG: Agent-based Universal Retrieval Augmented Generation. In Proceedings of the 2024 Annual International\nACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region (Tokyo, Japan) (SIGIR-AP 2024). Association\nfor Computing Machinery, New York, NY, USA, 2\u201311. doi:10.1145/3673791.3698416\n[29] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna\nLengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang,\nTimoth\u00e9e Lacroix, and William El Sayed. 2023. Mistral 7B. arXiv:2310.06825 [cs.CL] https://arxiv.org/abs/2310.06825\n[30] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las\nCasas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\u00e9lio Renard Lavaud, Lucile Saulnier, Marie-Anne\nLachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang,\nTimoth\u00e9e Lacroix, and William El Sayed. 2024. Mixtral of Experts. arXiv:2401.04088 [cs.LG] https://arxiv.org/abs/2401.04088\n[31] Ziyan Jiang, Xueguang Ma, and Wenhu Chen. 2024.\nLongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs.\narXiv:2406.15319 [cs.CL] https://arxiv.org/abs/2406.15319\n[32] Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active\nRetrieval Augmented Generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan\nPino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 7969\u20137992. doi:10.18653/v1/2023.emnlp-main.495\n[33] Chao Jin, Zili Zhang, Xuanlin Jiang, Fangyue Liu, Xin Liu, Xuanzhe Liu, and Xin Jin. 2024. RAGCache: Efficient Knowledge Caching for Retrieval-\nAugmented Generation. arXiv:2404.12457 [cs.DC] https://arxiv.org/abs/2404.12457\n[34] Hailey Joren, Jianyi Zhang, Chun-Sung Ferng, Da-Cheng Juan, Ankur Taly, and Cyrus Rashtchian. 2025. Sufficient Context: A New Lens on Retrieval\nAugmented Generation Systems. In The Thirteenth International Conference on Learning Representations. https://openreview.net/forum?id=Jjr2Odj8DJ\n[35] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading\nComprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Regina Barzilay\nand Min-Yen Kan (Eds.). Association for Computational Linguistics, Vancouver, Canada, 1601\u20131611. doi:10.18653/v1/P17-1147\n[36] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval\nfor Open-Domain Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Bonnie\nWebber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, Online, 6769\u20136781. doi:10.18653/v1/2020.emnlp-\nmain.550\n[37] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob\nDevlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov.\n2019. Natural Questions: A Benchmark for Question Answering Research. Transactions of the Association for Computational Linguistics 7 (2019),\n452\u2013466. doi:10.1162/tacl_a_00276\n[38] Hyunji Lee, Sohee Yang, Hanseok Oh, and Minjoon Seo. 2022. Generative Multi-hop Retrieval. In Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, Abu\nDhabi, United Arab Emirates, 1417\u20131436. doi:10.18653/v1/2022.emnlp-main.92\nManuscript submitted to ACM\n\nRetrieval-Augmented Generation: A Survey\n25\n[39] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics, Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (Eds.). Association for\nComputational Linguistics, Online, 7871\u20137880. doi:10.18653/v1/2020.acl-main.703\n[40] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim\nRockt\u00e4schel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems 33\n(2020), 9459\u20139474.\n[41] Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky. 2024. Retrieval Augmented Generation or Long-Context LLMs? A\nComprehensive Study and Hybrid Approach. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry\nTrack, Franck Dernoncourt, Daniel Preo\u0163iuc-Pietro, and Anastasia Shimorina (Eds.). Association for Computational Linguistics, Miami, Florida, US,\n881\u2013893. doi:10.18653/v1/2024.emnlp-industry.66\n[42] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When Not to Trust Language Models:\nInvestigating Effectiveness of Parametric and Non-Parametric Memories. In Proceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics,\nToronto, Canada, 9802\u20139822. doi:10.18653/v1/2023.acl-long.546\n[43] Nicholas Matsumoto, Jay Moran, Hyunjun Choi, Miguel E Hernandez, Mythreye Venkatesan, Paul Wang, and Jason H Moore. 2024. KRAGEN: a\nknowledge graph-enhanced RAG framework for biomedical problem solving using large language models. Bioinformatics 40, 6 (2024), btae353.\n[44] Chuan Meng, Negar Arabzadeh, Arian Askari, Mohammad Aliannejadi, and Maarten de Rijke. 2024. Ranked List Truncation for Large Language\nModel-based Re-Ranking. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval\n(Washington DC, USA) (SIGIR \u201924). Association for Computing Machinery, New York, NY, USA, 141\u2013151. doi:10.1145/3626772.3657864\n[45] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\nFActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. In Proceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore,\n12076\u201312100. doi:10.18653/v1/2023.emnlp-main.741\n[46] Cheng Niu, Yuanhao Wu, Juno Zhu, Siliang Xu, KaShun Shum, Randy Zhong, Juntong Song, and Tong Zhang. 2024. RAGTruth: A Hallucination\nCorpus for Developing Trustworthy Retrieval-Augmented Language Models. In Proceedings of the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics,\nBangkok, Thailand, 10862\u201310878. doi:10.18653/v1/2024.acl-long.585\n[47] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt,\nSam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff\nBelgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa\nBrakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson,\nRory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester\nCho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien\nDeville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,\nSim\u00f3n Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha\nGontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse\nHan, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton,\nKenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino\nJomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, \u0141ukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan\nKilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, \u0141ukasz Kondraciuk,\nAndrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung,\nDaniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim\nMalfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney,\nChristine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin,\nVinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David M\u00e9ly, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,\nArvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O\u2019Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano,\nGiambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres,\nMichael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power,\nElizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri\nRoussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam,\nKyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama,\nIan Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak,\nMadeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cer\u00f3n Uribe,\nAndrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ\nWeinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah\nManuscript submitted to ACM\n\nRetrieval-Augmented Generation: A Survey\n26\nWong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan\nZellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2024. GPT-4 Technical\nReport. arXiv:2303.08774 [cs.CL] https://arxiv.org/abs/2303.08774\n[48] Shintaro Ozaki, Yuta Kato, Siyuan Feng, Masayo Tomita, Kazuki Hayashi, Wataru Hashimoto, Ryoma Obara, Masafumi Oyamada, Katsuhiko Hayashi,\nHidetaka Kamigaito, and Taro Watanabe. 2025. Understanding the Impact of Confidence in Retrieval Augmented Generation: A Case Study in the\nMedical Domain. arXiv:2412.20309 [cs.CL] https://arxiv.org/abs/2412.20309\n[49] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin,\nJean Maillard, Vassilis Plachouras, Tim Rockt\u00e4schel, and Sebastian Riedel. 2021. KILT: a Benchmark for Knowledge Intensive Language Tasks. In\nProceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,\nKristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and\nYichao Zhou (Eds.). Association for Computational Linguistics, Online, 2523\u20132544. doi:10.18653/v1/2021.naacl-main.200\n[50] Zackary Rackauckas. 2024. Rag-Fusion: A New Take on Retrieval Augmented Generation. International Journal on Natural Language Computing 13,\n1 (Feb. 2024), 37\u201347. doi:10.5121/ijnlc.2024.13103\n[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text transformer. Journal of machine learning research 21, 140 (2020), 1\u201367.\n[52] David Rau, Herv\u00e9 D\u00e9jean, Nadezhda Chirkova, Thibault Formal, Shuai Wang, St\u00e9phane Clinchant, and Vassilina Nikoulina. 2024. BERGEN:\nA Benchmarking Library for Retrieval-Augmented Generation. In Findings of the Association for Computational Linguistics: EMNLP 2024, Yaser\nAl-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 7640\u20137663. doi:10.18653/v1/\n2024.findings-emnlp.449\n[53] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy P. Lillicrap, Jean-Baptiste Alayrac, Radu Soricut, Angeliki Lazaridou,\nOrhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud, Andrew M. Dai, Katie Millican, Ethan Dyer, Mia Glaese,\nThibault Sottiaux, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James Molloy, Jilin Chen, Michael Isard, Paul Barham, Tom\nHennigan, Ross McIlroy, Melvin Johnson, Johan Schalkwyk, Eli Collins, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Clemens Meyer,\nGregory Thornton, Zhen Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher, Ankesh Anand, Richard Ives, James Keeling, Karel Lenc,\nSalem Haykal, Siamak Shakeri, Pranav Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezener, and et al. 2024. Gemini 1.5:\nUnlocking multimodal understanding across millions of tokens of context. CoRR abs/2403.05530 (2024). https://doi.org/10.48550/arXiv.2403.05530\n[54] Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends\u00ae in Information\nRetrieval 3, 4 (2009), 333\u2013389.\n[55] Jon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. 2024. ARES: An Automated Evaluation Framework for Retrieval-Augmented\nGeneration Systems. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies (Volume 1: Long Papers), Kevin Duh, Helena Gomez, and Steven Bethard (Eds.). Association for Computational Linguistics,\nMexico City, Mexico, 338\u2013354. doi:10.18653/v1/2024.naacl-long.20\n[56] Alireza Salemi and Hamed Zamani. 2024. Evaluating Retrieval Quality in Retrieval-Augmented Generation. In Proceedings of the 47th International\nACM SIGIR Conference on Research and Development in Information Retrieval (Washington DC, USA) (SIGIR \u201924). Association for Computing\nMachinery, New York, NY, USA, 2395\u20132400. doi:10.1145/3626772.3657957\n[57] Alireza Salemi and Hamed Zamani. 2024. Towards a Search Engine for Machines: Unified Ranking for Multiple Retrieval-Augmented Large Language\nModels. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (Washington DC, USA)\n(SIGIR \u201924). Association for Computing Machinery, New York, NY, USA, 741\u2013751. doi:10.1145/3626772.3657733\n[58] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Richard James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2024. REPLUG:\nRetrieval-Augmented Black-Box Language Models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies (Volume 1: Long Papers), Kevin Duh, Helena Gomez, and Steven Bethard (Eds.). Association\nfor Computational Linguistics, Mexico City, Mexico, 8371\u20138384. doi:10.18653/v1/2024.naacl-long.463\n[59] Zhengliang Shi, Shuo Zhang, Weiwei Sun, Shen Gao, Pengjie Ren, Zhumin Chen, and Zhaochun Ren. 2024. Generate-then-Ground in Retrieval-\nAugmented Generation for Multi-hop Question Answering. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand,\n7339\u20137353. doi:10.18653/v1/2024.acl-long.397\n[60] Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. 2022. ASQA: Factoid Questions Meet Long-Form Answers. In Proceedings of the\n2022 Conference on Empirical Methods in Natural Language Processing, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for\nComputational Linguistics, Abu Dhabi, United Arab Emirates, 8273\u20138288. doi:10.18653/v1/2022.emnlp-main.566\n[61] Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, and Yiqun Liu. 2024. DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time\nInformation Needs of Large Language Models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 12991\u201313013.\ndoi:10.18653/v1/2024.acl-long.702\n[62] Viju Sudhi, Sinchana Ramakanth Bhat, Max Rudat, and Roman Teucher. 2024. RAG-Ex: A Generic Framework for Explaining Retrieval Augmented\nGeneration. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (Washington DC,\nUSA) (SIGIR \u201924). Association for Computing Machinery, New York, NY, USA, 2776\u20132780. doi:10.1145/3626772.3657660\nManuscript submitted to ACM\n\nRetrieval-Augmented Generation: A Survey\n27\n[63] Yixuan Tang and Yi Yang. 2024. MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries. In First Conference on\nLanguage Modeling. https://openreview.net/forum?id=t4eB3zYWBK\n[64] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi\u00e8re, Mihir Sanjay\nKale, Juliette Love, Pouya Tafti, L\u00e9onard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex\nCastro-Ros, Ambrose Slone, Am\u00e9lie H\u00e9liou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan,\nChristopher A. Choquette-Choo, Cl\u00e9ment Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan,\nGeorge Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James\nKeeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine\nLee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Miku\u0142a, Mateo Wirth, Michael Sharman, Nikolai\nChinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona\nComanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas,\nShree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed,\nZhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Cl\u00e9ment Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin\nGhahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen\nKenealy. 2024. Gemma: Open Models Based on Gemini Research and Technology. arXiv:2403.08295 [cs.CL] https://arxiv.org/abs/2403.08295\n[65] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a Large-scale Dataset for Fact Extraction and\nVERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), Marilyn Walker, Heng Ji, and Amanda Stent (Eds.). Association for Computational Linguistics, New Orleans,\nLouisiana, 809\u2013819. doi:10.18653/v1/N18-1074\n[66] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,\nShruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin\nFu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor\nKerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich,\nYinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi\nRungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina\nWilliams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez,\nRobert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv:2307.09288 [cs.CL]\nhttps://arxiv.org/abs/2307.09288\n[67] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. MuSiQue: Multihop Questions via Single-hop Question\nComposition. Transactions of the Association for Computational Linguistics 10 (2022), 539\u2013554. doi:10.1162/tacl_a_00475\n[68] Shuai Wang, Ekaterina Khramtsova, Shengyao Zhuang, and Guido Zuccon. 2024. FeB4RAG: Evaluating Federated Search in the Context of Retrieval\nAugmented Generation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval\n(Washington DC, USA) (SIGIR \u201924). Association for Computing Machinery, New York, NY, USA, 763\u2013773. doi:10.1145/3626772.3657853\n[69] Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez, and Graham Neubig. 2023. Learning to filter context for retrieval-augmented\ngeneration. arXiv preprint arXiv:2311.08377 (2023).\n[70] Zheng Wang, Shu Teo, Jieer Ouyang, Yongjun Xu, and Wei Shi. 2024. M-RAG: Reinforcing Large Language Model Performance through Retrieval-\nAugmented Generation with Multiple Partitions. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 1966\u20131978.\ndoi:10.18653/v1/2024.acl-long.108\n[71] Zilong Wang, Zifeng Wang, Long Le, Steven Zheng, Swaroop Mishra, Vincent Perot, Yuwei Zhang, Anush Mattapalli, Ankur Taly, Jingbo Shang,\nChen-Yu Lee, and Tomas Pfister. 2025. Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting. In The Thirteenth International\nConference on Learning Representations. https://openreview.net/forum?id=xgQfWbV6Ey\n[72] Junde Wu, Jiayuan Zhu, and Yunli Qi. 2024. Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented\nGeneration. CoRR abs/2408.04187 (2024). https://doi.org/10.48550/arXiv.2408.04187\n[73] Ran Xu, Hui Liu, Sreyashi Nag, Zhenwei Dai, Yaochen Xie, Xianfeng Tang, Chen Luo, Yang Li, Joyce C. Ho, Carl Yang, and Qi He. 2025. SimRAG:\nSelf-Improving Retrieval-Augmented Generation for Adapting Large Language Models to Specialized Domains. In Proceedings of the 2025 Conference\nof the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers),\nLuis Chiruzzo, Alan Ritter, and Lu Wang (Eds.). Association for Computational Linguistics, Albuquerque, New Mexico, 11534\u201311550.\nhttps:\n//aclanthology.org/2025.naacl-long.575/\n[74] Sheng Xu, Mike Chen, and Shuwen Chen. 2024. Enhancing Retrieval-Augmented Generation Models with Knowledge Graphs: Innovative Practices\nThrough a Dual-Pathway Approach. In Advanced Intelligent Computing Technology and Applications: 20th International Conference, ICIC 2024, Tianjin,\nChina, August 5\u20138, 2024, Proceedings, Part VI (Tianjin, China). Springer-Verlag, Berlin, Heidelberg, 398\u2013409. doi:10.1007/978-981-97-5678-0_34\n[75] Shicheng Xu, Liang Pang, Jun Xu, Huawei Shen, and Xueqi Cheng. 2024. List-aware Reranking-Truncation Joint Model for Search and Retrieval-\naugmented Generation. In Proceedings of the ACM Web Conference 2024 (Singapore, Singapore) (WWW \u201924). Association for Computing Machinery,\nNew York, NY, USA, 1330\u20131340. doi:10.1145/3589334.3645336\nManuscript submitted to ACM\n\nRetrieval-Augmented Generation: A Survey\n28\n[76] Shicheng Xu, Liang Pang, Mo Yu, Fandong Meng, Huawei Shen, Xueqi Cheng, and Jie Zhou. 2024. Unsupervised Information Refinement Training\nof Large Language Models for Retrieval-Augmented Generation. In Proceedings of the 62nd Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok,\nThailand, 133\u2013145. doi:10.18653/v1/2024.acl-long.9\n[77] Zhentao Xu, Mark Jerome Cruz, Matthew Guevara, Tie Wang, Manasi Deshpande, Xiaofeng Wang, and Zheng Li. 2024. Retrieval-Augmented\nGeneration with Knowledge Graphs for Customer Service Question Answering. In Proceedings of the 47th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval (SIGIR 2024). ACM, 2905\u20132909. doi:10.1145/3626772.3661370\n[78] Jiaqi Xue, Mengxin Zheng, Yebowen Hu, Fei Liu, Xun Chen, and Qian Lou. 2024. BadRAG: Identifying Vulnerabilities in Retrieval Augmented\nGeneration of Large Language Models. https://openreview.net/forum?id=G2p8TLuJgy\n[79] Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024. Corrective Retrieval Augmented Generation. https://openreview.net/forum?id=\nJnWJbrnaUE\n[80] Diji Yang, Jinmeng Rao, Kezhen Chen, Xiaoyuan Guo, Yawen Zhang, Jie Yang, and Yi Zhang. 2024. IM-RAG: Multi-Round Retrieval-Augmented\nGeneration Through Learning Inner Monologues. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval (Washington DC, USA) (SIGIR \u201924). Association for Computing Machinery, New York, NY, USA, 730\u2013740. doi:10.1145/3626772.\n3657760\n[81] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A\nDataset for Diverse, Explainable Multi-hop Question Answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language\nProcessing, Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun\u2019ichi Tsujii (Eds.). Association for Computational Linguistics, Brussels, Belgium,\n2369\u20132380. doi:10.18653/v1/D18-1259\n[82] Fuda Ye, Shuangyin Li, Yongqi Zhang, and Lei Chen. 2024. R2AG: Incorporating Retrieval Information into Retrieval Augmented Generation. In\nFindings of the Association for Computational Linguistics: EMNLP 2024, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for\nComputational Linguistics, Miami, Florida, USA, 11584\u201311596. doi:10.18653/v1/2024.findings-emnlp.678\n[83] Yue Yu, Wei Ping, Zihan Liu, Boxin Wang, Jiaxuan You, Chao Zhang, Mohammad Shoeybi, and Bryan Catanzaro. 2024.\nRankRAG: Uni-\nfying Context Ranking with Retrieval-Augmented Generation in LLMs. In NeurIPS.\nhttp://papers.nips.cc/paper_files/paper/2024/hash/\ndb93ccb6cf392f352570dd5af0a223d3-Abstract-Conference.html\n[84] Hamed Zamani and Michael Bendersky. 2024. Stochastic RAG: End-to-End Retrieval-Augmented Generation through Expected Utility Maximization.\nIn Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (Washington DC, USA) (SIGIR\n\u201924). Association for Computing Machinery, New York, NY, USA, 2641\u20132646. doi:10.1145/3626772.3657923\n[85] Shenglai Zeng, Jiankun Zhang, Pengfei He, Yiding Liu, Yue Xing, Han Xu, Jie Ren, Yi Chang, Shuaiqiang Wang, Dawei Yin, and Jiliang Tang. 2024.\nThe Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG). In Findings of the Association for Computational\nLinguistics: ACL 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand,\n4505\u20134524. doi:10.18653/v1/2024.findings-acl.267\n[86] Zihan Zhang, Meng Fang, and Ling Chen. 2024. RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain\nQuestion Answering. In Findings of the Association for Computational Linguistics: ACL 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.).\nAssociation for Computational Linguistics, Bangkok, Thailand, 6963\u20136975. doi:10.18653/v1/2024.findings-acl.415\n[87] Xinping Zhao, Dongfang Li, Yan Zhong, Boren Hu, Yibin Chen, Baotian Hu, and Min Zhang. 2024. SEER: Self-Aligned Evidence Extraction for\nRetrieval-Augmented Generation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, Yaser Al-Onaizan, Mohit\nBansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 3027\u20133041. doi:10.18653/v1/2024.emnlp-main.178\n[88] Yuanhang Zheng, Peng Li, Wei Liu, Yang Liu, Jian Luan, and Bin Wang. 2024. ToolRerank: Adaptive and Hierarchy-Aware Reranking for Tool\nRetrieval. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING\n2024), Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue (Eds.). ELRA and ICCL, Torino, Italia,\n16263\u201316273. https://aclanthology.org/2024.lrec-main.1413/\n[89] Kun Zhu, Xiaocheng Feng, Xiyuan Du, Yuxuan Gu, Weijiang Yu, Haotian Wang, Qianglong Chen, Zheng Chu, Jingchang Chen, and Bing Qin. 2024.\nAn Information Bottleneck Perspective for Effective Noise Filtering on Retrieval-Augmented Generation. In Proceedings of the 62nd Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for\nComputational Linguistics, Bangkok, Thailand, 1044\u20131069. doi:10.18653/v1/2024.acl-long.59\n[90] Yun Zhu, Jia-Chen Gu, Caitlin Sikora, Ho Ko, Yinxiao Liu, Chu-Cheng Lin, Lei Shu, Liangchen Luo, Lei Meng, Bang Liu, and Jindong Chen. 2025.\nAccelerating Inference of Retrieval-Augmented Generation via Sparse Context Selection. In The Thirteenth International Conference on Learning\nRepresentations. https://openreview.net/forum?id=HE6pJoNnFp\nA\nAppendix\nTo support transparency and reproducibility, we include in the Appendix the original benchmark scores reported in the\nprimary publications of each RAG framework. These tables serve as the empirical source for the relative improvement\nManuscript submitted to ACM\n\nRetrieval-Augmented Generation: A Survey\n29\nanalyses presented in Sections 5. All values are cited from original papers, preserving reported metrics such as F1, EM,\nAccuracy, and FactScore. Where applicable, dataset splits, backbone models, and evaluation metrics are clearly labeled\nto ensure traceability.\nManuscript submitted to ACM\n\nRetrieval-Augmented Generation: A Survey\n30\nTable 5. Reported Performance Scores for Short-Form QA Frameworks. Accuracy and Exact Match (EM) scores as reported in\nthe original publications of short-form RAG frameworks. These values were used to compute the normalized improvements presented\nin Section 5.\nTaxonomy\nFramework\nBackbone\nDataset\nMetric\nRaw LLM\nLLM+Retrieval\nFramework Score\nRetriever-Based RAG\nRQ-RAG\nLLaMA2-7B\nPopQA\nAcc\n14.7\n39.8\n57.1\nRQ-RAG\nLLaMA2-7B\nARC-Challenge\nAcc\n21.8\n28.7\n68.3\nSimRAG\nLLaMA3-8B\nARC-Challenge\nAcc\n\u2013\n71.08\n81.4\nSimRAG\nLLaMA3-8B\nSciQ\nEM\n\u2013\n20.8\n57.5\nSimRAG\nGemma2-27B\nARC-Challenge\nAcc\n\u2013\n85.75\n88.65\nSimRAG\nGemma2-27B\nSciQ\nEM\n\u2013\n44.8\n58.1\nRe2G\nBART Large\nNQ\nAcc\n45.22\n\u2013\n51.73\nRe2G\nBART Large\nTriviaQA\nAcc\n60.99\n\u2013\n76.27\nFILCO\nLLaMA2-7B (Top-5)\nNQ\nEM\n\u2013\n47.6\n61.8\nFILCO\nLLaMA2-7B (Top-5)\nTriviaQA\nEM\n\u2013\n67.3\n71.1\nGenerator-Based RAG\nSELF-RAG\nLLaMA2-7B\nPopQA\nAcc\n14.7\n38.2\n54.9\nSELF-RAG\nLLaMA2-7B\nTriviaQA\nAcc\n30.5\n42.5\n66.4\nSELF-RAG\nLLaMA2-7B\nARC-Challenge\nAcc\n21.8\n48.0\n67.4\nSELF-RAG\nLLaMA2-13B\nPopQA\nAcc\n14.7\n45.7\n55.8\nSELF-RAG\nLLaMA2-13B\nTriviaQA\nAcc\n38.5\n47.0\n69.3\nxRAG\nMistral-7B\nNQ\nEM\n30.25\n42.71\n39.1\nxRAG\nMistral-7B\nTriviaQA\nEM\n57.08\n65.88\n65.77\nxRAG\nMistral-7B\nWebQA\nEM\n34.89\n37.84\n39.4\nxRAG\nMixtral-8x7B\nNQ\nEM\n41.99\n45.15\n47.28\nxRAG\nMixtral-8x7B\nTriviaQA\nEM\n71.1\n70.34\n74.14\nxRAG\nMixtral-8x7B\nWebQA\nEM\n40.31\n41.26\n44.5\nFiD-Light\nFiD+DPR\nTriviaQA\nEM\n48.6\n\u2013\n57.6\nFiD-Light\nFiD+DPR\nNQ\nEM\n41.9\n\u2013\n53.2\nR2AG\nLLaMA2-7B\nNQ\nAcc\n0.38\n\u2013\n0.693\nSELF-RAG\nLLaMA2-7B\nNQ\nAcc\n0.38\n\u2013\n0.188\nHybrid RAG\nStochastic RAG\nFiD-Light (T5-Base)\nNQ\nEM\n\u2013\n45.6\n46.2\nStochastic RAG\nFiD-Light (T5-Base)\nTriviaQA\nEM\n\u2013\n57.6\n59.7\nStochastic RAG\nFiD-Light (T5-XL)\nNQ\nEM\n\u2013\n51.1\n53.0\nStochastic RAG\nFiD-Light (T5-XL)\nTriviaQA\nEM\n\u2013\n63.7\n64.7\nCRAG\nLLaMA2-7B\nNQ\nAcc\n0.38\n\u2013\n0.397\nCRAG\nLLaMA2-7B\nPopQA\nAcc\n14.7\n40.3\n59.3\nCRAG\nLLaMA2-7B\nARC-Challenge\nAcc\n21.8\n46.7\n54.8\nSelf-CRAG\nLLaMA2-7B\nPopQA\nAcc\n14.7\n40.3\n61.8\nSelf-CRAG\nLLaMA2-7B\nARC-Challenge\nAcc\n21.8\n46.7\n67.2\nTA-ARE\nGPT-3.5\nRetrievalQA\nAcc\n1.2\n38.2\n35.8\nTA-ARE\nGPT-4\nRetrievalQA\nAcc\n2.4\n46.0\n46.4\nTA-ARE\nLLaMA2-7B\nRetrievalQA\nAcc\n2.0\n36.0\n30.7\nRobustness-Based RAG\nRAAT\nLLaMA2-7B\nRAG-Bench (TQA/NQ/WebQ)\nEM\n38.37\n65.4\n83.07\nManuscript submitted to ACM\n\nRetrieval-Augmented Generation: A Survey\n31\nTable 6. Reported Performance Scores for Multi-Hop QA Frameworks. Raw F1 and EM scores extracted from the original\npapers of multi-hop RAG systems, across datasets such as HotpotQA, 2Wiki, and MuSiQue. These scores form the basis of the\ncomparative analysis in Section 5.\nTaxonomy\nFramework\nBackbone\nDataset\nMetric\nRaw LLM\nLLM + Retrieval\nFramework Score\nRetriever-Based\nRQ-RAG\nLLaMA2-7B\nHotpotQA\nF1\n6.6\n16.7\n62.6\nRQ-RAG\nLLaMA2-7B\n2Wiki\nF1\n16\n18.7\n44.8\nRQ-RAG\nLLaMA2-7B\nMuSiQue\nF1\n3\n7.4\n41.7\nRankRAG\nLLaMA3-8B\nHotpotQA\nF1\n\u2013\n43.3\n46.7\nRankRAG\nLLaMA3-8B\n2Wiki\nF1\n\u2013\n27.9\n36.9\nRankRAG\nLLaMA3-70B\nHotpotQA\nF1\n\u2013\n44.6\n55.4\nRankRAG\nLLaMA3-70B\n2Wiki\nF1\n\u2013\n31.9\n43.9\nLQR\nLLaMA3-8B\nMuSiQue\nF1\n10.7\n22.8\n41.97\nLQR\nLLaMA3-8B\nHotpotQA\nF1\n22.71\n46.15\n69.96\nLQR\nLLaMA3-8B\n2Wiki\nF1\n32.04\n47.9\n54.65\nLongRAG\nGPT-4o\nHotpotQA\nEM\n42.4\n\u2013\n64.3\nLongRAG\nGemini-1.5-Pro\nHotpotQA\nEM\n33.9\n\u2013\n57.5\nSEER\nLLaMA2-7B-Chat\nHotpotQA\nF1\n0.5471\n0.5826\n0.604\nFILCO\nLLaMA2-7B\nHotpotQA\nEM\n\u2013\n61.5\n65\nGenerator-Based\nR2AG\nLLaMA2-7B\nHotpotQA\nF1\n8.52\n\u2013\n36.05\nR2AG\nLLaMA2-7B\nMuSiQue\nF1\n2.41\n\u2013\n16.87\nR2AG\nLLaMA2-7B\n2Wiki\nF1\n6.34\n\u2013\n34.52\nxRAG\nMistral-7B\nHotpotQA\nEM\n27.02\n38.79\n34.05\nxRAG\nMixtral-8x7B\nHotpotQA\nEM\n32.87\n43.46\n39.66\nINFO-RAG\nLLaMA2-7B\nHotpotQA\nEM\n39.4\n\u2013\n46.56\nINFO-RAG\nLLaMA2-13B\nHotpotQA\nEM\n42.12\n\u2013\n51.48\nINFO-RAG\nLLaMA2-13B-chat\nHotpotQA\nEM\n61.23\n\u2013\n61.91\nINFO-RAG\nLLaMA2-7B\nMuSiQue\nEM\n25.95\n\u2013\n30.19\nINFO-RAG\nLLaMA2-13B\nMuSiQue\nEM\n25.78\n\u2013\n35.02\nINFO-RAG\nLLaMA2-13B-chat\nMuSiQue\nEM\n47.06\n\u2013\n47.93\nHybrid\nDRAGIN\nLLaMA2-13B-chat\nHotpotQA\nF1\n30.97\n37.06\n42.38\nDRAGIN\nLLaMA2-13B-chat\n2Wiki\nF1\n27.21\n33.64\n39.31\nDRAGIN\nLLaMA2-7B-chat\nHotpotQA\nF1\n27.45\n24.99\n33.44\nDRAGIN\nLLaMA2-7B-chat\n2Wiki\nF1\n22.32\n25.49\n29.26\nDRAGIN\nVicuna-13B-v1.5\nHotpotQA\nF1\n32.56\n35.31\n41.64\nDRAGIN\nVicuna-13B-v1.5\n2Wiki\nF1\n22.32\n25.64\n35.16\nFLAREdirect\nGPT-3.5\n2Wiki\nF1\n36.8\n48.8\n59.7\nFLAREinstruct\nGPT-3.5\n2Wiki\nF1\n36.8\n48.8\n49.8\nGenGround\nGPT-3.5\nHotpotQA\nF1\n42.28\n47.8\n52.26\nGenGround\nGPT-3.5\nMuSiQue\nF1\n20.13\n20.11\n27.36\nGenGround\nGPT-3.5\n2Wiki\nF1\n41.19\n44.77\n50.21\nGenGround\nGPT-3.5\nStrategyQA\nF1\n68.13\n71.78\n77.12\nStochastic RAG\nFiD-Light (T5-Base)\nHotpotQA\nEM\n25.6\n\u2013\n27.3\nStochastic RAG\nFiD-Light (T5-XL)\nHotpotQA\nEM\n29.2\n\u2013\n31.1\nManuscript submitted to ACM\n\nRetrieval-Augmented Generation: A Survey\n32\nTable 7. Reported Robustness Scores for RAG Frameworks. Precision, recall, and FactScore values extracted from original\npublications, across multiple datasets. These scores serve as the empirical basis for the comparative robustness analysis in Section 5.\nTaxonomy\nFramework\nBackbone\nMetric (Dataset)\nLLM + Retrieval\nLLM + Retrieval + Framework\nRetriever-Based RAG\nFILCO\nRAG\nPrecision (FEVER)\n1.2\n5.1\nRe2G\nKGI0\nPrecision (NQ)\n64.65\n70.92\nRe2G\nKGI0\nRecall (NQ)\n69.6\n74.79\nRe2G\nKGI0\nR-Precision (NQ)\n61.13\n72.01\nRe2G\nKGI0\nRecall (TriviaQA)\n63.12\n73.16\nRe2G\nKGI0\nR-Precision (FEVER)\n80.34\n90.06\nRe2G\nKGI0\nRecall (FEVER)\n86.53\n92.91\nGenerator-Based RAG\nSELF-RAG\nLLaMA2-7B\nPrecision (ASQA)\n2.9\n66.9\nSELF-RAG\nLLaMA2-7B\nRecall (ASQA)\n4\n67.8\nSELF-RAG\nLLaMA2-7B\nFactScore (ASQA)\n78\n81.2\nSELF-RAG\nLLaMA2-13B\nPrecision (ASQA)\n2.3\n70.3\nSELF-RAG\nLLaMA2-13B\nRecall (ASQA)\n3.6\n71.3\nSELF-RAG\nLLaMA2-13B\nFactScore (ASQA)\n77.5\n80.2\nFiD-Light\nT5-Base\nFactScore (FEVER)\n\u2013\n80.6\nFiD-Light\nT5-XL\nFactScore (FEVER)\n\u2013\n84.5\nRAG w/ Rich Ans. Encoding\nRAG\nRecall (MSMARCO)\n25.3\n27.5\nRAG w/ Rich Ans. Encoding\nRAG\nRecall (KILT WoW)\n61.98\n68.63\nGenRT\nRAG\nRecall (NQ)\n59.4\n60.78\nGenRT\nRAG\nRecall (TriviaQA)\n68.22\n70.01\nHybrid RAG\nCRAG\nLLaMA2-7B\nFactScore (Biography)\n59.2\n74.1\nSelf-RAG\nLLaMA2-7B\nFactScore (Biography)\n59.2\n81.2\nSelf-CRAG\nLLaMA2-7B\nFactScore (Biography)\n59.2\n86.2\nFlare Instruct\nGPT-3.5\nPrecision (2Wiki)\n48.6\n49.1\nFlare Instruct\nGPT-3.5\nRecall (2Wiki)\n51.5\n52.5\nFlare Direct\nGPT-3.5\nPrecision (2Wiki)\n48.6\n59.1\nFlare Direct\nGPT-3.5\nRecall (2Wiki)\n51.5\n62.6\nStochastic RAG\nFiD-Light (T5-Base)\nFactScore (FEVER)\n80.6\n81.3\nStochastic RAG\nFiD-Light (T5-XL)\nFactScore (FEVER)\n84.5\n84.8\nDRAGIN\nLLaMA2-13B\nPrecision (HotPotQA)\n0.3711\n0.4401\nDRAGIN\nLLaMA2-13B\nRecall (HotPotQA)\n0.374\n0.411\nDRAGIN\nVICUNA-13B\nPrecision (HotPotQA)\n0.3457\n0.4226\nDRAGIN\nVICUNA-13B\nRecall (HotPotQA)\n0.352\n0.389\nManuscript submitted to ACM\n"}