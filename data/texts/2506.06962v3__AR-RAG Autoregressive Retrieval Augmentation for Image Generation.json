{"metadata": {"pdf_filename": "2506.06962v3__AR-RAG Autoregressive Retrieval Augmentation for Image Generation.pdf", "source": "arXiv"}, "text": "arXiv:2506.06962v3  [cs.CV]  14 Jun 2025\nAR-RAG: Autoregressive Retrieval Augmentation for\nImage Generation\nJingyuan Qi* 1\nZhiyang Xu* 1\nQifan Wang2\nLifu Huang3\n1Virginia Tech\n2Meta\n3 UC Davis\njingyq1@vt.edu\n(a) Vanilla Image Generation\nPrompt\n(c) Patch-based Autoregressive Retrieval Augmentation (Ours)\n...\nAugmentation\nGeneration\nPrompt\nGenerated Image\nGenerated Image\n(b) Image-Based Retrieval Augmentation\nPrompt\nGenerated Image\nRetrieved Images\n...\nQuery\nKey\nQuery\nKey\nQuery\nValue\nKey\nRetrieval\nValue\nValue\n?\n?\nNext Image Patch\n?\nFigure 1: Comparison between Autoregressive Retrieval Augmentation (AR-RAG) for image\ngeneration in (c) and existing image generation paradigms in (a) (b). In AR-RAG, image patches in\nred boxes denote retrieval queries and keys, image patches in blue boxes are retrieved values, and\ngray boxes with the question mark are next image patches to be predicted. (Caption: A white cat is\nplaying basketball on the court.)\nAbstract\nWe introduce Autoregressive Retrieval Augmentation (AR-RAG), a novel\nparadigm that enhances image generation by autoregressively incorporating k-\nnearest neighbor retrievals at the patch level. Unlike prior methods that perform\na single, static retrieval before generation and condition the entire generation on\nfixed reference images, AR-RAG performs context-aware retrievals at each gen-\neration step, using prior-generated patches as queries to retrieve and incorporate\nthe most relevant patch-level visual references, enabling the model to respond to\nevolving generation needs while avoiding limitations (e.g., over-copying, stylis-\ntic bias, etc.) prevalent in existing methods. To realize AR-RAG, we propose\ntwo parallel frameworks: (1) Distribution-Augmentation in Decoding (DAiD),\na training-free plug-and-use decoding strategy that directly merges the distribu-\ntion of model-predicted patches with the distribution of retrieved patches, and\n(2) Feature-Augmentation in Decoding (FAiD), a parameter-efficient fine-tuning\nmethod that progressively smooths the features of retrieved patches via multi-scale\nconvolution operations and leverages them to augment the image generation pro-\ncess. We validate the effectiveness of AR-RAG on widely adopted benchmarks,\n1Jingyuan Qi and Zhiyang Xu contributed equally to this work.\nPreprint. Under review.\n\nincluding Midjourney-30K, GenEval and DPG-Bench, demonstrating significant\nperformance gains over state-of-the-art image generation models.1\n1\nIntroduction\nRecent advancements in image generation have demonstrated remarkable capabilities in producing\nphotorealistic images based on user prompts [31, 28, 7, 37, 10, 41, 9, 43, 45, 27, 6]. However,\ndespite these improvements, the generated images often exhibit local distortions and inconsistencies,\nparticularly in visual objects that possess complex structures [11], frequently interact with other\nobjects and the surrounding scene [22, 26], or are underrepresented in the training data [8]. A\npromising approach to mitigating these challenges is retrieval-augmented generation (RAG), which\nenhances the generation process by incorporating real-world images as additional references [8, 3].\nWhile RAG has been extensively explored in the language domain [23, 13], its application to image\nand multimodal generation remains largely underdeveloped. A few existing studies [3, 8, 46, 48, 49]\nbridge this gap by performing a single-step retrieval based on the input prompt prior to generation,\nconditioning the entire image generation process on fixed visual cues (Figure 1 (b)). However, as\ndemonstrated in our pilot study (Section 5.2), such static, coarse-grained retrieval approaches [3, 8, 49]\nfrequently introduce irrelevant or weakly aligned visual contents that persist throughout generation.\nSince the retrieved images are selected once, before decoding begins, and remain unchanged, these\nmethods cannot respond to the evolving generation needs, resulting in over-copying of irrelevant\ndetails, stylistic bias, and the hallucination of unrelated visual elements. For example, as shown in\nFigure 1(b), a basketball player present in the retrieved references, despite being irrelevant to the\ninput prompt, unintentionally appears in the generated image.\nIn this paper, we propose Autoregressive Retrieval Augmentation (AR-RAG), a novel retrieval-\naugmented paradigm for image generation that dynamically and autoregressively incorporates patch-\nlevel k-nearest-neighbor (k-NN) retrievals throughout the generation process (Figure 1(c)). In contrast\nto prior methods that rely on static, coarse-grained retrievals of entire reference images, typically\nusing captions as retrieval queries and keys, AR-RAG performs fine-grained, step-wise retrieval\nat the image patch level. Specifically, as generation unfolds, AR-RAG leverages the already-\ngenerated surrounding patches as localized queries to retrieve contextually similar patches from\na pre-constructed patch-level database. This database is built by encoding real-world images into\nlatent patch features, where each entry contains a patch embedding as a value and the embeddings\nof its h-hop spatial neighbors as a key. During the generation of the next target patch (gray boxes\nin Figure 1(c)), AR-RAG retrieves the top-K most relevant patches (blue boxes) by measuring\nsimilarity between the surrounding generated context patches (red boxes) and database keys (also\nred boxes). These retrieved patches are then integrated into the model to inform and enhance the\nprediction of the next patch, enabling the model to dynamically adjust to local generation needs.\nBy conditioning on the evolving generation context as retrieval queries, AR-RAG ensures that\nretrieved visual references remain relevant throughout the generation process, encouraging local\nsemantic coherence. Moreover, the patch-level retrieval allows for precise integration of visual\nelements without overcommitting to entire reference images, avoiding the limitations of over-copying\nor irrelevant conditioning observed in static retrieval.\nTo realize the AR-RAG framework, we introduce two parallel implementations: (1) Distribution-\nAugmentation in Decoding (DAiD), a training-free, plug-and-play decoding strategy that merges\nthe model\u2019s predicted patch distribution with that of the retrieved patches. Specifically, the top-K\nretrieved patches are assigned probabilities inversely proportional to their normalized \u21132 distances\ncomputed from the query and key patch embeddings. These probabilities are then linearly combined\nwith the model\u2019s native output distribution to guide the next patch prediction, enabling retrieval-aware\ngeneration without any additional training. (2) Feature-Augmentation in Decoding (FAiD), a\nparameter-efficient fine-tuning approach that integrates retrieved patches into the generation process\nthrough learned smoothing and blending mechanisms. Specifically, when generating the next image\ntoken, FAiD operates in two stages: (1) refining the retrieved patch features by adjusting them to\nbetter fit the local context of the already generated surrounding patches, based on parameterized\nconvolutional operations of varying kernel sizes; and (2) blending the refined features of retrieved\npatches with the model\u2019s predicted feature representation for the next patch, based on compatibility\n1Code and model checkpoints can be found at https://github.com/PLUM-Lab/AR-RAG.\n2\n\nscores computed for each retrieved patch to quantify their alignment with the current generation\ncontext. To enable iterative refinement, we insert multiple FAiD modules at selected transformer\nlayers, where the output of each FAiD module, i.e., the context-aware retrieved features blended\nat that layer, is forwarded as input to the next FAiD module in deeper layers. This progressive\nretrieval refinement mechanism allows the model to incrementally enhance its predictions as patch-\nlevel representations evolve through the network. We evaluate AR-RAG on three widely adopted\nbenchmarks, including Midjourney-30K 2, Geneval [14], and DPG-Bench [18]. Experimental results\ndemonstrate that both DAiD and FAiD significantly improve the coherence and naturalness of\ngenerated images while introducing only marginal computational overhead.\nThe contributions of our work can be summarized as follows:\n\u2022 We propose AR-RAG, the first patch-level autoregressive retrieval augmentation framework which\ndynamically retrieves and integrates fine-grained visual content to enhance image generation,\nwhile avoiding limitations (e.g., over-copying, stylistic bias, etc.) prevalent in existing image-level\nretrieval augmentation methods.\n\u2022 We introduce Distribution-Augmentation in Decoding (DAiD), a training-free, plug-and-play\ndecoding strategy that directly integrates the distribution of retrieved patches into that predicted by\nthe image generation models, enabling easy integration into existing architectures.\n\u2022 We introduce Feature-Augmentation in Decoding (FAiD), a parameter-efficient fine-tuning frame-\nwork that progressively refines and blends retrieval signals via lightweight convolutional modules,\nenhancing spatial coherence and visual quality across layers.\n\u2022 Extensive experiments and analysis show that AR-RAG significantly improves performance of\nstate-of-the-art image generation model across diverse metrics. In particular, Janus-Pro with FAiD\nachieves 6.67 FID on Midjourney-30K and 0.78 overall score on GenEval, establishing a new state\nof the art among autoregressive image generation models of comparable scale.\n2\nPreliminary\nAutoregressive Image Generation Models\nWe implement both DAiD and FAiD based on Janus-\nPro [9], an autoregressive (AR) unified generation model, due to its strong performance. Janus-Pro is\ninitialized from a transformer-based pre-trained large-language model [2], and employs a quantized\nautoencoder [37] to encode images into discrete image tokens. During multimodal pretraining, the\nmodel learns to predict a sequence of discrete image tokens [v1, v2, ...vN] conditioned on an input\ntext prompt [t1, t2, ...tM]. The training objective is formally defined as:\narg max\n\u03d5\nD\nX\nN\nX\nn=1\nP\u03d5(vn|t1, t2, ..., tM, v1, ...vn\u22121)\n(1)\nwhere D is the training corpus. This is the same training objective used in our FAiD method in\nSection 3.3. We argue that DAiD and FAiD can be extended to any image generation model that\nautoregressively predicts probability distributions of discrete image tokens such as LlamaGen [37],\nShow-o [44] and VAR [38].\nQuantized Autoencoder\nThe quantized autoencoder used in Janus-Pro consists of an encoder \u03b8enc,\na decoder \u03b8dec, and a codebook Z. The encoder, a convolutional neural network, downsamples and\ncompresses raw pixel inputs into compact patch representations. During the quantization process, each\npatch representation is mapped to an index in the codebook by identifying its nearest neighbor vector\nin the codebook. In the decoding stage, these patch indices are mapped back to their corresponding\nvector representations via the codebook, and the decoder, another convolutional neural network,\nreconstructs the image from these compact representations. In our implementation, we leverage this\nautoencoder to build the coupled database for Janus-pro which is detailed in Section 3.1.\n3\nAR-RAG: Patch-based Autoregressive Retrieval Augmentation\n3.1\nPatch-based Retrieval Database Construction\nWe build a patch-based retrieval database based on several large-scale, real-world image datasets,\nincluding CC12M [5] and JourneyDB [36]. Specifically, for each image I, we encode it into N\n2https://huggingface.co/datasets/playgroundai/MJHQ-30K\n3\n\nPatch-based Retrieval\nh-hop surrounding\npatches\nAR Model\nDmodel\nDistribution-Augmentation in Decoding (DAiD)\nGenerated Next \nImage Patches\nDmerge\n\uff1f\nGenerated Patches\nDRetrieval\nFigure 2: The decoding process in Distribution-Augmentation in Decoding (DAiD).\npatches using the quantized autoencoder [37], \u03b8Enc, from Janus-Pro: V = \u03b8enc(I) \u2208R\n\u221a\nN\u00d7\n\u221a\nN\u00d7d,\nwhere d is the hidden dimension, and Vij corresponds to the latent representation of the patch at\nposition (i, j). We utilize each patch vector Vij as the value of a database entry and the representation\nof its h-hop surrounding patches as the key. Here, the h-hop surrounding patch representation is\nformed by concatenating the vectors of adjacent patches centering around (i, j) in a top-to-bottom,\nleft-to-right order. For example, for a patch at position (i, j), the 1-hop surrounding representation\nspans 8 surrounding patches [V(i\u22121)(j\u22121) : V(i\u22121)(j) : V(i\u22121)(j+1) : V(i)(j\u22121) : V(i)(j+1) :\nV(i+1)(j\u22121) : V(i+1)(j) : V(i+1)(j+1)] where : denotes the concatenation operation of image patch\nfeatures. If a patch is located at the edge of the image and lacks certain surrounding patches, we\nsubstitute each missing surrounding patch with a zero vector 0.\n3.2\nDistribution-Augmentation in Decoding (DAiD)\nGiven a text prompt T, Janus-Pro autoregressively predicts a sequence of image tokens [v1, v2, ...vN]\nwhere per-token probability is defined in Equation 1. As shown in Figure 2, DAiD augments this\nprocess by incorporating probability distributions from retrieved image patches. Specifically, when\nJanus-Pro predicts the next image token vij, we first utilize the codebook Z to convert vij\u2019s h-\nhop already generated surrounding patches into patch representations. If no surrounding image\ntokens are available at a given position (e.g., when i = 0 or j = 0), we use the zero vector 0 as a\nplaceholder. Once we compute the representation of vij\u2019s h-hop surrounding patches, we leverage it\nas the retrieval query and retrieve the top-K most similar patch representations from the database\nconstructed in Section 3.1 using l2 distance. We denote the representations of the top-K retrieved\npatches as [\u02c6v1, \u02c6v2, ..., \u02c6vK] and their corresponding l2 distances as [s1, s2, ..., sK]. These retrieved\nrepresentations are then mapped back to discrete token indices using the codebook: \u02c6vk = Z(\u02c6vk).\nTo augment the generation process with the retrieved image tokens [\u02c6v1, \u02c6v2, ..., \u02c6vK], we create a\nretrieval-based distribution Dretrieval \u2208R|Z| over the entire codebook Z, where |Z| is the codebook\nsize. Tokens not included in the top-K retrieved set are assigned a probability of 0. For tokens within\nthe top-K, we compute their probabilities using a softmax over their l2 distance to the query, scaled\nby a retrieval temperature hyperparameter \u03c4:\nDretrieval[v] =\n\u001ap(\u02c6vk)\nif v = \u02c6vk for some m \u2208{1, 2, ..., K}\n0\notherwise\n(2)\np(\u02c6vk) =\nexp(\u2212sk/\u03c4)\nPK\nk=1 exp(\u2212sk/\u03c4)\n,\n(3)\nThis creates a sparse distribution where only the top-K retrieved tokens have non-zero probabilities.\nFinally, we merge this retrieval distribution with the model\u2019s predicted distribution Dmodel using a\nweighted average:\nDmerge = (1 \u2212\u03bb) \u00b7 Dmodel + \u03bb \u00b7 Dretrieval,\n(4)\nwhere \u03bb \u2208[0, 1] is the retrieval weight hyperparameter controlling the influence of retrieved patches\non the final distribution. The next token is then sampled from this merged distribution: vij \u223cDmerge.\n3.3\nFeature-Augmentation in Decoding (FAiD)\nWhile DAiD offers a training free approach to directly augment the probability distribution of\npredicted patches using retrieved ones, it suffers from noise propagation and limited flexibility in\n4\n\n\uff1f\nRetrieval Database\n...\nPatch-based Image Retriever\nGenerated Patches\nConvolutional Layer\nMLP Layer\nFeature-Augmentation in Decoding (FAiD)\nDecoder\nLayer\nRMS Norm\nSelf-Attn\nRMS Norm\nFFD\nRMS Norm\nPrevious Patch Embeddings\nRetrieval Embeddings\nJanus-Pro\nSPB\n...\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n1\n2\n10\n1\n2\n9\n10\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nFigure 3: Overall architecture of Feature-Augmentation in Decoding (FAiD).\nfully leveraging the fine-grained visual information in the retrieved patches. We thus further propose\nFAiD, a feature-based autoregressive augmentation strategy to enhance the image generation process.\nAs illustrated in Figure 3, when predicting the next token vij during image generation, we employ\nthe same retrieval process described in Section 3.2 to obtain the top-K most relevant patches and\ntheir representations [\u02c6v1, \u02c6v2, ..., \u02c6vK] from our database. To effectively incorporate them into the\nautoregressive generation process, FAiD consists of two steps: (1) refining retrieved patches to ensure\ncoherence with the surrounding context of vij in the generated image, and (2) adaptively blending the\nrepresentation of refined patches with the hidden state of the predicted next patch based on learned\ncompatibility scores. To enable progressive refinement of retrieved information as representations\nevolve through the network, we insert a FAiD module for every L/b decoder layers of the generation\nmodel, where L denotes the total number of decoder layers and b is a hyperparameter.\nMulti-Scale Feature Smoothing\nThe key of effective patch integration lies in ensuring spatial\ncoherence between retrieved patches and the surrounding image context. To achieve this, we propose\nmulti-scale feature smoothing (Algorithm 1 in Appendix A), where multi-scale convolutions are\napplied to retrieved patches within the generation context, so that the retrieved visual features are\nsmoothed to preserve structural and stylistic consistency with the surrounding context of the predicted\ntoken. Specifically, at each step when predicting the next image token vij, we first construct a 2D\nspatial representation Hl \u2208R\n\u221a\nN\u00d7\n\u221a\nN\u00d7D of the current partially-generated image by arranging\ntheir hidden states [hl\n1, hl\n2, ..., hl\nn\u22121, hl\nn] from the current decoder layer l. We use 0 vectors as\nplaceholders for positions that have not yet been generated. Then, we transform the retrieved patch\nrepresentations [\u02c6v1, \u02c6v2, ..., \u02c6vK] into the generation model\u2019s hidden space by mapping each patch\n\u02c6vk to a discrete token index via the codebook Z and embedding it through the pretrained image\nembedding layer Embimg:\n[\u02c6h1, \u02c6h2, ..., \u02c6hK] = Embimg([Z(\u02c6v1), Z(\u02c6v2), ..., Z(\u02c6vK)])\n(5)\nFor each retrieved patch \u02c6hk, we create a copy of Hl where position (i, j) (the location of vij) is\nreplaced with \u02c6hk. We then apply convolution operations at multiple scales (2 \u00d7 2 through Q \u00d7 Q) to\ncapture contextual patterns at different resolutions. To maintain computational efficiency, we only\nperform convolution operations when the kernel covers position (i, j), rather than processing the\nentire image. Each convolution kernel Convq\u00d7q produces a refined representation \u02c6hq\nk for the retrieved\npatch at scale q. The final refined representation for each retrieved patch is computed as a weighted\nsum of these multi-scale features:\n\u02c6hk \u2190\nQ\nX\nq=2\nsoftmax(\u2126)q \u00b7 \u02c6hq\nk\n(6)\nwhere \u2126= [\u03c92, ..., \u03c9Q] are learnable parameters that determine the importance of each scale.\n5\n\nFeature Augmentation\nAfter feature smoothing, some of the retrieved patch features may still not\nbe able to fit into the surrounding neighbors and hence we need to lower their impact in the final repre-\nsentation. Thus, we compute a compatibility score for each of the refined patches. This is achieved by\nprojecting each refined retrieved patch representation through a linear transformation parameterized\nby a weight matrix W \u2208R1\u00d7D, yielding the score sk = \u02c6hkWT . The final representation for the\nnext image token vij after layer j is computed as:\nh(l+1)\nij\n= hl\nij + \u2206hl\nij +\nK\nX\nk=1\nsk\u02c6hk\n(7)\nHere, hl\nij is the residual, \u2206hl\nij is the updated representation from the transformer layer l, and\nPK\nk=1 sk\u02c6hk is the contribution of the retrieved image patches.\n4\nExperiment Setup\nPatch-based Retrieval Database\nTo construct our patch-level retrieval database, we randomly\nsample 5.7 million images from CC12M [5], 3.3 million from JourneyDB [36], and 4.6 million from\nDataComp [12], while ensuring that any samples included in the testing set are excluded to prevent\ndata leakage. Each image is encoded into a sequence of patch-level representations and image tokens\nusing the same image tokenizer employed in the Janus-Pro model. For efficient similarity search, we\nimplement our retriever using the FAISS library [21].\nTraining Setup\nWe adopt Janus-Pro-1B [9] and Show-o [44] as our backbone models and fine-tune\nthem on a dataset of 50,000 image-caption pairs sampled from CC12M [5] and Midjourney-v6 3.\nWe empirically determine the optimal hyperparameters for DAiD and FAiD, and the complete\nhyperparameter optimization experiment results can be found in Appendix C.2. Further details\nregarding the training dataset construction and implementation can be found in Appendix B.3.\nBaselines\nTo evaluate the effectiveness of our proposed methods, we adopt several state-of-the-art\nimage generation approaches as baselines, including non-retrieval models such as LlamaGen [37],\nLDM [32], Stable Diffusion (SDv1.5 and SDv3) [31, 10], PixArt-alpha [7], DALL-E 2 [30], Show-\no[44], and Janus-Pro [9], and image-based retrieval augmentation methods, including RDM [3],\nRA-CM3 [46], and ImageRAG[33]. Since pretrained models of RA-CM3 are not publicly available,\nwe try our best to replicate their method based on Janus-Pro to ensure a fair comparison. More details\nof training and implementation of RA-CM3 can be found in Appendix B.1.\nEvaluation Benchmarks and Metrics\nTo comprehensively evaluate our proposed methods, we\nemploy three benchmarks: (1) GenEval [14], which assesses models\u2019 ability to generate images with\nspecific attributes and relationships described in text prompts; (2) DPG-Bench [18], which evaluates\nperformance on detailed prompts with complex requirements; and (3) Midjourney-30k [40], where we\nemploy three complementary metrics: FID [17] for measuring statistical similarity between generated\nand real image distributions, CMMD [20] for assessing alignment with human perception using CLIP\nembeddings, and FWD [39] for evaluating spatial and frequency coherence through wavelet packet\ncoefficients. For all three metrics, lower scores indicate higher quality generated images. Detailed\ndescriptions of these benchmarks and metrics can be found in Appendix B.4.\n5\nResults and Discussion\n5.1\nText-to-Image Generation Results\nTables 1, 2, and 3 present performance comparisons across multiple benchmarks, where our AR-\nRAG methods consistently outperform existing approaches. Notably, previous retrieval-augmented\napproaches such as RDM and ImageRAG perform worse than their non-retrieval counterparts (LDM\nand SDXL, respectively) on both GenEval and DPG-Bench. We provide detailed analysis for existing\nimage-level retrieval methods and highlight the unique advantages of our AR-RAG frameworks in the\nfollowing discussion and Section 5.2. Appendix C.1 provides a benchmark analysis to demonstrate\nthe effectiveness of patch-level retrieval in our AR-RAG methods.\n3https://huggingface.co/datasets/brivangl/midjourney-v6-llava\n6\n\nMethod\nParams\nSingle Obj.\nTwo Obj.\nCounting\nColors\nPosition\nColor Attri.\nOverall \u2191\nNon Retrieval-Augmented Model\nPixArt-\u03b1\n0.6B\n0.98\n0.50\n0.44\n0.80\n0.08\n0.07\n0.48\nLlamaGen\n0.8B\n0.71\n0.34\n0.21\n0.58\n0.07\n0.04\n0.32\nSDv1.5\n0.9B\n0.97\n0.38\n0.35\n0.76\n0.04\n0.06\n0.43\nSDv2.1\n0.9B\n0.98\n0.51\n0.44\n0.85\n0.07\n0.17\n0.50\nJanus-Pro\n1.0B\n0.98\n0.77\n0.52\n0.84\n0.61\n0.55\n0.71\nShow-o\n1.3B\n0.98\n0.80\n0.66\n0.84\n0.31\n0.50\n0.68\nLDM\n1.4B\n0.92\n0.29\n0.23\n0.7\n0.02\n0.05\n0.37\nSD3 (d=24)\n2.0B\n0.98\n0.74\n0.63\n0.67\n0.34\n0.36\n0.62\nSDXL\n2.6B\n0.98\n0.74\n0.39\n0.85\n0.15\n0.23\n0.55\nDALL-E 2\n6.5B\n0.94\n0.66\n0.49\n0.77\n0.10\n0.19\n0.52\nDALL-E 3\n-\n0.96\n0.87\n0.47\n0.83\n0.43\n0.45\n0.67\nTransfusion\n7.3B\n-\n-\n-\n-\n-\n-\n0.63\nChameleon\n34B\n-\n-\n-\n-\n-\n-\n0.39\nRetrieval-Augmented Model\nRDM\n1.4B\n0.91\n0.21\n0.28\n0.71\n0.02\n0.04\n0.36\nImageRAG\n3.5B\n0.93\n0.06\n0.03\n0.37\n0.01\n0.03\n0.24\nJanus-Pro\n+ RA-CM3\n1.0B\n0.98\n0.78\n0.41\n0.84\n0.42\n0.49\n0.65 (-0.06)\n+ DAiD (ours)\n1.0B\n0.98\n0.82\n0.54\n0.87\n0.63\n0.49\n0.72 (+0.01)\n+ FAiD (ours)\n1.2B\n1.00\n0.88\n0.50\n0.86\n0.70\n0.73\n0.78 (+0.07)\nTable 1: Evaluation of text-to-image generation ability on GenEval benchmark. Note our methods\nare based on Janus-Pro highlighted in gray.\nMethod\nParams\nGlobal\nEntity\nAttribute\nRelation\nOther\nOverall \u2191\nNon Retrieval-Augmented Model\nPixArt-\u03b1\n0.6B\n74.97\n97.32\n78.60\n82.57\n76.96\n71.11\nSDv1.5\n0.9B\n74.63\n74.23\n75.39\n73.49\n67.81\n63.18\nJanus-Pro\n1.0B\n81.76\n84.53\n84.34\n92.22\n75.20\n77.26\nLumina-Next\n2.0B\n82.82\n88.65\n86.44\n80.53\n81.82\n74.63\nSDXL\n3.5B\n83.27\n82.43\n80.91\n86.76\n80.41\n74.65\nRetrieval-Augmented Model\nRDM\n1.4B\n62.36\n40.46\n60.20\n69.16\n24.68\n26.51\nImageRAG\n3.5B\n61.35\n32.77\n53.87\n60.38\n18.42\n19.82\nJanus-Pro\n+ RA-CM3\n1B\n81.76\n81.03\n83.32\n90.60\n70.80\n73.76 (-3.50)\n+DAiD (ours)\n1.0B\n83.58\n84.46\n84.76\n91.49\n76.40\n77.88 (+0.62)\n+FAiD (ours)\n1.2B\n82.67\n85.80\n85.38\n92.30\n76.80\n79.36 (+2.10)\nTable 2: Evaluation of text-to-image generation ability on DPG-Bench. Note our methods are based\non Janus-Pro highlighted in gray.\nOn GenEval, our methods show significant improvements in categories such as \u201cTwo Obj.\u201d and\n\u201cPosition,\u201d which demand accurate multi-object generation and spatial arrangement. These gains are\nlargely due to the local and dynamic nature of our autoregressive patch-level retrieval. Consider the\nprompt \u201ca green couch and an orange umbrella\u201d, a combination that rarely co-occurs in real-world\nimages. Static full-image retrieval methods may retrieve references containing only one of the objects.\nTaking these references as a global visual prior throughout the generation can lead the model to\noverfit to irrelevant layouts or dominant visual structures in the retrieved examples. On DPG-Bench,\nwhich features dense and highly detailed prompts, the performance gap between our method and\nprior retrieval-augmented approaches becomes even more substantial. Similar as GenEval, existing\nModel\nParams\nCMMD \u2193\nFID \u2193\nFWD \u2193\nRDM\n1.4B\n0.71\n19.17\n34.95\nImageRAG\n3.5B\n0.32\n19.39\n62.65\nShow-o\n1.3B\n0.09\n11.47\n2.57\n+ DAiD (ours)\n1.3B\n0.08\n9.28\n2.49\n+ FAiD (ours)\n1.5B\n0.06\n7.93\n1.73\nJanus-Pro\n1.0B\n0.12\n14.33\n28.41\n+ RA-CM3\n1.0B\n0.13\n12.40\n20.57\n+ DAiD (ours)\n1.0B\n0.11\n9.15\n28.00\n+ FAiD (ours)\n1.2B\n0.07\n6.67\n9.40\nTable 3: Evaluation of text-to-image generation\nability on the Midjourney-30K benchmark.\nimage-level retrieval augmentation methods strug-\ngle to retrieve meaningful references when the num-\nber of distinct entities and attributes in a prompt\nincreases.\nIn contrast, our autoregressive aug-\nmentation framework overcomes this limitation by\ndynamically retrieving patch-level visual features\nbased on the evolving image context rather than\nthe original prompt, enabling more targeted and\neffective augmentation.\nOn Midjourney-30K, our proposed methods con-\nsistently outperform both Janus-Pro and Show-o\nbaselines across all three evaluation metrics. No-\ntably, despite operating locally at the patch level, our approach leads to a significant reduction in FID\n7\n\nscores, indicating improved global visual quality and closer alignment with the distribution of real\nimages. This suggests that context-aware, auto-regressive retrieval and refinement can propagate to\nenhance holistic image fidelity. Furthermore, the improvements in CMMD and FWD metrics confirm\nour method\u2019s effectiveness in reducing visual distortions and enhancing coherence. These results also\ndemonstrate that AR-RAG delivers robust and architecture-agnostic improvements, validating its\nbroad applicability across different image generation backbones.\n5.2\nQualitative Analysis\nA photo of a\nbench.\nA realistic portrait\nof taylor swift\nwith a red scarf.\nThe morning light\nfilters cast a soft\nglow on a pair of\nhigh-top sneakers.\nA solitary camel\nslowly ambles\nbeside a plush,\nround red couch.\nJanus-Pro\nDAiD\nFAiD\nA photo of a sheep.\nFigure 4: Qualitative results of DAiD, FAiD and baselines.\nFigure 4 illustrates these quan-\ntitative improvements with rep-\nresentative examples from DPG-\nBench (left three columns) and\nGenEval (right two columns).\nThese\nexamples\ndemonstrate\nhow autoregressive retrieval aug-\nmentation improves the vanilla\nimage generation models. The\nvanilla model struggles with\nobject interactions (e.g., col-\numn 3,\nwhere shoes merge\nwith a coffee machine in the\nbackground), complex structures\n(e.g., columns 2 and 5, where\ncamels and sheep have anatom-\nically incorrect numbers of or-\ngans), and implausible configu-\nrations (e.g., column 4, where a chair exhibits an impossible design). Both DAiD and FAiD\nsubstantially reduce such local distortions, with FAiD yielding the highest visual quality. These\nresults confirm that autoregressive retrieval effectively maintains object consistency and structural\nintegrity throughout the generation process, particularly for complex objects and multi-object scenes.\n(d) A photo of a green couch and an orange umbrella.\n(c) A photo of a green cup and a yellow bowl.\nRetrieved Image\nAR-RAG\nImageRAG\nRetrieved Image\nAR-RAG\nImageRAG\n(a) A photo of an apple.\nRetrieved Image\nAR-RAG\nImageRAG\nRetrieved Image\nAR-RAG\n(b) A photo of a white dog and a blue potted plant.\nImageRAG\nFigure 5: Images generated by ImageRAG [33] and our AR-RAG. ImageRAG excessively copies\nretrieved images and does not follow user prompts.\nFigure 5 presents a comparative analysis of conventional image-level and our autoregressive patch-\nlevel retrieval augmentation methods. By comprehensively examining images produced by Im-\nageRAG alongside their corresponding retrieved reference images, we identify two critical challenges\ninherent in image-level retrieval augmentation approaches. First, these methods tend to overcopy\nirrelevant visual elements from retrieved reference images into the generation outputs. As illustrated\nin Figure 5 (a), when generating an image of an apple, image-level retrieval approaches retrieve\na reference image showing an apple on a tree branch and subsequently incorporate both the apple\nand the surrounding branches, despite the prompt making no mention of them. Similarly, for the\nprompt \u201ca green cup and a yellow bowl\u201d in Figure 5 (b), the image-level retrieval augmentation\napproach retrieves a green Starbucks cup and reproduces the pattern on the cup in the generated image,\n8\n\ndespite this element not being part of the original instruction. This overcopying behavior directly\ncompromises the instruction-following capability of generative models. Figure 5 (c) demonstrates\nthat when prompted to generate \u201cA photo of a white dog and a blue potted plant,\u201d image-level retrieval\nmethods produce an image containing only the white dog, omitting the blue potted plant entirely.\nSimilarly, for \u201ca photo of a green couch and an orange umbrella\u201d in Figure 5 (d), the generated image\nfails to include the umbrella. This degradation in instruction following occurs because image-level\nretrieval biases the generation process toward the compositional structure of retrieved reference\nimages, which may not align with the multi-object relationships specified in the prompt. In contrast,\nby autoregressively retrieving and integrating visual information at the fine-grained patch level rather\nthan the image level, AR-RAG enables selective incorporation of relevant visual elements while\nmaintaining independence from irrelevant contextual features present in the reference images.\n5.3\nInference Time Cost\nSingle GPU (L40)\nModel\nTotal (s)\nAverage (s)\nImageRAG\n879.64\n8.80\nJanus-Pro\n457.74\n4.58\n+ DAiD\n459.34\n4.59 (+0.22%)\n+ FAiD\n623.01\n6.23 (+36.03%)\nTable 4: Inference time for generat-\ning 100 images on a single L40 card.\nTable 4 shows the inference time comparisons across different\nmodels when generating 100 images using both a single L40\nGPU. The DAiD method introduces only a minimal increase\nin inference time compared to the base Janus-Pro-1B model,\nwith an average overhead of just 0.22%, demonstrating that\nDAiD maintains high computational efficiency. FAiD shows\na more noticeable overhead of 36.03% on a single GPU due\nto its autoregressive retrieval and feature blending operations.\nHowever, this increase remains reasonable given the substan-\ntial performance gains in generation quality. Overall, both DAiD and FAiD do not significantly\ncompromise the inference efficiency of Janus-Pro, making them practical for real-world applications.\n6\nRelated Work\nRetrieval-augmented generation (RAG) has emerged as a powerful paradigm that enhances generative\nmodels by incorporating external knowledge during decoding [23, 13, 16, 47, 46, 15, 24, 25, 42].\nOriginally developed for natural language processing, RAG enables models to retrieve relevant\ndocuments to supplement parametric knowledge during response generation [4], and has been widely\nadopted in many downstream tasks, such as knowledge-intensive tasks [23], document fusion [19],\nmodel pretraining [16], dialogue generation [35, 1], and so on.\nBeyond the text domain, prior research has explored enhancing image generation by incorporating\nexternal visual references. Early approaches [8, 3] condition the diffusion process on retrieved\nimages, typically encoded via CLIP or VAE encoders, to guide generation toward higher visual\nfidelity. KNN-Diffusion [34] extends this idea by leveraging k-nearest neighbor images to improve\nzero-shot generalization to novel domains. Building on this retrieval-augmented framework, more\nrecent methods [49, 33] introduce adaptive retrieval pipelines that iteratively refine retrieved images\nbased on feedback from multimodal large language models (MLLMs) analyzing the generated outputs.\nThese methods enable context-aware and prompt-sensitive guidance during generation. Another line\nof work [46] encodes multimodal retrievals into discrete visual and text tokens, and uses them directly\nas contextual input to augment the generation process of a multimodal large language model. All of\nthese works differe from our method by that our method works on patch-level, enabling more fine\ngrain retrievals and can dynamically adjust retrievals based on evolving generation states.\n7\nConclusion\nIn this work, we propose Autoregressive Retrieval Augmentation (AR-RAG), a novel retrieval\nparadigm that enhances image synthesis by leveraging k-nearest neighbor retrievals at the patch level.\nUnlike traditional image-level retrieval approaches, AR-RAG enables fine-grained visual element\nintegration while maintaining compositional flexibility. We introduce two parallel frameworks: (1)\nDistribution-Augmentation in Decoding (DAiD), a training-free approach that integrates retrieved\npatch distributions directly into generation, and (2) Feature-Augmentation in Decoding (FAiD), which\nemploys parameter-efficient fine-tuning with multi-scale feature smoothing and compatibility-based\nfeature augmentation. Extensive experiments across GenEval, DPG-Bench, and Midjourney-30K\ndemonstrate that AR-RAG significantly outperforms both conventional and retrieval-augmented\nbaselines, particularly in handling complex prompts with multiple objects and specific spatial rela-\n9\n\ntionships. Our methods substantially reduce local distortions in generated images, improving object\nconsistency and structural integrity.\nReferences\n[1] Trevor Ashby, Adithya Kulkarni, Jingyuan Qi, Minqian Liu, Eunah Cho, Vaibhav Kumar, and\nLifu Huang. Towards effective long conversation generation with dynamic topic tracking and\nrecommendation. In Proceedings of the 17th International Natural Language Generation\nConference, pages 540\u2013556, 2024.\n[2] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui\nDing, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language models\nwith longtermism. arXiv preprint arXiv:2401.02954, 2024.\n[3] Andreas Blattmann, Robin Rombach, Kaan Oktay, Jonas M\u00fcller, and Bj\u00f6rn Ommer. Semi-\nparametric neural image synthesis. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.\n[4] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie\nMillican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark,\nDiego De Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang,\nLoren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving,\nOriol Vinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and Laurent Sifre.\nImproving language models by retrieving from trillions of tokens. In Kamalika Chaudhuri,\nStefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings\nof the 39th International Conference on Machine Learning, volume 162 of Proceedings of\nMachine Learning Research, pages 2206\u20132240. PMLR, 17\u201323 Jul 2022.\n[5] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing\nweb-scale image-text pre-training to recognize long-tail visual concepts. CoRR, abs/2102.08981,\n2021.\n[6] Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi\nZhou, Saining Xie, Silvio Savarese, Le Xue, Caiming Xiong, and Ran Xu. Blip3-o: A family of\nfully open unified multimodal models-architecture, training and dataset, 2025.\n[7] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang,\nJames T. Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-\u03b1: Fast training of diffusion\ntransformer for photorealistic text-to-image synthesis. CoRR, abs/2310.00426, 2023.\n[8] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W. Cohen. Re-imagen: Retrieval-\naugmented text-to-image generator. In The Eleventh International Conference on Learning\nRepresentations, 2023.\n[9] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu,\nand Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and\nmodel scaling. CoRR, abs/2501.17811, 2025.\n[10] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M\u00fcller, Harry Saini,\nYam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion\nEnglish, and Robin Rombach. Scaling rectified flow transformers for high-resolution image\nsynthesis. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna,\nAustria, July 21-27, 2024. OpenReview.net, 2024.\n[11] Wan-Cyuan Fan, Yen-Chun Chen, Dongdong Chen, Yu Cheng, Lu Yuan, and Yu-Chiang Frank\nWang.\nFrido: Feature pyramid diffusion for complex scene image synthesis.\nCoRR,\nabs/2208.13753, 2022.\n[12] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao\nNguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim\nEntezari, Giannis Daras, Sarah M Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe,\nStephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga\n10\n\nSaukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont,\nSewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt.\nDatacomp: In search of the next generation of multimodal datasets. In Thirty-seventh Conference\non Neural Information Processing Systems Datasets and Benchmarks Track, 2023.\n[13] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei\nSun, Qianyu Guo, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large\nlanguage models: A survey. CoRR, abs/2312.10997, 2023.\n[14] Dhruba Ghosh, Hanna Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework\nfor evaluating text-to-image alignment. CoRR, abs/2310.11513, 2023.\n[15] Liangke Gui, Borui Wang, Qiuyuan Huang, Alex Hauptmann, Yonatan Bisk, and Jianfeng\nGao. Kat: A knowledge augmented transformer for vision-and-language. arXiv preprint\narXiv:2112.08614, 2021.\n[16] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Retrieval\naugmented language model pre-training. In Proceedings of the 37th International Conference\non Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings\nof Machine Learning Research, pages 3929\u20133938. PMLR, 2020.\n[17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGans trained by a two time-scale update rule converge to a local nash equilibrium. In Proceedings\nof the 31st International Conference on Neural Information Processing Systems, NIPS\u201917, page\n6629\u20136640, Red Hook, NY, USA, 2017. Curran Associates Inc.\n[18] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion\nmodels with llm for enhanced semantic alignment, 2024.\n[19] Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models\nfor open domain question answering. In Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty,\neditors, Proceedings of the 16th Conference of the European Chapter of the Association for\nComputational Linguistics: Main Volume, pages 874\u2013880, Online, April 2021. Association for\nComputational Linguistics.\n[20] Sadeep Jayasumana, Srikumar Ramalingam, Andreas Veit, Daniel Glasner, Ayan Chakrabarti,\nand Sanjiv Kumar. Rethinking fid: Towards a better evaluation metric for image generation. In\nCVPR, pages 9307\u20139315, 2024.\n[21] Jeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Billion-scale similarity search with GPUs.\nIEEE Transactions on Big Data, 7(3):535\u2013547, 2019.\n[22] Xuan Ju, Ailing Zeng, Chenchen Zhao, Jianan Wang, Lei Zhang, and Qiang Xu. Humansd: A\nnative skeleton-guided diffusion model for human image generation. CoRR, abs/2304.04269,\n2023.\n[23] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, and\nDouwe Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Hugo\nLarochelle, Marc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin,\neditors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural\nInformation Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\n[24] Weizhe Lin, Jinghong Chen, Jingbiao Mei, Alexandru Coca, and Bill Byrne. Fine-grained late-\ninteraction multi-modal retrieval for retrieval augmented visual question answering. Advances\nin Neural Information Processing Systems, 36:22820\u201322840, 2023.\n[25] Weizhe Lin, Jingbiao Mei, Jinghong Chen, and Bill Byrne. Preflmr: Scaling up fine-grained\nlate-interaction multi-modal retrievers. arXiv preprint arXiv:2402.08327, 2024.\n[26] Wenquan Lu, Yufei Xu, Jing Zhang, Chaoyue Wang, and Dacheng Tao. Handrefiner: Refining\nmalformed hands in generated images by diffusion-based conditional inpainting. In Jianfei Cai,\nMohan S. Kankanhalli, Balakrishnan Prabhakaran, Susanne Boll, Ramanathan Subramanian,\n11\n\nLiang Zheng, Vivek K. Singh, Pablo C\u00e9sar, Lexing Xie, and Dong Xu, editors, Proceedings of\nthe 32nd ACM International Conference on Multimedia, MM 2024, Melbourne, VIC, Australia,\n28 October 2024 - 1 November 2024, pages 7085\u20137093. ACM, 2024.\n[27] Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang\nWang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, Ji Hou, and Saining Xie. Transfer\nbetween modalities with metaqueries. CoRR, abs/2504.06256, 2025.\n[28] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller,\nJoe Penna, and Robin Rombach. SDXL: improving latent diffusion models for high-resolution\nimage synthesis. In The Twelfth International Conference on Learning Representations, ICLR\n2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024.\n[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. pages 8748\u20138763, 2021.\n[30] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\ntext-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.\nHigh-resolution image synthesis with latent diffusion models. CoRR, abs/2112.10752, 2021.\n[32] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), pages 10684\u201310695, June\n2022.\n[33] Rotem Shalev-Arkushin, Rinon Gal, Amit H. Bermano, and Ohad Fried. Imagerag: Dynamic\nimage retrieval for reference-guided image generation, 2025.\n[34] Shelly Sheynin, Oron Ashual, Adam Polyak, Uriel Singer, Oran Gafni, Eliya Nachmani, and\nYaniv Taigman. kNN-diffusion: Image generation via large-scale retrieval. In The Eleventh\nInternational Conference on Learning Representations, 2023.\n[35] Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval aug-\nmentation reduces hallucination in conversation. In Marie-Francine Moens, Xuanjing Huang,\nLucia Specia, and Scott Wen-tau Yih, editors, Findings of the Association for Computational\nLinguistics: EMNLP 2021, pages 3784\u20133803, Punta Cana, Dominican Republic, November\n2021. Association for Computational Linguistics.\n[36] Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang,\nAojun Zhou, Zipeng Qin, Yi Wang, Jifeng Dai, Yu Qiao, Limin Wang, and Hongsheng Li.\nJourneydb: A benchmark for generative image understanding. In Alice Oh, Tristan Naumann,\nAmir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural\nInformation Processing Systems 36: Annual Conference on Neural Information Processing\nSystems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023.\n[37] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan\nYuan. Autoregressive model beats diffusion: Llama for scalable image generation. CoRR,\nabs/2406.06525, 2024.\n[38] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive\nmodeling: Scalable image generation via next-scale prediction. In A. Globerson, L. Mackey,\nD. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural\nInformation Processing Systems, volume 37, pages 84839\u201384865. Curran Associates, Inc.,\n2024.\n[39] Lokesh Veeramacheneni, Moritz Wolter, Hilde Kuehne, and Juergen Gall. Fr\u00e9chet wavelet\ndistance: A domain-agnostic metric for image generation. In The Thirteenth International\nConference on Learning Representations, 2025.\n[40] Vivym.\nMidjourney prompts dataset.\nhttps://huggingface.co/datasets/vivym/\nmidjourney-prompts, 2023. Accessed: 2024-04-11.\n12\n\n[41] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan\nZhang, Yueze Wang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya\nWu, Bo Zhao, Bowen Zhang, Liangdong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu,\nYonghua Lin, Tiejun Huang, and Zhongyuan Wang. Emu3: Next-token prediction is all you\nneed. CoRR, abs/2409.18869, 2024.\n[42] Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge Zhang, Jie Fu, Alan Ritter, and Wenhu\nChen. Uniir: Training and benchmarking universal multimodal information retrievers. arXiv\npreprint arXiv:2311.17136, 2023.\n[43] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang,\nMuyang Li, Ligeng Zhu, Yao Lu, and Song Han. SANA: efficient high-resolution image\nsynthesis with linear diffusion transformers. CoRR, abs/2410.10629, 2024.\n[44] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong\nLin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single\ntransformer to unify multimodal understanding and generation. CoRR, abs/2408.12528, 2024.\n[45] Zhiyang Xu, Minqian Liu, Ying Shen, Joy Rimchala, Jiaxin Zhang, Qifan Wang, Yu Cheng, and\nLifu Huang. Modality-specialized synergizers for interleaved vision-language generalists. In\nThe Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore,\nApril 24-28, 2025. OpenReview.net, 2025.\n[46] Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Richard James, Jure Leskovec, Percy\nLiang, Mike Lewis, Luke Zettlemoyer, and Wen-Tau Yih. Retrieval-augmented multimodal\nlanguage modeling. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt,\nSivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning,\nICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine\nLearning Research, pages 39755\u201339769. PMLR, 2023.\n[47] Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. Making retrieval-augmented\nlanguage models robust to irrelevant context. CoRR, abs/2310.01558, 2023.\n[48] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang,\nArun Babu, Binh Tang, Brian Karrer, Shelly Sheynin, Candace Ross, Adam Polyak, Russell\nHowes, Vasu Sharma, Puxin Xu, Hovhannes Tamoyan, Oron Ashual, Uriel Singer, Shang-Wen\nLi, Susan Zhang, Richard James, Gargi Ghosh, Yaniv Taigman, Maryam Fazel-Zarandi, Asli\nCelikyilmaz, Luke Zettlemoyer, and Armen Aghajanyan. Scaling autoregressive multi-modal\nmodels: Pretraining and instruction tuning. CoRR, abs/2309.02591, 2023.\n[49] Huaying Yuan, Ziliang Zhao, Shuting Wang, Shitao Xiao, Minheng Ni, Zheng Liu, and Zhicheng\nDou. FineRAG: Fine-grained retrieval-augmented text-to-image generation. In Owen Ram-\nbow, Leo Wanner, Marianna Apidianaki, Hend Al-Khalifa, Barbara Di Eugenio, and Steven\nSchockaert, editors, Proceedings of the 31st International Conference on Computational Lin-\nguistics, pages 11196\u201311205, Abu Dhabi, UAE, January 2025. Association for Computational\nLinguistics.\n13\n\nA\nMulti-Scale Feature Smoothing Algorithm\nAlgorithm 1: Multi-Scale Feature Smoothing\nInput: Image Representations Hl \u2208R\n\u221a\nN\u00d7\n\u221a\nN\u00d7D,\nRetrieved Patch Representations\n[\u02c6h1, \u02c6h2, . . . , \u02c6hK], Next Patch Index (i, j)\nOutput: Updated hidden states [\u02c6h1, \u02c6h2, . . . , \u02c6hK]\n1 foreach \u02c6hi \u2208[\u02c6h1, . . . , \u02c6hK] do\n2\nfor q = 2 to Q do\n3\nInitialize tensor: M \u21900 \u2208RQ\u00d7Q\u00d7D;\n4\nInitialize tensor: \u02c6hq \u21900 \u2208RD;\n5\nfor m = q down to 1 do\n6\nfor n = q down to 1 do\n7\nHl\nloc \u2190Hl[i \u2212m : i + q \u2212m, j \u2212n :\nj + q \u2212n];\n8\nMmn \u2190Conv1\nq\u00d7q(Hl\nloc);\n9\n\u02c6hq += Conv2\nq\u00d7q(M);\n10\n\u02c6hi \u2190\n\u02c6hq\nQ\u22121;\nAlgorithm A illustrates the multi-scale fea-\nture smoothing, which is the core computa-\ntional procedure for refining retrieved patch\nrepresentations within their generation con-\ntext. This algorithm ensures that retrieved\nvisual elements are spatially and stylisti-\ncally coherent with the surrounding image\ncontent through systematic multi-scale con-\nvolution operations.\nThe algorithm processes each retrieved\npatch representation \u02c6hi independently, ap-\nplying convolution operations at multiple\nscales ranging from 2\u00d72 to Q\u00d7Q kernels.\nFor each scale q, the algorithm initializes\na temporary feature tensor M \u2208RQ\u00d7Q\u00d7D\nand an accumulation vector \u02c6hq \u2208RD. The\nnested loops over indices m and n system-\natically extract local patch features from\ndifferent spatial windows around the target\nposition (i, j). Each extraction operation\nHl\nloc \u2190Hl[i \u2212m : i + q \u2212m, j \u2212n :\nj + q \u2212n] captures a local neighborhood\nof size q \u00d7 q centered at varying offsets from the target position.\nThe extracted local features undergo two-stage convolution processing. The first convolution operation\nConv1\nq\u00d7q transforms the local patch features into an intermediate representation stored in Mmn,\neffectively capturing contextual relationships within each local window. The second convolution\noperation Conv2\nq\u00d7q processes the accumulated intermediate features to produce scale-specific refined\nrepresentations. This two-stage design enables the algorithm to first capture local contextual patterns\nand then integrate them into a coherent scale-specific feature representation.\nAfter processing all scales for a given retrieved patch, the algorithm computes the final refined\nrepresentation by averaging the scale-specific features. The normalization factor (Q \u22121) accounts\nfor the number of scales processed, ensuring consistent feature magnitudes across different retrieved\npatches. This averaging operation effectively combines multi-scale contextual information into a\nsingle refined representation that preserves both fine-grained details from smaller kernel sizes and\nbroader contextual patterns from larger kernel sizes. The resulting refined patch representations\nmaintain spatial coherence with the surrounding generation context while preserving the essential\nvisual characteristics of the retrieved content.\nB\nExperiment Setup\nB.1\nRA-CM3 Implementation Details\nSince the pretrained RA-CM3 model is not publicly available, we implement our own version\nfollowing the methodology described in the original paper to serve as a representative baseline for\nimage-level retrieval-augmented generation. Our implementation uses Janus-Pro as the backbone\nmodel to ensure fair comparison with our proposed methods, as both approaches operate on the same\nfoundation architecture.\nWe construct an image-level retrieval database using the same CC12M [5] and JourneyDB [36]\ndatasets employed for our patch-level retrieval database to maintain consistency in the underlying\ndata distribution. All images in the database are encoded into 512 dimensional vector representations\nusing a pretrained CLIP [29] model. For each training instance in our 50,000 sample training set, we\nretrieve the most relevant reference image by encoding the corresponding text prompt with the same\nCLIP model, extracting the [CLS] token as the text representation, and computing cosine similarity\nscores between the text representation and all image representations in the database. The image\n14\n\nwith the highest similarity score is selected as the retrieved reference. Each retrieved image is then\nprocessed through the quantized autoencoder from Janus-Pro to obtain image tokens [v1, . . . , vN] =\nZ(\u03b8Enc(I)), which are subsequently encoded into 2048 dimensional vector representations in the\nlanguage model\u2019s latent space using the image embedding and aligning layers in Janus-Pro. These\nretrieved image representations are concatenated with the text embeddings of the input prompts\nto form the augmented input for training the retrieval-enhanced model, which is the same training\nstrategy used in RA-CM3.\nDuring inference, given a text prompt for image generation, we follow the same retrieval process\nused in training. The input prompt is encoded using the CLIP text encoder, and we compute cosine\nsimilarity with all images in the database to identify the most relevant reference image. The retrieved\nimage is processed through the same pipeline to obtain its representation in the language model\u2019s\nlatent space. This representation is then prepended to the text prompt embedding to provide the model\nwith both textual and visual context for generation. The augmented input is fed into the fine-tuned\nJanus-Pro model to generate the output image following the standard autoregressive generation\nprocedure.\nB.2\nShow-o Implementation Details\nOur patch-based autoregressive retrieval augmentation methods can be theoretically adapted to\nany model that generates images through discrete tokens. To demonstrate this generalizability,\nwe implement both DAiD and FAiD on the Show-o [44] model, which generates images through\na masked token decoding process rather than strict left-to-right autoregression. Show-o decodes\nmultiple image tokens simultaneously at each time step by converting masked tokens to specific image\ntokens based on a learned probability matrix. This fundamental difference in generation strategy\nnecessitates several architectural adaptations to effectively incorporate our patch-based retrieval\nmechanisms while maintaining the model\u2019s inherent generation capabilities.\nDAiD on Show-o\nThe implementation of DAiD on Show-o requires three key modifications to\naccommodate its non-autoregressive generation strategy. First, instead of constructing retrieval queries\nfrom upper-left neighboring patches as in autoregressive models, we utilize all eight surrounding\npatches to form the h-hop neighborhood representation for each target token position (i, j). This\ncomprehensive neighborhood encoding is computed as [V(i\u22121)(j\u22121) : V(i\u22121)(j) : V(i\u22121)(j+1) :\nV(i)(j\u22121) : V(i)(j+1) : V(i+1)(j\u22121) : V(i+1)(j) : V(i+1)(j+1)], where missing positions are filled\nwith zero vectors 0. Second, to mitigate retrieval noise arising from sparse neighborhood information\nin early time steps, we apply patch-level retrieval only during the final half of Show-o\u2019s decoding\nprocess when sufficient contextual information is available. Third, since Show-o simultaneously\npredicts tokens for all patch positions at each time step rather than sequentially, we perform retrieval\nfor all patch positions concurrently. At each qualifying time step t, for every patch position (i, j)\nin the partially generated image, we extract the eight-neighborhood representation as the retrieval\nquery and obtain the top-K most similar patches [\u02c6v(i,j)\n1\n, \u02c6v(i,j)\n2\n, ..., \u02c6v(i,j)\nK\n] from our database. We\nthen construct position-specific retrieval distributions D(i,j)\nretrieval \u2208R|Z| using the same softmax\nformulation over retrieval distances as described in the main paper. These retrieval distributions are\nmerged with Show-o\u2019s predicted distributions for each patch position using the weighted average\nD(i,j)\nmerge = (1 \u2212\u03bb) \u00b7 D(i,j)\nmodel + \u03bb \u00b7 D(i,j)\nretrieval, where \u03bb controls the retrieval influence across all positions.\nFAiD on Show-o\nThe adaptation of FAiD to Show-o involves both training and inference modifica-\ntions to accommodate the model\u2019s masked token generation process. During training, we prepare\nthe training dataset by applying Show-o\u2019s noise injection process to generate intermediate noisy\nrepresentations at each time step, which serve as ground truth targets for the denoising process. For\neach training instance, we save these intermediate representations and apply patch-level retrieval\nto obtain relevant patches for all time steps. The training objective remains consistent with the\nstandard Show-o formulation, but with augmented input representations that incorporate retrieved\npatch information. We insert FAiD modules into every L/b decoder layers of Show-o\u2019s \u03a6 model,\nwhere each module processes all patch positions simultaneously rather than focusing on a single next\ntoken. At each qualifying time step and for each FAiD-equipped layer l, we construct the 2D spatial\nrepresentation Hl \u2208R\n\u221a\nN\u00d7\n\u221a\nN\u00d7D from the current hidden states and perform multi-scale feature\nsmoothing for all patch positions. For each position (i, j) and its corresponding retrieved patches\n15\n\n[\u02c6h(i,j)\n1\n, \u02c6h(i,j)\n2\n, ..., \u02c6h(i,j)\nK\n], we apply the convolution operations {Conv2\u00d72, Conv3\u00d73, ..., ConvQ\u00d7Q}\nto capture contextual patterns at multiple scales. The refined representations are computed as\n\u02c6h(i,j)\nk\n\u2190PQ\nq=2 softmax(\u2126)q \u00b7 \u02c6h(i,j)\nk,q , where \u02c6h(i,j)\nk,q\nrepresents the output of the q \u00d7 q convolution\nfor patch k at position (i, j). The final augmented representation for each position is calculated as\nh(l+1)\nij\n= hl\nij + \u2206hl\nij + PK\nk=1 s(i,j)\nk\n\u02c6h(i,j)\nk\n, where \u2206hl\nij represents the standard transformer layer\nupdates including self-attention and feed-forward components, and s(i,j)\nk\nare position-specific com-\npatibility scores computed through learned linear projections. During inference, we follow the same\nprocedure but apply retrieval and feature blending only during the final half of the generation time\nsteps to ensure sufficient contextual information is available for effective patch integration.\nB.3\nTraining Setup\nTraining Datasets\nFor model training, we utilize two large-scale image-caption datasets:\nCC12M [5] and Midjourney-v6 4. From the training sets of these datasets, we randomly sam-\nple a total of 50, 000 image-caption pairs (25, 000 from each dataset) to fine-tune our model. Each\nimage is encoded into 576 patch features and corresponding image tokens with the same image\ntokenizer [37] employed in the Janus-Pro model. For each image patch, we further retrieve the top-K\nimage tokens from our retrieval database that exhibit similar neighborhood relationships. Conse-\nquently, each training instance comprises: (1) a textual image caption that serves as the conditioning\ninput, (2) a sequence of 576 image tokens representing the ground-truth image, where each token is\npaired with K relevant image tokens retrieved from the database based on similar contextual features.\nTraining Details\nFor the implementation of our FAiD approach, we fine-tune two pre-trained text-\nto-image generation models using the training dataset of 50K text-image pairs that we constructed.\nWe select Janus-Pro-1B [9] and Show-o [44] as our base models. The fine-tuning process is conducted\non 4 NVIDIA A100 (80GB) GPUs with a global batch size of 256 for a single epoch. We utilize the\nAdamW optimizer without weight decay, incorporating a 10% linear warm-up schedule followed by\na constant learning rate of 2e-4.\nB.4\nEvaluation Benchmarks and Metrics\nTo comprehensively evaluate our proposed methods, we adopt multiple widely used benchmarks that\nassess different aspects of image generation quality:\n\u2022 GenEval [14] is a benchmark designed to evaluate models\u2019 ability to understand and generate\nimages based on specific attributes and relationships described in text prompts. It comprises\nmultiple categories such as single object generation, two-object composition, counting, colors,\npositioning, color attribution and so on. Performance is measured as the percentage of generated\nimages that correctly align with the text descriptions.\n\u2022 DPG-Bench [18] (Detailed Prompt Generation Benchmark) evaluates how well image generation\nmodels handle detailed prompts with complex requirements, covering categories such as global\nimage quality, entity generation, attribute accuracy, relationship modeling, and other complex\ngeneration tasks. Scores are reported as percentages.\n\u2022 For the Midjourney-30k benchmark [40], we employ three complementary metrics to evaluate the\nquality of generated images, including (1) Fr\u00e9chet Inception Distance (FID) [17], which measures\nthe statistical similarity between the distributions of generated and real images in the feature space\nof a pre-trained Inception network; (2) CLIP-MMD (CMMD) [20], which measures the distance\nbetween real and generated images using CLIP embeddings and the Maximum Mean Discrepancy,\nand is specifically designed to better align with human perception of image quality and addresses\nseveral limitations of FID, including poor sample efficiency and incorrect normality assumptions;\nand (3) Fr\u00e9chet Wavelet Distance (FWD) [39], which measures the distance between real and\ngenerated images in the wavelet packet coefficient space. FWD captures both spatial and frequency\ninformation without relying on pre-trained networks, making it domain-agnostic and robust to\ndomain shifts across various image types. For all three metrics, lower scores indicate higher-quality\nimage generation, with both CMMD and FWD particularly effective in capturing distortions in\ngenerated images in ways that better correlate with human judgements.\n4https://huggingface.co/datasets/brivangl/midjourney-v6-llava\n16\n\nC\nExperiment Results and Discussion\nC.1\nAccuracy of Patch-based Autoregressive Retrieval\nFigure 6: l2 distance between ground-truth tokens\nand top-10 retrieved tokens (blue line) compared\nto randomly sampled tokens (red dashed line). The\ncurved arrow indicates a broken y-axis that accom-\nmodates the large gap between the retrieved token\nand the random token baseline.\nTo assess the effectiveness of our patch-level\nautoregressive retrieval mechanism, we conduct\na comparative analysis between the top-K re-\ntrieved image tokens and the ground-truth to-\nkens to be generated. Specifically, we randomly\nsampled 1, 000 instances from our training set,\neach comprising 576 image tokens and 576 \u00d7 k\nretrieved tokens. To demonstrate the accuracy\nof the retrieved image tokens, for each ground-\ntruth image token, we also randomly sample\na vocabulary code as non-relevant tokens. Us-\ning the shared codebook, we transform all im-\nage tokens into vector representations and com-\npute the l2 distances between each ground-truth\nimage token and its top-K retrieved counter-\nparts. Similarly, we also compute the mean of\nthe l2 distance between each ground-truth token\nand the randomly sampled tokens. As shown\nin Figure 6, the l2 distance between retrieved\ntokens and ground-truth image tokens is signif-\nicantly smaller than the distance between ran-\ndomly sampled tokens and ground-truth tokens.\nAs k increases, the distance between the k-th\nretrieved token and the ground-truth token also increases, demonstrating the effectiveness of the\nretrieval approach and our assumption that image patches with similar neighbors usually exhibit\ninherent similarities.\nC.2\nHyperparameter Optimization\nFigure 7: Hyperparameter optimization results for DAiD and FAiD on FID scores. Left: FID\nscores for DAiD across different combinations of retrieval temperature \u03c4 and merging weight \u03bb.\nRight: FID scores for FAiD across varying levels of hop h and numbers of blender modules b. All\nexperiments conducted on the Midjourney-10K benchmark, with optimal configurations highlighted\nby red borders.\nBoth DAiD and FAiD require careful optimization of distinct sets of hyperparameters. For DAiD,\nwe optimize the retrieval temperature \u03c4 and merging weight \u03bb, which control the retrieval-based\nprobability distribution sharpness and the balance between retrieval and model predictions, respec-\ntively. For FAiD, we focus on the level of hop (h) and number of blender modules (b), determining\nthe spatial context incorporated during retrieval and extent of feature blending. To identify optimal\n17\n\nconfigurations, we conducted a systematic ablation study on the Midjourney-10K benchmark using\nFr\u00e9chet Inception Distance (FID) as the performance metric.\nFigure 7 presents the FID scores for DAiD across different combinations of \u03bb and \u03c4, and for FAiD\nacross varying levels of (h) and (b), where composite hop levels such as \u201c12\u201d indicate combined\nuse of multiple hop distances. Analysis of the DAiD results reveals that performance degrades\nas \u03bb increases, suggesting that modest integration of retrieval information enhances performance\nwhile excessive reliance impairs generative flexibility. The retrieval temperature \u03c4 demonstrates\nless pronounced effects, though a moderate value of 0.6 provides marginal benefits. For FAiD,\nconfigurations incorporating multiple hop levels generally outperform single hop levels, with the \u201c12\u201d\nconfiguration yielding optimal results. Regarding blender modules, an intermediate value consistently\ndelivers the best performance, implying that moderate feature blending optimizes the incorporation of\nretrieved patches while avoiding both under-utilization and over-smoothing. Based on this analysis,\nwe selected \u03bb = 0.05) and \u03c4 = 0.6 for DAiD, and hop levels \u201c12\u201d with 2 blender modules for FAiD,\nachieving FID scores of 14.12 and 13.13, respectively. These configurations effectively harness\nretrieval information while preserving the generative strengths of the underlying Janus-Pro model, as\ndemonstrated by their superior performance on the benchmark.\nD\nLimitations\nWhile our AR-RAG framework demonstrates strong performance across multiple benchmarks,\nseveral limitations should be acknowledged. First, our approach relies on discrete image tokenization\nand targets discrete token-based models, so it may not be directly applied to continuous diffusion\nmodels operating in latent spaces. Second, due to computational resource limitations, our retrieval\ndatabase remains smaller than billion-scale databases. This limitation may introduce visual pattern\nbiases, as the database may not fully capture the diversity of real-world visual patterns, potentially\naffecting the generation of underrepresented visual elements. Third, our implementation focuses\nexclusively on 2D image generation. While the underlying patch-based retrieval concept could\ntheoretically extend to other structured generation tasks such as 3D point cloud generation, we have\nnot explored these applications.\nE\nBroader impacts\nWe propose a novel retrieval-augmented approach to enhance existing image generation models.\nOur method is both highly efficient and readily adaptable to a wide range of applications, making it\nvaluable for both academic research and industrial deployment. However, as our approach builds upon\nexisting generative models, it may inherit their biases and could potentially produce inappropriate\noutputs in the absence of additional safety mechanisms. Furthermore, the large-scale retrieval\ndatabase may contain unsafe or undesirable content, which can be reflected in the retrieved image\npatches. To ensure safe deployment in real-world scenarios, additional safeguards and filtering\nmeasures are necessary to mitigate these risks.\n18\n"}