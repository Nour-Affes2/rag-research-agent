{"metadata": {"pdf_filename": "2312.05708v1__Context Tuning for Retrieval Augmented Generation.pdf", "source": "arXiv"}, "text": "Context Tuning for Retrieval Augmented Generation\nRaviteja Anantha, Tharun Bethi, Danil Vodianik, Srinivas Chappidi\nApple\nAbstract\nLarge language models (LLMs) have the re-\nmarkable ability to solve new tasks with just a\nfew examples, but they need access to the right\ntools. Retrieval Augmented Generation (RAG)\naddresses this problem by retrieving a list of\nrelevant tools for a given task. However, RAG\u2019s\ntool retrieval step requires all the required in-\nformation to be explicitly present in the query.\nThis is a limitation, as semantic search, the\nwidely adopted tool retrieval method, can fail\nwhen the query is incomplete or lacks context.\nTo address this limitation, we propose Context\nTuning for RAG, which employs a smart con-\ntext retrieval system to fetch relevant informa-\ntion that improves both tool retrieval and plan\ngeneration. Our lightweight context retrieval\nmodel uses numerical, categorical, and habitual\nusage signals to retrieve and rank context items.\nOur empirical results demonstrate that context\ntuning significantly enhances semantic search,\nachieving a 3.5-fold and 1.5-fold improvement\nin Recall@K for context retrieval and tool re-\ntrieval tasks respectively, and resulting in an\n11.6% increase in LLM-based planner accu-\nracy. Additionally, we show that our proposed\nlightweight model using Reciprocal Rank Fu-\nsion (RRF) with LambdaMART outperforms\nGPT-4 based retrieval. Moreover, we observe\ncontext augmentation at plan generation, even\nafter tool retrieval, reduces hallucination.\n1\nIntroduction\nLarge language models (LLMs) excel in a variety\nof tasks ranging from response generation and log-\nical reasoning to program synthesis. One of the\nimportant active areas of LLM research is to uti-\nlize them as planning agents (Huang et al., 2022).\nPlanning is an essential functionality for processing\ncomplex natural language instructions. A planner\nshould possess the ability to select the appropriate\ntools to complete each sub-task. While LLMs ex-\nhibit exceptional generation capabilities, they have\ninherent limitations, such as lacking up-to-date in-\nformation and exhibiting a tendency to hallucinate\ntools. By providing LLMs with a relevant set of\ntools based on the given task (Schick et al., 2023;\nLu et al., 2023), one can alleviate the issue of out-\ndated information. The set of methods to augment\nLLM input with retrieved information, such as rel-\nevant tools, is referred to as Retrieval Augmented\nGeneration (RAG) (Guu et al., 2020; Lewis et al.,\n2020). RAG consists of three primary components:\nTool Retrieval, Plan Generation, and Execution.1\nIn this study, we focus on enhancing tool retrieval,\nwith the goal of achieving subsequent improve-\nments in plan generation.\nExisting RAG methodologies rely heavily on se-\nmantic search for tool retrieval, but this approach\nhas limitations, especially when queries lack speci-\nficity or context. To this end, we present Context\nTuning, a component in RAG that precedes tool\nretrieval, to provide contextual understanding and\ncontext seeking abilities to improve tool retrieval\nand plan generation. Our contribution can be sum-\nmarized as follows:\n1. We empirically show that traditional RAG\nis inadequate for implicit/context-seeking\nqueries and present context tuning as a viable\nsolution;\n2. We provide a systematic comparison of vari-\nous context retrieval methods applied on both\nlightweight models and LLMs;\n3. We share empirically the insight that Chain of\nThought (CoT) augmentation improves con-\ntext retrieval when no fine-tuning is applied,\nwhereas fine-tuning the retrieval model re-\nmoves the need for CoT augmentation;\n4. We propose a lightweight model using Re-\nciprocal Rank Fusion (RRF) (Cormack et al.,\n1Typically, the query along with retrieved tools undergo\ndynamic prompt construction before presented to an LLM.\nThis process is called Query Decoration/Transformation. We\nomit that in this work for the sake of simplicity.\narXiv:2312.05708v1  [cs.IR]  9 Dec 2023\n\n2009) with LambdaMART (Burges, 2010),\nwhich outperforms GPT-4 (OpenAI, 2023)\nsystem, and finally;\n5. We show that context augmentation at plan\ngeneration reduces hallucinations.\n2\nRelated Work\nUsing retrieval to incorporate tools into plan gen-\neration with LLMs has emerged as a burgeoning\narea of research, with ongoing investigations aimed\nat enhancing both the retrieval component and the\nLLMs themselves. Our work falls within the for-\nmer category, placing a particular emphasis on\nrefining retrieval methodologies to enhance con-\ntextual understanding of implicit and ambiguous\nqueries that demand context-seeking capabilities.\nThe integration of tools into generation has been\ndemonstrated to enhance the capabilities of LLM-\nbased planners in recent studies (Schick et al., 2023;\nLu et al., 2023). However, these works primarily fo-\ncus on well-defined or unambiguous queries, where\nretrieving supplementary information to augment\nthe query is not strictly required. For question an-\nswering (QA) tasks, incorporating any off-the-shelf\ndocument retriever has been shown to improve\nLLM generation, with the addition of re-ranking\nfurther boosting performance (Ram et al., 2023).\nWhile re-ranking is preferred, employing any pre-\ntrained retriever, particularly a text-based retriever,\nwould be sub-optimal due to the inadequate in-\nformation expected from ambiguous queries. Our\nwork demonstrates the inadequacy of text-based\nretrievers for context retrieval and the necessity of\nmore advanced retrieval models.\nTo address the lack of context inherent in under-\nspecified queries, some studies have explored the\nuse of CoT (Wei et al., 2022) mechanisms to gener-\nate text that closely approximates the semantic sim-\nilarity of relevant context (Ma et al., 2023). While\nCoT augmentation improves upon baseline meth-\nods, such as vanilla semantic search, CoT may\npotentially increase the input length to the LLM,\nwhich has a limited context window size. Addi-\ntionally, studies have demonstrated that the place-\nment of relevant information impacts LLM gen-\neration (Liu et al., 2023). Therefore, it is prefer-\nable to avoid increasing input sequence length if\nthe same or better results can be achieved with-\nout query augmentation. Distillation-based query\naugmentation approaches have been proposed to\naddress this problem (Srinivasan et al., 2023). Our\nwork unveils that fine-tuning semantic search ob-\nviates the necessity for query augmentation while\nachieving comparable performance.\nRecent studies have shown LLMs can act as\nzero-shot rankers through pairwise ranking prompt-\ning (Qin et al., 2023). While addition of rank-\ning for retrieval component has shown improve-\nment in QA tasks, direct use of LLMs for the\nranking task, in addition to plan generation, incurs\ntwice the inference cost. We empirically show that\nour proposed lightweight context tuning method,\nLambdaMART (Burges, 2010) based RRF (Cor-\nmack et al., 2009), outperforms both fine-tuning\napproach and GPT-4 (OpenAI, 2023) based CoT\nAugmentation.\n3\nMethodology\nOur experiments train and evaluate tool retrieval\nand planning with and without context tuning. Fig-\nure 1 illustrates how a context-seeking query uses\ncontext retrieval to enhance tool retrieval and plan\ngeneration.\n3.1\nData Generation\nOur study employed a data generation methodology\nusing synthetic application data, aimed at simulat-\ning real-world scenarios for a digital assistant. The\ndata encompasses 7 commonly used applications:\nmail, calendar, google, music, reminders, notes,\nand phone call. We generated this data using GPT-\n4, ensuring diversity in the dataset to reflect a wide\nrange of user personalities. The synthetic dataset\ncontained a diverse range of context items spanning\nvarious applications. A total of 791 distinct per-\nsonas were synthesized, yielding 4,338 unique im-\nplicit queries for training and 936 implicit queries\nfor evaluation.\nAdditionally, we developed a toolbox containing\nAPIs for each of the applications we considered.\nThis toolbox was created using in-context learn-\ning with GPT-4 and contained a total of 59 APIs\ndistributed across the applications.\nTo simulate user interaction with a virtual assis-\ntant, GPT-4 was also utilized to generate realistic\nqueries grounded in the application data. Following\nthis, we employed GPT-4 to retrieve the appropri-\nate tool from the generated toolbox in response\nto these queries. Finally, GPT-4 was used to re-\nsolve the tool\u2019s API with the correct parameters.\nThis methodology provided a comprehensive and\nrealistic dataset, essential for the evaluation of our\n\nFigure 1: Context-tuned RAG pipeline illustrating end-to-end processing of a complex request with progressive\nplan generation.\ncontext tuning approach in RAG-based planning\nsystems.2\n3.2\nContext Tuning\nTo compare various context retrieval methods, we\nemploy both text-based and vector-based retrieval\nbaselines. We simulate different context stores by\nstructuring context data per persona and train mod-\nels to perform federated search. We use query and\npersona meta-signals, such as frequency, usage his-\ntory, and correlation with geo-temporal features,\nto perform retrieval. We evaluate context retrieval\nusing the Recall@K and Normalized Discounted\nCumulative Gain (NDCG@K) metrics.\nBM25\nFor text-based search, we use an improved\nversion of BM25, called BM25T (Trotman et al.,\n2014).\nSemantic Search\nFor vector-based search, we\nemploy the widely adopted Semantic Search ap-\nproach.\nWe use GTR-T5-XL (Ni et al., 2021)\nto generate query and context item embeddings,\nwhich are then ranked using cosine similarity to se-\nlect the top-K results. We evaluate both pre-trained\nand fine-tuned variants of this method.\nCoT Augmentation\nTo enhance the likelihood\nof semantic alignment with pertinent contextual\nelements, we augment the under-specified or im-\nplicit query with GPT-4 (OpenAI, 2023) generated\nCoT.3 We evaluate both pre-trained and fine-tuned\nsemantic search versions utilizing CoT.\nLambdaMART with RRF\nReciprocal Rank Fu-\nsion (RRF) (Cormack et al., 2009) is shown to\noutperform individual rank learning methods. To\nleverage this advantage, we propose a lightweight\n2Refer to Appendix A for more details on data generation.\n3Please refer Appendix A.6 for the GPT-4 prompt used\nand Table 5 for CoT examples.\nmodel that uses LambdaMART (Burges, 2010) for\ninitial ranking of data across context stores, fol-\nlowed by re-ranking using RRF.\n3.3\nTool Retrieval\nWhile advanced ranking models can enhance the\nrecall of tool retrieval, we employ the pre-trained\nGTR-T5-XL model for semantic search using co-\nsine similarity to retrieve the top-K tools. Extend-\ning the tool retrieval process to incorporate ranking\nshould be a straightforward endeavor. We evaluate\ntool retrieval performance with and without context\nretrieval using Recall@K.\n3.4\nPlanner\nThe planner\u2019s objective is to select the most appro-\npriate tool from the retrieved tool list and gener-\nate a well-formed plan. A plan comprises an API\ncall constructed using the chosen tool and parame-\nters extracted from the query and retrieved context.\nWe fine-tune OpenLLaMA-v2-7B (Touvron et al.,\n2023) for plan generation. To assess the planner\u2019s\nperformance, we employ the Abstract Syntax Tree\n(AST) matching strategy to compute plan accuracy.\nA hallucination is defined as a plan generated using\nan imaginary tool.\n4\nResults\n4.1\nContext Retrieval\nConsistent with expectations, vector-based search\nsurpasses text-based search, as shown in Table 1.\nNevertheless, both approaches struggle to retrieve\nrelevant context for under-specified queries. Fine-\ntuned semantic search and CoT augmentation with\npre-trained semantic search both significantly en-\nhance retrieval performance. Notably, when fine-\ntuning is employed, CoT augmentation yields only\nmarginal gains, suggesting that comparable im-\n\nTable 1: A comparison of various Context Retrieval\nmethods using Recall@K and NDCG@K metrics. The\ncontext-seeking query is used as input to perform a\nfederated search across different context stores, after\nwhich semantic search or ranking is applied.\nRetrieval Method\nRecall@K\nNDCG@K\nK=3\nK=5\nK=10\nK=3\nK=5\nK=10\nBM25\n11.35\n13.47\n14.92\n56.45\n52.33\n50.91\nSemantic Search\n23.74\n25.38\n26.99\n65.44\n64.31\n64.02\nCoT Augmentation\n71.77\n85.61\n94.41\n93.67\n91.78\n88.40\nFinetuned Semantic\nSearch\n73.48\n88.52\n95.13\n93.81\n94.07\n94.23\nFinetuned w/ CoT\nAugmentation\n73.55\n88.53\n95.17\n93.92\n94.11\n94.22\nLambdaMART-\nRRF\n81.27\n92.65\n98.77\n96.39\n97.11\n98.24\nFigure 2: Evaluation of tool retrieval using Recall@k,\nwith and without context tuning.\nprovements could be achieved without augmenting\nthe input sequence with CoT.\nOur proposed approach utilizing LambdaMART\nwith RRF outperforms both fine-tuned semantic\nsearch and CoT augmentation. Additionally, we ob-\nserve that for fine-tuned methods, both Recall@K\nand NDCG@K increase with K, whereas for pre-\ntrained methods, NDCG@K decreases with an in-\ncrease in K and Recall@K.\n4.2\nTool Retrieval\nFigure 2 illustrates the performance of tool retrieval\nusing semantic search. Incorporating relevant con-\ntext into tool retrieval consistently yields substan-\ntial gains across various K-values.\n4.3\nPlanner\nTo establish the planner\u2019s lower bound, we remove\nthe retrieval step, while the upper bound is set by\ndirectly utilizing context and/or tool labels, effec-\nTable 2: End-to-end planner evaluation both with and\nwithout context tuning. \u201cLower Bound\" excludes re-\ntrieval and performs direct plan generation while \u201cUpper\nBound\" assumes perfect context and tool retrieval.\nSetting\nAST-based\nPlan Acc \u2191\nExact Match \u2191\nHallucination \u2193\nLower Bound\n43.77\n39.45\n2.59\nRAG-based\nPlanner\n76.39\n58.12\n1.76\nContext-tuned\nRAG Planner\n85.24\n67.33\n0.93\nUpper Bound\n91.47\n72.65\n0.85\nContext-tuned\nUpper Bound\n91.62\n72.84\n0.53\ntively employing oracle retrievers. Table 2 encap-\nsulates the end-to-end evaluation of the fine-tuned\nplanner, demonstrating that the context-tuned plan-\nner significantly outperforms the planner based on\ntraditional RAG using semantic search. Notably,\neven when the correct tool is retrieved, incorpo-\nrating relevant context in plan generation, as evi-\ndenced by the upper bound, helps in reducing hal-\nlucination.\n5\nConclusion\nOur work introduces context tuning, a novel compo-\nnent that enhances RAG-based planning by equip-\nping it with essential context-seeking capabilities\nto address incomplete or under-specified queries.\nThrough a systematic comparison of various re-\ntrieval methods applied to both lightweight models\nand LLMs, we demonstrate the effectiveness of\ncontext tuning in improving contextual understand-\ning. Our empirical observations reveal that CoT\naugmentation enhances context retrieval when fine-\ntuning is not applied, while fine-tuning the retrieval\nmodel eliminates the need for CoT augmentation.\nFurthermore, we observe that context augmenta-\ntion at the plan generation stage reduces halluci-\nnations. Finally, we showcase the superiority of\nour proposed lightweight model using RRF with\nLambdaMART over the GPT-4-based system.\nLimitations\nThe current work does not utilize conversation his-\ntory, which is crucial for handling explicit multi-\nturn instructions that contain anaphora or ellipsis.\nThis limitation also hinders the model\u2019s ability to\neffectively process and respond to complex tasks\nthat require multi-hop context retrieval. Addition-\n\nally, the absence of conversation history impedes\nthe model\u2019s ability to adapt to topic shifts that may\noccur throughout a dialogue.\nFurthermore, the performance of the planner\nmodel is constrained by the length of the context\nwindow. While employing LLMs with longer con-\ntext windows can enhance performance, it also in-\ncreases model size and computational complexity.\nTo address this limitation, incorporating context\ncompression techniques could potentially improve\nend-to-end performance without incurring signifi-\ncant increases in model size.\nDue to privacy constraints, we simulated real-\nworld data by generating synthetic user profiles\nand personas that mirrored real-world use cases for\na digital assistant.\nEthics Statement\nTo safeguard privacy, this study exclusively utilizes\nsynthetically generated data, eliminating the use of\nreal user information under ethical considerations.\nAcknowledgements\nWe would like to thank Stephen Pulman, Barry\nTheobald and Joel Moniz for their valuable feed-\nback.\nReferences\nChristopher J.C. Burges. 2010. From ranknet to lamb-\ndarank to lambdamart: An overview. Microsoft Re-\nsearch Technical Report MSR-TR-2010-82.\nGordon V. Cormack, Charles L. A. Clarke, and Stefan\nBuettcher. 2009. Reciprocal rank fusion outperforms\ncondorcet and individual rank learning methods. In\nProceedings of the 32nd International ACM SIGIR\nConference on Research and Development in Infor-\nmation Retrieval., pages 758\u2013759.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training.\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and\nIgor Mordatch. 2022. Language models as zero-shot\nplanners: Extracting actionable knowledge for em-\nbodied agents.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K\u00fcttler, Mike Lewis, Wen tau Yih, Tim Rock-\nt\u00e4schel, Sebastian Riedel, and Douwe Kiela. 2020.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks.\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin\nParanjape, Michele Bevilacqua, Fabio Petroni, and\nPercy Liang. 2023. Lost in the middle: How lan-\nguage models use long contexts.\narXiv preprint\narXiv:2307.03172.\nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-\nWei Chang, Ying Nian Wu, Song-Chun Zhu, and Jian-\nfeng Gao. 2023. Chameleon: Plug-and-play compo-\nsitional reasoning with large language models. arXiv\npreprint arXiv:2304.09842.\nXinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao,\nand Nan Duan. 2023. Query rewriting for retrieval-\naugmented large language models. arXiv preprint\narXiv:2305.14283.\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-\ntavo Hern\u00e1ndez \u00c1brego, Ji Ma, Vincent Y. Zhao,\nYi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei\nYang. 2021. Large dual encoders are generalizable\nretrievers.\nOpenAI. 2023. Gpt-4 technical report.\nZhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang,\nJunru Wu, Jiaming Shen, Tianqi Liu, Jialu Liu, Don-\nald Metzler, Xuanhui Wang, and Michael Bender-\nsky. 2023. Large language models are effective text\nrankers with pairwise ranking prompting.\narXiv\npreprint arXiv:2306.17563v1.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. 2023. In-context retrieval-augmented lan-\nguage models. arXiv preprint arXiv:2302.00083.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\narXiv preprint arXiv:2302.04761.\nKrishna Srinivasan, Karthik Raman, Anupam Samanta,\nLingrui Liao, Luca Bertelli, and Mike Bendersky.\n2023. Quill: Query intent with large language mod-\nels using retrieval augmentation and multi-stage dis-\ntillation. arXiv preprint arXiv:2210.15718v1.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nAndrew Trotman, Antti Puurula, and Blake Burgess.\n2014. Improvements to bm25 and language models\nexamined. In Proceedings of the 32nd International\nACM SIGIR Conference on Research and Develop-\nment in Information Retrieval., pages 58\u201365.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2022.\nChain-of-thought prompting\nelicits reasoning in large language models. arXiv\npreprint arXiv:2201.11903.\n\nA\nData Generation Details\nA.1\nImplicit Query Dataset\nFor our experiments, we created a synthetic dataset\nto simulate realistic interactions across various ap-\nplications commonly found with digital assistants.\nThe dataset is structured to encompass a diverse\nrange of contexts, representing different synthetic\nuser activities and interactions.\nData Points:\nA total of 791 unique personas were\nsynthesized, covering seven key applications: Mail,\nCalendar, Google, Music, Reminders, Notes, and\nPhone Calls. The final dataset contained 4,338 train\nand 936 test data points.\nGeneration Method:\nWe utilized GPT-4 to gen-\nerate the data. We ensured high diversity in the\ndataset is met through manual inspection, this is\nessential to accurately reflect a wide range of syn-\nthetic user personalities and interaction patterns.\nData Representation:\nEach data point in the\ndataset contains multiple contextual information\nfields, relevant to the specific application and syn-\nthetic user\u2019s activity. An example of persona in\nJSON format is shown in Figure 3.\nFigure 3: Snippet of a persona\nTable 3 shows the distribution of context items\nper application in our dataset.\nA.2\nPersona Data Creation Example Prompt\nI'm working on generating synthetic data\nfor a user (also known as persona)\nand the persona 's\nApplication\nAvg. Context Items\nMail\n2.93\nCalendar\n5.63\nGoogle\n9.57\nNotes\n2.23\nMusic\n4.38\nReminders\n4.81\nPhonecall\n2.34\nTable 3: Distribution of context items per application.\niPhone Data.\nHere are the characteristics of the\npersona that we would like to\ngenerate the data for:\nage: 22\nfavorite_music_genre: Pop\nfavorite_movie_genre: Romance\nfavorite_cuisine: Italian\nfavorite_sport: Tennis\nprofession: Software Developer\nhobbies: ['Cooking ', 'Swimming ', '\nReading ']\nI want to generate data for ios App\ncalled Music with bundle id as com.\napple.music.\nCan you generate around 5 recently\nplayed songs\nInstructions:\n1. Today 's date is 2023 -12 -07\n11:18:19.028759 , Please generate any\ntimes or dates in the past 15 days.\n2. 'played_time ' should be in yyyy -MM -dd\nHH:mm:ss.SSS format\nUse the following schema:\nThe output should be formatted as a JSON\ninstance that conforms to the JSON\nschema below.\nAs an example , for the schema {\"\nproperties \": {\"foo\": {\" title \": \"Foo\n\", \"description \": \"a list of strings\n\", \"type\": \"array\", \"items \": {\"type\n\": \"string \"}}}, \"required \": [\"foo \"]}\nthe object {\"foo\": [\"bar\", \"baz \"]} is a\nwell -formatted instance of the\nschema. The object {\" properties \": {\"\nfoo\": [\"bar\", \"baz \"]}} is not well -\nformatted.\nHere is the output schema:\n```\n{\" $defs \": {\" MusicAppData \": {\" properties\n\": {\" recent_songs \": {\" items \": {\"$ref\n\": \"#/ $defs/Song\"}, \"title \": \"Recent\nSongs\", \"type\": \"array\"}, \"\ncurrent_playing \": {\"$ref\": \"#/ $defs/\nSong\"}}, \"required \": [\"\ncurrent_playing \"], \"title \": \"\n\nMusicAppData\", \"type\": \"object\"}, \"\nSong\": {\" properties \": {\" played_time\n\": {\" default \": \"\", \"title\": \"Played\nTime\", \"type\": \"string\"}, \"\nalbum_title \": {\" default \": \"\", \"title\n\": \"Album Title\", \"type\": \"string\"},\n\"artist \": {\" default \": \"\", \"title \":\n\"Artist\", \"type\": \"string\"}, \"\nsong_name \": {\" default \": \"\", \"title \":\n\"Song Name\", \"type\": \"string\"}, \"id\n\": {\" default \": \"\", \"title\": \"Id\", \"\ntype\": \"string \"}}, \"title\": \"Song\",\n\"type\": \"object \"}}, \"properties \": {\"\napp_name \": {\" default \": \"\", \"title \":\n\"App Name\", \"type\": \"string\"}, \"\napp_bundle_id \": {\" default \": \"\", \"\ntitle \": \"App Bundle Id\", \"type\": \"\nstring\"}, \"app_data \": {\"$ref\": \"#/\n$defs/MusicAppData \"}}, \"required \":\n[\" app_data \"]}\n```\nDo not include any explanations , only\nprovide a RFC8259 compliant JSON\nresponse following this format\nwithout deviation.\nA.3\nSynthetic Toolbox Generation\nYou are an intelligent AI assistant\ntasked with generating APIs for iOS\nthat can be used to interact with\nApplications. For example , if I ask\nyou to generate APIs for Messages\niOS Application , you would generate\na comprehensive set of APIs that can\nperform any action on the app. Some\nexamples below are:\napi: read_message\ndescription: Messages App 's read_message\nAPI is used to read messages from a\nparticular contact\narguments:\n- contact: contact from which the\nmessage was received\napi: read_unread_messages\ndescription: Messages App 's\nread_unread_messages API is used to\nread all unread messages on your\niPhone\narguments:\n-\napi: send_message\ndescription: Messages App 's send_message\nAPI is used to send message to a\nparticular contact\narguments:\n- text: text to be sent to the\ncontact\n- contact: contact information\napi: send_group_message\ndescription: Messages App 's\nsend_group_message API is used to\nsend a message to a list of contacts\n.\narguments:\n- text: text to be sent to the group\n- contacts: list of contacts in the\ngroup\napi: search_messages\ndescription: Messages App 's\nsearch_messages API is used to\nsearch messages by text , recipient ,\nsender.\narguments:\n- text: text to be searched.\n- recipient: search messages by\nrecipient name\n- sender: Search messages by sender\nname\nSimilarly , can you generate the APIs for\nthe following Application: {\napplication }?\nDo not include any explanations. Only\nprovide the APIs in YAML format as\nabove.\nThe following table represents the distribution\nof APIs:\nApplication\nAPIs Count\nMusic\n11\nGoogle\n10\nNotes\n9\nMail\n8\nPhoneCall\n8\nCalendar\n7\nReminders\n6\nTable 4: Distribution of APIs generated by Synthetic\nToolbox Generation\nA.4\nTool Retrieval\nI have the following toolbox defined\nwith the available APIs:\n{tools}\nFor the following query:\n{query}\nSuggest the most appropriate api? If\nthere is no API available in the\ntoolbox , then output default.\nOnly output the API name without any\nexplanations\nA.5\nPlan Resolution\nYou are an intelligent AI Planner\nhelping me come up with a plan and\nresolve the variables.\nI have the following query:\n{query}\n\nI have selected the following tool to\nperform the task:\n{tool}\nCan you come up with fully resolved plan\nusing the following schema?\n{format_instructions}\nA.6\nPrompt to generate CoT\nYou are an expert in processing context -\nseeking or under -specified queries\nby finding missing context in the\nquery. As an expert , your task is to\ngenerate concise chain of thought\nwhich when used to augment the\ncontext -seeking query , increases the\nsemantic similarity of the updated\nquery with relevant context items.\nPlease only use the following\ncontext types: 'Mail ', 'Calendar ', '\nReminders ', 'Notes ', 'Photos ', '\nPhoneCall ', 'Message ', 'Messenger ',\n'Maps ', 'Google Maps ', 'Music ', '\nSpotify ', 'Find My ', 'Workout '; and\ndo not create new context types.\nContext -seeking Query: {query}\nYour expert Chain of Thought:\nExamples showing generated implicit queries\nalong with CoT, context and plan labels are shown\nin Table 5.\n\nTable 5: A sample of context-seeking or under-specified queries along with CoT produced by GPT-4. The columns\nfor context and tools show labels for those retrieval tasks.\nImplicit Query\nCoT\nRelevant Context\nTop-3 Relevant Tools\nWhen is my next\nguitar lesson?\nCheck the \u2019Calendar\u2019 for any\nupcoming guitar lessons.\nIf not there, check \u2019Reminders\u2019\nfor any alerts set about the lesson.\nThe user has a reminder\ntitled \u201cGuitar Class\"\n[\u2019Reminders\u2019, \u2019Calendar\u2019,\n\u2019Notes\u2019]\nI need to check my\ndiet plan again.\nI may have noted down the\ndiet plan in \u2019Notes\u2019. If not\nthere, perhaps I saved a photo\nof it in \u2019Photos\u2019.\nThe user has a note titled\n\u201cIntermittent Fasting Plan.\"\nThe user also has an\nimage titled \u201cKeto Diet.\"\n[\u2019Photos\u2019, \u2019Notes\u2019,\n\u2019Mail\u2019]\nI\u2019m running late.\nCheck \u2019Calendar\u2019 for any\nscheduled meetings. If so, verify\n\u2019Maps\u2019 or \u2019Google Maps\u2019 to\ngauge current traffic situation\nand estimated time of arrival.\nUse \u2019Messages\u2019 or \u2019Messenger\u2019\nor \u2019Mail\u2019 to inform the meeting\nattendees that you are\n\u201crunning late\".\nThe user has an upcoming\nmeeting titled \u201cLLM\nDiscussion\" organized by\n\u201cJohn Doe.\"\n[\u2019Calendar\u2019, \u2019Mail\u2019,\n\u2019Messages\u2019]\n"}